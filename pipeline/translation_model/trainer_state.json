{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 75000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0004,
      "grad_norm": 1.7767713069915771,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 0.7178,
      "step": 10
    },
    {
      "epoch": 0.0008,
      "grad_norm": 2.8282699584960938,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.8036,
      "step": 20
    },
    {
      "epoch": 0.0012,
      "grad_norm": 3.1357614994049072,
      "learning_rate": 3e-06,
      "loss": 0.6692,
      "step": 30
    },
    {
      "epoch": 0.0016,
      "grad_norm": 2.835677146911621,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.7019,
      "step": 40
    },
    {
      "epoch": 0.002,
      "grad_norm": 3.092571258544922,
      "learning_rate": 5e-06,
      "loss": 0.6381,
      "step": 50
    },
    {
      "epoch": 0.0024,
      "grad_norm": 3.152463674545288,
      "learning_rate": 6e-06,
      "loss": 0.7536,
      "step": 60
    },
    {
      "epoch": 0.0028,
      "grad_norm": 2.63167405128479,
      "learning_rate": 7.000000000000001e-06,
      "loss": 0.6803,
      "step": 70
    },
    {
      "epoch": 0.0032,
      "grad_norm": 2.915207624435425,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.7472,
      "step": 80
    },
    {
      "epoch": 0.0036,
      "grad_norm": 3.262517213821411,
      "learning_rate": 9e-06,
      "loss": 0.7385,
      "step": 90
    },
    {
      "epoch": 0.004,
      "grad_norm": 2.1479265689849854,
      "learning_rate": 1e-05,
      "loss": 0.7899,
      "step": 100
    },
    {
      "epoch": 0.0044,
      "grad_norm": 3.057431221008301,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 0.6281,
      "step": 110
    },
    {
      "epoch": 0.0048,
      "grad_norm": 2.3018991947174072,
      "learning_rate": 1.2e-05,
      "loss": 0.7429,
      "step": 120
    },
    {
      "epoch": 0.0052,
      "grad_norm": 2.5962581634521484,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 0.677,
      "step": 130
    },
    {
      "epoch": 0.0056,
      "grad_norm": 3.7814853191375732,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 0.7476,
      "step": 140
    },
    {
      "epoch": 0.006,
      "grad_norm": 2.874826192855835,
      "learning_rate": 1.5e-05,
      "loss": 0.7399,
      "step": 150
    },
    {
      "epoch": 0.0064,
      "grad_norm": 2.356583833694458,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.6426,
      "step": 160
    },
    {
      "epoch": 0.0068,
      "grad_norm": 3.2635233402252197,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 0.6655,
      "step": 170
    },
    {
      "epoch": 0.0072,
      "grad_norm": 4.96164608001709,
      "learning_rate": 1.8e-05,
      "loss": 0.7412,
      "step": 180
    },
    {
      "epoch": 0.0076,
      "grad_norm": 3.6747074127197266,
      "learning_rate": 1.9e-05,
      "loss": 0.7805,
      "step": 190
    },
    {
      "epoch": 0.008,
      "grad_norm": 3.2858901023864746,
      "learning_rate": 2e-05,
      "loss": 0.6081,
      "step": 200
    },
    {
      "epoch": 0.0084,
      "grad_norm": 2.9479434490203857,
      "learning_rate": 2.1e-05,
      "loss": 0.6736,
      "step": 210
    },
    {
      "epoch": 0.0088,
      "grad_norm": 3.6122419834136963,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.6738,
      "step": 220
    },
    {
      "epoch": 0.0092,
      "grad_norm": 2.756411552429199,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 0.6854,
      "step": 230
    },
    {
      "epoch": 0.0096,
      "grad_norm": 3.429105281829834,
      "learning_rate": 2.4e-05,
      "loss": 0.6824,
      "step": 240
    },
    {
      "epoch": 0.01,
      "grad_norm": 3.1458332538604736,
      "learning_rate": 2.5e-05,
      "loss": 0.6647,
      "step": 250
    },
    {
      "epoch": 0.0104,
      "grad_norm": 2.314635753631592,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 0.656,
      "step": 260
    },
    {
      "epoch": 0.0108,
      "grad_norm": 4.2024006843566895,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 0.7709,
      "step": 270
    },
    {
      "epoch": 0.0112,
      "grad_norm": 3.2036733627319336,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.6399,
      "step": 280
    },
    {
      "epoch": 0.0116,
      "grad_norm": 2.3784027099609375,
      "learning_rate": 2.9e-05,
      "loss": 0.6769,
      "step": 290
    },
    {
      "epoch": 0.012,
      "grad_norm": 2.3417654037475586,
      "learning_rate": 3e-05,
      "loss": 0.5706,
      "step": 300
    },
    {
      "epoch": 0.0124,
      "grad_norm": 2.4232640266418457,
      "learning_rate": 3.1e-05,
      "loss": 0.6754,
      "step": 310
    },
    {
      "epoch": 0.0128,
      "grad_norm": 2.68308687210083,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 0.6992,
      "step": 320
    },
    {
      "epoch": 0.0132,
      "grad_norm": 2.9686193466186523,
      "learning_rate": 3.3e-05,
      "loss": 0.7128,
      "step": 330
    },
    {
      "epoch": 0.0136,
      "grad_norm": 2.9075756072998047,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.775,
      "step": 340
    },
    {
      "epoch": 0.014,
      "grad_norm": 3.2501955032348633,
      "learning_rate": 3.5e-05,
      "loss": 0.7183,
      "step": 350
    },
    {
      "epoch": 0.0144,
      "grad_norm": 3.0888845920562744,
      "learning_rate": 3.6e-05,
      "loss": 0.6117,
      "step": 360
    },
    {
      "epoch": 0.0148,
      "grad_norm": 3.041250467300415,
      "learning_rate": 3.7e-05,
      "loss": 0.7588,
      "step": 370
    },
    {
      "epoch": 0.0152,
      "grad_norm": 3.331752061843872,
      "learning_rate": 3.8e-05,
      "loss": 0.6721,
      "step": 380
    },
    {
      "epoch": 0.0156,
      "grad_norm": 2.9469454288482666,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 0.7161,
      "step": 390
    },
    {
      "epoch": 0.016,
      "grad_norm": 2.820601463317871,
      "learning_rate": 4e-05,
      "loss": 0.7617,
      "step": 400
    },
    {
      "epoch": 0.0164,
      "grad_norm": 3.1810832023620605,
      "learning_rate": 4.1e-05,
      "loss": 0.6171,
      "step": 410
    },
    {
      "epoch": 0.0168,
      "grad_norm": 3.013425588607788,
      "learning_rate": 4.2e-05,
      "loss": 0.7273,
      "step": 420
    },
    {
      "epoch": 0.0172,
      "grad_norm": 2.938775062561035,
      "learning_rate": 4.3e-05,
      "loss": 0.7934,
      "step": 430
    },
    {
      "epoch": 0.0176,
      "grad_norm": 3.328301191329956,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 0.7025,
      "step": 440
    },
    {
      "epoch": 0.018,
      "grad_norm": 3.137857675552368,
      "learning_rate": 4.5e-05,
      "loss": 0.7789,
      "step": 450
    },
    {
      "epoch": 0.0184,
      "grad_norm": 2.514267683029175,
      "learning_rate": 4.600000000000001e-05,
      "loss": 0.6091,
      "step": 460
    },
    {
      "epoch": 0.0188,
      "grad_norm": 2.4291977882385254,
      "learning_rate": 4.7e-05,
      "loss": 0.6984,
      "step": 470
    },
    {
      "epoch": 0.0192,
      "grad_norm": 2.325990915298462,
      "learning_rate": 4.8e-05,
      "loss": 0.5871,
      "step": 480
    },
    {
      "epoch": 0.0196,
      "grad_norm": 2.648569107055664,
      "learning_rate": 4.9e-05,
      "loss": 0.6737,
      "step": 490
    },
    {
      "epoch": 0.02,
      "grad_norm": 2.6714463233947754,
      "learning_rate": 5e-05,
      "loss": 0.6523,
      "step": 500
    },
    {
      "epoch": 0.0204,
      "grad_norm": 2.315145969390869,
      "learning_rate": 4.999328859060403e-05,
      "loss": 0.7386,
      "step": 510
    },
    {
      "epoch": 0.0208,
      "grad_norm": 2.8502516746520996,
      "learning_rate": 4.998657718120805e-05,
      "loss": 0.7403,
      "step": 520
    },
    {
      "epoch": 0.0212,
      "grad_norm": 2.904428005218506,
      "learning_rate": 4.997986577181208e-05,
      "loss": 0.7437,
      "step": 530
    },
    {
      "epoch": 0.0216,
      "grad_norm": 2.7352147102355957,
      "learning_rate": 4.997315436241611e-05,
      "loss": 0.78,
      "step": 540
    },
    {
      "epoch": 0.022,
      "grad_norm": 3.842226982116699,
      "learning_rate": 4.996644295302014e-05,
      "loss": 0.7251,
      "step": 550
    },
    {
      "epoch": 0.0224,
      "grad_norm": 3.2370338439941406,
      "learning_rate": 4.995973154362416e-05,
      "loss": 0.6735,
      "step": 560
    },
    {
      "epoch": 0.0228,
      "grad_norm": 3.038020372390747,
      "learning_rate": 4.995302013422819e-05,
      "loss": 0.7163,
      "step": 570
    },
    {
      "epoch": 0.0232,
      "grad_norm": 2.9836220741271973,
      "learning_rate": 4.994630872483222e-05,
      "loss": 0.6552,
      "step": 580
    },
    {
      "epoch": 0.0236,
      "grad_norm": 2.85068678855896,
      "learning_rate": 4.9939597315436246e-05,
      "loss": 0.7284,
      "step": 590
    },
    {
      "epoch": 0.024,
      "grad_norm": 3.5124945640563965,
      "learning_rate": 4.9932885906040274e-05,
      "loss": 0.7571,
      "step": 600
    },
    {
      "epoch": 0.0244,
      "grad_norm": 2.6888701915740967,
      "learning_rate": 4.9926174496644296e-05,
      "loss": 0.6702,
      "step": 610
    },
    {
      "epoch": 0.0248,
      "grad_norm": 2.793132781982422,
      "learning_rate": 4.9919463087248325e-05,
      "loss": 0.6413,
      "step": 620
    },
    {
      "epoch": 0.0252,
      "grad_norm": 2.975951910018921,
      "learning_rate": 4.991275167785235e-05,
      "loss": 0.6853,
      "step": 630
    },
    {
      "epoch": 0.0256,
      "grad_norm": 4.372642993927002,
      "learning_rate": 4.9906040268456375e-05,
      "loss": 0.7701,
      "step": 640
    },
    {
      "epoch": 0.026,
      "grad_norm": 2.9030609130859375,
      "learning_rate": 4.989932885906041e-05,
      "loss": 0.6549,
      "step": 650
    },
    {
      "epoch": 0.0264,
      "grad_norm": 2.351149559020996,
      "learning_rate": 4.989261744966443e-05,
      "loss": 0.7401,
      "step": 660
    },
    {
      "epoch": 0.0268,
      "grad_norm": 2.8497314453125,
      "learning_rate": 4.988590604026846e-05,
      "loss": 0.5975,
      "step": 670
    },
    {
      "epoch": 0.0272,
      "grad_norm": 2.4830615520477295,
      "learning_rate": 4.987919463087248e-05,
      "loss": 0.6671,
      "step": 680
    },
    {
      "epoch": 0.0276,
      "grad_norm": 3.1542062759399414,
      "learning_rate": 4.987248322147651e-05,
      "loss": 0.7525,
      "step": 690
    },
    {
      "epoch": 0.028,
      "grad_norm": 3.3422534465789795,
      "learning_rate": 4.986577181208054e-05,
      "loss": 0.6714,
      "step": 700
    },
    {
      "epoch": 0.0284,
      "grad_norm": 2.8551294803619385,
      "learning_rate": 4.985906040268457e-05,
      "loss": 0.6999,
      "step": 710
    },
    {
      "epoch": 0.0288,
      "grad_norm": 3.7947065830230713,
      "learning_rate": 4.9852348993288597e-05,
      "loss": 0.7387,
      "step": 720
    },
    {
      "epoch": 0.0292,
      "grad_norm": 3.0695934295654297,
      "learning_rate": 4.984563758389262e-05,
      "loss": 0.7147,
      "step": 730
    },
    {
      "epoch": 0.0296,
      "grad_norm": 3.432049512863159,
      "learning_rate": 4.983892617449665e-05,
      "loss": 0.6935,
      "step": 740
    },
    {
      "epoch": 0.03,
      "grad_norm": 3.147735834121704,
      "learning_rate": 4.983221476510067e-05,
      "loss": 0.6213,
      "step": 750
    },
    {
      "epoch": 0.0304,
      "grad_norm": 2.4304308891296387,
      "learning_rate": 4.98255033557047e-05,
      "loss": 0.6585,
      "step": 760
    },
    {
      "epoch": 0.0308,
      "grad_norm": 3.4971747398376465,
      "learning_rate": 4.981879194630873e-05,
      "loss": 0.6246,
      "step": 770
    },
    {
      "epoch": 0.0312,
      "grad_norm": 2.1454129219055176,
      "learning_rate": 4.9812080536912754e-05,
      "loss": 0.5632,
      "step": 780
    },
    {
      "epoch": 0.0316,
      "grad_norm": 2.93698787689209,
      "learning_rate": 4.980536912751678e-05,
      "loss": 0.7152,
      "step": 790
    },
    {
      "epoch": 0.032,
      "grad_norm": 2.5699143409729004,
      "learning_rate": 4.9798657718120805e-05,
      "loss": 0.6198,
      "step": 800
    },
    {
      "epoch": 0.0324,
      "grad_norm": 2.9035885334014893,
      "learning_rate": 4.979194630872483e-05,
      "loss": 0.7536,
      "step": 810
    },
    {
      "epoch": 0.0328,
      "grad_norm": 2.900344133377075,
      "learning_rate": 4.978523489932886e-05,
      "loss": 0.734,
      "step": 820
    },
    {
      "epoch": 0.0332,
      "grad_norm": 2.461041212081909,
      "learning_rate": 4.977852348993289e-05,
      "loss": 0.6983,
      "step": 830
    },
    {
      "epoch": 0.0336,
      "grad_norm": 7.721332550048828,
      "learning_rate": 4.977181208053692e-05,
      "loss": 0.7096,
      "step": 840
    },
    {
      "epoch": 0.034,
      "grad_norm": 3.0495307445526123,
      "learning_rate": 4.976510067114094e-05,
      "loss": 0.6451,
      "step": 850
    },
    {
      "epoch": 0.0344,
      "grad_norm": 3.440345525741577,
      "learning_rate": 4.975838926174497e-05,
      "loss": 0.6744,
      "step": 860
    },
    {
      "epoch": 0.0348,
      "grad_norm": 3.6245436668395996,
      "learning_rate": 4.975167785234899e-05,
      "loss": 0.741,
      "step": 870
    },
    {
      "epoch": 0.0352,
      "grad_norm": 2.6425540447235107,
      "learning_rate": 4.974496644295302e-05,
      "loss": 0.6751,
      "step": 880
    },
    {
      "epoch": 0.0356,
      "grad_norm": 2.907264232635498,
      "learning_rate": 4.9738255033557055e-05,
      "loss": 0.7356,
      "step": 890
    },
    {
      "epoch": 0.036,
      "grad_norm": 2.900075674057007,
      "learning_rate": 4.9731543624161077e-05,
      "loss": 0.8103,
      "step": 900
    },
    {
      "epoch": 0.0364,
      "grad_norm": 2.9960854053497314,
      "learning_rate": 4.9724832214765105e-05,
      "loss": 0.6187,
      "step": 910
    },
    {
      "epoch": 0.0368,
      "grad_norm": 2.3687961101531982,
      "learning_rate": 4.971812080536913e-05,
      "loss": 0.5854,
      "step": 920
    },
    {
      "epoch": 0.0372,
      "grad_norm": 3.2412898540496826,
      "learning_rate": 4.9711409395973155e-05,
      "loss": 0.6539,
      "step": 930
    },
    {
      "epoch": 0.0376,
      "grad_norm": 2.545761823654175,
      "learning_rate": 4.9704697986577184e-05,
      "loss": 0.6218,
      "step": 940
    },
    {
      "epoch": 0.038,
      "grad_norm": 2.499500036239624,
      "learning_rate": 4.969798657718121e-05,
      "loss": 0.6336,
      "step": 950
    },
    {
      "epoch": 0.0384,
      "grad_norm": 2.584944486618042,
      "learning_rate": 4.969127516778524e-05,
      "loss": 0.7385,
      "step": 960
    },
    {
      "epoch": 0.0388,
      "grad_norm": 3.566843271255493,
      "learning_rate": 4.968456375838926e-05,
      "loss": 0.8027,
      "step": 970
    },
    {
      "epoch": 0.0392,
      "grad_norm": 3.3510336875915527,
      "learning_rate": 4.967785234899329e-05,
      "loss": 0.7267,
      "step": 980
    },
    {
      "epoch": 0.0396,
      "grad_norm": 2.98109769821167,
      "learning_rate": 4.967114093959731e-05,
      "loss": 0.6824,
      "step": 990
    },
    {
      "epoch": 0.04,
      "grad_norm": 3.3511688709259033,
      "learning_rate": 4.966442953020135e-05,
      "loss": 0.676,
      "step": 1000
    },
    {
      "epoch": 0.0404,
      "grad_norm": 3.0142359733581543,
      "learning_rate": 4.965771812080537e-05,
      "loss": 0.6303,
      "step": 1010
    },
    {
      "epoch": 0.0408,
      "grad_norm": 3.015610456466675,
      "learning_rate": 4.96510067114094e-05,
      "loss": 0.723,
      "step": 1020
    },
    {
      "epoch": 0.0412,
      "grad_norm": 2.8864407539367676,
      "learning_rate": 4.964429530201343e-05,
      "loss": 0.7271,
      "step": 1030
    },
    {
      "epoch": 0.0416,
      "grad_norm": 2.152879238128662,
      "learning_rate": 4.963758389261745e-05,
      "loss": 0.6364,
      "step": 1040
    },
    {
      "epoch": 0.042,
      "grad_norm": 3.513256788253784,
      "learning_rate": 4.963087248322148e-05,
      "loss": 0.653,
      "step": 1050
    },
    {
      "epoch": 0.0424,
      "grad_norm": 2.3892269134521484,
      "learning_rate": 4.9624161073825506e-05,
      "loss": 0.751,
      "step": 1060
    },
    {
      "epoch": 0.0428,
      "grad_norm": 3.249577760696411,
      "learning_rate": 4.9617449664429535e-05,
      "loss": 0.7665,
      "step": 1070
    },
    {
      "epoch": 0.0432,
      "grad_norm": 2.6081604957580566,
      "learning_rate": 4.961073825503356e-05,
      "loss": 0.7088,
      "step": 1080
    },
    {
      "epoch": 0.0436,
      "grad_norm": 1.458239197731018,
      "learning_rate": 4.9604026845637585e-05,
      "loss": 0.5944,
      "step": 1090
    },
    {
      "epoch": 0.044,
      "grad_norm": 2.3045854568481445,
      "learning_rate": 4.9597315436241614e-05,
      "loss": 0.7451,
      "step": 1100
    },
    {
      "epoch": 0.0444,
      "grad_norm": 3.2001423835754395,
      "learning_rate": 4.9590604026845635e-05,
      "loss": 0.6904,
      "step": 1110
    },
    {
      "epoch": 0.0448,
      "grad_norm": 2.583134412765503,
      "learning_rate": 4.958389261744967e-05,
      "loss": 0.6501,
      "step": 1120
    },
    {
      "epoch": 0.0452,
      "grad_norm": 2.6392319202423096,
      "learning_rate": 4.957718120805369e-05,
      "loss": 0.5896,
      "step": 1130
    },
    {
      "epoch": 0.0456,
      "grad_norm": 4.062599182128906,
      "learning_rate": 4.957046979865772e-05,
      "loss": 0.6559,
      "step": 1140
    },
    {
      "epoch": 0.046,
      "grad_norm": 2.5654048919677734,
      "learning_rate": 4.956375838926175e-05,
      "loss": 0.6905,
      "step": 1150
    },
    {
      "epoch": 0.0464,
      "grad_norm": 3.119119644165039,
      "learning_rate": 4.955704697986577e-05,
      "loss": 0.645,
      "step": 1160
    },
    {
      "epoch": 0.0468,
      "grad_norm": 1.9018151760101318,
      "learning_rate": 4.95503355704698e-05,
      "loss": 0.6528,
      "step": 1170
    },
    {
      "epoch": 0.0472,
      "grad_norm": 2.9239461421966553,
      "learning_rate": 4.954362416107383e-05,
      "loss": 0.7002,
      "step": 1180
    },
    {
      "epoch": 0.0476,
      "grad_norm": 2.8018550872802734,
      "learning_rate": 4.953691275167786e-05,
      "loss": 0.6958,
      "step": 1190
    },
    {
      "epoch": 0.048,
      "grad_norm": 3.3720967769622803,
      "learning_rate": 4.953020134228188e-05,
      "loss": 0.6559,
      "step": 1200
    },
    {
      "epoch": 0.0484,
      "grad_norm": 3.6903061866760254,
      "learning_rate": 4.952348993288591e-05,
      "loss": 0.7476,
      "step": 1210
    },
    {
      "epoch": 0.0488,
      "grad_norm": 3.2284295558929443,
      "learning_rate": 4.9516778523489936e-05,
      "loss": 0.7627,
      "step": 1220
    },
    {
      "epoch": 0.0492,
      "grad_norm": 2.902026891708374,
      "learning_rate": 4.951006711409396e-05,
      "loss": 0.5723,
      "step": 1230
    },
    {
      "epoch": 0.0496,
      "grad_norm": 3.192260980606079,
      "learning_rate": 4.950335570469799e-05,
      "loss": 0.6573,
      "step": 1240
    },
    {
      "epoch": 0.05,
      "grad_norm": 3.27384877204895,
      "learning_rate": 4.9496644295302015e-05,
      "loss": 0.7422,
      "step": 1250
    },
    {
      "epoch": 0.0504,
      "grad_norm": 3.0165534019470215,
      "learning_rate": 4.948993288590604e-05,
      "loss": 0.5667,
      "step": 1260
    },
    {
      "epoch": 0.0508,
      "grad_norm": 2.834174871444702,
      "learning_rate": 4.948322147651007e-05,
      "loss": 0.7091,
      "step": 1270
    },
    {
      "epoch": 0.0512,
      "grad_norm": 2.3970413208007812,
      "learning_rate": 4.9476510067114094e-05,
      "loss": 0.7371,
      "step": 1280
    },
    {
      "epoch": 0.0516,
      "grad_norm": 3.029937505722046,
      "learning_rate": 4.946979865771812e-05,
      "loss": 0.7664,
      "step": 1290
    },
    {
      "epoch": 0.052,
      "grad_norm": 2.7290561199188232,
      "learning_rate": 4.946308724832215e-05,
      "loss": 0.6566,
      "step": 1300
    },
    {
      "epoch": 0.0524,
      "grad_norm": 2.949441909790039,
      "learning_rate": 4.945637583892618e-05,
      "loss": 0.712,
      "step": 1310
    },
    {
      "epoch": 0.0528,
      "grad_norm": 3.378096580505371,
      "learning_rate": 4.94496644295302e-05,
      "loss": 0.6811,
      "step": 1320
    },
    {
      "epoch": 0.0532,
      "grad_norm": 2.833674669265747,
      "learning_rate": 4.944295302013423e-05,
      "loss": 0.6475,
      "step": 1330
    },
    {
      "epoch": 0.0536,
      "grad_norm": 3.2082440853118896,
      "learning_rate": 4.943624161073826e-05,
      "loss": 0.7782,
      "step": 1340
    },
    {
      "epoch": 0.054,
      "grad_norm": 3.4138877391815186,
      "learning_rate": 4.9429530201342287e-05,
      "loss": 0.7696,
      "step": 1350
    },
    {
      "epoch": 0.0544,
      "grad_norm": 2.769991636276245,
      "learning_rate": 4.9422818791946315e-05,
      "loss": 0.5642,
      "step": 1360
    },
    {
      "epoch": 0.0548,
      "grad_norm": 3.0043723583221436,
      "learning_rate": 4.941610738255034e-05,
      "loss": 0.566,
      "step": 1370
    },
    {
      "epoch": 0.0552,
      "grad_norm": 3.7946243286132812,
      "learning_rate": 4.9409395973154365e-05,
      "loss": 0.736,
      "step": 1380
    },
    {
      "epoch": 0.0556,
      "grad_norm": 3.097018241882324,
      "learning_rate": 4.940268456375839e-05,
      "loss": 0.6705,
      "step": 1390
    },
    {
      "epoch": 0.056,
      "grad_norm": 4.021025657653809,
      "learning_rate": 4.9395973154362416e-05,
      "loss": 0.6117,
      "step": 1400
    },
    {
      "epoch": 0.0564,
      "grad_norm": 2.4163119792938232,
      "learning_rate": 4.9389261744966444e-05,
      "loss": 0.6447,
      "step": 1410
    },
    {
      "epoch": 0.0568,
      "grad_norm": 3.68705153465271,
      "learning_rate": 4.938255033557047e-05,
      "loss": 0.7671,
      "step": 1420
    },
    {
      "epoch": 0.0572,
      "grad_norm": 3.26973819732666,
      "learning_rate": 4.93758389261745e-05,
      "loss": 0.6971,
      "step": 1430
    },
    {
      "epoch": 0.0576,
      "grad_norm": 3.0076770782470703,
      "learning_rate": 4.936912751677852e-05,
      "loss": 0.5931,
      "step": 1440
    },
    {
      "epoch": 0.058,
      "grad_norm": 3.384672164916992,
      "learning_rate": 4.936241610738255e-05,
      "loss": 0.6401,
      "step": 1450
    },
    {
      "epoch": 0.0584,
      "grad_norm": 2.7601754665374756,
      "learning_rate": 4.935570469798658e-05,
      "loss": 0.7406,
      "step": 1460
    },
    {
      "epoch": 0.0588,
      "grad_norm": 1.8118196725845337,
      "learning_rate": 4.934899328859061e-05,
      "loss": 0.6881,
      "step": 1470
    },
    {
      "epoch": 0.0592,
      "grad_norm": 3.9912405014038086,
      "learning_rate": 4.934228187919464e-05,
      "loss": 0.8586,
      "step": 1480
    },
    {
      "epoch": 0.0596,
      "grad_norm": 3.0074355602264404,
      "learning_rate": 4.933557046979866e-05,
      "loss": 0.7407,
      "step": 1490
    },
    {
      "epoch": 0.06,
      "grad_norm": 2.883770227432251,
      "learning_rate": 4.932885906040269e-05,
      "loss": 0.6254,
      "step": 1500
    },
    {
      "epoch": 0.0604,
      "grad_norm": 2.3869383335113525,
      "learning_rate": 4.932214765100671e-05,
      "loss": 0.6755,
      "step": 1510
    },
    {
      "epoch": 0.0608,
      "grad_norm": 3.3864080905914307,
      "learning_rate": 4.931543624161074e-05,
      "loss": 0.6542,
      "step": 1520
    },
    {
      "epoch": 0.0612,
      "grad_norm": 3.0434741973876953,
      "learning_rate": 4.9308724832214767e-05,
      "loss": 0.6753,
      "step": 1530
    },
    {
      "epoch": 0.0616,
      "grad_norm": 2.737760066986084,
      "learning_rate": 4.9302013422818795e-05,
      "loss": 0.6429,
      "step": 1540
    },
    {
      "epoch": 0.062,
      "grad_norm": 2.90203857421875,
      "learning_rate": 4.9295302013422824e-05,
      "loss": 0.6671,
      "step": 1550
    },
    {
      "epoch": 0.0624,
      "grad_norm": 2.872119188308716,
      "learning_rate": 4.9288590604026845e-05,
      "loss": 0.6489,
      "step": 1560
    },
    {
      "epoch": 0.0628,
      "grad_norm": 2.4512453079223633,
      "learning_rate": 4.9281879194630874e-05,
      "loss": 0.6443,
      "step": 1570
    },
    {
      "epoch": 0.0632,
      "grad_norm": 2.903815507888794,
      "learning_rate": 4.92751677852349e-05,
      "loss": 0.7958,
      "step": 1580
    },
    {
      "epoch": 0.0636,
      "grad_norm": 3.2222793102264404,
      "learning_rate": 4.926845637583893e-05,
      "loss": 0.7047,
      "step": 1590
    },
    {
      "epoch": 0.064,
      "grad_norm": 2.9729082584381104,
      "learning_rate": 4.926174496644296e-05,
      "loss": 0.6318,
      "step": 1600
    },
    {
      "epoch": 0.0644,
      "grad_norm": 3.0157744884490967,
      "learning_rate": 4.925503355704698e-05,
      "loss": 0.6381,
      "step": 1610
    },
    {
      "epoch": 0.0648,
      "grad_norm": 2.896710157394409,
      "learning_rate": 4.924832214765101e-05,
      "loss": 0.6883,
      "step": 1620
    },
    {
      "epoch": 0.0652,
      "grad_norm": 2.372964382171631,
      "learning_rate": 4.924161073825503e-05,
      "loss": 0.5517,
      "step": 1630
    },
    {
      "epoch": 0.0656,
      "grad_norm": 3.493133783340454,
      "learning_rate": 4.923489932885906e-05,
      "loss": 0.6638,
      "step": 1640
    },
    {
      "epoch": 0.066,
      "grad_norm": 3.272348642349243,
      "learning_rate": 4.922818791946309e-05,
      "loss": 0.6801,
      "step": 1650
    },
    {
      "epoch": 0.0664,
      "grad_norm": 3.628736734390259,
      "learning_rate": 4.922147651006712e-05,
      "loss": 0.72,
      "step": 1660
    },
    {
      "epoch": 0.0668,
      "grad_norm": 3.1446452140808105,
      "learning_rate": 4.9214765100671146e-05,
      "loss": 0.633,
      "step": 1670
    },
    {
      "epoch": 0.0672,
      "grad_norm": 3.6962366104125977,
      "learning_rate": 4.920805369127517e-05,
      "loss": 0.7526,
      "step": 1680
    },
    {
      "epoch": 0.0676,
      "grad_norm": 2.770535945892334,
      "learning_rate": 4.9201342281879196e-05,
      "loss": 0.6414,
      "step": 1690
    },
    {
      "epoch": 0.068,
      "grad_norm": 2.6174685955047607,
      "learning_rate": 4.9194630872483225e-05,
      "loss": 0.6815,
      "step": 1700
    },
    {
      "epoch": 0.0684,
      "grad_norm": 3.011491298675537,
      "learning_rate": 4.918791946308725e-05,
      "loss": 0.6765,
      "step": 1710
    },
    {
      "epoch": 0.0688,
      "grad_norm": 2.432894229888916,
      "learning_rate": 4.918120805369128e-05,
      "loss": 0.6051,
      "step": 1720
    },
    {
      "epoch": 0.0692,
      "grad_norm": 2.90551495552063,
      "learning_rate": 4.9174496644295304e-05,
      "loss": 0.6808,
      "step": 1730
    },
    {
      "epoch": 0.0696,
      "grad_norm": 3.169940710067749,
      "learning_rate": 4.916778523489933e-05,
      "loss": 0.6911,
      "step": 1740
    },
    {
      "epoch": 0.07,
      "grad_norm": 2.5717155933380127,
      "learning_rate": 4.9161073825503354e-05,
      "loss": 0.6438,
      "step": 1750
    },
    {
      "epoch": 0.0704,
      "grad_norm": 2.7125229835510254,
      "learning_rate": 4.915436241610738e-05,
      "loss": 0.6107,
      "step": 1760
    },
    {
      "epoch": 0.0708,
      "grad_norm": 3.723893165588379,
      "learning_rate": 4.914765100671141e-05,
      "loss": 0.8393,
      "step": 1770
    },
    {
      "epoch": 0.0712,
      "grad_norm": 3.1033647060394287,
      "learning_rate": 4.914093959731544e-05,
      "loss": 0.6911,
      "step": 1780
    },
    {
      "epoch": 0.0716,
      "grad_norm": 2.751441240310669,
      "learning_rate": 4.913422818791947e-05,
      "loss": 0.6686,
      "step": 1790
    },
    {
      "epoch": 0.072,
      "grad_norm": 2.2836148738861084,
      "learning_rate": 4.912751677852349e-05,
      "loss": 0.7281,
      "step": 1800
    },
    {
      "epoch": 0.0724,
      "grad_norm": 3.4097962379455566,
      "learning_rate": 4.912080536912752e-05,
      "loss": 0.7097,
      "step": 1810
    },
    {
      "epoch": 0.0728,
      "grad_norm": 3.521564245223999,
      "learning_rate": 4.911409395973155e-05,
      "loss": 0.7124,
      "step": 1820
    },
    {
      "epoch": 0.0732,
      "grad_norm": 2.4805614948272705,
      "learning_rate": 4.9107382550335576e-05,
      "loss": 0.642,
      "step": 1830
    },
    {
      "epoch": 0.0736,
      "grad_norm": 2.5907094478607178,
      "learning_rate": 4.91006711409396e-05,
      "loss": 0.6762,
      "step": 1840
    },
    {
      "epoch": 0.074,
      "grad_norm": 3.217616319656372,
      "learning_rate": 4.9093959731543626e-05,
      "loss": 0.7253,
      "step": 1850
    },
    {
      "epoch": 0.0744,
      "grad_norm": 3.277120590209961,
      "learning_rate": 4.9087248322147654e-05,
      "loss": 0.7254,
      "step": 1860
    },
    {
      "epoch": 0.0748,
      "grad_norm": 3.0838282108306885,
      "learning_rate": 4.9080536912751676e-05,
      "loss": 0.6835,
      "step": 1870
    },
    {
      "epoch": 0.0752,
      "grad_norm": 3.310657501220703,
      "learning_rate": 4.907382550335571e-05,
      "loss": 0.6204,
      "step": 1880
    },
    {
      "epoch": 0.0756,
      "grad_norm": 3.580707311630249,
      "learning_rate": 4.906711409395973e-05,
      "loss": 0.7392,
      "step": 1890
    },
    {
      "epoch": 0.076,
      "grad_norm": 2.8016695976257324,
      "learning_rate": 4.906040268456376e-05,
      "loss": 0.6716,
      "step": 1900
    },
    {
      "epoch": 0.0764,
      "grad_norm": 2.938154697418213,
      "learning_rate": 4.905369127516779e-05,
      "loss": 0.7032,
      "step": 1910
    },
    {
      "epoch": 0.0768,
      "grad_norm": 2.858914613723755,
      "learning_rate": 4.904697986577181e-05,
      "loss": 0.7051,
      "step": 1920
    },
    {
      "epoch": 0.0772,
      "grad_norm": 5.030903339385986,
      "learning_rate": 4.904026845637584e-05,
      "loss": 0.7133,
      "step": 1930
    },
    {
      "epoch": 0.0776,
      "grad_norm": 2.755545139312744,
      "learning_rate": 4.903355704697987e-05,
      "loss": 0.6193,
      "step": 1940
    },
    {
      "epoch": 0.078,
      "grad_norm": 2.8350343704223633,
      "learning_rate": 4.90268456375839e-05,
      "loss": 0.6892,
      "step": 1950
    },
    {
      "epoch": 0.0784,
      "grad_norm": 3.015109062194824,
      "learning_rate": 4.902013422818792e-05,
      "loss": 0.6898,
      "step": 1960
    },
    {
      "epoch": 0.0788,
      "grad_norm": 2.6532692909240723,
      "learning_rate": 4.901342281879195e-05,
      "loss": 0.7277,
      "step": 1970
    },
    {
      "epoch": 0.0792,
      "grad_norm": 3.0147576332092285,
      "learning_rate": 4.900671140939598e-05,
      "loss": 0.7765,
      "step": 1980
    },
    {
      "epoch": 0.0796,
      "grad_norm": 2.790961980819702,
      "learning_rate": 4.9e-05,
      "loss": 0.6576,
      "step": 1990
    },
    {
      "epoch": 0.08,
      "grad_norm": 3.0146515369415283,
      "learning_rate": 4.8993288590604034e-05,
      "loss": 0.7455,
      "step": 2000
    },
    {
      "epoch": 0.0804,
      "grad_norm": 2.3147683143615723,
      "learning_rate": 4.8986577181208056e-05,
      "loss": 0.7074,
      "step": 2010
    },
    {
      "epoch": 0.0808,
      "grad_norm": 2.7056994438171387,
      "learning_rate": 4.8979865771812084e-05,
      "loss": 0.741,
      "step": 2020
    },
    {
      "epoch": 0.0812,
      "grad_norm": 2.1692490577697754,
      "learning_rate": 4.8973154362416106e-05,
      "loss": 0.6493,
      "step": 2030
    },
    {
      "epoch": 0.0816,
      "grad_norm": 2.537099599838257,
      "learning_rate": 4.8966442953020134e-05,
      "loss": 0.6077,
      "step": 2040
    },
    {
      "epoch": 0.082,
      "grad_norm": 2.5624117851257324,
      "learning_rate": 4.895973154362416e-05,
      "loss": 0.6718,
      "step": 2050
    },
    {
      "epoch": 0.0824,
      "grad_norm": 3.539318323135376,
      "learning_rate": 4.895302013422819e-05,
      "loss": 0.7712,
      "step": 2060
    },
    {
      "epoch": 0.0828,
      "grad_norm": 3.1747026443481445,
      "learning_rate": 4.894630872483222e-05,
      "loss": 0.6646,
      "step": 2070
    },
    {
      "epoch": 0.0832,
      "grad_norm": 3.0877175331115723,
      "learning_rate": 4.893959731543624e-05,
      "loss": 0.7515,
      "step": 2080
    },
    {
      "epoch": 0.0836,
      "grad_norm": 2.6681487560272217,
      "learning_rate": 4.893288590604027e-05,
      "loss": 0.651,
      "step": 2090
    },
    {
      "epoch": 0.084,
      "grad_norm": 2.085801124572754,
      "learning_rate": 4.89261744966443e-05,
      "loss": 0.6453,
      "step": 2100
    },
    {
      "epoch": 0.0844,
      "grad_norm": 2.709275007247925,
      "learning_rate": 4.891946308724832e-05,
      "loss": 0.6288,
      "step": 2110
    },
    {
      "epoch": 0.0848,
      "grad_norm": 2.638193368911743,
      "learning_rate": 4.8912751677852356e-05,
      "loss": 0.746,
      "step": 2120
    },
    {
      "epoch": 0.0852,
      "grad_norm": 2.827627182006836,
      "learning_rate": 4.890604026845638e-05,
      "loss": 0.7541,
      "step": 2130
    },
    {
      "epoch": 0.0856,
      "grad_norm": 2.910961627960205,
      "learning_rate": 4.8899328859060406e-05,
      "loss": 0.7395,
      "step": 2140
    },
    {
      "epoch": 0.086,
      "grad_norm": 2.4630911350250244,
      "learning_rate": 4.889261744966443e-05,
      "loss": 0.7112,
      "step": 2150
    },
    {
      "epoch": 0.0864,
      "grad_norm": 3.3968164920806885,
      "learning_rate": 4.888590604026846e-05,
      "loss": 0.6774,
      "step": 2160
    },
    {
      "epoch": 0.0868,
      "grad_norm": 2.332977056503296,
      "learning_rate": 4.8879194630872485e-05,
      "loss": 0.7671,
      "step": 2170
    },
    {
      "epoch": 0.0872,
      "grad_norm": 2.9160869121551514,
      "learning_rate": 4.8872483221476514e-05,
      "loss": 0.5901,
      "step": 2180
    },
    {
      "epoch": 0.0876,
      "grad_norm": 3.240205764770508,
      "learning_rate": 4.886577181208054e-05,
      "loss": 0.6449,
      "step": 2190
    },
    {
      "epoch": 0.088,
      "grad_norm": 3.0669920444488525,
      "learning_rate": 4.8859060402684564e-05,
      "loss": 0.7429,
      "step": 2200
    },
    {
      "epoch": 0.0884,
      "grad_norm": 3.1228246688842773,
      "learning_rate": 4.885234899328859e-05,
      "loss": 0.6565,
      "step": 2210
    },
    {
      "epoch": 0.0888,
      "grad_norm": 2.649892568588257,
      "learning_rate": 4.8845637583892614e-05,
      "loss": 0.6765,
      "step": 2220
    },
    {
      "epoch": 0.0892,
      "grad_norm": 4.822502136230469,
      "learning_rate": 4.883892617449665e-05,
      "loss": 0.7071,
      "step": 2230
    },
    {
      "epoch": 0.0896,
      "grad_norm": 3.1461093425750732,
      "learning_rate": 4.883221476510068e-05,
      "loss": 0.6453,
      "step": 2240
    },
    {
      "epoch": 0.09,
      "grad_norm": 3.479581594467163,
      "learning_rate": 4.88255033557047e-05,
      "loss": 0.7102,
      "step": 2250
    },
    {
      "epoch": 0.0904,
      "grad_norm": 3.267988681793213,
      "learning_rate": 4.881879194630873e-05,
      "loss": 0.6857,
      "step": 2260
    },
    {
      "epoch": 0.0908,
      "grad_norm": 2.5747482776641846,
      "learning_rate": 4.881208053691275e-05,
      "loss": 0.6575,
      "step": 2270
    },
    {
      "epoch": 0.0912,
      "grad_norm": 2.967221736907959,
      "learning_rate": 4.880536912751678e-05,
      "loss": 0.7087,
      "step": 2280
    },
    {
      "epoch": 0.0916,
      "grad_norm": 2.377208709716797,
      "learning_rate": 4.879865771812081e-05,
      "loss": 0.5802,
      "step": 2290
    },
    {
      "epoch": 0.092,
      "grad_norm": 2.511301040649414,
      "learning_rate": 4.8791946308724836e-05,
      "loss": 0.6333,
      "step": 2300
    },
    {
      "epoch": 0.0924,
      "grad_norm": 3.41379976272583,
      "learning_rate": 4.8785234899328864e-05,
      "loss": 0.6986,
      "step": 2310
    },
    {
      "epoch": 0.0928,
      "grad_norm": 2.6903185844421387,
      "learning_rate": 4.8778523489932886e-05,
      "loss": 0.6772,
      "step": 2320
    },
    {
      "epoch": 0.0932,
      "grad_norm": 3.2137370109558105,
      "learning_rate": 4.8771812080536915e-05,
      "loss": 0.642,
      "step": 2330
    },
    {
      "epoch": 0.0936,
      "grad_norm": 3.3741557598114014,
      "learning_rate": 4.8765100671140937e-05,
      "loss": 0.6296,
      "step": 2340
    },
    {
      "epoch": 0.094,
      "grad_norm": 3.4002745151519775,
      "learning_rate": 4.875838926174497e-05,
      "loss": 0.87,
      "step": 2350
    },
    {
      "epoch": 0.0944,
      "grad_norm": 3.5382473468780518,
      "learning_rate": 4.8751677852349e-05,
      "loss": 0.6726,
      "step": 2360
    },
    {
      "epoch": 0.0948,
      "grad_norm": 2.9860174655914307,
      "learning_rate": 4.874496644295302e-05,
      "loss": 0.7704,
      "step": 2370
    },
    {
      "epoch": 0.0952,
      "grad_norm": 3.2274837493896484,
      "learning_rate": 4.873825503355705e-05,
      "loss": 0.7309,
      "step": 2380
    },
    {
      "epoch": 0.0956,
      "grad_norm": 2.711787223815918,
      "learning_rate": 4.873154362416107e-05,
      "loss": 0.7208,
      "step": 2390
    },
    {
      "epoch": 0.096,
      "grad_norm": 3.064453601837158,
      "learning_rate": 4.87248322147651e-05,
      "loss": 0.6399,
      "step": 2400
    },
    {
      "epoch": 0.0964,
      "grad_norm": 3.225698232650757,
      "learning_rate": 4.871812080536913e-05,
      "loss": 0.6837,
      "step": 2410
    },
    {
      "epoch": 0.0968,
      "grad_norm": 2.464232921600342,
      "learning_rate": 4.871140939597316e-05,
      "loss": 0.5909,
      "step": 2420
    },
    {
      "epoch": 0.0972,
      "grad_norm": 3.6726508140563965,
      "learning_rate": 4.870469798657719e-05,
      "loss": 0.695,
      "step": 2430
    },
    {
      "epoch": 0.0976,
      "grad_norm": 3.25003719329834,
      "learning_rate": 4.869798657718121e-05,
      "loss": 0.74,
      "step": 2440
    },
    {
      "epoch": 0.098,
      "grad_norm": 3.1707139015197754,
      "learning_rate": 4.869127516778524e-05,
      "loss": 0.7037,
      "step": 2450
    },
    {
      "epoch": 0.0984,
      "grad_norm": 4.273855686187744,
      "learning_rate": 4.868456375838926e-05,
      "loss": 0.6881,
      "step": 2460
    },
    {
      "epoch": 0.0988,
      "grad_norm": 3.695524215698242,
      "learning_rate": 4.8677852348993294e-05,
      "loss": 0.7411,
      "step": 2470
    },
    {
      "epoch": 0.0992,
      "grad_norm": 3.2419068813323975,
      "learning_rate": 4.8671140939597316e-05,
      "loss": 0.6892,
      "step": 2480
    },
    {
      "epoch": 0.0996,
      "grad_norm": 2.2988321781158447,
      "learning_rate": 4.8664429530201344e-05,
      "loss": 0.6157,
      "step": 2490
    },
    {
      "epoch": 0.1,
      "grad_norm": 2.6247317790985107,
      "learning_rate": 4.865771812080537e-05,
      "loss": 0.7135,
      "step": 2500
    },
    {
      "epoch": 0.1004,
      "grad_norm": 3.5512607097625732,
      "learning_rate": 4.8651006711409395e-05,
      "loss": 0.5961,
      "step": 2510
    },
    {
      "epoch": 0.1008,
      "grad_norm": 2.7370338439941406,
      "learning_rate": 4.864429530201342e-05,
      "loss": 0.7282,
      "step": 2520
    },
    {
      "epoch": 0.1012,
      "grad_norm": 2.9635517597198486,
      "learning_rate": 4.863758389261745e-05,
      "loss": 0.7582,
      "step": 2530
    },
    {
      "epoch": 0.1016,
      "grad_norm": 3.1433939933776855,
      "learning_rate": 4.863087248322148e-05,
      "loss": 0.6783,
      "step": 2540
    },
    {
      "epoch": 0.102,
      "grad_norm": 3.2900195121765137,
      "learning_rate": 4.862416107382551e-05,
      "loss": 0.7308,
      "step": 2550
    },
    {
      "epoch": 0.1024,
      "grad_norm": 3.1182451248168945,
      "learning_rate": 4.861744966442953e-05,
      "loss": 0.6952,
      "step": 2560
    },
    {
      "epoch": 0.1028,
      "grad_norm": 2.7856059074401855,
      "learning_rate": 4.861073825503356e-05,
      "loss": 0.7061,
      "step": 2570
    },
    {
      "epoch": 0.1032,
      "grad_norm": 2.562927484512329,
      "learning_rate": 4.860402684563759e-05,
      "loss": 0.78,
      "step": 2580
    },
    {
      "epoch": 0.1036,
      "grad_norm": 2.353792667388916,
      "learning_rate": 4.8597315436241616e-05,
      "loss": 0.6847,
      "step": 2590
    },
    {
      "epoch": 0.104,
      "grad_norm": 2.8726003170013428,
      "learning_rate": 4.859060402684564e-05,
      "loss": 0.7051,
      "step": 2600
    },
    {
      "epoch": 0.1044,
      "grad_norm": 2.4028773307800293,
      "learning_rate": 4.858389261744967e-05,
      "loss": 0.6643,
      "step": 2610
    },
    {
      "epoch": 0.1048,
      "grad_norm": 2.849152088165283,
      "learning_rate": 4.8577181208053695e-05,
      "loss": 0.6296,
      "step": 2620
    },
    {
      "epoch": 0.1052,
      "grad_norm": 1.9481240510940552,
      "learning_rate": 4.857046979865772e-05,
      "loss": 0.7025,
      "step": 2630
    },
    {
      "epoch": 0.1056,
      "grad_norm": 3.1251838207244873,
      "learning_rate": 4.8563758389261746e-05,
      "loss": 0.7063,
      "step": 2640
    },
    {
      "epoch": 0.106,
      "grad_norm": 2.1797170639038086,
      "learning_rate": 4.8557046979865774e-05,
      "loss": 0.5711,
      "step": 2650
    },
    {
      "epoch": 0.1064,
      "grad_norm": 3.3186933994293213,
      "learning_rate": 4.85503355704698e-05,
      "loss": 0.7541,
      "step": 2660
    },
    {
      "epoch": 0.1068,
      "grad_norm": 3.3451902866363525,
      "learning_rate": 4.8543624161073824e-05,
      "loss": 0.7746,
      "step": 2670
    },
    {
      "epoch": 0.1072,
      "grad_norm": 2.9854543209075928,
      "learning_rate": 4.853691275167785e-05,
      "loss": 0.7053,
      "step": 2680
    },
    {
      "epoch": 0.1076,
      "grad_norm": 3.3328940868377686,
      "learning_rate": 4.853020134228188e-05,
      "loss": 0.7426,
      "step": 2690
    },
    {
      "epoch": 0.108,
      "grad_norm": 2.8671364784240723,
      "learning_rate": 4.852348993288591e-05,
      "loss": 0.6955,
      "step": 2700
    },
    {
      "epoch": 0.1084,
      "grad_norm": 2.363246202468872,
      "learning_rate": 4.851677852348994e-05,
      "loss": 0.6512,
      "step": 2710
    },
    {
      "epoch": 0.1088,
      "grad_norm": 3.223161458969116,
      "learning_rate": 4.851006711409396e-05,
      "loss": 0.7045,
      "step": 2720
    },
    {
      "epoch": 0.1092,
      "grad_norm": 3.093397855758667,
      "learning_rate": 4.850335570469799e-05,
      "loss": 0.6124,
      "step": 2730
    },
    {
      "epoch": 0.1096,
      "grad_norm": 2.558640956878662,
      "learning_rate": 4.849664429530202e-05,
      "loss": 0.645,
      "step": 2740
    },
    {
      "epoch": 0.11,
      "grad_norm": 2.3476476669311523,
      "learning_rate": 4.848993288590604e-05,
      "loss": 0.6214,
      "step": 2750
    },
    {
      "epoch": 0.1104,
      "grad_norm": 4.074769973754883,
      "learning_rate": 4.848322147651007e-05,
      "loss": 0.7081,
      "step": 2760
    },
    {
      "epoch": 0.1108,
      "grad_norm": 3.445545196533203,
      "learning_rate": 4.8476510067114096e-05,
      "loss": 0.6239,
      "step": 2770
    },
    {
      "epoch": 0.1112,
      "grad_norm": 2.767840623855591,
      "learning_rate": 4.8469798657718125e-05,
      "loss": 0.6858,
      "step": 2780
    },
    {
      "epoch": 0.1116,
      "grad_norm": 3.2101964950561523,
      "learning_rate": 4.846308724832215e-05,
      "loss": 0.7403,
      "step": 2790
    },
    {
      "epoch": 0.112,
      "grad_norm": 3.0328307151794434,
      "learning_rate": 4.8456375838926175e-05,
      "loss": 0.7042,
      "step": 2800
    },
    {
      "epoch": 0.1124,
      "grad_norm": 3.3703536987304688,
      "learning_rate": 4.8449664429530204e-05,
      "loss": 0.7175,
      "step": 2810
    },
    {
      "epoch": 0.1128,
      "grad_norm": 2.583719491958618,
      "learning_rate": 4.844295302013423e-05,
      "loss": 0.6468,
      "step": 2820
    },
    {
      "epoch": 0.1132,
      "grad_norm": 3.2900986671447754,
      "learning_rate": 4.843624161073826e-05,
      "loss": 0.6086,
      "step": 2830
    },
    {
      "epoch": 0.1136,
      "grad_norm": 4.414258003234863,
      "learning_rate": 4.842953020134228e-05,
      "loss": 0.7188,
      "step": 2840
    },
    {
      "epoch": 0.114,
      "grad_norm": 3.2076306343078613,
      "learning_rate": 4.842281879194631e-05,
      "loss": 0.6595,
      "step": 2850
    },
    {
      "epoch": 0.1144,
      "grad_norm": 2.8682548999786377,
      "learning_rate": 4.841610738255033e-05,
      "loss": 0.6822,
      "step": 2860
    },
    {
      "epoch": 0.1148,
      "grad_norm": 2.827568769454956,
      "learning_rate": 4.840939597315436e-05,
      "loss": 0.7316,
      "step": 2870
    },
    {
      "epoch": 0.1152,
      "grad_norm": 2.896655321121216,
      "learning_rate": 4.84026845637584e-05,
      "loss": 0.6812,
      "step": 2880
    },
    {
      "epoch": 0.1156,
      "grad_norm": 2.593930721282959,
      "learning_rate": 4.839597315436242e-05,
      "loss": 0.6417,
      "step": 2890
    },
    {
      "epoch": 0.116,
      "grad_norm": 3.4195613861083984,
      "learning_rate": 4.838926174496645e-05,
      "loss": 0.6697,
      "step": 2900
    },
    {
      "epoch": 0.1164,
      "grad_norm": 2.5640311241149902,
      "learning_rate": 4.838255033557047e-05,
      "loss": 0.6474,
      "step": 2910
    },
    {
      "epoch": 0.1168,
      "grad_norm": 3.4680540561676025,
      "learning_rate": 4.83758389261745e-05,
      "loss": 0.7543,
      "step": 2920
    },
    {
      "epoch": 0.1172,
      "grad_norm": 2.8872485160827637,
      "learning_rate": 4.8369127516778526e-05,
      "loss": 0.6738,
      "step": 2930
    },
    {
      "epoch": 0.1176,
      "grad_norm": 3.5463147163391113,
      "learning_rate": 4.8362416107382555e-05,
      "loss": 0.7454,
      "step": 2940
    },
    {
      "epoch": 0.118,
      "grad_norm": 2.5011799335479736,
      "learning_rate": 4.835570469798658e-05,
      "loss": 0.6536,
      "step": 2950
    },
    {
      "epoch": 0.1184,
      "grad_norm": 3.124119520187378,
      "learning_rate": 4.8348993288590605e-05,
      "loss": 0.75,
      "step": 2960
    },
    {
      "epoch": 0.1188,
      "grad_norm": 3.7930121421813965,
      "learning_rate": 4.8342281879194633e-05,
      "loss": 0.5613,
      "step": 2970
    },
    {
      "epoch": 0.1192,
      "grad_norm": 3.1016950607299805,
      "learning_rate": 4.8335570469798655e-05,
      "loss": 0.6279,
      "step": 2980
    },
    {
      "epoch": 0.1196,
      "grad_norm": 2.075267791748047,
      "learning_rate": 4.8328859060402684e-05,
      "loss": 0.6897,
      "step": 2990
    },
    {
      "epoch": 0.12,
      "grad_norm": 3.177865743637085,
      "learning_rate": 4.832214765100672e-05,
      "loss": 0.5384,
      "step": 3000
    },
    {
      "epoch": 0.1204,
      "grad_norm": 3.5029237270355225,
      "learning_rate": 4.831543624161074e-05,
      "loss": 0.6344,
      "step": 3010
    },
    {
      "epoch": 0.1208,
      "grad_norm": 2.7524564266204834,
      "learning_rate": 4.830872483221477e-05,
      "loss": 0.6839,
      "step": 3020
    },
    {
      "epoch": 0.1212,
      "grad_norm": 3.750103235244751,
      "learning_rate": 4.830201342281879e-05,
      "loss": 0.6949,
      "step": 3030
    },
    {
      "epoch": 0.1216,
      "grad_norm": 3.1158223152160645,
      "learning_rate": 4.829530201342282e-05,
      "loss": 0.742,
      "step": 3040
    },
    {
      "epoch": 0.122,
      "grad_norm": 2.881248950958252,
      "learning_rate": 4.828859060402685e-05,
      "loss": 0.6606,
      "step": 3050
    },
    {
      "epoch": 0.1224,
      "grad_norm": 2.784236192703247,
      "learning_rate": 4.828187919463088e-05,
      "loss": 0.6677,
      "step": 3060
    },
    {
      "epoch": 0.1228,
      "grad_norm": 2.4256114959716797,
      "learning_rate": 4.8275167785234905e-05,
      "loss": 0.606,
      "step": 3070
    },
    {
      "epoch": 0.1232,
      "grad_norm": 2.7989399433135986,
      "learning_rate": 4.826845637583893e-05,
      "loss": 0.776,
      "step": 3080
    },
    {
      "epoch": 0.1236,
      "grad_norm": 2.4904932975769043,
      "learning_rate": 4.8261744966442956e-05,
      "loss": 0.7599,
      "step": 3090
    },
    {
      "epoch": 0.124,
      "grad_norm": 3.9875807762145996,
      "learning_rate": 4.825503355704698e-05,
      "loss": 0.6931,
      "step": 3100
    },
    {
      "epoch": 0.1244,
      "grad_norm": 2.2792208194732666,
      "learning_rate": 4.824832214765101e-05,
      "loss": 0.6879,
      "step": 3110
    },
    {
      "epoch": 0.1248,
      "grad_norm": 3.1078379154205322,
      "learning_rate": 4.824161073825504e-05,
      "loss": 0.6867,
      "step": 3120
    },
    {
      "epoch": 0.1252,
      "grad_norm": 2.724163055419922,
      "learning_rate": 4.823489932885906e-05,
      "loss": 0.6072,
      "step": 3130
    },
    {
      "epoch": 0.1256,
      "grad_norm": 4.33550500869751,
      "learning_rate": 4.822818791946309e-05,
      "loss": 0.6485,
      "step": 3140
    },
    {
      "epoch": 0.126,
      "grad_norm": 2.9756290912628174,
      "learning_rate": 4.822147651006711e-05,
      "loss": 0.7301,
      "step": 3150
    },
    {
      "epoch": 0.1264,
      "grad_norm": 2.820227861404419,
      "learning_rate": 4.821476510067114e-05,
      "loss": 0.6996,
      "step": 3160
    },
    {
      "epoch": 0.1268,
      "grad_norm": 2.948432445526123,
      "learning_rate": 4.820805369127517e-05,
      "loss": 0.6743,
      "step": 3170
    },
    {
      "epoch": 0.1272,
      "grad_norm": 2.4918928146362305,
      "learning_rate": 4.82013422818792e-05,
      "loss": 0.6119,
      "step": 3180
    },
    {
      "epoch": 0.1276,
      "grad_norm": 3.454108715057373,
      "learning_rate": 4.819463087248323e-05,
      "loss": 0.7455,
      "step": 3190
    },
    {
      "epoch": 0.128,
      "grad_norm": 2.8591911792755127,
      "learning_rate": 4.818791946308725e-05,
      "loss": 0.6481,
      "step": 3200
    },
    {
      "epoch": 0.1284,
      "grad_norm": 2.2465908527374268,
      "learning_rate": 4.818120805369128e-05,
      "loss": 0.6528,
      "step": 3210
    },
    {
      "epoch": 0.1288,
      "grad_norm": 2.6218502521514893,
      "learning_rate": 4.81744966442953e-05,
      "loss": 0.6632,
      "step": 3220
    },
    {
      "epoch": 0.1292,
      "grad_norm": 3.1664819717407227,
      "learning_rate": 4.8167785234899335e-05,
      "loss": 0.6883,
      "step": 3230
    },
    {
      "epoch": 0.1296,
      "grad_norm": 3.6683452129364014,
      "learning_rate": 4.816107382550336e-05,
      "loss": 0.6849,
      "step": 3240
    },
    {
      "epoch": 0.13,
      "grad_norm": 3.621248245239258,
      "learning_rate": 4.8154362416107385e-05,
      "loss": 0.6803,
      "step": 3250
    },
    {
      "epoch": 0.1304,
      "grad_norm": 2.712918996810913,
      "learning_rate": 4.8147651006711414e-05,
      "loss": 0.7185,
      "step": 3260
    },
    {
      "epoch": 0.1308,
      "grad_norm": 3.2979607582092285,
      "learning_rate": 4.8140939597315436e-05,
      "loss": 0.8025,
      "step": 3270
    },
    {
      "epoch": 0.1312,
      "grad_norm": 2.858055353164673,
      "learning_rate": 4.8134228187919464e-05,
      "loss": 0.6554,
      "step": 3280
    },
    {
      "epoch": 0.1316,
      "grad_norm": 3.2352194786071777,
      "learning_rate": 4.812751677852349e-05,
      "loss": 0.6809,
      "step": 3290
    },
    {
      "epoch": 0.132,
      "grad_norm": 2.8176686763763428,
      "learning_rate": 4.812080536912752e-05,
      "loss": 0.6527,
      "step": 3300
    },
    {
      "epoch": 0.1324,
      "grad_norm": 2.9746062755584717,
      "learning_rate": 4.811409395973154e-05,
      "loss": 0.6464,
      "step": 3310
    },
    {
      "epoch": 0.1328,
      "grad_norm": 3.4130001068115234,
      "learning_rate": 4.810738255033557e-05,
      "loss": 0.6993,
      "step": 3320
    },
    {
      "epoch": 0.1332,
      "grad_norm": 2.863912582397461,
      "learning_rate": 4.81006711409396e-05,
      "loss": 0.7066,
      "step": 3330
    },
    {
      "epoch": 0.1336,
      "grad_norm": 3.292854070663452,
      "learning_rate": 4.809395973154362e-05,
      "loss": 0.6589,
      "step": 3340
    },
    {
      "epoch": 0.134,
      "grad_norm": 2.8542261123657227,
      "learning_rate": 4.808724832214766e-05,
      "loss": 0.6638,
      "step": 3350
    },
    {
      "epoch": 0.1344,
      "grad_norm": 2.8622794151306152,
      "learning_rate": 4.808053691275168e-05,
      "loss": 0.5961,
      "step": 3360
    },
    {
      "epoch": 0.1348,
      "grad_norm": 2.948528528213501,
      "learning_rate": 4.807382550335571e-05,
      "loss": 0.7254,
      "step": 3370
    },
    {
      "epoch": 0.1352,
      "grad_norm": 3.157020092010498,
      "learning_rate": 4.8067114093959736e-05,
      "loss": 0.6241,
      "step": 3380
    },
    {
      "epoch": 0.1356,
      "grad_norm": 2.6070587635040283,
      "learning_rate": 4.806040268456376e-05,
      "loss": 0.667,
      "step": 3390
    },
    {
      "epoch": 0.136,
      "grad_norm": 2.650608777999878,
      "learning_rate": 4.8053691275167786e-05,
      "loss": 0.6591,
      "step": 3400
    },
    {
      "epoch": 0.1364,
      "grad_norm": 3.1023201942443848,
      "learning_rate": 4.8046979865771815e-05,
      "loss": 0.6485,
      "step": 3410
    },
    {
      "epoch": 0.1368,
      "grad_norm": 2.621793508529663,
      "learning_rate": 4.8040268456375843e-05,
      "loss": 0.6468,
      "step": 3420
    },
    {
      "epoch": 0.1372,
      "grad_norm": 2.629180669784546,
      "learning_rate": 4.8033557046979865e-05,
      "loss": 0.7291,
      "step": 3430
    },
    {
      "epoch": 0.1376,
      "grad_norm": 3.2673604488372803,
      "learning_rate": 4.8026845637583894e-05,
      "loss": 0.7298,
      "step": 3440
    },
    {
      "epoch": 0.138,
      "grad_norm": 2.790562391281128,
      "learning_rate": 4.802013422818792e-05,
      "loss": 0.6215,
      "step": 3450
    },
    {
      "epoch": 0.1384,
      "grad_norm": 3.336378812789917,
      "learning_rate": 4.801342281879195e-05,
      "loss": 0.6757,
      "step": 3460
    },
    {
      "epoch": 0.1388,
      "grad_norm": 3.0014448165893555,
      "learning_rate": 4.800671140939598e-05,
      "loss": 0.6964,
      "step": 3470
    },
    {
      "epoch": 0.1392,
      "grad_norm": 2.4150373935699463,
      "learning_rate": 4.8e-05,
      "loss": 0.684,
      "step": 3480
    },
    {
      "epoch": 0.1396,
      "grad_norm": 2.42401385307312,
      "learning_rate": 4.799328859060403e-05,
      "loss": 0.5667,
      "step": 3490
    },
    {
      "epoch": 0.14,
      "grad_norm": 2.549682855606079,
      "learning_rate": 4.798657718120805e-05,
      "loss": 0.6158,
      "step": 3500
    },
    {
      "epoch": 0.1404,
      "grad_norm": 2.566399574279785,
      "learning_rate": 4.797986577181208e-05,
      "loss": 0.648,
      "step": 3510
    },
    {
      "epoch": 0.1408,
      "grad_norm": 2.9486639499664307,
      "learning_rate": 4.797315436241611e-05,
      "loss": 0.5798,
      "step": 3520
    },
    {
      "epoch": 0.1412,
      "grad_norm": 2.5631062984466553,
      "learning_rate": 4.796644295302014e-05,
      "loss": 0.5763,
      "step": 3530
    },
    {
      "epoch": 0.1416,
      "grad_norm": 2.535041332244873,
      "learning_rate": 4.7959731543624166e-05,
      "loss": 0.6629,
      "step": 3540
    },
    {
      "epoch": 0.142,
      "grad_norm": 3.3906095027923584,
      "learning_rate": 4.795302013422819e-05,
      "loss": 0.5941,
      "step": 3550
    },
    {
      "epoch": 0.1424,
      "grad_norm": 3.8973007202148438,
      "learning_rate": 4.7946308724832216e-05,
      "loss": 0.7891,
      "step": 3560
    },
    {
      "epoch": 0.1428,
      "grad_norm": 2.131363868713379,
      "learning_rate": 4.7939597315436245e-05,
      "loss": 0.6539,
      "step": 3570
    },
    {
      "epoch": 0.1432,
      "grad_norm": 1.850480079650879,
      "learning_rate": 4.793288590604027e-05,
      "loss": 0.6697,
      "step": 3580
    },
    {
      "epoch": 0.1436,
      "grad_norm": 2.8676581382751465,
      "learning_rate": 4.79261744966443e-05,
      "loss": 0.5906,
      "step": 3590
    },
    {
      "epoch": 0.144,
      "grad_norm": 2.7604265213012695,
      "learning_rate": 4.7919463087248323e-05,
      "loss": 0.6957,
      "step": 3600
    },
    {
      "epoch": 0.1444,
      "grad_norm": 3.0629475116729736,
      "learning_rate": 4.791275167785235e-05,
      "loss": 0.6381,
      "step": 3610
    },
    {
      "epoch": 0.1448,
      "grad_norm": 2.7032954692840576,
      "learning_rate": 4.7906040268456374e-05,
      "loss": 0.6934,
      "step": 3620
    },
    {
      "epoch": 0.1452,
      "grad_norm": 3.242450714111328,
      "learning_rate": 4.78993288590604e-05,
      "loss": 0.7403,
      "step": 3630
    },
    {
      "epoch": 0.1456,
      "grad_norm": 2.5876975059509277,
      "learning_rate": 4.789261744966443e-05,
      "loss": 0.6382,
      "step": 3640
    },
    {
      "epoch": 0.146,
      "grad_norm": 2.3977155685424805,
      "learning_rate": 4.788590604026846e-05,
      "loss": 0.7291,
      "step": 3650
    },
    {
      "epoch": 0.1464,
      "grad_norm": 3.489356517791748,
      "learning_rate": 4.787919463087249e-05,
      "loss": 0.7292,
      "step": 3660
    },
    {
      "epoch": 0.1468,
      "grad_norm": 3.3000051975250244,
      "learning_rate": 4.787248322147651e-05,
      "loss": 0.7366,
      "step": 3670
    },
    {
      "epoch": 0.1472,
      "grad_norm": 5.025209426879883,
      "learning_rate": 4.786577181208054e-05,
      "loss": 0.7681,
      "step": 3680
    },
    {
      "epoch": 0.1476,
      "grad_norm": 2.7519025802612305,
      "learning_rate": 4.785906040268457e-05,
      "loss": 0.643,
      "step": 3690
    },
    {
      "epoch": 0.148,
      "grad_norm": 3.408956289291382,
      "learning_rate": 4.7852348993288595e-05,
      "loss": 0.6657,
      "step": 3700
    },
    {
      "epoch": 0.1484,
      "grad_norm": 2.8606457710266113,
      "learning_rate": 4.7845637583892624e-05,
      "loss": 0.735,
      "step": 3710
    },
    {
      "epoch": 0.1488,
      "grad_norm": 3.4142820835113525,
      "learning_rate": 4.7838926174496646e-05,
      "loss": 0.6778,
      "step": 3720
    },
    {
      "epoch": 0.1492,
      "grad_norm": 2.0950613021850586,
      "learning_rate": 4.7832214765100674e-05,
      "loss": 0.5927,
      "step": 3730
    },
    {
      "epoch": 0.1496,
      "grad_norm": 2.5949466228485107,
      "learning_rate": 4.7825503355704696e-05,
      "loss": 0.6225,
      "step": 3740
    },
    {
      "epoch": 0.15,
      "grad_norm": 2.710686683654785,
      "learning_rate": 4.7818791946308725e-05,
      "loss": 0.7253,
      "step": 3750
    },
    {
      "epoch": 0.1504,
      "grad_norm": 3.800701856613159,
      "learning_rate": 4.781208053691276e-05,
      "loss": 0.706,
      "step": 3760
    },
    {
      "epoch": 0.1508,
      "grad_norm": 2.9027984142303467,
      "learning_rate": 4.780536912751678e-05,
      "loss": 0.637,
      "step": 3770
    },
    {
      "epoch": 0.1512,
      "grad_norm": 3.198089122772217,
      "learning_rate": 4.779865771812081e-05,
      "loss": 0.6506,
      "step": 3780
    },
    {
      "epoch": 0.1516,
      "grad_norm": 3.240888833999634,
      "learning_rate": 4.779194630872483e-05,
      "loss": 0.6831,
      "step": 3790
    },
    {
      "epoch": 0.152,
      "grad_norm": 2.7929129600524902,
      "learning_rate": 4.778523489932886e-05,
      "loss": 0.6189,
      "step": 3800
    },
    {
      "epoch": 0.1524,
      "grad_norm": 3.129035234451294,
      "learning_rate": 4.777852348993289e-05,
      "loss": 0.7176,
      "step": 3810
    },
    {
      "epoch": 0.1528,
      "grad_norm": 3.680332899093628,
      "learning_rate": 4.777181208053692e-05,
      "loss": 0.6664,
      "step": 3820
    },
    {
      "epoch": 0.1532,
      "grad_norm": 3.2434377670288086,
      "learning_rate": 4.7765100671140946e-05,
      "loss": 0.6377,
      "step": 3830
    },
    {
      "epoch": 0.1536,
      "grad_norm": 2.9676482677459717,
      "learning_rate": 4.775838926174497e-05,
      "loss": 0.7724,
      "step": 3840
    },
    {
      "epoch": 0.154,
      "grad_norm": 2.9132542610168457,
      "learning_rate": 4.7751677852348996e-05,
      "loss": 0.6454,
      "step": 3850
    },
    {
      "epoch": 0.1544,
      "grad_norm": 2.30350923538208,
      "learning_rate": 4.774496644295302e-05,
      "loss": 0.6306,
      "step": 3860
    },
    {
      "epoch": 0.1548,
      "grad_norm": 2.9474637508392334,
      "learning_rate": 4.773825503355705e-05,
      "loss": 0.6498,
      "step": 3870
    },
    {
      "epoch": 0.1552,
      "grad_norm": 3.2591824531555176,
      "learning_rate": 4.7731543624161075e-05,
      "loss": 0.6739,
      "step": 3880
    },
    {
      "epoch": 0.1556,
      "grad_norm": 2.815370559692383,
      "learning_rate": 4.7724832214765104e-05,
      "loss": 0.6602,
      "step": 3890
    },
    {
      "epoch": 0.156,
      "grad_norm": 3.0829432010650635,
      "learning_rate": 4.771812080536913e-05,
      "loss": 0.6,
      "step": 3900
    },
    {
      "epoch": 0.1564,
      "grad_norm": 3.0838570594787598,
      "learning_rate": 4.7711409395973154e-05,
      "loss": 0.6136,
      "step": 3910
    },
    {
      "epoch": 0.1568,
      "grad_norm": 2.176417112350464,
      "learning_rate": 4.770469798657718e-05,
      "loss": 0.6372,
      "step": 3920
    },
    {
      "epoch": 0.1572,
      "grad_norm": 3.4783244132995605,
      "learning_rate": 4.769798657718121e-05,
      "loss": 0.596,
      "step": 3930
    },
    {
      "epoch": 0.1576,
      "grad_norm": 2.7939813137054443,
      "learning_rate": 4.769127516778524e-05,
      "loss": 0.5513,
      "step": 3940
    },
    {
      "epoch": 0.158,
      "grad_norm": 2.9925858974456787,
      "learning_rate": 4.768456375838926e-05,
      "loss": 0.758,
      "step": 3950
    },
    {
      "epoch": 0.1584,
      "grad_norm": 3.273785352706909,
      "learning_rate": 4.767785234899329e-05,
      "loss": 0.6091,
      "step": 3960
    },
    {
      "epoch": 0.1588,
      "grad_norm": 2.711167097091675,
      "learning_rate": 4.767114093959732e-05,
      "loss": 0.6099,
      "step": 3970
    },
    {
      "epoch": 0.1592,
      "grad_norm": 1.990360975265503,
      "learning_rate": 4.766442953020134e-05,
      "loss": 0.66,
      "step": 3980
    },
    {
      "epoch": 0.1596,
      "grad_norm": 2.3746285438537598,
      "learning_rate": 4.7657718120805376e-05,
      "loss": 0.6732,
      "step": 3990
    },
    {
      "epoch": 0.16,
      "grad_norm": 3.5018060207366943,
      "learning_rate": 4.76510067114094e-05,
      "loss": 0.5983,
      "step": 4000
    },
    {
      "epoch": 0.1604,
      "grad_norm": 2.579200267791748,
      "learning_rate": 4.7644295302013426e-05,
      "loss": 0.7244,
      "step": 4010
    },
    {
      "epoch": 0.1608,
      "grad_norm": 2.9552738666534424,
      "learning_rate": 4.7637583892617455e-05,
      "loss": 0.6571,
      "step": 4020
    },
    {
      "epoch": 0.1612,
      "grad_norm": 3.1078526973724365,
      "learning_rate": 4.7630872483221476e-05,
      "loss": 0.7615,
      "step": 4030
    },
    {
      "epoch": 0.1616,
      "grad_norm": 3.4753870964050293,
      "learning_rate": 4.7624161073825505e-05,
      "loss": 0.6582,
      "step": 4040
    },
    {
      "epoch": 0.162,
      "grad_norm": 2.301274061203003,
      "learning_rate": 4.7617449664429534e-05,
      "loss": 0.6479,
      "step": 4050
    },
    {
      "epoch": 0.1624,
      "grad_norm": 2.395944118499756,
      "learning_rate": 4.761073825503356e-05,
      "loss": 0.7244,
      "step": 4060
    },
    {
      "epoch": 0.1628,
      "grad_norm": 2.0687649250030518,
      "learning_rate": 4.7604026845637584e-05,
      "loss": 0.6794,
      "step": 4070
    },
    {
      "epoch": 0.1632,
      "grad_norm": 3.6476211547851562,
      "learning_rate": 4.759731543624161e-05,
      "loss": 0.827,
      "step": 4080
    },
    {
      "epoch": 0.1636,
      "grad_norm": 3.204324722290039,
      "learning_rate": 4.759060402684564e-05,
      "loss": 0.6892,
      "step": 4090
    },
    {
      "epoch": 0.164,
      "grad_norm": 3.658255100250244,
      "learning_rate": 4.758389261744966e-05,
      "loss": 0.7176,
      "step": 4100
    },
    {
      "epoch": 0.1644,
      "grad_norm": 3.558881998062134,
      "learning_rate": 4.75771812080537e-05,
      "loss": 0.7296,
      "step": 4110
    },
    {
      "epoch": 0.1648,
      "grad_norm": 2.6831467151641846,
      "learning_rate": 4.757046979865772e-05,
      "loss": 0.6851,
      "step": 4120
    },
    {
      "epoch": 0.1652,
      "grad_norm": 2.6287379264831543,
      "learning_rate": 4.756375838926175e-05,
      "loss": 0.7224,
      "step": 4130
    },
    {
      "epoch": 0.1656,
      "grad_norm": 3.014986991882324,
      "learning_rate": 4.755704697986577e-05,
      "loss": 0.854,
      "step": 4140
    },
    {
      "epoch": 0.166,
      "grad_norm": 3.2711238861083984,
      "learning_rate": 4.75503355704698e-05,
      "loss": 0.6186,
      "step": 4150
    },
    {
      "epoch": 0.1664,
      "grad_norm": 3.119485855102539,
      "learning_rate": 4.754362416107383e-05,
      "loss": 0.6245,
      "step": 4160
    },
    {
      "epoch": 0.1668,
      "grad_norm": 3.3244004249572754,
      "learning_rate": 4.7536912751677856e-05,
      "loss": 0.659,
      "step": 4170
    },
    {
      "epoch": 0.1672,
      "grad_norm": 4.7159318923950195,
      "learning_rate": 4.7530201342281884e-05,
      "loss": 0.6683,
      "step": 4180
    },
    {
      "epoch": 0.1676,
      "grad_norm": 2.6464684009552,
      "learning_rate": 4.7523489932885906e-05,
      "loss": 0.6319,
      "step": 4190
    },
    {
      "epoch": 0.168,
      "grad_norm": 2.431180715560913,
      "learning_rate": 4.7516778523489935e-05,
      "loss": 0.7905,
      "step": 4200
    },
    {
      "epoch": 0.1684,
      "grad_norm": 2.255610466003418,
      "learning_rate": 4.751006711409396e-05,
      "loss": 0.6349,
      "step": 4210
    },
    {
      "epoch": 0.1688,
      "grad_norm": 2.187929630279541,
      "learning_rate": 4.7503355704697985e-05,
      "loss": 0.6178,
      "step": 4220
    },
    {
      "epoch": 0.1692,
      "grad_norm": 2.126277208328247,
      "learning_rate": 4.749664429530202e-05,
      "loss": 0.755,
      "step": 4230
    },
    {
      "epoch": 0.1696,
      "grad_norm": 3.561457395553589,
      "learning_rate": 4.748993288590604e-05,
      "loss": 0.7047,
      "step": 4240
    },
    {
      "epoch": 0.17,
      "grad_norm": 2.801785707473755,
      "learning_rate": 4.748322147651007e-05,
      "loss": 0.7228,
      "step": 4250
    },
    {
      "epoch": 0.1704,
      "grad_norm": 4.260695934295654,
      "learning_rate": 4.747651006711409e-05,
      "loss": 0.7424,
      "step": 4260
    },
    {
      "epoch": 0.1708,
      "grad_norm": 3.0242185592651367,
      "learning_rate": 4.746979865771812e-05,
      "loss": 0.6453,
      "step": 4270
    },
    {
      "epoch": 0.1712,
      "grad_norm": 2.6189117431640625,
      "learning_rate": 4.746308724832215e-05,
      "loss": 0.6542,
      "step": 4280
    },
    {
      "epoch": 0.1716,
      "grad_norm": 3.044543743133545,
      "learning_rate": 4.745637583892618e-05,
      "loss": 0.7455,
      "step": 4290
    },
    {
      "epoch": 0.172,
      "grad_norm": 2.6135616302490234,
      "learning_rate": 4.7449664429530207e-05,
      "loss": 0.6292,
      "step": 4300
    },
    {
      "epoch": 0.1724,
      "grad_norm": 3.9813642501831055,
      "learning_rate": 4.744295302013423e-05,
      "loss": 0.7012,
      "step": 4310
    },
    {
      "epoch": 0.1728,
      "grad_norm": 2.3806729316711426,
      "learning_rate": 4.743624161073826e-05,
      "loss": 0.5554,
      "step": 4320
    },
    {
      "epoch": 0.1732,
      "grad_norm": 3.5967981815338135,
      "learning_rate": 4.742953020134228e-05,
      "loss": 0.7867,
      "step": 4330
    },
    {
      "epoch": 0.1736,
      "grad_norm": 2.615541934967041,
      "learning_rate": 4.7422818791946314e-05,
      "loss": 0.5812,
      "step": 4340
    },
    {
      "epoch": 0.174,
      "grad_norm": 2.8125524520874023,
      "learning_rate": 4.741610738255034e-05,
      "loss": 0.7322,
      "step": 4350
    },
    {
      "epoch": 0.1744,
      "grad_norm": 2.600607395172119,
      "learning_rate": 4.7409395973154364e-05,
      "loss": 0.7425,
      "step": 4360
    },
    {
      "epoch": 0.1748,
      "grad_norm": 2.7216906547546387,
      "learning_rate": 4.740268456375839e-05,
      "loss": 0.7223,
      "step": 4370
    },
    {
      "epoch": 0.1752,
      "grad_norm": 3.38735032081604,
      "learning_rate": 4.7395973154362415e-05,
      "loss": 0.7145,
      "step": 4380
    },
    {
      "epoch": 0.1756,
      "grad_norm": 3.270146131515503,
      "learning_rate": 4.738926174496644e-05,
      "loss": 0.6969,
      "step": 4390
    },
    {
      "epoch": 0.176,
      "grad_norm": 2.4294066429138184,
      "learning_rate": 4.738255033557047e-05,
      "loss": 0.693,
      "step": 4400
    },
    {
      "epoch": 0.1764,
      "grad_norm": 3.5040857791900635,
      "learning_rate": 4.73758389261745e-05,
      "loss": 0.7963,
      "step": 4410
    },
    {
      "epoch": 0.1768,
      "grad_norm": 2.3901748657226562,
      "learning_rate": 4.736912751677853e-05,
      "loss": 0.7239,
      "step": 4420
    },
    {
      "epoch": 0.1772,
      "grad_norm": 3.173476457595825,
      "learning_rate": 4.736241610738255e-05,
      "loss": 0.646,
      "step": 4430
    },
    {
      "epoch": 0.1776,
      "grad_norm": 2.2773776054382324,
      "learning_rate": 4.735570469798658e-05,
      "loss": 0.6602,
      "step": 4440
    },
    {
      "epoch": 0.178,
      "grad_norm": 3.647456169128418,
      "learning_rate": 4.73489932885906e-05,
      "loss": 0.6499,
      "step": 4450
    },
    {
      "epoch": 0.1784,
      "grad_norm": 2.341747760772705,
      "learning_rate": 4.7342281879194636e-05,
      "loss": 0.6353,
      "step": 4460
    },
    {
      "epoch": 0.1788,
      "grad_norm": 3.1014466285705566,
      "learning_rate": 4.7335570469798665e-05,
      "loss": 0.6608,
      "step": 4470
    },
    {
      "epoch": 0.1792,
      "grad_norm": 2.850029468536377,
      "learning_rate": 4.7328859060402687e-05,
      "loss": 0.6385,
      "step": 4480
    },
    {
      "epoch": 0.1796,
      "grad_norm": 2.8033828735351562,
      "learning_rate": 4.7322147651006715e-05,
      "loss": 0.5315,
      "step": 4490
    },
    {
      "epoch": 0.18,
      "grad_norm": 3.2132904529571533,
      "learning_rate": 4.731543624161074e-05,
      "loss": 0.7281,
      "step": 4500
    },
    {
      "epoch": 0.1804,
      "grad_norm": 3.174916982650757,
      "learning_rate": 4.7308724832214765e-05,
      "loss": 0.6856,
      "step": 4510
    },
    {
      "epoch": 0.1808,
      "grad_norm": 2.5964438915252686,
      "learning_rate": 4.7302013422818794e-05,
      "loss": 0.6289,
      "step": 4520
    },
    {
      "epoch": 0.1812,
      "grad_norm": 2.948853015899658,
      "learning_rate": 4.729530201342282e-05,
      "loss": 0.6991,
      "step": 4530
    },
    {
      "epoch": 0.1816,
      "grad_norm": 2.678985118865967,
      "learning_rate": 4.728859060402685e-05,
      "loss": 0.6816,
      "step": 4540
    },
    {
      "epoch": 0.182,
      "grad_norm": 3.121001720428467,
      "learning_rate": 4.728187919463087e-05,
      "loss": 0.6407,
      "step": 4550
    },
    {
      "epoch": 0.1824,
      "grad_norm": 2.7342379093170166,
      "learning_rate": 4.72751677852349e-05,
      "loss": 0.6027,
      "step": 4560
    },
    {
      "epoch": 0.1828,
      "grad_norm": 2.6769347190856934,
      "learning_rate": 4.726845637583892e-05,
      "loss": 0.6026,
      "step": 4570
    },
    {
      "epoch": 0.1832,
      "grad_norm": 3.034291982650757,
      "learning_rate": 4.726174496644296e-05,
      "loss": 0.704,
      "step": 4580
    },
    {
      "epoch": 0.1836,
      "grad_norm": 3.6204113960266113,
      "learning_rate": 4.725503355704699e-05,
      "loss": 0.6267,
      "step": 4590
    },
    {
      "epoch": 0.184,
      "grad_norm": 2.9889872074127197,
      "learning_rate": 4.724832214765101e-05,
      "loss": 0.6792,
      "step": 4600
    },
    {
      "epoch": 0.1844,
      "grad_norm": 3.1542792320251465,
      "learning_rate": 4.724161073825504e-05,
      "loss": 0.6794,
      "step": 4610
    },
    {
      "epoch": 0.1848,
      "grad_norm": 2.457693576812744,
      "learning_rate": 4.723489932885906e-05,
      "loss": 0.7043,
      "step": 4620
    },
    {
      "epoch": 0.1852,
      "grad_norm": 3.091993570327759,
      "learning_rate": 4.722818791946309e-05,
      "loss": 0.7281,
      "step": 4630
    },
    {
      "epoch": 0.1856,
      "grad_norm": 2.061450958251953,
      "learning_rate": 4.7221476510067116e-05,
      "loss": 0.6527,
      "step": 4640
    },
    {
      "epoch": 0.186,
      "grad_norm": 3.103177785873413,
      "learning_rate": 4.7214765100671145e-05,
      "loss": 0.6877,
      "step": 4650
    },
    {
      "epoch": 0.1864,
      "grad_norm": 3.0168559551239014,
      "learning_rate": 4.720805369127517e-05,
      "loss": 0.6705,
      "step": 4660
    },
    {
      "epoch": 0.1868,
      "grad_norm": 2.4407479763031006,
      "learning_rate": 4.7201342281879195e-05,
      "loss": 0.7532,
      "step": 4670
    },
    {
      "epoch": 0.1872,
      "grad_norm": 2.62876296043396,
      "learning_rate": 4.7194630872483224e-05,
      "loss": 0.5835,
      "step": 4680
    },
    {
      "epoch": 0.1876,
      "grad_norm": 2.7947194576263428,
      "learning_rate": 4.718791946308725e-05,
      "loss": 0.6651,
      "step": 4690
    },
    {
      "epoch": 0.188,
      "grad_norm": 3.5918760299682617,
      "learning_rate": 4.718120805369128e-05,
      "loss": 0.7184,
      "step": 4700
    },
    {
      "epoch": 0.1884,
      "grad_norm": 3.6243574619293213,
      "learning_rate": 4.71744966442953e-05,
      "loss": 0.6542,
      "step": 4710
    },
    {
      "epoch": 0.1888,
      "grad_norm": 3.116671323776245,
      "learning_rate": 4.716778523489933e-05,
      "loss": 0.7077,
      "step": 4720
    },
    {
      "epoch": 0.1892,
      "grad_norm": 2.5090160369873047,
      "learning_rate": 4.716107382550336e-05,
      "loss": 0.6738,
      "step": 4730
    },
    {
      "epoch": 0.1896,
      "grad_norm": 4.056818962097168,
      "learning_rate": 4.715436241610738e-05,
      "loss": 0.6481,
      "step": 4740
    },
    {
      "epoch": 0.19,
      "grad_norm": 2.6122124195098877,
      "learning_rate": 4.714765100671141e-05,
      "loss": 0.6187,
      "step": 4750
    },
    {
      "epoch": 0.1904,
      "grad_norm": 3.04734206199646,
      "learning_rate": 4.714093959731544e-05,
      "loss": 0.7601,
      "step": 4760
    },
    {
      "epoch": 0.1908,
      "grad_norm": 3.013529062271118,
      "learning_rate": 4.713422818791947e-05,
      "loss": 0.6909,
      "step": 4770
    },
    {
      "epoch": 0.1912,
      "grad_norm": 2.938337564468384,
      "learning_rate": 4.712751677852349e-05,
      "loss": 0.6684,
      "step": 4780
    },
    {
      "epoch": 0.1916,
      "grad_norm": 2.4408740997314453,
      "learning_rate": 4.712080536912752e-05,
      "loss": 0.7357,
      "step": 4790
    },
    {
      "epoch": 0.192,
      "grad_norm": 3.38387131690979,
      "learning_rate": 4.7114093959731546e-05,
      "loss": 0.694,
      "step": 4800
    },
    {
      "epoch": 0.1924,
      "grad_norm": 3.1332907676696777,
      "learning_rate": 4.7107382550335574e-05,
      "loss": 0.7352,
      "step": 4810
    },
    {
      "epoch": 0.1928,
      "grad_norm": 3.299900531768799,
      "learning_rate": 4.71006711409396e-05,
      "loss": 0.6359,
      "step": 4820
    },
    {
      "epoch": 0.1932,
      "grad_norm": 3.4943501949310303,
      "learning_rate": 4.7093959731543625e-05,
      "loss": 0.6635,
      "step": 4830
    },
    {
      "epoch": 0.1936,
      "grad_norm": 2.9179015159606934,
      "learning_rate": 4.708724832214765e-05,
      "loss": 0.585,
      "step": 4840
    },
    {
      "epoch": 0.194,
      "grad_norm": 2.951326608657837,
      "learning_rate": 4.708053691275168e-05,
      "loss": 0.6415,
      "step": 4850
    },
    {
      "epoch": 0.1944,
      "grad_norm": 3.0583372116088867,
      "learning_rate": 4.7073825503355704e-05,
      "loss": 0.8354,
      "step": 4860
    },
    {
      "epoch": 0.1948,
      "grad_norm": 2.6159586906433105,
      "learning_rate": 4.706711409395973e-05,
      "loss": 0.6382,
      "step": 4870
    },
    {
      "epoch": 0.1952,
      "grad_norm": 3.1506552696228027,
      "learning_rate": 4.706040268456376e-05,
      "loss": 0.7081,
      "step": 4880
    },
    {
      "epoch": 0.1956,
      "grad_norm": 2.7190141677856445,
      "learning_rate": 4.705369127516779e-05,
      "loss": 0.6608,
      "step": 4890
    },
    {
      "epoch": 0.196,
      "grad_norm": 3.8014535903930664,
      "learning_rate": 4.704697986577181e-05,
      "loss": 0.7427,
      "step": 4900
    },
    {
      "epoch": 0.1964,
      "grad_norm": 3.1697967052459717,
      "learning_rate": 4.704026845637584e-05,
      "loss": 0.6355,
      "step": 4910
    },
    {
      "epoch": 0.1968,
      "grad_norm": 1.8619661331176758,
      "learning_rate": 4.703355704697987e-05,
      "loss": 0.6399,
      "step": 4920
    },
    {
      "epoch": 0.1972,
      "grad_norm": 3.1725549697875977,
      "learning_rate": 4.7026845637583897e-05,
      "loss": 0.6042,
      "step": 4930
    },
    {
      "epoch": 0.1976,
      "grad_norm": 3.1189260482788086,
      "learning_rate": 4.7020134228187925e-05,
      "loss": 0.7714,
      "step": 4940
    },
    {
      "epoch": 0.198,
      "grad_norm": 2.576641798019409,
      "learning_rate": 4.701342281879195e-05,
      "loss": 0.6491,
      "step": 4950
    },
    {
      "epoch": 0.1984,
      "grad_norm": 3.1936867237091064,
      "learning_rate": 4.7006711409395975e-05,
      "loss": 0.6883,
      "step": 4960
    },
    {
      "epoch": 0.1988,
      "grad_norm": 3.253382682800293,
      "learning_rate": 4.7e-05,
      "loss": 0.6632,
      "step": 4970
    },
    {
      "epoch": 0.1992,
      "grad_norm": 2.8342132568359375,
      "learning_rate": 4.6993288590604026e-05,
      "loss": 0.5947,
      "step": 4980
    },
    {
      "epoch": 0.1996,
      "grad_norm": 3.172300338745117,
      "learning_rate": 4.698657718120806e-05,
      "loss": 0.7682,
      "step": 4990
    },
    {
      "epoch": 0.2,
      "grad_norm": 3.4803569316864014,
      "learning_rate": 4.697986577181208e-05,
      "loss": 0.7646,
      "step": 5000
    },
    {
      "epoch": 0.2004,
      "grad_norm": 3.6276440620422363,
      "learning_rate": 4.697315436241611e-05,
      "loss": 0.7547,
      "step": 5010
    },
    {
      "epoch": 0.2008,
      "grad_norm": 2.927718162536621,
      "learning_rate": 4.696644295302013e-05,
      "loss": 0.8397,
      "step": 5020
    },
    {
      "epoch": 0.2012,
      "grad_norm": 2.6839609146118164,
      "learning_rate": 4.695973154362416e-05,
      "loss": 0.8003,
      "step": 5030
    },
    {
      "epoch": 0.2016,
      "grad_norm": 3.1213409900665283,
      "learning_rate": 4.695302013422819e-05,
      "loss": 0.5915,
      "step": 5040
    },
    {
      "epoch": 0.202,
      "grad_norm": 2.904341697692871,
      "learning_rate": 4.694630872483222e-05,
      "loss": 0.6815,
      "step": 5050
    },
    {
      "epoch": 0.2024,
      "grad_norm": 3.510446786880493,
      "learning_rate": 4.693959731543625e-05,
      "loss": 0.7659,
      "step": 5060
    },
    {
      "epoch": 0.2028,
      "grad_norm": 2.4245760440826416,
      "learning_rate": 4.693288590604027e-05,
      "loss": 0.708,
      "step": 5070
    },
    {
      "epoch": 0.2032,
      "grad_norm": 3.139171600341797,
      "learning_rate": 4.69261744966443e-05,
      "loss": 0.7312,
      "step": 5080
    },
    {
      "epoch": 0.2036,
      "grad_norm": 3.6553001403808594,
      "learning_rate": 4.691946308724832e-05,
      "loss": 0.7485,
      "step": 5090
    },
    {
      "epoch": 0.204,
      "grad_norm": 3.2393124103546143,
      "learning_rate": 4.691275167785235e-05,
      "loss": 0.6724,
      "step": 5100
    },
    {
      "epoch": 0.2044,
      "grad_norm": 3.652843713760376,
      "learning_rate": 4.690604026845638e-05,
      "loss": 0.5987,
      "step": 5110
    },
    {
      "epoch": 0.2048,
      "grad_norm": 2.658134698867798,
      "learning_rate": 4.6899328859060405e-05,
      "loss": 0.6408,
      "step": 5120
    },
    {
      "epoch": 0.2052,
      "grad_norm": 2.561502695083618,
      "learning_rate": 4.6892617449664434e-05,
      "loss": 0.6149,
      "step": 5130
    },
    {
      "epoch": 0.2056,
      "grad_norm": 2.641754388809204,
      "learning_rate": 4.6885906040268455e-05,
      "loss": 0.6424,
      "step": 5140
    },
    {
      "epoch": 0.206,
      "grad_norm": 3.2366268634796143,
      "learning_rate": 4.6879194630872484e-05,
      "loss": 0.7339,
      "step": 5150
    },
    {
      "epoch": 0.2064,
      "grad_norm": 2.9446911811828613,
      "learning_rate": 4.687248322147651e-05,
      "loss": 0.7079,
      "step": 5160
    },
    {
      "epoch": 0.2068,
      "grad_norm": 3.6319494247436523,
      "learning_rate": 4.686577181208054e-05,
      "loss": 0.7142,
      "step": 5170
    },
    {
      "epoch": 0.2072,
      "grad_norm": 3.135855197906494,
      "learning_rate": 4.685906040268457e-05,
      "loss": 0.8152,
      "step": 5180
    },
    {
      "epoch": 0.2076,
      "grad_norm": 2.622804880142212,
      "learning_rate": 4.685234899328859e-05,
      "loss": 0.6188,
      "step": 5190
    },
    {
      "epoch": 0.208,
      "grad_norm": 2.7712039947509766,
      "learning_rate": 4.684563758389262e-05,
      "loss": 0.7203,
      "step": 5200
    },
    {
      "epoch": 0.2084,
      "grad_norm": 2.8157835006713867,
      "learning_rate": 4.683892617449664e-05,
      "loss": 0.6311,
      "step": 5210
    },
    {
      "epoch": 0.2088,
      "grad_norm": 2.527897357940674,
      "learning_rate": 4.683221476510068e-05,
      "loss": 0.63,
      "step": 5220
    },
    {
      "epoch": 0.2092,
      "grad_norm": 3.4294421672821045,
      "learning_rate": 4.6825503355704706e-05,
      "loss": 0.62,
      "step": 5230
    },
    {
      "epoch": 0.2096,
      "grad_norm": 2.634782314300537,
      "learning_rate": 4.681879194630873e-05,
      "loss": 0.6922,
      "step": 5240
    },
    {
      "epoch": 0.21,
      "grad_norm": 3.4847424030303955,
      "learning_rate": 4.6812080536912756e-05,
      "loss": 0.6946,
      "step": 5250
    },
    {
      "epoch": 0.2104,
      "grad_norm": 3.28163480758667,
      "learning_rate": 4.680536912751678e-05,
      "loss": 0.7668,
      "step": 5260
    },
    {
      "epoch": 0.2108,
      "grad_norm": 2.751269578933716,
      "learning_rate": 4.6798657718120806e-05,
      "loss": 0.6991,
      "step": 5270
    },
    {
      "epoch": 0.2112,
      "grad_norm": 2.952765941619873,
      "learning_rate": 4.6791946308724835e-05,
      "loss": 0.6649,
      "step": 5280
    },
    {
      "epoch": 0.2116,
      "grad_norm": 3.976536273956299,
      "learning_rate": 4.678523489932886e-05,
      "loss": 0.737,
      "step": 5290
    },
    {
      "epoch": 0.212,
      "grad_norm": 2.9495062828063965,
      "learning_rate": 4.677852348993289e-05,
      "loss": 0.5719,
      "step": 5300
    },
    {
      "epoch": 0.2124,
      "grad_norm": 2.2860677242279053,
      "learning_rate": 4.6771812080536914e-05,
      "loss": 0.6507,
      "step": 5310
    },
    {
      "epoch": 0.2128,
      "grad_norm": 3.527737617492676,
      "learning_rate": 4.676510067114094e-05,
      "loss": 0.6192,
      "step": 5320
    },
    {
      "epoch": 0.2132,
      "grad_norm": 2.8382508754730225,
      "learning_rate": 4.6758389261744964e-05,
      "loss": 0.7068,
      "step": 5330
    },
    {
      "epoch": 0.2136,
      "grad_norm": 3.531381607055664,
      "learning_rate": 4.6751677852349e-05,
      "loss": 0.6843,
      "step": 5340
    },
    {
      "epoch": 0.214,
      "grad_norm": 3.1332826614379883,
      "learning_rate": 4.674496644295302e-05,
      "loss": 0.7741,
      "step": 5350
    },
    {
      "epoch": 0.2144,
      "grad_norm": 2.9361841678619385,
      "learning_rate": 4.673825503355705e-05,
      "loss": 0.696,
      "step": 5360
    },
    {
      "epoch": 0.2148,
      "grad_norm": 2.59313702583313,
      "learning_rate": 4.673154362416108e-05,
      "loss": 0.7024,
      "step": 5370
    },
    {
      "epoch": 0.2152,
      "grad_norm": 2.7116808891296387,
      "learning_rate": 4.67248322147651e-05,
      "loss": 0.7159,
      "step": 5380
    },
    {
      "epoch": 0.2156,
      "grad_norm": 3.5002758502960205,
      "learning_rate": 4.671812080536913e-05,
      "loss": 0.7359,
      "step": 5390
    },
    {
      "epoch": 0.216,
      "grad_norm": 3.3009798526763916,
      "learning_rate": 4.671140939597316e-05,
      "loss": 0.7222,
      "step": 5400
    },
    {
      "epoch": 0.2164,
      "grad_norm": 3.064408540725708,
      "learning_rate": 4.6704697986577186e-05,
      "loss": 0.6241,
      "step": 5410
    },
    {
      "epoch": 0.2168,
      "grad_norm": 4.15158224105835,
      "learning_rate": 4.6697986577181214e-05,
      "loss": 0.6532,
      "step": 5420
    },
    {
      "epoch": 0.2172,
      "grad_norm": 2.5392818450927734,
      "learning_rate": 4.6691275167785236e-05,
      "loss": 0.6274,
      "step": 5430
    },
    {
      "epoch": 0.2176,
      "grad_norm": 4.091337203979492,
      "learning_rate": 4.6684563758389264e-05,
      "loss": 0.6557,
      "step": 5440
    },
    {
      "epoch": 0.218,
      "grad_norm": 2.7626919746398926,
      "learning_rate": 4.6677852348993286e-05,
      "loss": 0.738,
      "step": 5450
    },
    {
      "epoch": 0.2184,
      "grad_norm": 2.645911693572998,
      "learning_rate": 4.667114093959732e-05,
      "loss": 0.6741,
      "step": 5460
    },
    {
      "epoch": 0.2188,
      "grad_norm": 3.042515277862549,
      "learning_rate": 4.666442953020134e-05,
      "loss": 0.7859,
      "step": 5470
    },
    {
      "epoch": 0.2192,
      "grad_norm": 3.241403579711914,
      "learning_rate": 4.665771812080537e-05,
      "loss": 0.6336,
      "step": 5480
    },
    {
      "epoch": 0.2196,
      "grad_norm": 3.617866039276123,
      "learning_rate": 4.66510067114094e-05,
      "loss": 0.6593,
      "step": 5490
    },
    {
      "epoch": 0.22,
      "grad_norm": 2.7307724952697754,
      "learning_rate": 4.664429530201342e-05,
      "loss": 0.6929,
      "step": 5500
    },
    {
      "epoch": 0.2204,
      "grad_norm": 3.012559413909912,
      "learning_rate": 4.663758389261745e-05,
      "loss": 0.671,
      "step": 5510
    },
    {
      "epoch": 0.2208,
      "grad_norm": 3.3183608055114746,
      "learning_rate": 4.663087248322148e-05,
      "loss": 0.7608,
      "step": 5520
    },
    {
      "epoch": 0.2212,
      "grad_norm": 3.2394983768463135,
      "learning_rate": 4.662416107382551e-05,
      "loss": 0.77,
      "step": 5530
    },
    {
      "epoch": 0.2216,
      "grad_norm": 3.358062982559204,
      "learning_rate": 4.661744966442953e-05,
      "loss": 0.6388,
      "step": 5540
    },
    {
      "epoch": 0.222,
      "grad_norm": 2.4739251136779785,
      "learning_rate": 4.661073825503356e-05,
      "loss": 0.684,
      "step": 5550
    },
    {
      "epoch": 0.2224,
      "grad_norm": 2.918252944946289,
      "learning_rate": 4.660402684563759e-05,
      "loss": 0.5904,
      "step": 5560
    },
    {
      "epoch": 0.2228,
      "grad_norm": 2.8871920108795166,
      "learning_rate": 4.6597315436241615e-05,
      "loss": 0.8039,
      "step": 5570
    },
    {
      "epoch": 0.2232,
      "grad_norm": 2.929218053817749,
      "learning_rate": 4.6590604026845644e-05,
      "loss": 0.7356,
      "step": 5580
    },
    {
      "epoch": 0.2236,
      "grad_norm": 3.3982646465301514,
      "learning_rate": 4.6583892617449666e-05,
      "loss": 0.7022,
      "step": 5590
    },
    {
      "epoch": 0.224,
      "grad_norm": 2.8458807468414307,
      "learning_rate": 4.6577181208053694e-05,
      "loss": 0.7159,
      "step": 5600
    },
    {
      "epoch": 0.2244,
      "grad_norm": 3.0133285522460938,
      "learning_rate": 4.6570469798657716e-05,
      "loss": 0.8095,
      "step": 5610
    },
    {
      "epoch": 0.2248,
      "grad_norm": 2.8241450786590576,
      "learning_rate": 4.6563758389261744e-05,
      "loss": 0.6601,
      "step": 5620
    },
    {
      "epoch": 0.2252,
      "grad_norm": 2.120976209640503,
      "learning_rate": 4.655704697986577e-05,
      "loss": 0.6994,
      "step": 5630
    },
    {
      "epoch": 0.2256,
      "grad_norm": 3.1032373905181885,
      "learning_rate": 4.65503355704698e-05,
      "loss": 0.56,
      "step": 5640
    },
    {
      "epoch": 0.226,
      "grad_norm": 3.12233567237854,
      "learning_rate": 4.654362416107383e-05,
      "loss": 0.6476,
      "step": 5650
    },
    {
      "epoch": 0.2264,
      "grad_norm": 2.684650182723999,
      "learning_rate": 4.653691275167785e-05,
      "loss": 0.6342,
      "step": 5660
    },
    {
      "epoch": 0.2268,
      "grad_norm": 2.7455005645751953,
      "learning_rate": 4.653020134228188e-05,
      "loss": 0.6159,
      "step": 5670
    },
    {
      "epoch": 0.2272,
      "grad_norm": 3.1386492252349854,
      "learning_rate": 4.652348993288591e-05,
      "loss": 0.7223,
      "step": 5680
    },
    {
      "epoch": 0.2276,
      "grad_norm": 2.67630672454834,
      "learning_rate": 4.651677852348994e-05,
      "loss": 0.6822,
      "step": 5690
    },
    {
      "epoch": 0.228,
      "grad_norm": 2.6600053310394287,
      "learning_rate": 4.6510067114093966e-05,
      "loss": 0.7485,
      "step": 5700
    },
    {
      "epoch": 0.2284,
      "grad_norm": 2.4585227966308594,
      "learning_rate": 4.650335570469799e-05,
      "loss": 0.6535,
      "step": 5710
    },
    {
      "epoch": 0.2288,
      "grad_norm": 2.7673659324645996,
      "learning_rate": 4.6496644295302016e-05,
      "loss": 0.7123,
      "step": 5720
    },
    {
      "epoch": 0.2292,
      "grad_norm": 2.9790990352630615,
      "learning_rate": 4.648993288590604e-05,
      "loss": 0.6851,
      "step": 5730
    },
    {
      "epoch": 0.2296,
      "grad_norm": 3.1866161823272705,
      "learning_rate": 4.648322147651007e-05,
      "loss": 0.6891,
      "step": 5740
    },
    {
      "epoch": 0.23,
      "grad_norm": 3.8962647914886475,
      "learning_rate": 4.6476510067114095e-05,
      "loss": 0.752,
      "step": 5750
    },
    {
      "epoch": 0.2304,
      "grad_norm": 2.29579758644104,
      "learning_rate": 4.6469798657718124e-05,
      "loss": 0.6882,
      "step": 5760
    },
    {
      "epoch": 0.2308,
      "grad_norm": 3.163827896118164,
      "learning_rate": 4.646308724832215e-05,
      "loss": 0.6535,
      "step": 5770
    },
    {
      "epoch": 0.2312,
      "grad_norm": 2.4524145126342773,
      "learning_rate": 4.6456375838926174e-05,
      "loss": 0.6818,
      "step": 5780
    },
    {
      "epoch": 0.2316,
      "grad_norm": 2.6332826614379883,
      "learning_rate": 4.64496644295302e-05,
      "loss": 0.5809,
      "step": 5790
    },
    {
      "epoch": 0.232,
      "grad_norm": 3.2767739295959473,
      "learning_rate": 4.644295302013423e-05,
      "loss": 0.7244,
      "step": 5800
    },
    {
      "epoch": 0.2324,
      "grad_norm": 2.61075496673584,
      "learning_rate": 4.643624161073826e-05,
      "loss": 0.6394,
      "step": 5810
    },
    {
      "epoch": 0.2328,
      "grad_norm": 2.237744092941284,
      "learning_rate": 4.642953020134229e-05,
      "loss": 0.7119,
      "step": 5820
    },
    {
      "epoch": 0.2332,
      "grad_norm": 3.6209850311279297,
      "learning_rate": 4.642281879194631e-05,
      "loss": 0.6616,
      "step": 5830
    },
    {
      "epoch": 0.2336,
      "grad_norm": 3.22088623046875,
      "learning_rate": 4.641610738255034e-05,
      "loss": 0.7019,
      "step": 5840
    },
    {
      "epoch": 0.234,
      "grad_norm": 2.7640929222106934,
      "learning_rate": 4.640939597315436e-05,
      "loss": 0.6753,
      "step": 5850
    },
    {
      "epoch": 0.2344,
      "grad_norm": 2.8228394985198975,
      "learning_rate": 4.640268456375839e-05,
      "loss": 0.6416,
      "step": 5860
    },
    {
      "epoch": 0.2348,
      "grad_norm": 3.130629062652588,
      "learning_rate": 4.6395973154362424e-05,
      "loss": 0.6843,
      "step": 5870
    },
    {
      "epoch": 0.2352,
      "grad_norm": 3.7707619667053223,
      "learning_rate": 4.6389261744966446e-05,
      "loss": 0.7823,
      "step": 5880
    },
    {
      "epoch": 0.2356,
      "grad_norm": 2.1792826652526855,
      "learning_rate": 4.6382550335570474e-05,
      "loss": 0.7601,
      "step": 5890
    },
    {
      "epoch": 0.236,
      "grad_norm": 3.4497880935668945,
      "learning_rate": 4.6375838926174496e-05,
      "loss": 0.8113,
      "step": 5900
    },
    {
      "epoch": 0.2364,
      "grad_norm": 2.728868246078491,
      "learning_rate": 4.6369127516778525e-05,
      "loss": 0.6624,
      "step": 5910
    },
    {
      "epoch": 0.2368,
      "grad_norm": 3.179262161254883,
      "learning_rate": 4.636241610738255e-05,
      "loss": 0.6963,
      "step": 5920
    },
    {
      "epoch": 0.2372,
      "grad_norm": 3.6483871936798096,
      "learning_rate": 4.635570469798658e-05,
      "loss": 0.7435,
      "step": 5930
    },
    {
      "epoch": 0.2376,
      "grad_norm": 3.431669235229492,
      "learning_rate": 4.634899328859061e-05,
      "loss": 0.8013,
      "step": 5940
    },
    {
      "epoch": 0.238,
      "grad_norm": 2.7760519981384277,
      "learning_rate": 4.634228187919463e-05,
      "loss": 0.6691,
      "step": 5950
    },
    {
      "epoch": 0.2384,
      "grad_norm": 2.999640464782715,
      "learning_rate": 4.633557046979866e-05,
      "loss": 0.6985,
      "step": 5960
    },
    {
      "epoch": 0.2388,
      "grad_norm": 2.8490302562713623,
      "learning_rate": 4.632885906040268e-05,
      "loss": 0.7391,
      "step": 5970
    },
    {
      "epoch": 0.2392,
      "grad_norm": 3.56278657913208,
      "learning_rate": 4.632214765100671e-05,
      "loss": 0.7384,
      "step": 5980
    },
    {
      "epoch": 0.2396,
      "grad_norm": 3.5788393020629883,
      "learning_rate": 4.631543624161074e-05,
      "loss": 0.7456,
      "step": 5990
    },
    {
      "epoch": 0.24,
      "grad_norm": 2.238428831100464,
      "learning_rate": 4.630872483221477e-05,
      "loss": 0.6309,
      "step": 6000
    },
    {
      "epoch": 0.2404,
      "grad_norm": 3.1027188301086426,
      "learning_rate": 4.63020134228188e-05,
      "loss": 0.6188,
      "step": 6010
    },
    {
      "epoch": 0.2408,
      "grad_norm": 2.6712095737457275,
      "learning_rate": 4.629530201342282e-05,
      "loss": 0.7564,
      "step": 6020
    },
    {
      "epoch": 0.2412,
      "grad_norm": 2.2734084129333496,
      "learning_rate": 4.628859060402685e-05,
      "loss": 0.6421,
      "step": 6030
    },
    {
      "epoch": 0.2416,
      "grad_norm": 3.163996458053589,
      "learning_rate": 4.6281879194630876e-05,
      "loss": 0.6903,
      "step": 6040
    },
    {
      "epoch": 0.242,
      "grad_norm": 3.4924256801605225,
      "learning_rate": 4.6275167785234904e-05,
      "loss": 0.7335,
      "step": 6050
    },
    {
      "epoch": 0.2424,
      "grad_norm": 2.728912830352783,
      "learning_rate": 4.626845637583893e-05,
      "loss": 0.8188,
      "step": 6060
    },
    {
      "epoch": 0.2428,
      "grad_norm": 3.4210801124572754,
      "learning_rate": 4.6261744966442954e-05,
      "loss": 0.654,
      "step": 6070
    },
    {
      "epoch": 0.2432,
      "grad_norm": 3.0886635780334473,
      "learning_rate": 4.625503355704698e-05,
      "loss": 0.6584,
      "step": 6080
    },
    {
      "epoch": 0.2436,
      "grad_norm": 2.8154492378234863,
      "learning_rate": 4.6248322147651005e-05,
      "loss": 0.6963,
      "step": 6090
    },
    {
      "epoch": 0.244,
      "grad_norm": 2.5245211124420166,
      "learning_rate": 4.624161073825504e-05,
      "loss": 0.62,
      "step": 6100
    },
    {
      "epoch": 0.2444,
      "grad_norm": 3.0163657665252686,
      "learning_rate": 4.623489932885906e-05,
      "loss": 0.561,
      "step": 6110
    },
    {
      "epoch": 0.2448,
      "grad_norm": 2.8988375663757324,
      "learning_rate": 4.622818791946309e-05,
      "loss": 0.8196,
      "step": 6120
    },
    {
      "epoch": 0.2452,
      "grad_norm": 3.3456575870513916,
      "learning_rate": 4.622147651006712e-05,
      "loss": 0.6926,
      "step": 6130
    },
    {
      "epoch": 0.2456,
      "grad_norm": 3.538771152496338,
      "learning_rate": 4.621476510067114e-05,
      "loss": 0.7607,
      "step": 6140
    },
    {
      "epoch": 0.246,
      "grad_norm": 3.52553129196167,
      "learning_rate": 4.620805369127517e-05,
      "loss": 0.702,
      "step": 6150
    },
    {
      "epoch": 0.2464,
      "grad_norm": 3.1946308612823486,
      "learning_rate": 4.62013422818792e-05,
      "loss": 0.726,
      "step": 6160
    },
    {
      "epoch": 0.2468,
      "grad_norm": 2.2313895225524902,
      "learning_rate": 4.6194630872483226e-05,
      "loss": 0.6783,
      "step": 6170
    },
    {
      "epoch": 0.2472,
      "grad_norm": 3.0542430877685547,
      "learning_rate": 4.618791946308725e-05,
      "loss": 0.7808,
      "step": 6180
    },
    {
      "epoch": 0.2476,
      "grad_norm": 3.480752944946289,
      "learning_rate": 4.618120805369128e-05,
      "loss": 0.7875,
      "step": 6190
    },
    {
      "epoch": 0.248,
      "grad_norm": 3.080986976623535,
      "learning_rate": 4.6174496644295305e-05,
      "loss": 0.6259,
      "step": 6200
    },
    {
      "epoch": 0.2484,
      "grad_norm": 2.2448928356170654,
      "learning_rate": 4.616778523489933e-05,
      "loss": 0.7001,
      "step": 6210
    },
    {
      "epoch": 0.2488,
      "grad_norm": 2.705523729324341,
      "learning_rate": 4.616107382550336e-05,
      "loss": 0.6653,
      "step": 6220
    },
    {
      "epoch": 0.2492,
      "grad_norm": 2.8899641036987305,
      "learning_rate": 4.6154362416107384e-05,
      "loss": 0.7801,
      "step": 6230
    },
    {
      "epoch": 0.2496,
      "grad_norm": 3.2302892208099365,
      "learning_rate": 4.614765100671141e-05,
      "loss": 0.7064,
      "step": 6240
    },
    {
      "epoch": 0.25,
      "grad_norm": 3.097515344619751,
      "learning_rate": 4.6140939597315434e-05,
      "loss": 0.6922,
      "step": 6250
    },
    {
      "epoch": 0.2504,
      "grad_norm": 2.0735394954681396,
      "learning_rate": 4.613422818791946e-05,
      "loss": 0.6221,
      "step": 6260
    },
    {
      "epoch": 0.2508,
      "grad_norm": 2.416699171066284,
      "learning_rate": 4.612751677852349e-05,
      "loss": 0.6141,
      "step": 6270
    },
    {
      "epoch": 0.2512,
      "grad_norm": 2.7272109985351562,
      "learning_rate": 4.612080536912752e-05,
      "loss": 0.6913,
      "step": 6280
    },
    {
      "epoch": 0.2516,
      "grad_norm": 2.8716421127319336,
      "learning_rate": 4.611409395973155e-05,
      "loss": 0.7403,
      "step": 6290
    },
    {
      "epoch": 0.252,
      "grad_norm": 3.226917266845703,
      "learning_rate": 4.610738255033557e-05,
      "loss": 0.7422,
      "step": 6300
    },
    {
      "epoch": 0.2524,
      "grad_norm": 3.3641791343688965,
      "learning_rate": 4.61006711409396e-05,
      "loss": 0.6391,
      "step": 6310
    },
    {
      "epoch": 0.2528,
      "grad_norm": 3.737044095993042,
      "learning_rate": 4.609395973154363e-05,
      "loss": 0.7552,
      "step": 6320
    },
    {
      "epoch": 0.2532,
      "grad_norm": 2.1738057136535645,
      "learning_rate": 4.608724832214765e-05,
      "loss": 0.7186,
      "step": 6330
    },
    {
      "epoch": 0.2536,
      "grad_norm": 2.937985897064209,
      "learning_rate": 4.6080536912751685e-05,
      "loss": 0.708,
      "step": 6340
    },
    {
      "epoch": 0.254,
      "grad_norm": 3.1830153465270996,
      "learning_rate": 4.6073825503355706e-05,
      "loss": 0.8711,
      "step": 6350
    },
    {
      "epoch": 0.2544,
      "grad_norm": 3.313655138015747,
      "learning_rate": 4.6067114093959735e-05,
      "loss": 0.7294,
      "step": 6360
    },
    {
      "epoch": 0.2548,
      "grad_norm": 3.1977458000183105,
      "learning_rate": 4.606040268456376e-05,
      "loss": 0.7827,
      "step": 6370
    },
    {
      "epoch": 0.2552,
      "grad_norm": 2.6477394104003906,
      "learning_rate": 4.6053691275167785e-05,
      "loss": 0.6976,
      "step": 6380
    },
    {
      "epoch": 0.2556,
      "grad_norm": 2.297224998474121,
      "learning_rate": 4.6046979865771814e-05,
      "loss": 0.6721,
      "step": 6390
    },
    {
      "epoch": 0.256,
      "grad_norm": 3.222804546356201,
      "learning_rate": 4.604026845637584e-05,
      "loss": 0.6306,
      "step": 6400
    },
    {
      "epoch": 0.2564,
      "grad_norm": 1.9098259210586548,
      "learning_rate": 4.603355704697987e-05,
      "loss": 0.729,
      "step": 6410
    },
    {
      "epoch": 0.2568,
      "grad_norm": 2.846100330352783,
      "learning_rate": 4.602684563758389e-05,
      "loss": 0.6101,
      "step": 6420
    },
    {
      "epoch": 0.2572,
      "grad_norm": 3.0379278659820557,
      "learning_rate": 4.602013422818792e-05,
      "loss": 0.5954,
      "step": 6430
    },
    {
      "epoch": 0.2576,
      "grad_norm": 5.165643692016602,
      "learning_rate": 4.601342281879194e-05,
      "loss": 0.6949,
      "step": 6440
    },
    {
      "epoch": 0.258,
      "grad_norm": 3.628221273422241,
      "learning_rate": 4.600671140939598e-05,
      "loss": 0.6211,
      "step": 6450
    },
    {
      "epoch": 0.2584,
      "grad_norm": 2.761169195175171,
      "learning_rate": 4.600000000000001e-05,
      "loss": 0.6851,
      "step": 6460
    },
    {
      "epoch": 0.2588,
      "grad_norm": 3.1537506580352783,
      "learning_rate": 4.599328859060403e-05,
      "loss": 0.6922,
      "step": 6470
    },
    {
      "epoch": 0.2592,
      "grad_norm": 2.1790077686309814,
      "learning_rate": 4.598657718120806e-05,
      "loss": 0.6156,
      "step": 6480
    },
    {
      "epoch": 0.2596,
      "grad_norm": 3.1280343532562256,
      "learning_rate": 4.597986577181208e-05,
      "loss": 0.5654,
      "step": 6490
    },
    {
      "epoch": 0.26,
      "grad_norm": 2.909027099609375,
      "learning_rate": 4.597315436241611e-05,
      "loss": 0.7033,
      "step": 6500
    },
    {
      "epoch": 0.2604,
      "grad_norm": 2.3974530696868896,
      "learning_rate": 4.5966442953020136e-05,
      "loss": 0.5686,
      "step": 6510
    },
    {
      "epoch": 0.2608,
      "grad_norm": 1.587440848350525,
      "learning_rate": 4.5959731543624165e-05,
      "loss": 0.7039,
      "step": 6520
    },
    {
      "epoch": 0.2612,
      "grad_norm": 3.2597873210906982,
      "learning_rate": 4.595302013422819e-05,
      "loss": 0.6306,
      "step": 6530
    },
    {
      "epoch": 0.2616,
      "grad_norm": 2.6462740898132324,
      "learning_rate": 4.5946308724832215e-05,
      "loss": 0.5637,
      "step": 6540
    },
    {
      "epoch": 0.262,
      "grad_norm": 4.30956506729126,
      "learning_rate": 4.5939597315436243e-05,
      "loss": 0.7006,
      "step": 6550
    },
    {
      "epoch": 0.2624,
      "grad_norm": 3.0732321739196777,
      "learning_rate": 4.5932885906040265e-05,
      "loss": 0.6755,
      "step": 6560
    },
    {
      "epoch": 0.2628,
      "grad_norm": 2.2806222438812256,
      "learning_rate": 4.59261744966443e-05,
      "loss": 0.6027,
      "step": 6570
    },
    {
      "epoch": 0.2632,
      "grad_norm": 3.683293104171753,
      "learning_rate": 4.591946308724833e-05,
      "loss": 0.7183,
      "step": 6580
    },
    {
      "epoch": 0.2636,
      "grad_norm": 3.2982654571533203,
      "learning_rate": 4.591275167785235e-05,
      "loss": 0.7046,
      "step": 6590
    },
    {
      "epoch": 0.264,
      "grad_norm": 2.6568548679351807,
      "learning_rate": 4.590604026845638e-05,
      "loss": 0.6116,
      "step": 6600
    },
    {
      "epoch": 0.2644,
      "grad_norm": 2.812986135482788,
      "learning_rate": 4.58993288590604e-05,
      "loss": 0.6675,
      "step": 6610
    },
    {
      "epoch": 0.2648,
      "grad_norm": 3.5512869358062744,
      "learning_rate": 4.589261744966443e-05,
      "loss": 0.6614,
      "step": 6620
    },
    {
      "epoch": 0.2652,
      "grad_norm": 2.5096471309661865,
      "learning_rate": 4.588590604026846e-05,
      "loss": 0.6891,
      "step": 6630
    },
    {
      "epoch": 0.2656,
      "grad_norm": 2.503103017807007,
      "learning_rate": 4.587919463087249e-05,
      "loss": 0.6625,
      "step": 6640
    },
    {
      "epoch": 0.266,
      "grad_norm": 2.561244487762451,
      "learning_rate": 4.5872483221476515e-05,
      "loss": 0.5868,
      "step": 6650
    },
    {
      "epoch": 0.2664,
      "grad_norm": 2.2931113243103027,
      "learning_rate": 4.586577181208054e-05,
      "loss": 0.6541,
      "step": 6660
    },
    {
      "epoch": 0.2668,
      "grad_norm": 3.5052456855773926,
      "learning_rate": 4.5859060402684566e-05,
      "loss": 0.6566,
      "step": 6670
    },
    {
      "epoch": 0.2672,
      "grad_norm": 2.8039307594299316,
      "learning_rate": 4.585234899328859e-05,
      "loss": 0.8009,
      "step": 6680
    },
    {
      "epoch": 0.2676,
      "grad_norm": 3.447399377822876,
      "learning_rate": 4.584563758389262e-05,
      "loss": 0.6279,
      "step": 6690
    },
    {
      "epoch": 0.268,
      "grad_norm": 2.723745346069336,
      "learning_rate": 4.583892617449665e-05,
      "loss": 0.5906,
      "step": 6700
    },
    {
      "epoch": 0.2684,
      "grad_norm": 2.6535162925720215,
      "learning_rate": 4.583221476510067e-05,
      "loss": 0.6777,
      "step": 6710
    },
    {
      "epoch": 0.2688,
      "grad_norm": 3.4900436401367188,
      "learning_rate": 4.58255033557047e-05,
      "loss": 0.7214,
      "step": 6720
    },
    {
      "epoch": 0.2692,
      "grad_norm": 3.775170087814331,
      "learning_rate": 4.581879194630872e-05,
      "loss": 0.6797,
      "step": 6730
    },
    {
      "epoch": 0.2696,
      "grad_norm": 2.7632644176483154,
      "learning_rate": 4.581208053691275e-05,
      "loss": 0.6559,
      "step": 6740
    },
    {
      "epoch": 0.27,
      "grad_norm": 2.2780025005340576,
      "learning_rate": 4.580536912751678e-05,
      "loss": 0.6899,
      "step": 6750
    },
    {
      "epoch": 0.2704,
      "grad_norm": 3.174551248550415,
      "learning_rate": 4.579865771812081e-05,
      "loss": 0.7638,
      "step": 6760
    },
    {
      "epoch": 0.2708,
      "grad_norm": 2.8612523078918457,
      "learning_rate": 4.579194630872484e-05,
      "loss": 0.6734,
      "step": 6770
    },
    {
      "epoch": 0.2712,
      "grad_norm": 3.7248616218566895,
      "learning_rate": 4.578523489932886e-05,
      "loss": 0.7101,
      "step": 6780
    },
    {
      "epoch": 0.2716,
      "grad_norm": 2.2831051349639893,
      "learning_rate": 4.577852348993289e-05,
      "loss": 0.6489,
      "step": 6790
    },
    {
      "epoch": 0.272,
      "grad_norm": 2.3469669818878174,
      "learning_rate": 4.5771812080536916e-05,
      "loss": 0.6399,
      "step": 6800
    },
    {
      "epoch": 0.2724,
      "grad_norm": 3.5253658294677734,
      "learning_rate": 4.5765100671140945e-05,
      "loss": 0.8217,
      "step": 6810
    },
    {
      "epoch": 0.2728,
      "grad_norm": 2.2952120304107666,
      "learning_rate": 4.575838926174497e-05,
      "loss": 0.6554,
      "step": 6820
    },
    {
      "epoch": 0.2732,
      "grad_norm": 2.2001473903656006,
      "learning_rate": 4.5751677852348995e-05,
      "loss": 0.7135,
      "step": 6830
    },
    {
      "epoch": 0.2736,
      "grad_norm": 2.1548261642456055,
      "learning_rate": 4.5744966442953024e-05,
      "loss": 0.6961,
      "step": 6840
    },
    {
      "epoch": 0.274,
      "grad_norm": 3.1912639141082764,
      "learning_rate": 4.5738255033557046e-05,
      "loss": 0.6677,
      "step": 6850
    },
    {
      "epoch": 0.2744,
      "grad_norm": 2.359264612197876,
      "learning_rate": 4.5731543624161074e-05,
      "loss": 0.7195,
      "step": 6860
    },
    {
      "epoch": 0.2748,
      "grad_norm": 3.0149636268615723,
      "learning_rate": 4.57248322147651e-05,
      "loss": 0.7276,
      "step": 6870
    },
    {
      "epoch": 0.2752,
      "grad_norm": 2.8773443698883057,
      "learning_rate": 4.571812080536913e-05,
      "loss": 0.6154,
      "step": 6880
    },
    {
      "epoch": 0.2756,
      "grad_norm": 2.247509717941284,
      "learning_rate": 4.571140939597316e-05,
      "loss": 0.656,
      "step": 6890
    },
    {
      "epoch": 0.276,
      "grad_norm": 3.6980836391448975,
      "learning_rate": 4.570469798657718e-05,
      "loss": 0.716,
      "step": 6900
    },
    {
      "epoch": 0.2764,
      "grad_norm": 2.5069172382354736,
      "learning_rate": 4.569798657718121e-05,
      "loss": 0.6631,
      "step": 6910
    },
    {
      "epoch": 0.2768,
      "grad_norm": 2.5650076866149902,
      "learning_rate": 4.569127516778524e-05,
      "loss": 0.6874,
      "step": 6920
    },
    {
      "epoch": 0.2772,
      "grad_norm": 3.0658438205718994,
      "learning_rate": 4.568456375838927e-05,
      "loss": 0.7526,
      "step": 6930
    },
    {
      "epoch": 0.2776,
      "grad_norm": 2.8614230155944824,
      "learning_rate": 4.567785234899329e-05,
      "loss": 0.7039,
      "step": 6940
    },
    {
      "epoch": 0.278,
      "grad_norm": 1.9913580417633057,
      "learning_rate": 4.567114093959732e-05,
      "loss": 0.6373,
      "step": 6950
    },
    {
      "epoch": 0.2784,
      "grad_norm": 2.6429507732391357,
      "learning_rate": 4.5664429530201346e-05,
      "loss": 0.7839,
      "step": 6960
    },
    {
      "epoch": 0.2788,
      "grad_norm": 2.709942579269409,
      "learning_rate": 4.565771812080537e-05,
      "loss": 0.6535,
      "step": 6970
    },
    {
      "epoch": 0.2792,
      "grad_norm": 2.2127315998077393,
      "learning_rate": 4.5651006711409396e-05,
      "loss": 0.6537,
      "step": 6980
    },
    {
      "epoch": 0.2796,
      "grad_norm": 2.264587879180908,
      "learning_rate": 4.5644295302013425e-05,
      "loss": 0.7383,
      "step": 6990
    },
    {
      "epoch": 0.28,
      "grad_norm": 3.133470296859741,
      "learning_rate": 4.5637583892617453e-05,
      "loss": 0.7121,
      "step": 7000
    },
    {
      "epoch": 0.2804,
      "grad_norm": 2.7930078506469727,
      "learning_rate": 4.5630872483221475e-05,
      "loss": 0.6476,
      "step": 7010
    },
    {
      "epoch": 0.2808,
      "grad_norm": 6.277763843536377,
      "learning_rate": 4.5624161073825504e-05,
      "loss": 0.7104,
      "step": 7020
    },
    {
      "epoch": 0.2812,
      "grad_norm": 2.3241348266601562,
      "learning_rate": 4.561744966442953e-05,
      "loss": 0.5862,
      "step": 7030
    },
    {
      "epoch": 0.2816,
      "grad_norm": 2.341637134552002,
      "learning_rate": 4.561073825503356e-05,
      "loss": 0.6814,
      "step": 7040
    },
    {
      "epoch": 0.282,
      "grad_norm": 3.159846544265747,
      "learning_rate": 4.560402684563759e-05,
      "loss": 0.7725,
      "step": 7050
    },
    {
      "epoch": 0.2824,
      "grad_norm": 3.398561716079712,
      "learning_rate": 4.559731543624161e-05,
      "loss": 0.7061,
      "step": 7060
    },
    {
      "epoch": 0.2828,
      "grad_norm": 2.3465728759765625,
      "learning_rate": 4.559060402684564e-05,
      "loss": 0.5957,
      "step": 7070
    },
    {
      "epoch": 0.2832,
      "grad_norm": 2.32661771774292,
      "learning_rate": 4.558389261744966e-05,
      "loss": 0.5845,
      "step": 7080
    },
    {
      "epoch": 0.2836,
      "grad_norm": 2.499436855316162,
      "learning_rate": 4.557718120805369e-05,
      "loss": 0.6495,
      "step": 7090
    },
    {
      "epoch": 0.284,
      "grad_norm": 2.7202670574188232,
      "learning_rate": 4.5570469798657725e-05,
      "loss": 0.7129,
      "step": 7100
    },
    {
      "epoch": 0.2844,
      "grad_norm": 3.181030035018921,
      "learning_rate": 4.556375838926175e-05,
      "loss": 0.6747,
      "step": 7110
    },
    {
      "epoch": 0.2848,
      "grad_norm": 2.759481191635132,
      "learning_rate": 4.5557046979865776e-05,
      "loss": 0.6529,
      "step": 7120
    },
    {
      "epoch": 0.2852,
      "grad_norm": 3.0978944301605225,
      "learning_rate": 4.55503355704698e-05,
      "loss": 0.6248,
      "step": 7130
    },
    {
      "epoch": 0.2856,
      "grad_norm": 2.8026723861694336,
      "learning_rate": 4.5543624161073826e-05,
      "loss": 0.7416,
      "step": 7140
    },
    {
      "epoch": 0.286,
      "grad_norm": 2.37890625,
      "learning_rate": 4.5536912751677855e-05,
      "loss": 0.7061,
      "step": 7150
    },
    {
      "epoch": 0.2864,
      "grad_norm": 2.2874882221221924,
      "learning_rate": 4.553020134228188e-05,
      "loss": 0.6027,
      "step": 7160
    },
    {
      "epoch": 0.2868,
      "grad_norm": 2.365546226501465,
      "learning_rate": 4.552348993288591e-05,
      "loss": 0.7153,
      "step": 7170
    },
    {
      "epoch": 0.2872,
      "grad_norm": 2.7548909187316895,
      "learning_rate": 4.5516778523489933e-05,
      "loss": 0.584,
      "step": 7180
    },
    {
      "epoch": 0.2876,
      "grad_norm": 2.7867372035980225,
      "learning_rate": 4.551006711409396e-05,
      "loss": 0.8006,
      "step": 7190
    },
    {
      "epoch": 0.288,
      "grad_norm": 2.6813454627990723,
      "learning_rate": 4.5503355704697984e-05,
      "loss": 0.6834,
      "step": 7200
    },
    {
      "epoch": 0.2884,
      "grad_norm": 2.5008881092071533,
      "learning_rate": 4.549664429530201e-05,
      "loss": 0.5785,
      "step": 7210
    },
    {
      "epoch": 0.2888,
      "grad_norm": 2.214496374130249,
      "learning_rate": 4.548993288590605e-05,
      "loss": 0.7657,
      "step": 7220
    },
    {
      "epoch": 0.2892,
      "grad_norm": 2.97524356842041,
      "learning_rate": 4.548322147651007e-05,
      "loss": 0.6599,
      "step": 7230
    },
    {
      "epoch": 0.2896,
      "grad_norm": 3.0333139896392822,
      "learning_rate": 4.54765100671141e-05,
      "loss": 0.7521,
      "step": 7240
    },
    {
      "epoch": 0.29,
      "grad_norm": 2.7214369773864746,
      "learning_rate": 4.546979865771812e-05,
      "loss": 0.6825,
      "step": 7250
    },
    {
      "epoch": 0.2904,
      "grad_norm": 3.124675989151001,
      "learning_rate": 4.546308724832215e-05,
      "loss": 0.6005,
      "step": 7260
    },
    {
      "epoch": 0.2908,
      "grad_norm": 4.132475852966309,
      "learning_rate": 4.545637583892618e-05,
      "loss": 0.6835,
      "step": 7270
    },
    {
      "epoch": 0.2912,
      "grad_norm": 3.3208861351013184,
      "learning_rate": 4.5449664429530205e-05,
      "loss": 0.7619,
      "step": 7280
    },
    {
      "epoch": 0.2916,
      "grad_norm": 2.567172050476074,
      "learning_rate": 4.5442953020134234e-05,
      "loss": 0.6325,
      "step": 7290
    },
    {
      "epoch": 0.292,
      "grad_norm": 2.890758514404297,
      "learning_rate": 4.5436241610738256e-05,
      "loss": 0.7029,
      "step": 7300
    },
    {
      "epoch": 0.2924,
      "grad_norm": 3.092758893966675,
      "learning_rate": 4.5429530201342284e-05,
      "loss": 0.6955,
      "step": 7310
    },
    {
      "epoch": 0.2928,
      "grad_norm": 2.3495965003967285,
      "learning_rate": 4.5422818791946306e-05,
      "loss": 0.7839,
      "step": 7320
    },
    {
      "epoch": 0.2932,
      "grad_norm": 2.941087245941162,
      "learning_rate": 4.541610738255034e-05,
      "loss": 0.6493,
      "step": 7330
    },
    {
      "epoch": 0.2936,
      "grad_norm": 3.3056302070617676,
      "learning_rate": 4.540939597315437e-05,
      "loss": 0.7037,
      "step": 7340
    },
    {
      "epoch": 0.294,
      "grad_norm": 2.9557089805603027,
      "learning_rate": 4.540268456375839e-05,
      "loss": 0.7567,
      "step": 7350
    },
    {
      "epoch": 0.2944,
      "grad_norm": 2.9357759952545166,
      "learning_rate": 4.539597315436242e-05,
      "loss": 0.7246,
      "step": 7360
    },
    {
      "epoch": 0.2948,
      "grad_norm": 3.1953697204589844,
      "learning_rate": 4.538926174496644e-05,
      "loss": 0.7237,
      "step": 7370
    },
    {
      "epoch": 0.2952,
      "grad_norm": 3.2418391704559326,
      "learning_rate": 4.538255033557047e-05,
      "loss": 0.5794,
      "step": 7380
    },
    {
      "epoch": 0.2956,
      "grad_norm": 2.6697444915771484,
      "learning_rate": 4.53758389261745e-05,
      "loss": 0.7122,
      "step": 7390
    },
    {
      "epoch": 0.296,
      "grad_norm": 3.199969530105591,
      "learning_rate": 4.536912751677853e-05,
      "loss": 0.7664,
      "step": 7400
    },
    {
      "epoch": 0.2964,
      "grad_norm": 2.3721635341644287,
      "learning_rate": 4.5362416107382556e-05,
      "loss": 0.6243,
      "step": 7410
    },
    {
      "epoch": 0.2968,
      "grad_norm": 2.7135777473449707,
      "learning_rate": 4.535570469798658e-05,
      "loss": 0.6053,
      "step": 7420
    },
    {
      "epoch": 0.2972,
      "grad_norm": 2.816427230834961,
      "learning_rate": 4.5348993288590606e-05,
      "loss": 0.6775,
      "step": 7430
    },
    {
      "epoch": 0.2976,
      "grad_norm": 2.775198221206665,
      "learning_rate": 4.534228187919463e-05,
      "loss": 0.5935,
      "step": 7440
    },
    {
      "epoch": 0.298,
      "grad_norm": 2.903493881225586,
      "learning_rate": 4.5335570469798664e-05,
      "loss": 0.6542,
      "step": 7450
    },
    {
      "epoch": 0.2984,
      "grad_norm": 3.502218723297119,
      "learning_rate": 4.5328859060402685e-05,
      "loss": 0.7697,
      "step": 7460
    },
    {
      "epoch": 0.2988,
      "grad_norm": 2.9663028717041016,
      "learning_rate": 4.5322147651006714e-05,
      "loss": 0.742,
      "step": 7470
    },
    {
      "epoch": 0.2992,
      "grad_norm": 2.476203680038452,
      "learning_rate": 4.531543624161074e-05,
      "loss": 0.6055,
      "step": 7480
    },
    {
      "epoch": 0.2996,
      "grad_norm": 2.8153998851776123,
      "learning_rate": 4.5308724832214764e-05,
      "loss": 0.6731,
      "step": 7490
    },
    {
      "epoch": 0.3,
      "grad_norm": 3.2229936122894287,
      "learning_rate": 4.530201342281879e-05,
      "loss": 0.666,
      "step": 7500
    },
    {
      "epoch": 0.3004,
      "grad_norm": 2.4590845108032227,
      "learning_rate": 4.529530201342282e-05,
      "loss": 0.6791,
      "step": 7510
    },
    {
      "epoch": 0.3008,
      "grad_norm": 1.9108203649520874,
      "learning_rate": 4.528859060402685e-05,
      "loss": 0.6578,
      "step": 7520
    },
    {
      "epoch": 0.3012,
      "grad_norm": 3.1331324577331543,
      "learning_rate": 4.528187919463088e-05,
      "loss": 0.7519,
      "step": 7530
    },
    {
      "epoch": 0.3016,
      "grad_norm": 2.9324655532836914,
      "learning_rate": 4.52751677852349e-05,
      "loss": 0.7324,
      "step": 7540
    },
    {
      "epoch": 0.302,
      "grad_norm": 2.876974582672119,
      "learning_rate": 4.526845637583893e-05,
      "loss": 0.6552,
      "step": 7550
    },
    {
      "epoch": 0.3024,
      "grad_norm": 2.8224010467529297,
      "learning_rate": 4.526174496644295e-05,
      "loss": 0.5868,
      "step": 7560
    },
    {
      "epoch": 0.3028,
      "grad_norm": 3.6197173595428467,
      "learning_rate": 4.5255033557046986e-05,
      "loss": 0.6764,
      "step": 7570
    },
    {
      "epoch": 0.3032,
      "grad_norm": 3.6130549907684326,
      "learning_rate": 4.524832214765101e-05,
      "loss": 0.6782,
      "step": 7580
    },
    {
      "epoch": 0.3036,
      "grad_norm": 3.5059866905212402,
      "learning_rate": 4.5241610738255036e-05,
      "loss": 0.6115,
      "step": 7590
    },
    {
      "epoch": 0.304,
      "grad_norm": 3.2236480712890625,
      "learning_rate": 4.5234899328859065e-05,
      "loss": 0.6655,
      "step": 7600
    },
    {
      "epoch": 0.3044,
      "grad_norm": 2.6583902835845947,
      "learning_rate": 4.5228187919463086e-05,
      "loss": 0.7352,
      "step": 7610
    },
    {
      "epoch": 0.3048,
      "grad_norm": 2.389186143875122,
      "learning_rate": 4.5221476510067115e-05,
      "loss": 0.6004,
      "step": 7620
    },
    {
      "epoch": 0.3052,
      "grad_norm": 2.955357551574707,
      "learning_rate": 4.5214765100671144e-05,
      "loss": 0.6955,
      "step": 7630
    },
    {
      "epoch": 0.3056,
      "grad_norm": 3.0746891498565674,
      "learning_rate": 4.520805369127517e-05,
      "loss": 0.7356,
      "step": 7640
    },
    {
      "epoch": 0.306,
      "grad_norm": 2.9929020404815674,
      "learning_rate": 4.5201342281879194e-05,
      "loss": 0.6091,
      "step": 7650
    },
    {
      "epoch": 0.3064,
      "grad_norm": 2.6721222400665283,
      "learning_rate": 4.519463087248322e-05,
      "loss": 0.6742,
      "step": 7660
    },
    {
      "epoch": 0.3068,
      "grad_norm": 2.3655519485473633,
      "learning_rate": 4.518791946308725e-05,
      "loss": 0.6182,
      "step": 7670
    },
    {
      "epoch": 0.3072,
      "grad_norm": 2.8443503379821777,
      "learning_rate": 4.518120805369128e-05,
      "loss": 0.6782,
      "step": 7680
    },
    {
      "epoch": 0.3076,
      "grad_norm": 2.7897119522094727,
      "learning_rate": 4.517449664429531e-05,
      "loss": 0.6876,
      "step": 7690
    },
    {
      "epoch": 0.308,
      "grad_norm": 3.4779863357543945,
      "learning_rate": 4.516778523489933e-05,
      "loss": 0.7248,
      "step": 7700
    },
    {
      "epoch": 0.3084,
      "grad_norm": 3.368823766708374,
      "learning_rate": 4.516107382550336e-05,
      "loss": 0.7393,
      "step": 7710
    },
    {
      "epoch": 0.3088,
      "grad_norm": 3.222318172454834,
      "learning_rate": 4.515436241610739e-05,
      "loss": 0.6745,
      "step": 7720
    },
    {
      "epoch": 0.3092,
      "grad_norm": 2.572038412094116,
      "learning_rate": 4.514765100671141e-05,
      "loss": 0.6192,
      "step": 7730
    },
    {
      "epoch": 0.3096,
      "grad_norm": 3.3785438537597656,
      "learning_rate": 4.514093959731544e-05,
      "loss": 0.6025,
      "step": 7740
    },
    {
      "epoch": 0.31,
      "grad_norm": 2.9366469383239746,
      "learning_rate": 4.5134228187919466e-05,
      "loss": 0.6884,
      "step": 7750
    },
    {
      "epoch": 0.3104,
      "grad_norm": 2.9851889610290527,
      "learning_rate": 4.5127516778523494e-05,
      "loss": 0.7351,
      "step": 7760
    },
    {
      "epoch": 0.3108,
      "grad_norm": 2.321772813796997,
      "learning_rate": 4.5120805369127516e-05,
      "loss": 0.6181,
      "step": 7770
    },
    {
      "epoch": 0.3112,
      "grad_norm": 2.4588799476623535,
      "learning_rate": 4.5114093959731545e-05,
      "loss": 0.739,
      "step": 7780
    },
    {
      "epoch": 0.3116,
      "grad_norm": 2.1027109622955322,
      "learning_rate": 4.510738255033557e-05,
      "loss": 0.6334,
      "step": 7790
    },
    {
      "epoch": 0.312,
      "grad_norm": 3.1179113388061523,
      "learning_rate": 4.51006711409396e-05,
      "loss": 0.6834,
      "step": 7800
    },
    {
      "epoch": 0.3124,
      "grad_norm": 3.189406394958496,
      "learning_rate": 4.509395973154363e-05,
      "loss": 0.6215,
      "step": 7810
    },
    {
      "epoch": 0.3128,
      "grad_norm": 3.2503583431243896,
      "learning_rate": 4.508724832214765e-05,
      "loss": 0.7234,
      "step": 7820
    },
    {
      "epoch": 0.3132,
      "grad_norm": 2.475177049636841,
      "learning_rate": 4.508053691275168e-05,
      "loss": 0.5839,
      "step": 7830
    },
    {
      "epoch": 0.3136,
      "grad_norm": 3.0908050537109375,
      "learning_rate": 4.50738255033557e-05,
      "loss": 0.6886,
      "step": 7840
    },
    {
      "epoch": 0.314,
      "grad_norm": 2.8746449947357178,
      "learning_rate": 4.506711409395973e-05,
      "loss": 0.6367,
      "step": 7850
    },
    {
      "epoch": 0.3144,
      "grad_norm": 3.273653268814087,
      "learning_rate": 4.506040268456376e-05,
      "loss": 0.6753,
      "step": 7860
    },
    {
      "epoch": 0.3148,
      "grad_norm": 2.3087377548217773,
      "learning_rate": 4.505369127516779e-05,
      "loss": 0.6848,
      "step": 7870
    },
    {
      "epoch": 0.3152,
      "grad_norm": 3.0625061988830566,
      "learning_rate": 4.5046979865771817e-05,
      "loss": 0.768,
      "step": 7880
    },
    {
      "epoch": 0.3156,
      "grad_norm": 2.808004856109619,
      "learning_rate": 4.504026845637584e-05,
      "loss": 0.6488,
      "step": 7890
    },
    {
      "epoch": 0.316,
      "grad_norm": 3.2218410968780518,
      "learning_rate": 4.503355704697987e-05,
      "loss": 0.7089,
      "step": 7900
    },
    {
      "epoch": 0.3164,
      "grad_norm": 4.297688007354736,
      "learning_rate": 4.5026845637583895e-05,
      "loss": 0.672,
      "step": 7910
    },
    {
      "epoch": 0.3168,
      "grad_norm": 3.174773693084717,
      "learning_rate": 4.5020134228187924e-05,
      "loss": 0.6249,
      "step": 7920
    },
    {
      "epoch": 0.3172,
      "grad_norm": 2.99768328666687,
      "learning_rate": 4.501342281879195e-05,
      "loss": 0.6488,
      "step": 7930
    },
    {
      "epoch": 0.3176,
      "grad_norm": 2.7974278926849365,
      "learning_rate": 4.5006711409395974e-05,
      "loss": 0.701,
      "step": 7940
    },
    {
      "epoch": 0.318,
      "grad_norm": 2.4046521186828613,
      "learning_rate": 4.5e-05,
      "loss": 0.6344,
      "step": 7950
    },
    {
      "epoch": 0.3184,
      "grad_norm": 3.137418508529663,
      "learning_rate": 4.4993288590604025e-05,
      "loss": 0.6415,
      "step": 7960
    },
    {
      "epoch": 0.3188,
      "grad_norm": 2.529649496078491,
      "learning_rate": 4.498657718120805e-05,
      "loss": 0.6291,
      "step": 7970
    },
    {
      "epoch": 0.3192,
      "grad_norm": 2.9405674934387207,
      "learning_rate": 4.497986577181209e-05,
      "loss": 0.6433,
      "step": 7980
    },
    {
      "epoch": 0.3196,
      "grad_norm": 2.2676215171813965,
      "learning_rate": 4.497315436241611e-05,
      "loss": 0.6045,
      "step": 7990
    },
    {
      "epoch": 0.32,
      "grad_norm": 3.2631349563598633,
      "learning_rate": 4.496644295302014e-05,
      "loss": 0.7187,
      "step": 8000
    },
    {
      "epoch": 0.3204,
      "grad_norm": 2.692157030105591,
      "learning_rate": 4.495973154362416e-05,
      "loss": 0.6513,
      "step": 8010
    },
    {
      "epoch": 0.3208,
      "grad_norm": 2.966430425643921,
      "learning_rate": 4.495302013422819e-05,
      "loss": 0.6544,
      "step": 8020
    },
    {
      "epoch": 0.3212,
      "grad_norm": 2.5784075260162354,
      "learning_rate": 4.494630872483222e-05,
      "loss": 0.6915,
      "step": 8030
    },
    {
      "epoch": 0.3216,
      "grad_norm": 2.5750374794006348,
      "learning_rate": 4.4939597315436246e-05,
      "loss": 0.6315,
      "step": 8040
    },
    {
      "epoch": 0.322,
      "grad_norm": 2.51029634475708,
      "learning_rate": 4.4932885906040275e-05,
      "loss": 0.6362,
      "step": 8050
    },
    {
      "epoch": 0.3224,
      "grad_norm": 3.0841808319091797,
      "learning_rate": 4.4926174496644297e-05,
      "loss": 0.7355,
      "step": 8060
    },
    {
      "epoch": 0.3228,
      "grad_norm": 3.6533124446868896,
      "learning_rate": 4.4919463087248325e-05,
      "loss": 0.7648,
      "step": 8070
    },
    {
      "epoch": 0.3232,
      "grad_norm": 3.4087984561920166,
      "learning_rate": 4.491275167785235e-05,
      "loss": 0.6045,
      "step": 8080
    },
    {
      "epoch": 0.3236,
      "grad_norm": 3.491645574569702,
      "learning_rate": 4.4906040268456375e-05,
      "loss": 0.6393,
      "step": 8090
    },
    {
      "epoch": 0.324,
      "grad_norm": 3.388828992843628,
      "learning_rate": 4.4899328859060404e-05,
      "loss": 0.691,
      "step": 8100
    },
    {
      "epoch": 0.3244,
      "grad_norm": 2.876035451889038,
      "learning_rate": 4.489261744966443e-05,
      "loss": 0.6338,
      "step": 8110
    },
    {
      "epoch": 0.3248,
      "grad_norm": 2.3220200538635254,
      "learning_rate": 4.488590604026846e-05,
      "loss": 0.7279,
      "step": 8120
    },
    {
      "epoch": 0.3252,
      "grad_norm": 3.304948091506958,
      "learning_rate": 4.487919463087248e-05,
      "loss": 0.6679,
      "step": 8130
    },
    {
      "epoch": 0.3256,
      "grad_norm": 2.6760315895080566,
      "learning_rate": 4.487248322147651e-05,
      "loss": 0.6569,
      "step": 8140
    },
    {
      "epoch": 0.326,
      "grad_norm": 2.767096757888794,
      "learning_rate": 4.486577181208054e-05,
      "loss": 0.6297,
      "step": 8150
    },
    {
      "epoch": 0.3264,
      "grad_norm": 2.774021863937378,
      "learning_rate": 4.485906040268457e-05,
      "loss": 0.6623,
      "step": 8160
    },
    {
      "epoch": 0.3268,
      "grad_norm": 2.0287106037139893,
      "learning_rate": 4.48523489932886e-05,
      "loss": 0.592,
      "step": 8170
    },
    {
      "epoch": 0.3272,
      "grad_norm": 3.152672052383423,
      "learning_rate": 4.484563758389262e-05,
      "loss": 0.6406,
      "step": 8180
    },
    {
      "epoch": 0.3276,
      "grad_norm": 3.2197701930999756,
      "learning_rate": 4.483892617449665e-05,
      "loss": 0.5958,
      "step": 8190
    },
    {
      "epoch": 0.328,
      "grad_norm": 2.3923215866088867,
      "learning_rate": 4.483221476510067e-05,
      "loss": 0.5925,
      "step": 8200
    },
    {
      "epoch": 0.3284,
      "grad_norm": 1.8721866607666016,
      "learning_rate": 4.4825503355704704e-05,
      "loss": 0.7115,
      "step": 8210
    },
    {
      "epoch": 0.3288,
      "grad_norm": 2.526820421218872,
      "learning_rate": 4.4818791946308726e-05,
      "loss": 0.614,
      "step": 8220
    },
    {
      "epoch": 0.3292,
      "grad_norm": 2.6230905055999756,
      "learning_rate": 4.4812080536912755e-05,
      "loss": 0.756,
      "step": 8230
    },
    {
      "epoch": 0.3296,
      "grad_norm": 2.6397387981414795,
      "learning_rate": 4.480536912751678e-05,
      "loss": 0.6892,
      "step": 8240
    },
    {
      "epoch": 0.33,
      "grad_norm": 3.566037893295288,
      "learning_rate": 4.4798657718120805e-05,
      "loss": 0.701,
      "step": 8250
    },
    {
      "epoch": 0.3304,
      "grad_norm": 3.826324939727783,
      "learning_rate": 4.4791946308724834e-05,
      "loss": 0.7526,
      "step": 8260
    },
    {
      "epoch": 0.3308,
      "grad_norm": 3.1601638793945312,
      "learning_rate": 4.478523489932886e-05,
      "loss": 0.6366,
      "step": 8270
    },
    {
      "epoch": 0.3312,
      "grad_norm": 2.8752105236053467,
      "learning_rate": 4.477852348993289e-05,
      "loss": 0.6511,
      "step": 8280
    },
    {
      "epoch": 0.3316,
      "grad_norm": 2.965930223464966,
      "learning_rate": 4.477181208053691e-05,
      "loss": 0.6774,
      "step": 8290
    },
    {
      "epoch": 0.332,
      "grad_norm": 3.3468434810638428,
      "learning_rate": 4.476510067114094e-05,
      "loss": 0.745,
      "step": 8300
    },
    {
      "epoch": 0.3324,
      "grad_norm": 1.955143928527832,
      "learning_rate": 4.475838926174497e-05,
      "loss": 0.6114,
      "step": 8310
    },
    {
      "epoch": 0.3328,
      "grad_norm": 2.414475917816162,
      "learning_rate": 4.475167785234899e-05,
      "loss": 0.5754,
      "step": 8320
    },
    {
      "epoch": 0.3332,
      "grad_norm": 2.6734671592712402,
      "learning_rate": 4.474496644295303e-05,
      "loss": 0.6161,
      "step": 8330
    },
    {
      "epoch": 0.3336,
      "grad_norm": 3.6816444396972656,
      "learning_rate": 4.473825503355705e-05,
      "loss": 0.7294,
      "step": 8340
    },
    {
      "epoch": 0.334,
      "grad_norm": 2.6015663146972656,
      "learning_rate": 4.473154362416108e-05,
      "loss": 0.6614,
      "step": 8350
    },
    {
      "epoch": 0.3344,
      "grad_norm": 3.030212163925171,
      "learning_rate": 4.4724832214765106e-05,
      "loss": 0.6372,
      "step": 8360
    },
    {
      "epoch": 0.3348,
      "grad_norm": 2.3249709606170654,
      "learning_rate": 4.471812080536913e-05,
      "loss": 0.6324,
      "step": 8370
    },
    {
      "epoch": 0.3352,
      "grad_norm": 3.730163335800171,
      "learning_rate": 4.4711409395973156e-05,
      "loss": 0.5877,
      "step": 8380
    },
    {
      "epoch": 0.3356,
      "grad_norm": 3.269788980484009,
      "learning_rate": 4.4704697986577184e-05,
      "loss": 0.6977,
      "step": 8390
    },
    {
      "epoch": 0.336,
      "grad_norm": 2.9653632640838623,
      "learning_rate": 4.469798657718121e-05,
      "loss": 0.7272,
      "step": 8400
    },
    {
      "epoch": 0.3364,
      "grad_norm": 2.415351390838623,
      "learning_rate": 4.4691275167785235e-05,
      "loss": 0.6909,
      "step": 8410
    },
    {
      "epoch": 0.3368,
      "grad_norm": 3.7607815265655518,
      "learning_rate": 4.468456375838926e-05,
      "loss": 0.6864,
      "step": 8420
    },
    {
      "epoch": 0.3372,
      "grad_norm": 3.5084731578826904,
      "learning_rate": 4.467785234899329e-05,
      "loss": 0.7072,
      "step": 8430
    },
    {
      "epoch": 0.3376,
      "grad_norm": 2.750438928604126,
      "learning_rate": 4.4671140939597314e-05,
      "loss": 0.7192,
      "step": 8440
    },
    {
      "epoch": 0.338,
      "grad_norm": 2.2175710201263428,
      "learning_rate": 4.466442953020135e-05,
      "loss": 0.6111,
      "step": 8450
    },
    {
      "epoch": 0.3384,
      "grad_norm": 3.9092743396759033,
      "learning_rate": 4.465771812080537e-05,
      "loss": 0.7306,
      "step": 8460
    },
    {
      "epoch": 0.3388,
      "grad_norm": 2.4453213214874268,
      "learning_rate": 4.46510067114094e-05,
      "loss": 0.6649,
      "step": 8470
    },
    {
      "epoch": 0.3392,
      "grad_norm": 3.177124500274658,
      "learning_rate": 4.464429530201342e-05,
      "loss": 0.6484,
      "step": 8480
    },
    {
      "epoch": 0.3396,
      "grad_norm": 3.602224111557007,
      "learning_rate": 4.463758389261745e-05,
      "loss": 0.6685,
      "step": 8490
    },
    {
      "epoch": 0.34,
      "grad_norm": 2.2807116508483887,
      "learning_rate": 4.463087248322148e-05,
      "loss": 0.6588,
      "step": 8500
    },
    {
      "epoch": 0.3404,
      "grad_norm": 2.6811821460723877,
      "learning_rate": 4.4624161073825507e-05,
      "loss": 0.6182,
      "step": 8510
    },
    {
      "epoch": 0.3408,
      "grad_norm": 3.393908977508545,
      "learning_rate": 4.4617449664429535e-05,
      "loss": 0.6619,
      "step": 8520
    },
    {
      "epoch": 0.3412,
      "grad_norm": 3.1166160106658936,
      "learning_rate": 4.461073825503356e-05,
      "loss": 0.6967,
      "step": 8530
    },
    {
      "epoch": 0.3416,
      "grad_norm": 3.291189670562744,
      "learning_rate": 4.4604026845637585e-05,
      "loss": 0.7883,
      "step": 8540
    },
    {
      "epoch": 0.342,
      "grad_norm": 2.850419282913208,
      "learning_rate": 4.459731543624161e-05,
      "loss": 0.5848,
      "step": 8550
    },
    {
      "epoch": 0.3424,
      "grad_norm": 3.4543309211730957,
      "learning_rate": 4.459060402684564e-05,
      "loss": 0.7259,
      "step": 8560
    },
    {
      "epoch": 0.3428,
      "grad_norm": 3.7348577976226807,
      "learning_rate": 4.458389261744967e-05,
      "loss": 0.6533,
      "step": 8570
    },
    {
      "epoch": 0.3432,
      "grad_norm": 2.98710298538208,
      "learning_rate": 4.457718120805369e-05,
      "loss": 0.6402,
      "step": 8580
    },
    {
      "epoch": 0.3436,
      "grad_norm": 4.217338562011719,
      "learning_rate": 4.457046979865772e-05,
      "loss": 0.678,
      "step": 8590
    },
    {
      "epoch": 0.344,
      "grad_norm": 2.075439453125,
      "learning_rate": 4.456375838926174e-05,
      "loss": 0.6039,
      "step": 8600
    },
    {
      "epoch": 0.3444,
      "grad_norm": 3.3156368732452393,
      "learning_rate": 4.455704697986577e-05,
      "loss": 0.6196,
      "step": 8610
    },
    {
      "epoch": 0.3448,
      "grad_norm": 3.840669870376587,
      "learning_rate": 4.45503355704698e-05,
      "loss": 0.7499,
      "step": 8620
    },
    {
      "epoch": 0.3452,
      "grad_norm": 2.8876912593841553,
      "learning_rate": 4.454362416107383e-05,
      "loss": 0.6826,
      "step": 8630
    },
    {
      "epoch": 0.3456,
      "grad_norm": 2.7748262882232666,
      "learning_rate": 4.453691275167786e-05,
      "loss": 0.6387,
      "step": 8640
    },
    {
      "epoch": 0.346,
      "grad_norm": 2.7554690837860107,
      "learning_rate": 4.453020134228188e-05,
      "loss": 0.711,
      "step": 8650
    },
    {
      "epoch": 0.3464,
      "grad_norm": 4.3545379638671875,
      "learning_rate": 4.452348993288591e-05,
      "loss": 0.6482,
      "step": 8660
    },
    {
      "epoch": 0.3468,
      "grad_norm": 3.288182258605957,
      "learning_rate": 4.451677852348993e-05,
      "loss": 0.7919,
      "step": 8670
    },
    {
      "epoch": 0.3472,
      "grad_norm": 2.45379900932312,
      "learning_rate": 4.4510067114093965e-05,
      "loss": 0.5885,
      "step": 8680
    },
    {
      "epoch": 0.3476,
      "grad_norm": 3.0854690074920654,
      "learning_rate": 4.450335570469799e-05,
      "loss": 0.6627,
      "step": 8690
    },
    {
      "epoch": 0.348,
      "grad_norm": 3.761150360107422,
      "learning_rate": 4.4496644295302015e-05,
      "loss": 0.6555,
      "step": 8700
    },
    {
      "epoch": 0.3484,
      "grad_norm": 2.768545150756836,
      "learning_rate": 4.4489932885906044e-05,
      "loss": 0.718,
      "step": 8710
    },
    {
      "epoch": 0.3488,
      "grad_norm": 2.392075777053833,
      "learning_rate": 4.4483221476510065e-05,
      "loss": 0.6124,
      "step": 8720
    },
    {
      "epoch": 0.3492,
      "grad_norm": 3.1045141220092773,
      "learning_rate": 4.4476510067114094e-05,
      "loss": 0.6829,
      "step": 8730
    },
    {
      "epoch": 0.3496,
      "grad_norm": 3.0064191818237305,
      "learning_rate": 4.446979865771812e-05,
      "loss": 0.5774,
      "step": 8740
    },
    {
      "epoch": 0.35,
      "grad_norm": 2.852748394012451,
      "learning_rate": 4.446308724832215e-05,
      "loss": 0.6714,
      "step": 8750
    },
    {
      "epoch": 0.3504,
      "grad_norm": 3.4916446208953857,
      "learning_rate": 4.445637583892618e-05,
      "loss": 0.5273,
      "step": 8760
    },
    {
      "epoch": 0.3508,
      "grad_norm": 2.645537853240967,
      "learning_rate": 4.44496644295302e-05,
      "loss": 0.6374,
      "step": 8770
    },
    {
      "epoch": 0.3512,
      "grad_norm": 2.509425640106201,
      "learning_rate": 4.444295302013423e-05,
      "loss": 0.6906,
      "step": 8780
    },
    {
      "epoch": 0.3516,
      "grad_norm": 2.30930495262146,
      "learning_rate": 4.443624161073825e-05,
      "loss": 0.6278,
      "step": 8790
    },
    {
      "epoch": 0.352,
      "grad_norm": 3.398022413253784,
      "learning_rate": 4.442953020134229e-05,
      "loss": 0.7314,
      "step": 8800
    },
    {
      "epoch": 0.3524,
      "grad_norm": 3.2030723094940186,
      "learning_rate": 4.4422818791946316e-05,
      "loss": 0.7608,
      "step": 8810
    },
    {
      "epoch": 0.3528,
      "grad_norm": 3.3309173583984375,
      "learning_rate": 4.441610738255034e-05,
      "loss": 0.6516,
      "step": 8820
    },
    {
      "epoch": 0.3532,
      "grad_norm": 10.271528244018555,
      "learning_rate": 4.4409395973154366e-05,
      "loss": 0.6962,
      "step": 8830
    },
    {
      "epoch": 0.3536,
      "grad_norm": 2.6590964794158936,
      "learning_rate": 4.440268456375839e-05,
      "loss": 0.6539,
      "step": 8840
    },
    {
      "epoch": 0.354,
      "grad_norm": 3.563685655593872,
      "learning_rate": 4.4395973154362416e-05,
      "loss": 0.7536,
      "step": 8850
    },
    {
      "epoch": 0.3544,
      "grad_norm": 3.2409136295318604,
      "learning_rate": 4.4389261744966445e-05,
      "loss": 0.6274,
      "step": 8860
    },
    {
      "epoch": 0.3548,
      "grad_norm": 2.335637331008911,
      "learning_rate": 4.438255033557047e-05,
      "loss": 0.6265,
      "step": 8870
    },
    {
      "epoch": 0.3552,
      "grad_norm": 3.070146083831787,
      "learning_rate": 4.43758389261745e-05,
      "loss": 0.6692,
      "step": 8880
    },
    {
      "epoch": 0.3556,
      "grad_norm": 2.231639862060547,
      "learning_rate": 4.4369127516778524e-05,
      "loss": 0.716,
      "step": 8890
    },
    {
      "epoch": 0.356,
      "grad_norm": 2.352287769317627,
      "learning_rate": 4.436241610738255e-05,
      "loss": 0.7279,
      "step": 8900
    },
    {
      "epoch": 0.3564,
      "grad_norm": 3.0410573482513428,
      "learning_rate": 4.435570469798658e-05,
      "loss": 0.769,
      "step": 8910
    },
    {
      "epoch": 0.3568,
      "grad_norm": 2.8950979709625244,
      "learning_rate": 4.434899328859061e-05,
      "loss": 0.606,
      "step": 8920
    },
    {
      "epoch": 0.3572,
      "grad_norm": 3.5650744438171387,
      "learning_rate": 4.434228187919463e-05,
      "loss": 0.597,
      "step": 8930
    },
    {
      "epoch": 0.3576,
      "grad_norm": 2.7155840396881104,
      "learning_rate": 4.433557046979866e-05,
      "loss": 0.7264,
      "step": 8940
    },
    {
      "epoch": 0.358,
      "grad_norm": 2.3392953872680664,
      "learning_rate": 4.432885906040269e-05,
      "loss": 0.4975,
      "step": 8950
    },
    {
      "epoch": 0.3584,
      "grad_norm": 4.0263671875,
      "learning_rate": 4.432214765100671e-05,
      "loss": 0.7785,
      "step": 8960
    },
    {
      "epoch": 0.3588,
      "grad_norm": 2.457289934158325,
      "learning_rate": 4.431543624161074e-05,
      "loss": 0.6965,
      "step": 8970
    },
    {
      "epoch": 0.3592,
      "grad_norm": 2.7021944522857666,
      "learning_rate": 4.430872483221477e-05,
      "loss": 0.6604,
      "step": 8980
    },
    {
      "epoch": 0.3596,
      "grad_norm": 2.50468111038208,
      "learning_rate": 4.4302013422818796e-05,
      "loss": 0.5805,
      "step": 8990
    },
    {
      "epoch": 0.36,
      "grad_norm": 2.8576557636260986,
      "learning_rate": 4.4295302013422824e-05,
      "loss": 0.8411,
      "step": 9000
    },
    {
      "epoch": 0.3604,
      "grad_norm": 2.5725913047790527,
      "learning_rate": 4.4288590604026846e-05,
      "loss": 0.645,
      "step": 9010
    },
    {
      "epoch": 0.3608,
      "grad_norm": 3.088406801223755,
      "learning_rate": 4.4281879194630874e-05,
      "loss": 0.6676,
      "step": 9020
    },
    {
      "epoch": 0.3612,
      "grad_norm": 3.173048973083496,
      "learning_rate": 4.42751677852349e-05,
      "loss": 0.7373,
      "step": 9030
    },
    {
      "epoch": 0.3616,
      "grad_norm": 3.0498666763305664,
      "learning_rate": 4.426845637583893e-05,
      "loss": 0.6177,
      "step": 9040
    },
    {
      "epoch": 0.362,
      "grad_norm": 2.6331422328948975,
      "learning_rate": 4.426174496644295e-05,
      "loss": 0.5993,
      "step": 9050
    },
    {
      "epoch": 0.3624,
      "grad_norm": 1.966631293296814,
      "learning_rate": 4.425503355704698e-05,
      "loss": 0.6377,
      "step": 9060
    },
    {
      "epoch": 0.3628,
      "grad_norm": 2.7019646167755127,
      "learning_rate": 4.424832214765101e-05,
      "loss": 0.5758,
      "step": 9070
    },
    {
      "epoch": 0.3632,
      "grad_norm": 2.7175846099853516,
      "learning_rate": 4.424161073825503e-05,
      "loss": 0.6728,
      "step": 9080
    },
    {
      "epoch": 0.3636,
      "grad_norm": 2.160550832748413,
      "learning_rate": 4.423489932885906e-05,
      "loss": 0.6016,
      "step": 9090
    },
    {
      "epoch": 0.364,
      "grad_norm": 3.402672529220581,
      "learning_rate": 4.422818791946309e-05,
      "loss": 0.7054,
      "step": 9100
    },
    {
      "epoch": 0.3644,
      "grad_norm": 3.6795458793640137,
      "learning_rate": 4.422147651006712e-05,
      "loss": 0.6996,
      "step": 9110
    },
    {
      "epoch": 0.3648,
      "grad_norm": 2.559206008911133,
      "learning_rate": 4.421476510067114e-05,
      "loss": 0.5727,
      "step": 9120
    },
    {
      "epoch": 0.3652,
      "grad_norm": 2.7985310554504395,
      "learning_rate": 4.420805369127517e-05,
      "loss": 0.5915,
      "step": 9130
    },
    {
      "epoch": 0.3656,
      "grad_norm": 3.4065051078796387,
      "learning_rate": 4.42013422818792e-05,
      "loss": 0.6639,
      "step": 9140
    },
    {
      "epoch": 0.366,
      "grad_norm": 3.288872480392456,
      "learning_rate": 4.4194630872483225e-05,
      "loss": 0.5587,
      "step": 9150
    },
    {
      "epoch": 0.3664,
      "grad_norm": 3.2786195278167725,
      "learning_rate": 4.4187919463087254e-05,
      "loss": 0.6362,
      "step": 9160
    },
    {
      "epoch": 0.3668,
      "grad_norm": 2.736536979675293,
      "learning_rate": 4.4181208053691276e-05,
      "loss": 0.6095,
      "step": 9170
    },
    {
      "epoch": 0.3672,
      "grad_norm": 2.976123094558716,
      "learning_rate": 4.4174496644295304e-05,
      "loss": 0.5973,
      "step": 9180
    },
    {
      "epoch": 0.3676,
      "grad_norm": 2.979620933532715,
      "learning_rate": 4.416778523489933e-05,
      "loss": 0.7705,
      "step": 9190
    },
    {
      "epoch": 0.368,
      "grad_norm": 2.650088310241699,
      "learning_rate": 4.4161073825503354e-05,
      "loss": 0.6476,
      "step": 9200
    },
    {
      "epoch": 0.3684,
      "grad_norm": 3.4955761432647705,
      "learning_rate": 4.415436241610739e-05,
      "loss": 0.6878,
      "step": 9210
    },
    {
      "epoch": 0.3688,
      "grad_norm": 1.9272823333740234,
      "learning_rate": 4.414765100671141e-05,
      "loss": 0.7181,
      "step": 9220
    },
    {
      "epoch": 0.3692,
      "grad_norm": 2.5337493419647217,
      "learning_rate": 4.414093959731544e-05,
      "loss": 0.6186,
      "step": 9230
    },
    {
      "epoch": 0.3696,
      "grad_norm": 3.0249340534210205,
      "learning_rate": 4.413422818791946e-05,
      "loss": 0.5996,
      "step": 9240
    },
    {
      "epoch": 0.37,
      "grad_norm": 2.956028699874878,
      "learning_rate": 4.412751677852349e-05,
      "loss": 0.673,
      "step": 9250
    },
    {
      "epoch": 0.3704,
      "grad_norm": 3.686694860458374,
      "learning_rate": 4.412080536912752e-05,
      "loss": 0.6731,
      "step": 9260
    },
    {
      "epoch": 0.3708,
      "grad_norm": 2.767923593521118,
      "learning_rate": 4.411409395973155e-05,
      "loss": 0.6243,
      "step": 9270
    },
    {
      "epoch": 0.3712,
      "grad_norm": 2.9961860179901123,
      "learning_rate": 4.4107382550335576e-05,
      "loss": 0.7026,
      "step": 9280
    },
    {
      "epoch": 0.3716,
      "grad_norm": 3.166997194290161,
      "learning_rate": 4.41006711409396e-05,
      "loss": 0.6034,
      "step": 9290
    },
    {
      "epoch": 0.372,
      "grad_norm": 2.0949456691741943,
      "learning_rate": 4.4093959731543626e-05,
      "loss": 0.6555,
      "step": 9300
    },
    {
      "epoch": 0.3724,
      "grad_norm": 2.579418897628784,
      "learning_rate": 4.408724832214765e-05,
      "loss": 0.6116,
      "step": 9310
    },
    {
      "epoch": 0.3728,
      "grad_norm": 2.8109066486358643,
      "learning_rate": 4.408053691275168e-05,
      "loss": 0.6287,
      "step": 9320
    },
    {
      "epoch": 0.3732,
      "grad_norm": 3.4075560569763184,
      "learning_rate": 4.407382550335571e-05,
      "loss": 0.6417,
      "step": 9330
    },
    {
      "epoch": 0.3736,
      "grad_norm": 3.6067867279052734,
      "learning_rate": 4.4067114093959734e-05,
      "loss": 0.7599,
      "step": 9340
    },
    {
      "epoch": 0.374,
      "grad_norm": 2.8494765758514404,
      "learning_rate": 4.406040268456376e-05,
      "loss": 0.6721,
      "step": 9350
    },
    {
      "epoch": 0.3744,
      "grad_norm": 2.9182190895080566,
      "learning_rate": 4.4053691275167784e-05,
      "loss": 0.722,
      "step": 9360
    },
    {
      "epoch": 0.3748,
      "grad_norm": 3.14128041267395,
      "learning_rate": 4.404697986577181e-05,
      "loss": 0.6622,
      "step": 9370
    },
    {
      "epoch": 0.3752,
      "grad_norm": 3.0079290866851807,
      "learning_rate": 4.404026845637584e-05,
      "loss": 0.6421,
      "step": 9380
    },
    {
      "epoch": 0.3756,
      "grad_norm": 3.445016622543335,
      "learning_rate": 4.403355704697987e-05,
      "loss": 0.7043,
      "step": 9390
    },
    {
      "epoch": 0.376,
      "grad_norm": 2.531857967376709,
      "learning_rate": 4.40268456375839e-05,
      "loss": 0.6605,
      "step": 9400
    },
    {
      "epoch": 0.3764,
      "grad_norm": 3.553251028060913,
      "learning_rate": 4.402013422818792e-05,
      "loss": 0.668,
      "step": 9410
    },
    {
      "epoch": 0.3768,
      "grad_norm": 2.6220145225524902,
      "learning_rate": 4.401342281879195e-05,
      "loss": 0.6273,
      "step": 9420
    },
    {
      "epoch": 0.3772,
      "grad_norm": 3.104736566543579,
      "learning_rate": 4.400671140939597e-05,
      "loss": 0.6915,
      "step": 9430
    },
    {
      "epoch": 0.3776,
      "grad_norm": 2.8774523735046387,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 0.6938,
      "step": 9440
    },
    {
      "epoch": 0.378,
      "grad_norm": 2.7643463611602783,
      "learning_rate": 4.3993288590604034e-05,
      "loss": 0.5719,
      "step": 9450
    },
    {
      "epoch": 0.3784,
      "grad_norm": 2.1864430904388428,
      "learning_rate": 4.3986577181208056e-05,
      "loss": 0.6951,
      "step": 9460
    },
    {
      "epoch": 0.3788,
      "grad_norm": 3.0773658752441406,
      "learning_rate": 4.3979865771812084e-05,
      "loss": 0.7003,
      "step": 9470
    },
    {
      "epoch": 0.3792,
      "grad_norm": 2.368169069290161,
      "learning_rate": 4.3973154362416106e-05,
      "loss": 0.6334,
      "step": 9480
    },
    {
      "epoch": 0.3796,
      "grad_norm": 3.901054620742798,
      "learning_rate": 4.3966442953020135e-05,
      "loss": 0.6996,
      "step": 9490
    },
    {
      "epoch": 0.38,
      "grad_norm": 3.2512025833129883,
      "learning_rate": 4.395973154362416e-05,
      "loss": 0.6403,
      "step": 9500
    },
    {
      "epoch": 0.3804,
      "grad_norm": 3.411621570587158,
      "learning_rate": 4.395302013422819e-05,
      "loss": 0.8275,
      "step": 9510
    },
    {
      "epoch": 0.3808,
      "grad_norm": 2.6432178020477295,
      "learning_rate": 4.394630872483222e-05,
      "loss": 0.6539,
      "step": 9520
    },
    {
      "epoch": 0.3812,
      "grad_norm": 3.079796075820923,
      "learning_rate": 4.393959731543624e-05,
      "loss": 0.5847,
      "step": 9530
    },
    {
      "epoch": 0.3816,
      "grad_norm": 3.586627244949341,
      "learning_rate": 4.393288590604027e-05,
      "loss": 0.758,
      "step": 9540
    },
    {
      "epoch": 0.382,
      "grad_norm": 2.7975094318389893,
      "learning_rate": 4.392617449664429e-05,
      "loss": 0.6966,
      "step": 9550
    },
    {
      "epoch": 0.3824,
      "grad_norm": 2.265700578689575,
      "learning_rate": 4.391946308724833e-05,
      "loss": 0.6211,
      "step": 9560
    },
    {
      "epoch": 0.3828,
      "grad_norm": 2.9479331970214844,
      "learning_rate": 4.391275167785235e-05,
      "loss": 0.6454,
      "step": 9570
    },
    {
      "epoch": 0.3832,
      "grad_norm": 2.377168655395508,
      "learning_rate": 4.390604026845638e-05,
      "loss": 0.6865,
      "step": 9580
    },
    {
      "epoch": 0.3836,
      "grad_norm": 1.7112507820129395,
      "learning_rate": 4.389932885906041e-05,
      "loss": 0.6938,
      "step": 9590
    },
    {
      "epoch": 0.384,
      "grad_norm": 3.174372911453247,
      "learning_rate": 4.389261744966443e-05,
      "loss": 0.7297,
      "step": 9600
    },
    {
      "epoch": 0.3844,
      "grad_norm": 3.235560178756714,
      "learning_rate": 4.388590604026846e-05,
      "loss": 0.6941,
      "step": 9610
    },
    {
      "epoch": 0.3848,
      "grad_norm": 3.1593337059020996,
      "learning_rate": 4.3879194630872486e-05,
      "loss": 0.6539,
      "step": 9620
    },
    {
      "epoch": 0.3852,
      "grad_norm": 2.545522689819336,
      "learning_rate": 4.3872483221476514e-05,
      "loss": 0.6458,
      "step": 9630
    },
    {
      "epoch": 0.3856,
      "grad_norm": 2.931644916534424,
      "learning_rate": 4.386577181208054e-05,
      "loss": 0.6293,
      "step": 9640
    },
    {
      "epoch": 0.386,
      "grad_norm": 2.837362051010132,
      "learning_rate": 4.3859060402684564e-05,
      "loss": 0.7218,
      "step": 9650
    },
    {
      "epoch": 0.3864,
      "grad_norm": 3.188666343688965,
      "learning_rate": 4.385234899328859e-05,
      "loss": 0.8009,
      "step": 9660
    },
    {
      "epoch": 0.3868,
      "grad_norm": 2.3999569416046143,
      "learning_rate": 4.3845637583892615e-05,
      "loss": 0.6169,
      "step": 9670
    },
    {
      "epoch": 0.3872,
      "grad_norm": 2.6268234252929688,
      "learning_rate": 4.383892617449665e-05,
      "loss": 0.6721,
      "step": 9680
    },
    {
      "epoch": 0.3876,
      "grad_norm": 3.548980712890625,
      "learning_rate": 4.383221476510067e-05,
      "loss": 0.6752,
      "step": 9690
    },
    {
      "epoch": 0.388,
      "grad_norm": 2.2191431522369385,
      "learning_rate": 4.38255033557047e-05,
      "loss": 0.6163,
      "step": 9700
    },
    {
      "epoch": 0.3884,
      "grad_norm": 2.9875028133392334,
      "learning_rate": 4.381879194630873e-05,
      "loss": 0.7105,
      "step": 9710
    },
    {
      "epoch": 0.3888,
      "grad_norm": 2.3433821201324463,
      "learning_rate": 4.381208053691275e-05,
      "loss": 0.6702,
      "step": 9720
    },
    {
      "epoch": 0.3892,
      "grad_norm": 2.78886342048645,
      "learning_rate": 4.380536912751678e-05,
      "loss": 0.6667,
      "step": 9730
    },
    {
      "epoch": 0.3896,
      "grad_norm": 3.21801495552063,
      "learning_rate": 4.379865771812081e-05,
      "loss": 0.5729,
      "step": 9740
    },
    {
      "epoch": 0.39,
      "grad_norm": 3.152588129043579,
      "learning_rate": 4.3791946308724836e-05,
      "loss": 0.6808,
      "step": 9750
    },
    {
      "epoch": 0.3904,
      "grad_norm": 3.1700854301452637,
      "learning_rate": 4.378523489932886e-05,
      "loss": 0.7152,
      "step": 9760
    },
    {
      "epoch": 0.3908,
      "grad_norm": 3.170530080795288,
      "learning_rate": 4.377852348993289e-05,
      "loss": 0.6615,
      "step": 9770
    },
    {
      "epoch": 0.3912,
      "grad_norm": 3.699812412261963,
      "learning_rate": 4.3771812080536915e-05,
      "loss": 0.7254,
      "step": 9780
    },
    {
      "epoch": 0.3916,
      "grad_norm": 3.281139373779297,
      "learning_rate": 4.3765100671140944e-05,
      "loss": 0.7204,
      "step": 9790
    },
    {
      "epoch": 0.392,
      "grad_norm": 2.0101253986358643,
      "learning_rate": 4.375838926174497e-05,
      "loss": 0.5643,
      "step": 9800
    },
    {
      "epoch": 0.3924,
      "grad_norm": 2.581711769104004,
      "learning_rate": 4.3751677852348994e-05,
      "loss": 0.6636,
      "step": 9810
    },
    {
      "epoch": 0.3928,
      "grad_norm": 2.4094696044921875,
      "learning_rate": 4.374496644295302e-05,
      "loss": 0.66,
      "step": 9820
    },
    {
      "epoch": 0.3932,
      "grad_norm": 3.096250295639038,
      "learning_rate": 4.373825503355705e-05,
      "loss": 0.5838,
      "step": 9830
    },
    {
      "epoch": 0.3936,
      "grad_norm": 2.49863338470459,
      "learning_rate": 4.373154362416107e-05,
      "loss": 0.7993,
      "step": 9840
    },
    {
      "epoch": 0.394,
      "grad_norm": 2.2400848865509033,
      "learning_rate": 4.37248322147651e-05,
      "loss": 0.6696,
      "step": 9850
    },
    {
      "epoch": 0.3944,
      "grad_norm": 2.1536712646484375,
      "learning_rate": 4.371812080536913e-05,
      "loss": 0.6758,
      "step": 9860
    },
    {
      "epoch": 0.3948,
      "grad_norm": 2.978696584701538,
      "learning_rate": 4.371140939597316e-05,
      "loss": 0.6439,
      "step": 9870
    },
    {
      "epoch": 0.3952,
      "grad_norm": 2.6571197509765625,
      "learning_rate": 4.370469798657718e-05,
      "loss": 0.5928,
      "step": 9880
    },
    {
      "epoch": 0.3956,
      "grad_norm": 2.354952335357666,
      "learning_rate": 4.369798657718121e-05,
      "loss": 0.7306,
      "step": 9890
    },
    {
      "epoch": 0.396,
      "grad_norm": 2.619699001312256,
      "learning_rate": 4.369127516778524e-05,
      "loss": 0.7338,
      "step": 9900
    },
    {
      "epoch": 0.3964,
      "grad_norm": 2.5453133583068848,
      "learning_rate": 4.3684563758389266e-05,
      "loss": 0.7328,
      "step": 9910
    },
    {
      "epoch": 0.3968,
      "grad_norm": 3.6945626735687256,
      "learning_rate": 4.3677852348993295e-05,
      "loss": 0.6461,
      "step": 9920
    },
    {
      "epoch": 0.3972,
      "grad_norm": 3.19553804397583,
      "learning_rate": 4.3671140939597316e-05,
      "loss": 0.6196,
      "step": 9930
    },
    {
      "epoch": 0.3976,
      "grad_norm": 2.7362539768218994,
      "learning_rate": 4.3664429530201345e-05,
      "loss": 0.6346,
      "step": 9940
    },
    {
      "epoch": 0.398,
      "grad_norm": 3.074028968811035,
      "learning_rate": 4.365771812080537e-05,
      "loss": 0.6823,
      "step": 9950
    },
    {
      "epoch": 0.3984,
      "grad_norm": 3.763017416000366,
      "learning_rate": 4.3651006711409395e-05,
      "loss": 0.6093,
      "step": 9960
    },
    {
      "epoch": 0.3988,
      "grad_norm": 3.160865545272827,
      "learning_rate": 4.3644295302013424e-05,
      "loss": 0.6201,
      "step": 9970
    },
    {
      "epoch": 0.3992,
      "grad_norm": 2.5485916137695312,
      "learning_rate": 4.363758389261745e-05,
      "loss": 0.6158,
      "step": 9980
    },
    {
      "epoch": 0.3996,
      "grad_norm": 3.2715651988983154,
      "learning_rate": 4.363087248322148e-05,
      "loss": 0.6844,
      "step": 9990
    },
    {
      "epoch": 0.4,
      "grad_norm": 1.933309555053711,
      "learning_rate": 4.36241610738255e-05,
      "loss": 0.6136,
      "step": 10000
    },
    {
      "epoch": 0.4004,
      "grad_norm": 3.757277488708496,
      "learning_rate": 4.361744966442953e-05,
      "loss": 0.7602,
      "step": 10010
    },
    {
      "epoch": 0.4008,
      "grad_norm": 3.3177037239074707,
      "learning_rate": 4.361073825503356e-05,
      "loss": 0.7025,
      "step": 10020
    },
    {
      "epoch": 0.4012,
      "grad_norm": 3.028618097305298,
      "learning_rate": 4.360402684563759e-05,
      "loss": 0.7182,
      "step": 10030
    },
    {
      "epoch": 0.4016,
      "grad_norm": 2.9230048656463623,
      "learning_rate": 4.359731543624162e-05,
      "loss": 0.6931,
      "step": 10040
    },
    {
      "epoch": 0.402,
      "grad_norm": 2.627016067504883,
      "learning_rate": 4.359060402684564e-05,
      "loss": 0.6478,
      "step": 10050
    },
    {
      "epoch": 0.4024,
      "grad_norm": 3.0624866485595703,
      "learning_rate": 4.358389261744967e-05,
      "loss": 0.7743,
      "step": 10060
    },
    {
      "epoch": 0.4028,
      "grad_norm": 3.094484806060791,
      "learning_rate": 4.357718120805369e-05,
      "loss": 0.5768,
      "step": 10070
    },
    {
      "epoch": 0.4032,
      "grad_norm": 2.7173943519592285,
      "learning_rate": 4.357046979865772e-05,
      "loss": 0.7435,
      "step": 10080
    },
    {
      "epoch": 0.4036,
      "grad_norm": 3.0595312118530273,
      "learning_rate": 4.356375838926175e-05,
      "loss": 0.7123,
      "step": 10090
    },
    {
      "epoch": 0.404,
      "grad_norm": 2.7466166019439697,
      "learning_rate": 4.3557046979865775e-05,
      "loss": 0.7321,
      "step": 10100
    },
    {
      "epoch": 0.4044,
      "grad_norm": 3.105459213256836,
      "learning_rate": 4.35503355704698e-05,
      "loss": 0.7495,
      "step": 10110
    },
    {
      "epoch": 0.4048,
      "grad_norm": 2.8808536529541016,
      "learning_rate": 4.3543624161073825e-05,
      "loss": 0.5963,
      "step": 10120
    },
    {
      "epoch": 0.4052,
      "grad_norm": 2.1787655353546143,
      "learning_rate": 4.3536912751677853e-05,
      "loss": 0.6781,
      "step": 10130
    },
    {
      "epoch": 0.4056,
      "grad_norm": 3.02227783203125,
      "learning_rate": 4.353020134228188e-05,
      "loss": 0.6872,
      "step": 10140
    },
    {
      "epoch": 0.406,
      "grad_norm": 3.0102198123931885,
      "learning_rate": 4.352348993288591e-05,
      "loss": 0.6542,
      "step": 10150
    },
    {
      "epoch": 0.4064,
      "grad_norm": 2.4032342433929443,
      "learning_rate": 4.351677852348994e-05,
      "loss": 0.622,
      "step": 10160
    },
    {
      "epoch": 0.4068,
      "grad_norm": 2.438778877258301,
      "learning_rate": 4.351006711409396e-05,
      "loss": 0.6329,
      "step": 10170
    },
    {
      "epoch": 0.4072,
      "grad_norm": 3.4094507694244385,
      "learning_rate": 4.350335570469799e-05,
      "loss": 0.668,
      "step": 10180
    },
    {
      "epoch": 0.4076,
      "grad_norm": 3.2483277320861816,
      "learning_rate": 4.349664429530201e-05,
      "loss": 0.6405,
      "step": 10190
    },
    {
      "epoch": 0.408,
      "grad_norm": 2.836049795150757,
      "learning_rate": 4.348993288590604e-05,
      "loss": 0.6562,
      "step": 10200
    },
    {
      "epoch": 0.4084,
      "grad_norm": 2.4984710216522217,
      "learning_rate": 4.348322147651007e-05,
      "loss": 0.5344,
      "step": 10210
    },
    {
      "epoch": 0.4088,
      "grad_norm": 3.1987483501434326,
      "learning_rate": 4.34765100671141e-05,
      "loss": 0.76,
      "step": 10220
    },
    {
      "epoch": 0.4092,
      "grad_norm": 2.894591808319092,
      "learning_rate": 4.3469798657718125e-05,
      "loss": 0.733,
      "step": 10230
    },
    {
      "epoch": 0.4096,
      "grad_norm": 2.9676361083984375,
      "learning_rate": 4.346308724832215e-05,
      "loss": 0.7023,
      "step": 10240
    },
    {
      "epoch": 0.41,
      "grad_norm": 3.018057107925415,
      "learning_rate": 4.3456375838926176e-05,
      "loss": 0.7131,
      "step": 10250
    },
    {
      "epoch": 0.4104,
      "grad_norm": 2.780655860900879,
      "learning_rate": 4.3449664429530204e-05,
      "loss": 0.62,
      "step": 10260
    },
    {
      "epoch": 0.4108,
      "grad_norm": 3.965998411178589,
      "learning_rate": 4.344295302013423e-05,
      "loss": 0.5576,
      "step": 10270
    },
    {
      "epoch": 0.4112,
      "grad_norm": 3.3808374404907227,
      "learning_rate": 4.343624161073826e-05,
      "loss": 0.6415,
      "step": 10280
    },
    {
      "epoch": 0.4116,
      "grad_norm": 3.2259230613708496,
      "learning_rate": 4.342953020134228e-05,
      "loss": 0.6395,
      "step": 10290
    },
    {
      "epoch": 0.412,
      "grad_norm": 2.630220890045166,
      "learning_rate": 4.342281879194631e-05,
      "loss": 0.7035,
      "step": 10300
    },
    {
      "epoch": 0.4124,
      "grad_norm": 3.6086199283599854,
      "learning_rate": 4.341610738255033e-05,
      "loss": 0.6211,
      "step": 10310
    },
    {
      "epoch": 0.4128,
      "grad_norm": 2.731428861618042,
      "learning_rate": 4.340939597315437e-05,
      "loss": 0.5183,
      "step": 10320
    },
    {
      "epoch": 0.4132,
      "grad_norm": 2.5284860134124756,
      "learning_rate": 4.340268456375839e-05,
      "loss": 0.7023,
      "step": 10330
    },
    {
      "epoch": 0.4136,
      "grad_norm": 2.808297634124756,
      "learning_rate": 4.339597315436242e-05,
      "loss": 0.6764,
      "step": 10340
    },
    {
      "epoch": 0.414,
      "grad_norm": 3.238206386566162,
      "learning_rate": 4.338926174496645e-05,
      "loss": 0.6031,
      "step": 10350
    },
    {
      "epoch": 0.4144,
      "grad_norm": 2.7188236713409424,
      "learning_rate": 4.338255033557047e-05,
      "loss": 0.6699,
      "step": 10360
    },
    {
      "epoch": 0.4148,
      "grad_norm": 3.3979132175445557,
      "learning_rate": 4.33758389261745e-05,
      "loss": 0.7621,
      "step": 10370
    },
    {
      "epoch": 0.4152,
      "grad_norm": 2.514876365661621,
      "learning_rate": 4.3369127516778526e-05,
      "loss": 0.7907,
      "step": 10380
    },
    {
      "epoch": 0.4156,
      "grad_norm": 3.2739875316619873,
      "learning_rate": 4.3362416107382555e-05,
      "loss": 0.8063,
      "step": 10390
    },
    {
      "epoch": 0.416,
      "grad_norm": 3.0470118522644043,
      "learning_rate": 4.335570469798658e-05,
      "loss": 0.7613,
      "step": 10400
    },
    {
      "epoch": 0.4164,
      "grad_norm": 3.3374056816101074,
      "learning_rate": 4.3348993288590605e-05,
      "loss": 0.6972,
      "step": 10410
    },
    {
      "epoch": 0.4168,
      "grad_norm": 2.305083990097046,
      "learning_rate": 4.3342281879194634e-05,
      "loss": 0.6005,
      "step": 10420
    },
    {
      "epoch": 0.4172,
      "grad_norm": 2.9357492923736572,
      "learning_rate": 4.3335570469798656e-05,
      "loss": 0.6737,
      "step": 10430
    },
    {
      "epoch": 0.4176,
      "grad_norm": 2.664020538330078,
      "learning_rate": 4.332885906040269e-05,
      "loss": 0.5822,
      "step": 10440
    },
    {
      "epoch": 0.418,
      "grad_norm": 3.082573890686035,
      "learning_rate": 4.332214765100671e-05,
      "loss": 0.6853,
      "step": 10450
    },
    {
      "epoch": 0.4184,
      "grad_norm": 2.3369345664978027,
      "learning_rate": 4.331543624161074e-05,
      "loss": 0.7228,
      "step": 10460
    },
    {
      "epoch": 0.4188,
      "grad_norm": 2.836937665939331,
      "learning_rate": 4.330872483221477e-05,
      "loss": 0.6465,
      "step": 10470
    },
    {
      "epoch": 0.4192,
      "grad_norm": 2.961242437362671,
      "learning_rate": 4.330201342281879e-05,
      "loss": 0.5859,
      "step": 10480
    },
    {
      "epoch": 0.4196,
      "grad_norm": 3.045546531677246,
      "learning_rate": 4.329530201342282e-05,
      "loss": 0.7264,
      "step": 10490
    },
    {
      "epoch": 0.42,
      "grad_norm": 2.5691590309143066,
      "learning_rate": 4.328859060402685e-05,
      "loss": 0.5426,
      "step": 10500
    },
    {
      "epoch": 0.4204,
      "grad_norm": 3.514522075653076,
      "learning_rate": 4.328187919463088e-05,
      "loss": 0.7678,
      "step": 10510
    },
    {
      "epoch": 0.4208,
      "grad_norm": 2.801483631134033,
      "learning_rate": 4.32751677852349e-05,
      "loss": 0.615,
      "step": 10520
    },
    {
      "epoch": 0.4212,
      "grad_norm": 2.0195820331573486,
      "learning_rate": 4.326845637583893e-05,
      "loss": 0.6043,
      "step": 10530
    },
    {
      "epoch": 0.4216,
      "grad_norm": 3.0451087951660156,
      "learning_rate": 4.3261744966442956e-05,
      "loss": 0.6858,
      "step": 10540
    },
    {
      "epoch": 0.422,
      "grad_norm": 2.180546522140503,
      "learning_rate": 4.325503355704698e-05,
      "loss": 0.6195,
      "step": 10550
    },
    {
      "epoch": 0.4224,
      "grad_norm": 2.9443554878234863,
      "learning_rate": 4.324832214765101e-05,
      "loss": 0.7027,
      "step": 10560
    },
    {
      "epoch": 0.4228,
      "grad_norm": 2.5776004791259766,
      "learning_rate": 4.3241610738255035e-05,
      "loss": 0.7381,
      "step": 10570
    },
    {
      "epoch": 0.4232,
      "grad_norm": 2.4500553607940674,
      "learning_rate": 4.3234899328859063e-05,
      "loss": 0.6486,
      "step": 10580
    },
    {
      "epoch": 0.4236,
      "grad_norm": 2.8643624782562256,
      "learning_rate": 4.3228187919463085e-05,
      "loss": 0.7216,
      "step": 10590
    },
    {
      "epoch": 0.424,
      "grad_norm": 3.079582691192627,
      "learning_rate": 4.3221476510067114e-05,
      "loss": 0.6199,
      "step": 10600
    },
    {
      "epoch": 0.4244,
      "grad_norm": 2.9769387245178223,
      "learning_rate": 4.321476510067114e-05,
      "loss": 0.6139,
      "step": 10610
    },
    {
      "epoch": 0.4248,
      "grad_norm": 2.6130874156951904,
      "learning_rate": 4.320805369127517e-05,
      "loss": 0.7607,
      "step": 10620
    },
    {
      "epoch": 0.4252,
      "grad_norm": 2.4797239303588867,
      "learning_rate": 4.32013422818792e-05,
      "loss": 0.693,
      "step": 10630
    },
    {
      "epoch": 0.4256,
      "grad_norm": 2.4834582805633545,
      "learning_rate": 4.319463087248322e-05,
      "loss": 0.7385,
      "step": 10640
    },
    {
      "epoch": 0.426,
      "grad_norm": 3.9159464836120605,
      "learning_rate": 4.318791946308725e-05,
      "loss": 0.7444,
      "step": 10650
    },
    {
      "epoch": 0.4264,
      "grad_norm": 2.5313000679016113,
      "learning_rate": 4.318120805369128e-05,
      "loss": 0.6558,
      "step": 10660
    },
    {
      "epoch": 0.4268,
      "grad_norm": 3.445981740951538,
      "learning_rate": 4.317449664429531e-05,
      "loss": 0.7344,
      "step": 10670
    },
    {
      "epoch": 0.4272,
      "grad_norm": 2.710998296737671,
      "learning_rate": 4.3167785234899335e-05,
      "loss": 0.6059,
      "step": 10680
    },
    {
      "epoch": 0.4276,
      "grad_norm": 3.1809918880462646,
      "learning_rate": 4.316107382550336e-05,
      "loss": 0.712,
      "step": 10690
    },
    {
      "epoch": 0.428,
      "grad_norm": 3.0731894969940186,
      "learning_rate": 4.3154362416107386e-05,
      "loss": 0.6182,
      "step": 10700
    },
    {
      "epoch": 0.4284,
      "grad_norm": 3.638399362564087,
      "learning_rate": 4.314765100671141e-05,
      "loss": 0.7796,
      "step": 10710
    },
    {
      "epoch": 0.4288,
      "grad_norm": 3.338089942932129,
      "learning_rate": 4.3140939597315436e-05,
      "loss": 0.6231,
      "step": 10720
    },
    {
      "epoch": 0.4292,
      "grad_norm": 3.652601718902588,
      "learning_rate": 4.3134228187919465e-05,
      "loss": 0.6289,
      "step": 10730
    },
    {
      "epoch": 0.4296,
      "grad_norm": 2.9633705615997314,
      "learning_rate": 4.312751677852349e-05,
      "loss": 0.7291,
      "step": 10740
    },
    {
      "epoch": 0.43,
      "grad_norm": 2.940420150756836,
      "learning_rate": 4.312080536912752e-05,
      "loss": 0.6587,
      "step": 10750
    },
    {
      "epoch": 0.4304,
      "grad_norm": 2.6928815841674805,
      "learning_rate": 4.3114093959731543e-05,
      "loss": 0.7224,
      "step": 10760
    },
    {
      "epoch": 0.4308,
      "grad_norm": 2.6649560928344727,
      "learning_rate": 4.310738255033557e-05,
      "loss": 0.6937,
      "step": 10770
    },
    {
      "epoch": 0.4312,
      "grad_norm": 3.0327064990997314,
      "learning_rate": 4.3100671140939594e-05,
      "loss": 0.6308,
      "step": 10780
    },
    {
      "epoch": 0.4316,
      "grad_norm": 3.310001850128174,
      "learning_rate": 4.309395973154363e-05,
      "loss": 0.6823,
      "step": 10790
    },
    {
      "epoch": 0.432,
      "grad_norm": 2.857778549194336,
      "learning_rate": 4.308724832214766e-05,
      "loss": 0.593,
      "step": 10800
    },
    {
      "epoch": 0.4324,
      "grad_norm": 3.2044434547424316,
      "learning_rate": 4.308053691275168e-05,
      "loss": 0.65,
      "step": 10810
    },
    {
      "epoch": 0.4328,
      "grad_norm": 3.1410164833068848,
      "learning_rate": 4.307382550335571e-05,
      "loss": 0.6147,
      "step": 10820
    },
    {
      "epoch": 0.4332,
      "grad_norm": 2.8457536697387695,
      "learning_rate": 4.306711409395973e-05,
      "loss": 0.6398,
      "step": 10830
    },
    {
      "epoch": 0.4336,
      "grad_norm": 2.9043009281158447,
      "learning_rate": 4.306040268456376e-05,
      "loss": 0.6311,
      "step": 10840
    },
    {
      "epoch": 0.434,
      "grad_norm": 3.223588228225708,
      "learning_rate": 4.305369127516779e-05,
      "loss": 0.5626,
      "step": 10850
    },
    {
      "epoch": 0.4344,
      "grad_norm": 2.6434357166290283,
      "learning_rate": 4.3046979865771815e-05,
      "loss": 0.7114,
      "step": 10860
    },
    {
      "epoch": 0.4348,
      "grad_norm": 2.9599456787109375,
      "learning_rate": 4.3040268456375844e-05,
      "loss": 0.6935,
      "step": 10870
    },
    {
      "epoch": 0.4352,
      "grad_norm": 2.7042367458343506,
      "learning_rate": 4.3033557046979866e-05,
      "loss": 0.6786,
      "step": 10880
    },
    {
      "epoch": 0.4356,
      "grad_norm": 3.100766658782959,
      "learning_rate": 4.3026845637583894e-05,
      "loss": 0.6622,
      "step": 10890
    },
    {
      "epoch": 0.436,
      "grad_norm": 3.3667328357696533,
      "learning_rate": 4.3020134228187916e-05,
      "loss": 0.7197,
      "step": 10900
    },
    {
      "epoch": 0.4364,
      "grad_norm": 3.7938380241394043,
      "learning_rate": 4.301342281879195e-05,
      "loss": 0.6709,
      "step": 10910
    },
    {
      "epoch": 0.4368,
      "grad_norm": 2.769591808319092,
      "learning_rate": 4.300671140939598e-05,
      "loss": 0.6864,
      "step": 10920
    },
    {
      "epoch": 0.4372,
      "grad_norm": 2.564970016479492,
      "learning_rate": 4.3e-05,
      "loss": 0.7019,
      "step": 10930
    },
    {
      "epoch": 0.4376,
      "grad_norm": 2.522500514984131,
      "learning_rate": 4.299328859060403e-05,
      "loss": 0.6795,
      "step": 10940
    },
    {
      "epoch": 0.438,
      "grad_norm": 3.710801601409912,
      "learning_rate": 4.298657718120805e-05,
      "loss": 0.7364,
      "step": 10950
    },
    {
      "epoch": 0.4384,
      "grad_norm": 3.0631463527679443,
      "learning_rate": 4.297986577181208e-05,
      "loss": 0.648,
      "step": 10960
    },
    {
      "epoch": 0.4388,
      "grad_norm": 3.604579210281372,
      "learning_rate": 4.297315436241611e-05,
      "loss": 0.7157,
      "step": 10970
    },
    {
      "epoch": 0.4392,
      "grad_norm": 3.2099251747131348,
      "learning_rate": 4.296644295302014e-05,
      "loss": 0.6615,
      "step": 10980
    },
    {
      "epoch": 0.4396,
      "grad_norm": 2.031069755554199,
      "learning_rate": 4.2959731543624166e-05,
      "loss": 0.5866,
      "step": 10990
    },
    {
      "epoch": 0.44,
      "grad_norm": 3.075267791748047,
      "learning_rate": 4.295302013422819e-05,
      "loss": 0.6904,
      "step": 11000
    },
    {
      "epoch": 0.4404,
      "grad_norm": 2.3367717266082764,
      "learning_rate": 4.2946308724832216e-05,
      "loss": 0.6357,
      "step": 11010
    },
    {
      "epoch": 0.4408,
      "grad_norm": 2.610440254211426,
      "learning_rate": 4.2939597315436245e-05,
      "loss": 0.6465,
      "step": 11020
    },
    {
      "epoch": 0.4412,
      "grad_norm": 3.2128663063049316,
      "learning_rate": 4.2932885906040274e-05,
      "loss": 0.5316,
      "step": 11030
    },
    {
      "epoch": 0.4416,
      "grad_norm": 3.2001454830169678,
      "learning_rate": 4.2926174496644295e-05,
      "loss": 0.6435,
      "step": 11040
    },
    {
      "epoch": 0.442,
      "grad_norm": 3.5125129222869873,
      "learning_rate": 4.2919463087248324e-05,
      "loss": 0.6356,
      "step": 11050
    },
    {
      "epoch": 0.4424,
      "grad_norm": 2.0023105144500732,
      "learning_rate": 4.291275167785235e-05,
      "loss": 0.675,
      "step": 11060
    },
    {
      "epoch": 0.4428,
      "grad_norm": 2.8103690147399902,
      "learning_rate": 4.2906040268456374e-05,
      "loss": 0.7589,
      "step": 11070
    },
    {
      "epoch": 0.4432,
      "grad_norm": 2.792534112930298,
      "learning_rate": 4.28993288590604e-05,
      "loss": 0.7081,
      "step": 11080
    },
    {
      "epoch": 0.4436,
      "grad_norm": 2.347929000854492,
      "learning_rate": 4.289261744966443e-05,
      "loss": 0.6565,
      "step": 11090
    },
    {
      "epoch": 0.444,
      "grad_norm": 2.981480121612549,
      "learning_rate": 4.288590604026846e-05,
      "loss": 0.6511,
      "step": 11100
    },
    {
      "epoch": 0.4444,
      "grad_norm": 3.4852025508880615,
      "learning_rate": 4.287919463087249e-05,
      "loss": 0.6668,
      "step": 11110
    },
    {
      "epoch": 0.4448,
      "grad_norm": 2.872483253479004,
      "learning_rate": 4.287248322147651e-05,
      "loss": 0.5564,
      "step": 11120
    },
    {
      "epoch": 0.4452,
      "grad_norm": 2.5012011528015137,
      "learning_rate": 4.286577181208054e-05,
      "loss": 0.5893,
      "step": 11130
    },
    {
      "epoch": 0.4456,
      "grad_norm": 1.9046223163604736,
      "learning_rate": 4.285906040268457e-05,
      "loss": 0.6262,
      "step": 11140
    },
    {
      "epoch": 0.446,
      "grad_norm": 3.466003179550171,
      "learning_rate": 4.2852348993288596e-05,
      "loss": 0.6969,
      "step": 11150
    },
    {
      "epoch": 0.4464,
      "grad_norm": 2.674623727798462,
      "learning_rate": 4.284563758389262e-05,
      "loss": 0.7178,
      "step": 11160
    },
    {
      "epoch": 0.4468,
      "grad_norm": 3.9606943130493164,
      "learning_rate": 4.2838926174496646e-05,
      "loss": 0.7247,
      "step": 11170
    },
    {
      "epoch": 0.4472,
      "grad_norm": 3.0593724250793457,
      "learning_rate": 4.2832214765100675e-05,
      "loss": 0.699,
      "step": 11180
    },
    {
      "epoch": 0.4476,
      "grad_norm": 2.3009114265441895,
      "learning_rate": 4.2825503355704696e-05,
      "loss": 0.6548,
      "step": 11190
    },
    {
      "epoch": 0.448,
      "grad_norm": 3.681002616882324,
      "learning_rate": 4.2818791946308725e-05,
      "loss": 0.7511,
      "step": 11200
    },
    {
      "epoch": 0.4484,
      "grad_norm": 2.490811347961426,
      "learning_rate": 4.2812080536912754e-05,
      "loss": 0.666,
      "step": 11210
    },
    {
      "epoch": 0.4488,
      "grad_norm": 2.3189284801483154,
      "learning_rate": 4.280536912751678e-05,
      "loss": 0.5955,
      "step": 11220
    },
    {
      "epoch": 0.4492,
      "grad_norm": 3.5755224227905273,
      "learning_rate": 4.2798657718120804e-05,
      "loss": 0.674,
      "step": 11230
    },
    {
      "epoch": 0.4496,
      "grad_norm": 2.931196451187134,
      "learning_rate": 4.279194630872483e-05,
      "loss": 0.6915,
      "step": 11240
    },
    {
      "epoch": 0.45,
      "grad_norm": 2.8307363986968994,
      "learning_rate": 4.278523489932886e-05,
      "loss": 0.6363,
      "step": 11250
    },
    {
      "epoch": 0.4504,
      "grad_norm": 3.946014642715454,
      "learning_rate": 4.277852348993289e-05,
      "loss": 0.7713,
      "step": 11260
    },
    {
      "epoch": 0.4508,
      "grad_norm": 2.5162229537963867,
      "learning_rate": 4.277181208053692e-05,
      "loss": 0.6756,
      "step": 11270
    },
    {
      "epoch": 0.4512,
      "grad_norm": 2.293208360671997,
      "learning_rate": 4.276510067114094e-05,
      "loss": 0.7725,
      "step": 11280
    },
    {
      "epoch": 0.4516,
      "grad_norm": 2.6537959575653076,
      "learning_rate": 4.275838926174497e-05,
      "loss": 0.6997,
      "step": 11290
    },
    {
      "epoch": 0.452,
      "grad_norm": 3.7868192195892334,
      "learning_rate": 4.2751677852349e-05,
      "loss": 0.6922,
      "step": 11300
    },
    {
      "epoch": 0.4524,
      "grad_norm": 3.2369890213012695,
      "learning_rate": 4.274496644295302e-05,
      "loss": 0.7184,
      "step": 11310
    },
    {
      "epoch": 0.4528,
      "grad_norm": 3.3568403720855713,
      "learning_rate": 4.2738255033557054e-05,
      "loss": 0.6785,
      "step": 11320
    },
    {
      "epoch": 0.4532,
      "grad_norm": 2.489518642425537,
      "learning_rate": 4.2731543624161076e-05,
      "loss": 0.7513,
      "step": 11330
    },
    {
      "epoch": 0.4536,
      "grad_norm": 2.5124735832214355,
      "learning_rate": 4.2724832214765104e-05,
      "loss": 0.6858,
      "step": 11340
    },
    {
      "epoch": 0.454,
      "grad_norm": 2.6239614486694336,
      "learning_rate": 4.2718120805369126e-05,
      "loss": 0.8111,
      "step": 11350
    },
    {
      "epoch": 0.4544,
      "grad_norm": 2.4432437419891357,
      "learning_rate": 4.2711409395973155e-05,
      "loss": 0.7184,
      "step": 11360
    },
    {
      "epoch": 0.4548,
      "grad_norm": 2.743807077407837,
      "learning_rate": 4.270469798657718e-05,
      "loss": 0.6593,
      "step": 11370
    },
    {
      "epoch": 0.4552,
      "grad_norm": 3.090569019317627,
      "learning_rate": 4.269798657718121e-05,
      "loss": 0.5885,
      "step": 11380
    },
    {
      "epoch": 0.4556,
      "grad_norm": 2.413336992263794,
      "learning_rate": 4.269127516778524e-05,
      "loss": 0.6754,
      "step": 11390
    },
    {
      "epoch": 0.456,
      "grad_norm": 2.8763022422790527,
      "learning_rate": 4.268456375838926e-05,
      "loss": 0.7338,
      "step": 11400
    },
    {
      "epoch": 0.4564,
      "grad_norm": 2.9722461700439453,
      "learning_rate": 4.267785234899329e-05,
      "loss": 0.6688,
      "step": 11410
    },
    {
      "epoch": 0.4568,
      "grad_norm": 3.1977756023406982,
      "learning_rate": 4.267114093959731e-05,
      "loss": 0.6549,
      "step": 11420
    },
    {
      "epoch": 0.4572,
      "grad_norm": 3.170464515686035,
      "learning_rate": 4.266442953020134e-05,
      "loss": 0.6272,
      "step": 11430
    },
    {
      "epoch": 0.4576,
      "grad_norm": 3.346452474594116,
      "learning_rate": 4.2657718120805376e-05,
      "loss": 0.5714,
      "step": 11440
    },
    {
      "epoch": 0.458,
      "grad_norm": 2.305863857269287,
      "learning_rate": 4.26510067114094e-05,
      "loss": 0.717,
      "step": 11450
    },
    {
      "epoch": 0.4584,
      "grad_norm": 3.350574493408203,
      "learning_rate": 4.2644295302013427e-05,
      "loss": 0.7334,
      "step": 11460
    },
    {
      "epoch": 0.4588,
      "grad_norm": 2.280362844467163,
      "learning_rate": 4.263758389261745e-05,
      "loss": 0.5478,
      "step": 11470
    },
    {
      "epoch": 0.4592,
      "grad_norm": 3.041353702545166,
      "learning_rate": 4.263087248322148e-05,
      "loss": 0.6557,
      "step": 11480
    },
    {
      "epoch": 0.4596,
      "grad_norm": 3.1770589351654053,
      "learning_rate": 4.2624161073825505e-05,
      "loss": 0.6818,
      "step": 11490
    },
    {
      "epoch": 0.46,
      "grad_norm": 3.082477331161499,
      "learning_rate": 4.2617449664429534e-05,
      "loss": 0.6286,
      "step": 11500
    },
    {
      "epoch": 0.4604,
      "grad_norm": 3.921769857406616,
      "learning_rate": 4.261073825503356e-05,
      "loss": 0.7292,
      "step": 11510
    },
    {
      "epoch": 0.4608,
      "grad_norm": 3.045888900756836,
      "learning_rate": 4.2604026845637584e-05,
      "loss": 0.6604,
      "step": 11520
    },
    {
      "epoch": 0.4612,
      "grad_norm": 2.7125418186187744,
      "learning_rate": 4.259731543624161e-05,
      "loss": 0.6498,
      "step": 11530
    },
    {
      "epoch": 0.4616,
      "grad_norm": 2.8345260620117188,
      "learning_rate": 4.2590604026845635e-05,
      "loss": 0.6383,
      "step": 11540
    },
    {
      "epoch": 0.462,
      "grad_norm": 2.7811172008514404,
      "learning_rate": 4.258389261744967e-05,
      "loss": 0.5603,
      "step": 11550
    },
    {
      "epoch": 0.4624,
      "grad_norm": 2.005525588989258,
      "learning_rate": 4.25771812080537e-05,
      "loss": 0.6477,
      "step": 11560
    },
    {
      "epoch": 0.4628,
      "grad_norm": 2.74569034576416,
      "learning_rate": 4.257046979865772e-05,
      "loss": 0.6331,
      "step": 11570
    },
    {
      "epoch": 0.4632,
      "grad_norm": 2.7275912761688232,
      "learning_rate": 4.256375838926175e-05,
      "loss": 0.7033,
      "step": 11580
    },
    {
      "epoch": 0.4636,
      "grad_norm": 3.1423120498657227,
      "learning_rate": 4.255704697986577e-05,
      "loss": 0.7698,
      "step": 11590
    },
    {
      "epoch": 0.464,
      "grad_norm": 2.959864616394043,
      "learning_rate": 4.25503355704698e-05,
      "loss": 0.8154,
      "step": 11600
    },
    {
      "epoch": 0.4644,
      "grad_norm": 3.069382905960083,
      "learning_rate": 4.254362416107383e-05,
      "loss": 0.6435,
      "step": 11610
    },
    {
      "epoch": 0.4648,
      "grad_norm": 3.154121160507202,
      "learning_rate": 4.2536912751677856e-05,
      "loss": 0.5892,
      "step": 11620
    },
    {
      "epoch": 0.4652,
      "grad_norm": 3.264619827270508,
      "learning_rate": 4.2530201342281885e-05,
      "loss": 0.6644,
      "step": 11630
    },
    {
      "epoch": 0.4656,
      "grad_norm": 2.8192851543426514,
      "learning_rate": 4.2523489932885907e-05,
      "loss": 0.681,
      "step": 11640
    },
    {
      "epoch": 0.466,
      "grad_norm": 2.4391825199127197,
      "learning_rate": 4.2516778523489935e-05,
      "loss": 0.601,
      "step": 11650
    },
    {
      "epoch": 0.4664,
      "grad_norm": 2.6535189151763916,
      "learning_rate": 4.251006711409396e-05,
      "loss": 0.6521,
      "step": 11660
    },
    {
      "epoch": 0.4668,
      "grad_norm": 3.0303287506103516,
      "learning_rate": 4.250335570469799e-05,
      "loss": 0.6285,
      "step": 11670
    },
    {
      "epoch": 0.4672,
      "grad_norm": 3.08133602142334,
      "learning_rate": 4.2496644295302014e-05,
      "loss": 0.6469,
      "step": 11680
    },
    {
      "epoch": 0.4676,
      "grad_norm": 3.9458680152893066,
      "learning_rate": 4.248993288590604e-05,
      "loss": 0.7432,
      "step": 11690
    },
    {
      "epoch": 0.468,
      "grad_norm": 2.8255813121795654,
      "learning_rate": 4.248322147651007e-05,
      "loss": 0.5707,
      "step": 11700
    },
    {
      "epoch": 0.4684,
      "grad_norm": 3.01556396484375,
      "learning_rate": 4.247651006711409e-05,
      "loss": 0.6632,
      "step": 11710
    },
    {
      "epoch": 0.4688,
      "grad_norm": 2.782303810119629,
      "learning_rate": 4.246979865771812e-05,
      "loss": 0.6819,
      "step": 11720
    },
    {
      "epoch": 0.4692,
      "grad_norm": 3.9307005405426025,
      "learning_rate": 4.246308724832215e-05,
      "loss": 0.7114,
      "step": 11730
    },
    {
      "epoch": 0.4696,
      "grad_norm": 2.3080759048461914,
      "learning_rate": 4.245637583892618e-05,
      "loss": 0.6627,
      "step": 11740
    },
    {
      "epoch": 0.47,
      "grad_norm": 3.150674343109131,
      "learning_rate": 4.244966442953021e-05,
      "loss": 0.6246,
      "step": 11750
    },
    {
      "epoch": 0.4704,
      "grad_norm": 2.924485206604004,
      "learning_rate": 4.244295302013423e-05,
      "loss": 0.6024,
      "step": 11760
    },
    {
      "epoch": 0.4708,
      "grad_norm": 2.9584133625030518,
      "learning_rate": 4.243624161073826e-05,
      "loss": 0.592,
      "step": 11770
    },
    {
      "epoch": 0.4712,
      "grad_norm": 2.6011345386505127,
      "learning_rate": 4.242953020134228e-05,
      "loss": 0.5855,
      "step": 11780
    },
    {
      "epoch": 0.4716,
      "grad_norm": 2.66503643989563,
      "learning_rate": 4.2422818791946314e-05,
      "loss": 0.734,
      "step": 11790
    },
    {
      "epoch": 0.472,
      "grad_norm": 2.3867557048797607,
      "learning_rate": 4.2416107382550336e-05,
      "loss": 0.6334,
      "step": 11800
    },
    {
      "epoch": 0.4724,
      "grad_norm": 3.0284063816070557,
      "learning_rate": 4.2409395973154365e-05,
      "loss": 0.7277,
      "step": 11810
    },
    {
      "epoch": 0.4728,
      "grad_norm": 3.5104172229766846,
      "learning_rate": 4.240268456375839e-05,
      "loss": 0.6615,
      "step": 11820
    },
    {
      "epoch": 0.4732,
      "grad_norm": 2.5222253799438477,
      "learning_rate": 4.2395973154362415e-05,
      "loss": 0.6531,
      "step": 11830
    },
    {
      "epoch": 0.4736,
      "grad_norm": 2.918550491333008,
      "learning_rate": 4.2389261744966444e-05,
      "loss": 0.7109,
      "step": 11840
    },
    {
      "epoch": 0.474,
      "grad_norm": 3.3829188346862793,
      "learning_rate": 4.238255033557047e-05,
      "loss": 0.682,
      "step": 11850
    },
    {
      "epoch": 0.4744,
      "grad_norm": 2.947268009185791,
      "learning_rate": 4.23758389261745e-05,
      "loss": 0.5762,
      "step": 11860
    },
    {
      "epoch": 0.4748,
      "grad_norm": 3.5326411724090576,
      "learning_rate": 4.236912751677852e-05,
      "loss": 0.7144,
      "step": 11870
    },
    {
      "epoch": 0.4752,
      "grad_norm": 3.6179120540618896,
      "learning_rate": 4.236241610738255e-05,
      "loss": 0.7064,
      "step": 11880
    },
    {
      "epoch": 0.4756,
      "grad_norm": 2.820882797241211,
      "learning_rate": 4.235570469798658e-05,
      "loss": 0.7268,
      "step": 11890
    },
    {
      "epoch": 0.476,
      "grad_norm": 3.3277857303619385,
      "learning_rate": 4.234899328859061e-05,
      "loss": 0.766,
      "step": 11900
    },
    {
      "epoch": 0.4764,
      "grad_norm": 2.451504707336426,
      "learning_rate": 4.234228187919464e-05,
      "loss": 0.6599,
      "step": 11910
    },
    {
      "epoch": 0.4768,
      "grad_norm": 2.7629997730255127,
      "learning_rate": 4.233557046979866e-05,
      "loss": 0.7227,
      "step": 11920
    },
    {
      "epoch": 0.4772,
      "grad_norm": 3.012280225753784,
      "learning_rate": 4.232885906040269e-05,
      "loss": 0.699,
      "step": 11930
    },
    {
      "epoch": 0.4776,
      "grad_norm": 2.6244277954101562,
      "learning_rate": 4.2322147651006716e-05,
      "loss": 0.7526,
      "step": 11940
    },
    {
      "epoch": 0.478,
      "grad_norm": 3.3826000690460205,
      "learning_rate": 4.231543624161074e-05,
      "loss": 0.6868,
      "step": 11950
    },
    {
      "epoch": 0.4784,
      "grad_norm": 2.807870626449585,
      "learning_rate": 4.2308724832214766e-05,
      "loss": 0.7093,
      "step": 11960
    },
    {
      "epoch": 0.4788,
      "grad_norm": 3.111165761947632,
      "learning_rate": 4.2302013422818794e-05,
      "loss": 0.6058,
      "step": 11970
    },
    {
      "epoch": 0.4792,
      "grad_norm": 2.787006139755249,
      "learning_rate": 4.229530201342282e-05,
      "loss": 0.666,
      "step": 11980
    },
    {
      "epoch": 0.4796,
      "grad_norm": 3.6593987941741943,
      "learning_rate": 4.2288590604026845e-05,
      "loss": 0.6217,
      "step": 11990
    },
    {
      "epoch": 0.48,
      "grad_norm": 2.240966558456421,
      "learning_rate": 4.228187919463087e-05,
      "loss": 0.6397,
      "step": 12000
    },
    {
      "epoch": 0.4804,
      "grad_norm": 2.5845022201538086,
      "learning_rate": 4.22751677852349e-05,
      "loss": 0.5897,
      "step": 12010
    },
    {
      "epoch": 0.4808,
      "grad_norm": 3.075031280517578,
      "learning_rate": 4.226845637583893e-05,
      "loss": 0.6125,
      "step": 12020
    },
    {
      "epoch": 0.4812,
      "grad_norm": 2.872205972671509,
      "learning_rate": 4.226174496644296e-05,
      "loss": 0.6398,
      "step": 12030
    },
    {
      "epoch": 0.4816,
      "grad_norm": 3.4873619079589844,
      "learning_rate": 4.225503355704698e-05,
      "loss": 0.7323,
      "step": 12040
    },
    {
      "epoch": 0.482,
      "grad_norm": 2.7586491107940674,
      "learning_rate": 4.224832214765101e-05,
      "loss": 0.7196,
      "step": 12050
    },
    {
      "epoch": 0.4824,
      "grad_norm": 3.3393542766571045,
      "learning_rate": 4.224161073825503e-05,
      "loss": 0.8085,
      "step": 12060
    },
    {
      "epoch": 0.4828,
      "grad_norm": 2.9035494327545166,
      "learning_rate": 4.223489932885906e-05,
      "loss": 0.6597,
      "step": 12070
    },
    {
      "epoch": 0.4832,
      "grad_norm": 3.148160219192505,
      "learning_rate": 4.222818791946309e-05,
      "loss": 0.7275,
      "step": 12080
    },
    {
      "epoch": 0.4836,
      "grad_norm": 2.9875314235687256,
      "learning_rate": 4.2221476510067117e-05,
      "loss": 0.677,
      "step": 12090
    },
    {
      "epoch": 0.484,
      "grad_norm": 2.6691436767578125,
      "learning_rate": 4.2214765100671145e-05,
      "loss": 0.6611,
      "step": 12100
    },
    {
      "epoch": 0.4844,
      "grad_norm": 2.468021869659424,
      "learning_rate": 4.220805369127517e-05,
      "loss": 0.6596,
      "step": 12110
    },
    {
      "epoch": 0.4848,
      "grad_norm": 3.0495004653930664,
      "learning_rate": 4.2201342281879195e-05,
      "loss": 0.7318,
      "step": 12120
    },
    {
      "epoch": 0.4852,
      "grad_norm": 2.9175171852111816,
      "learning_rate": 4.2194630872483224e-05,
      "loss": 0.6935,
      "step": 12130
    },
    {
      "epoch": 0.4856,
      "grad_norm": 2.674633264541626,
      "learning_rate": 4.218791946308725e-05,
      "loss": 0.7124,
      "step": 12140
    },
    {
      "epoch": 0.486,
      "grad_norm": 2.3267791271209717,
      "learning_rate": 4.218120805369128e-05,
      "loss": 0.7299,
      "step": 12150
    },
    {
      "epoch": 0.4864,
      "grad_norm": 2.9314377307891846,
      "learning_rate": 4.21744966442953e-05,
      "loss": 0.6899,
      "step": 12160
    },
    {
      "epoch": 0.4868,
      "grad_norm": 2.9026999473571777,
      "learning_rate": 4.216778523489933e-05,
      "loss": 0.666,
      "step": 12170
    },
    {
      "epoch": 0.4872,
      "grad_norm": 2.9922685623168945,
      "learning_rate": 4.216107382550335e-05,
      "loss": 0.6251,
      "step": 12180
    },
    {
      "epoch": 0.4876,
      "grad_norm": 2.163834571838379,
      "learning_rate": 4.215436241610738e-05,
      "loss": 0.6276,
      "step": 12190
    },
    {
      "epoch": 0.488,
      "grad_norm": 3.155103921890259,
      "learning_rate": 4.214765100671142e-05,
      "loss": 0.767,
      "step": 12200
    },
    {
      "epoch": 0.4884,
      "grad_norm": 2.5379412174224854,
      "learning_rate": 4.214093959731544e-05,
      "loss": 0.5919,
      "step": 12210
    },
    {
      "epoch": 0.4888,
      "grad_norm": 3.3839290142059326,
      "learning_rate": 4.213422818791947e-05,
      "loss": 0.6664,
      "step": 12220
    },
    {
      "epoch": 0.4892,
      "grad_norm": 3.4114902019500732,
      "learning_rate": 4.212751677852349e-05,
      "loss": 0.6656,
      "step": 12230
    },
    {
      "epoch": 0.4896,
      "grad_norm": 3.235342264175415,
      "learning_rate": 4.212080536912752e-05,
      "loss": 0.6138,
      "step": 12240
    },
    {
      "epoch": 0.49,
      "grad_norm": 2.5118086338043213,
      "learning_rate": 4.2114093959731546e-05,
      "loss": 0.6166,
      "step": 12250
    },
    {
      "epoch": 0.4904,
      "grad_norm": 2.2850592136383057,
      "learning_rate": 4.2107382550335575e-05,
      "loss": 0.5538,
      "step": 12260
    },
    {
      "epoch": 0.4908,
      "grad_norm": 2.5446248054504395,
      "learning_rate": 4.21006711409396e-05,
      "loss": 0.6517,
      "step": 12270
    },
    {
      "epoch": 0.4912,
      "grad_norm": 2.7787327766418457,
      "learning_rate": 4.2093959731543625e-05,
      "loss": 0.7757,
      "step": 12280
    },
    {
      "epoch": 0.4916,
      "grad_norm": 2.880540132522583,
      "learning_rate": 4.2087248322147654e-05,
      "loss": 0.6345,
      "step": 12290
    },
    {
      "epoch": 0.492,
      "grad_norm": 2.6339845657348633,
      "learning_rate": 4.2080536912751675e-05,
      "loss": 0.6467,
      "step": 12300
    },
    {
      "epoch": 0.4924,
      "grad_norm": 3.721351146697998,
      "learning_rate": 4.2073825503355704e-05,
      "loss": 0.6633,
      "step": 12310
    },
    {
      "epoch": 0.4928,
      "grad_norm": 2.5170814990997314,
      "learning_rate": 4.206711409395973e-05,
      "loss": 0.6824,
      "step": 12320
    },
    {
      "epoch": 0.4932,
      "grad_norm": 2.811676502227783,
      "learning_rate": 4.206040268456376e-05,
      "loss": 0.6657,
      "step": 12330
    },
    {
      "epoch": 0.4936,
      "grad_norm": 2.8941338062286377,
      "learning_rate": 4.205369127516779e-05,
      "loss": 0.6724,
      "step": 12340
    },
    {
      "epoch": 0.494,
      "grad_norm": 3.440402030944824,
      "learning_rate": 4.204697986577181e-05,
      "loss": 0.5712,
      "step": 12350
    },
    {
      "epoch": 0.4944,
      "grad_norm": 2.6278741359710693,
      "learning_rate": 4.204026845637584e-05,
      "loss": 0.724,
      "step": 12360
    },
    {
      "epoch": 0.4948,
      "grad_norm": 3.0148561000823975,
      "learning_rate": 4.203355704697987e-05,
      "loss": 0.672,
      "step": 12370
    },
    {
      "epoch": 0.4952,
      "grad_norm": 2.292038679122925,
      "learning_rate": 4.20268456375839e-05,
      "loss": 0.6789,
      "step": 12380
    },
    {
      "epoch": 0.4956,
      "grad_norm": 3.2420084476470947,
      "learning_rate": 4.2020134228187926e-05,
      "loss": 0.6826,
      "step": 12390
    },
    {
      "epoch": 0.496,
      "grad_norm": 3.228461503982544,
      "learning_rate": 4.201342281879195e-05,
      "loss": 0.6935,
      "step": 12400
    },
    {
      "epoch": 0.4964,
      "grad_norm": 2.4144864082336426,
      "learning_rate": 4.2006711409395976e-05,
      "loss": 0.6715,
      "step": 12410
    },
    {
      "epoch": 0.4968,
      "grad_norm": 3.7628977298736572,
      "learning_rate": 4.2e-05,
      "loss": 0.6392,
      "step": 12420
    },
    {
      "epoch": 0.4972,
      "grad_norm": 3.2715110778808594,
      "learning_rate": 4.199328859060403e-05,
      "loss": 0.6535,
      "step": 12430
    },
    {
      "epoch": 0.4976,
      "grad_norm": 2.4445626735687256,
      "learning_rate": 4.1986577181208055e-05,
      "loss": 0.6421,
      "step": 12440
    },
    {
      "epoch": 0.498,
      "grad_norm": 2.3125972747802734,
      "learning_rate": 4.197986577181208e-05,
      "loss": 0.6153,
      "step": 12450
    },
    {
      "epoch": 0.4984,
      "grad_norm": 2.1189610958099365,
      "learning_rate": 4.197315436241611e-05,
      "loss": 0.6357,
      "step": 12460
    },
    {
      "epoch": 0.4988,
      "grad_norm": 2.3306002616882324,
      "learning_rate": 4.1966442953020134e-05,
      "loss": 0.6574,
      "step": 12470
    },
    {
      "epoch": 0.4992,
      "grad_norm": 2.8506863117218018,
      "learning_rate": 4.195973154362416e-05,
      "loss": 0.6805,
      "step": 12480
    },
    {
      "epoch": 0.4996,
      "grad_norm": 2.701921224594116,
      "learning_rate": 4.195302013422819e-05,
      "loss": 0.7037,
      "step": 12490
    },
    {
      "epoch": 0.5,
      "grad_norm": 2.9851319789886475,
      "learning_rate": 4.194630872483222e-05,
      "loss": 0.7368,
      "step": 12500
    },
    {
      "epoch": 0.5004,
      "grad_norm": 3.481994390487671,
      "learning_rate": 4.193959731543624e-05,
      "loss": 0.7225,
      "step": 12510
    },
    {
      "epoch": 0.5008,
      "grad_norm": 3.4266114234924316,
      "learning_rate": 4.193288590604027e-05,
      "loss": 0.7191,
      "step": 12520
    },
    {
      "epoch": 0.5012,
      "grad_norm": 2.035188674926758,
      "learning_rate": 4.19261744966443e-05,
      "loss": 0.641,
      "step": 12530
    },
    {
      "epoch": 0.5016,
      "grad_norm": 2.5936689376831055,
      "learning_rate": 4.191946308724832e-05,
      "loss": 0.7499,
      "step": 12540
    },
    {
      "epoch": 0.502,
      "grad_norm": 3.187518358230591,
      "learning_rate": 4.1912751677852355e-05,
      "loss": 0.7202,
      "step": 12550
    },
    {
      "epoch": 0.5024,
      "grad_norm": 3.654719352722168,
      "learning_rate": 4.190604026845638e-05,
      "loss": 0.6795,
      "step": 12560
    },
    {
      "epoch": 0.5028,
      "grad_norm": 2.583889961242676,
      "learning_rate": 4.1899328859060406e-05,
      "loss": 0.6533,
      "step": 12570
    },
    {
      "epoch": 0.5032,
      "grad_norm": 3.0174992084503174,
      "learning_rate": 4.1892617449664434e-05,
      "loss": 0.6963,
      "step": 12580
    },
    {
      "epoch": 0.5036,
      "grad_norm": 2.697005033493042,
      "learning_rate": 4.1885906040268456e-05,
      "loss": 0.5748,
      "step": 12590
    },
    {
      "epoch": 0.504,
      "grad_norm": 2.9044747352600098,
      "learning_rate": 4.1879194630872484e-05,
      "loss": 0.5458,
      "step": 12600
    },
    {
      "epoch": 0.5044,
      "grad_norm": 3.169250965118408,
      "learning_rate": 4.187248322147651e-05,
      "loss": 0.5932,
      "step": 12610
    },
    {
      "epoch": 0.5048,
      "grad_norm": 2.824922800064087,
      "learning_rate": 4.186577181208054e-05,
      "loss": 0.6674,
      "step": 12620
    },
    {
      "epoch": 0.5052,
      "grad_norm": 3.689929723739624,
      "learning_rate": 4.185906040268456e-05,
      "loss": 0.6803,
      "step": 12630
    },
    {
      "epoch": 0.5056,
      "grad_norm": 3.7105441093444824,
      "learning_rate": 4.185234899328859e-05,
      "loss": 0.6857,
      "step": 12640
    },
    {
      "epoch": 0.506,
      "grad_norm": 2.3943371772766113,
      "learning_rate": 4.184563758389262e-05,
      "loss": 0.5789,
      "step": 12650
    },
    {
      "epoch": 0.5064,
      "grad_norm": 2.9567768573760986,
      "learning_rate": 4.183892617449664e-05,
      "loss": 0.6515,
      "step": 12660
    },
    {
      "epoch": 0.5068,
      "grad_norm": 3.271754026412964,
      "learning_rate": 4.183221476510068e-05,
      "loss": 0.7142,
      "step": 12670
    },
    {
      "epoch": 0.5072,
      "grad_norm": 3.347855567932129,
      "learning_rate": 4.18255033557047e-05,
      "loss": 0.6956,
      "step": 12680
    },
    {
      "epoch": 0.5076,
      "grad_norm": 3.1327743530273438,
      "learning_rate": 4.181879194630873e-05,
      "loss": 0.7806,
      "step": 12690
    },
    {
      "epoch": 0.508,
      "grad_norm": 2.8657302856445312,
      "learning_rate": 4.181208053691275e-05,
      "loss": 0.6717,
      "step": 12700
    },
    {
      "epoch": 0.5084,
      "grad_norm": 3.6400527954101562,
      "learning_rate": 4.180536912751678e-05,
      "loss": 0.6692,
      "step": 12710
    },
    {
      "epoch": 0.5088,
      "grad_norm": 2.418506622314453,
      "learning_rate": 4.179865771812081e-05,
      "loss": 0.7298,
      "step": 12720
    },
    {
      "epoch": 0.5092,
      "grad_norm": 3.271184206008911,
      "learning_rate": 4.1791946308724835e-05,
      "loss": 0.7389,
      "step": 12730
    },
    {
      "epoch": 0.5096,
      "grad_norm": 3.2523715496063232,
      "learning_rate": 4.1785234899328864e-05,
      "loss": 0.6657,
      "step": 12740
    },
    {
      "epoch": 0.51,
      "grad_norm": 1.8008517026901245,
      "learning_rate": 4.1778523489932886e-05,
      "loss": 0.6642,
      "step": 12750
    },
    {
      "epoch": 0.5104,
      "grad_norm": 3.74755597114563,
      "learning_rate": 4.1771812080536914e-05,
      "loss": 0.7461,
      "step": 12760
    },
    {
      "epoch": 0.5108,
      "grad_norm": 2.533442974090576,
      "learning_rate": 4.176510067114094e-05,
      "loss": 0.6055,
      "step": 12770
    },
    {
      "epoch": 0.5112,
      "grad_norm": 2.912459373474121,
      "learning_rate": 4.175838926174497e-05,
      "loss": 0.7389,
      "step": 12780
    },
    {
      "epoch": 0.5116,
      "grad_norm": 2.5629396438598633,
      "learning_rate": 4.1751677852349e-05,
      "loss": 0.7532,
      "step": 12790
    },
    {
      "epoch": 0.512,
      "grad_norm": 2.8226284980773926,
      "learning_rate": 4.174496644295302e-05,
      "loss": 0.6736,
      "step": 12800
    },
    {
      "epoch": 0.5124,
      "grad_norm": 3.9332475662231445,
      "learning_rate": 4.173825503355705e-05,
      "loss": 0.7697,
      "step": 12810
    },
    {
      "epoch": 0.5128,
      "grad_norm": 2.4943301677703857,
      "learning_rate": 4.173154362416107e-05,
      "loss": 0.5675,
      "step": 12820
    },
    {
      "epoch": 0.5132,
      "grad_norm": 2.5289056301116943,
      "learning_rate": 4.17248322147651e-05,
      "loss": 0.588,
      "step": 12830
    },
    {
      "epoch": 0.5136,
      "grad_norm": 2.76655650138855,
      "learning_rate": 4.171812080536913e-05,
      "loss": 0.671,
      "step": 12840
    },
    {
      "epoch": 0.514,
      "grad_norm": 3.119985342025757,
      "learning_rate": 4.171140939597316e-05,
      "loss": 0.751,
      "step": 12850
    },
    {
      "epoch": 0.5144,
      "grad_norm": 3.1363723278045654,
      "learning_rate": 4.1704697986577186e-05,
      "loss": 0.6302,
      "step": 12860
    },
    {
      "epoch": 0.5148,
      "grad_norm": 3.2723093032836914,
      "learning_rate": 4.169798657718121e-05,
      "loss": 0.6478,
      "step": 12870
    },
    {
      "epoch": 0.5152,
      "grad_norm": 2.5362231731414795,
      "learning_rate": 4.1691275167785236e-05,
      "loss": 0.6261,
      "step": 12880
    },
    {
      "epoch": 0.5156,
      "grad_norm": 2.4476096630096436,
      "learning_rate": 4.168456375838926e-05,
      "loss": 0.5858,
      "step": 12890
    },
    {
      "epoch": 0.516,
      "grad_norm": 3.541069507598877,
      "learning_rate": 4.1677852348993293e-05,
      "loss": 0.7253,
      "step": 12900
    },
    {
      "epoch": 0.5164,
      "grad_norm": 3.115278482437134,
      "learning_rate": 4.167114093959732e-05,
      "loss": 0.743,
      "step": 12910
    },
    {
      "epoch": 0.5168,
      "grad_norm": 2.4163131713867188,
      "learning_rate": 4.1664429530201344e-05,
      "loss": 0.643,
      "step": 12920
    },
    {
      "epoch": 0.5172,
      "grad_norm": 2.458488702774048,
      "learning_rate": 4.165771812080537e-05,
      "loss": 0.6286,
      "step": 12930
    },
    {
      "epoch": 0.5176,
      "grad_norm": 3.0070507526397705,
      "learning_rate": 4.1651006711409394e-05,
      "loss": 0.6596,
      "step": 12940
    },
    {
      "epoch": 0.518,
      "grad_norm": 2.1457250118255615,
      "learning_rate": 4.164429530201342e-05,
      "loss": 0.554,
      "step": 12950
    },
    {
      "epoch": 0.5184,
      "grad_norm": 2.5538315773010254,
      "learning_rate": 4.163758389261745e-05,
      "loss": 0.7073,
      "step": 12960
    },
    {
      "epoch": 0.5188,
      "grad_norm": 2.862097978591919,
      "learning_rate": 4.163087248322148e-05,
      "loss": 0.7471,
      "step": 12970
    },
    {
      "epoch": 0.5192,
      "grad_norm": 3.343538761138916,
      "learning_rate": 4.162416107382551e-05,
      "loss": 0.6964,
      "step": 12980
    },
    {
      "epoch": 0.5196,
      "grad_norm": 3.196449041366577,
      "learning_rate": 4.161744966442953e-05,
      "loss": 0.7064,
      "step": 12990
    },
    {
      "epoch": 0.52,
      "grad_norm": 3.198267698287964,
      "learning_rate": 4.161073825503356e-05,
      "loss": 0.6535,
      "step": 13000
    },
    {
      "epoch": 0.5204,
      "grad_norm": 2.7921299934387207,
      "learning_rate": 4.160402684563758e-05,
      "loss": 0.701,
      "step": 13010
    },
    {
      "epoch": 0.5208,
      "grad_norm": 3.532053232192993,
      "learning_rate": 4.1597315436241616e-05,
      "loss": 0.5908,
      "step": 13020
    },
    {
      "epoch": 0.5212,
      "grad_norm": 3.15445613861084,
      "learning_rate": 4.1590604026845644e-05,
      "loss": 0.65,
      "step": 13030
    },
    {
      "epoch": 0.5216,
      "grad_norm": 2.7793962955474854,
      "learning_rate": 4.1583892617449666e-05,
      "loss": 0.5134,
      "step": 13040
    },
    {
      "epoch": 0.522,
      "grad_norm": 2.742110252380371,
      "learning_rate": 4.1577181208053695e-05,
      "loss": 0.7241,
      "step": 13050
    },
    {
      "epoch": 0.5224,
      "grad_norm": 3.482818365097046,
      "learning_rate": 4.1570469798657716e-05,
      "loss": 0.6469,
      "step": 13060
    },
    {
      "epoch": 0.5228,
      "grad_norm": 2.621640205383301,
      "learning_rate": 4.1563758389261745e-05,
      "loss": 0.7007,
      "step": 13070
    },
    {
      "epoch": 0.5232,
      "grad_norm": 2.716034412384033,
      "learning_rate": 4.155704697986577e-05,
      "loss": 0.717,
      "step": 13080
    },
    {
      "epoch": 0.5236,
      "grad_norm": 2.415011167526245,
      "learning_rate": 4.15503355704698e-05,
      "loss": 0.7265,
      "step": 13090
    },
    {
      "epoch": 0.524,
      "grad_norm": 3.1066551208496094,
      "learning_rate": 4.154362416107383e-05,
      "loss": 0.6961,
      "step": 13100
    },
    {
      "epoch": 0.5244,
      "grad_norm": 2.432537078857422,
      "learning_rate": 4.153691275167785e-05,
      "loss": 0.7067,
      "step": 13110
    },
    {
      "epoch": 0.5248,
      "grad_norm": 3.1167070865631104,
      "learning_rate": 4.153020134228188e-05,
      "loss": 0.672,
      "step": 13120
    },
    {
      "epoch": 0.5252,
      "grad_norm": 3.0092885494232178,
      "learning_rate": 4.152348993288591e-05,
      "loss": 0.6101,
      "step": 13130
    },
    {
      "epoch": 0.5256,
      "grad_norm": 3.332148313522339,
      "learning_rate": 4.151677852348994e-05,
      "loss": 0.707,
      "step": 13140
    },
    {
      "epoch": 0.526,
      "grad_norm": 2.268836259841919,
      "learning_rate": 4.151006711409396e-05,
      "loss": 0.711,
      "step": 13150
    },
    {
      "epoch": 0.5264,
      "grad_norm": 2.4304983615875244,
      "learning_rate": 4.150335570469799e-05,
      "loss": 0.6313,
      "step": 13160
    },
    {
      "epoch": 0.5268,
      "grad_norm": 3.037510395050049,
      "learning_rate": 4.149664429530202e-05,
      "loss": 0.6687,
      "step": 13170
    },
    {
      "epoch": 0.5272,
      "grad_norm": 3.4334781169891357,
      "learning_rate": 4.148993288590604e-05,
      "loss": 0.6791,
      "step": 13180
    },
    {
      "epoch": 0.5276,
      "grad_norm": 3.2828245162963867,
      "learning_rate": 4.148322147651007e-05,
      "loss": 0.6598,
      "step": 13190
    },
    {
      "epoch": 0.528,
      "grad_norm": 2.3563179969787598,
      "learning_rate": 4.1476510067114096e-05,
      "loss": 0.6613,
      "step": 13200
    },
    {
      "epoch": 0.5284,
      "grad_norm": 2.8067402839660645,
      "learning_rate": 4.1469798657718124e-05,
      "loss": 0.55,
      "step": 13210
    },
    {
      "epoch": 0.5288,
      "grad_norm": 4.04012393951416,
      "learning_rate": 4.146308724832215e-05,
      "loss": 0.6849,
      "step": 13220
    },
    {
      "epoch": 0.5292,
      "grad_norm": 2.80513858795166,
      "learning_rate": 4.1456375838926174e-05,
      "loss": 0.6315,
      "step": 13230
    },
    {
      "epoch": 0.5296,
      "grad_norm": 2.3976359367370605,
      "learning_rate": 4.14496644295302e-05,
      "loss": 0.7274,
      "step": 13240
    },
    {
      "epoch": 0.53,
      "grad_norm": 2.1781411170959473,
      "learning_rate": 4.144295302013423e-05,
      "loss": 0.7375,
      "step": 13250
    },
    {
      "epoch": 0.5304,
      "grad_norm": 3.2483832836151123,
      "learning_rate": 4.143624161073826e-05,
      "loss": 0.58,
      "step": 13260
    },
    {
      "epoch": 0.5308,
      "grad_norm": 2.5512444972991943,
      "learning_rate": 4.142953020134228e-05,
      "loss": 0.6521,
      "step": 13270
    },
    {
      "epoch": 0.5312,
      "grad_norm": 3.5189731121063232,
      "learning_rate": 4.142281879194631e-05,
      "loss": 0.8342,
      "step": 13280
    },
    {
      "epoch": 0.5316,
      "grad_norm": 3.236978054046631,
      "learning_rate": 4.141610738255034e-05,
      "loss": 0.689,
      "step": 13290
    },
    {
      "epoch": 0.532,
      "grad_norm": 2.9251575469970703,
      "learning_rate": 4.140939597315436e-05,
      "loss": 0.6141,
      "step": 13300
    },
    {
      "epoch": 0.5324,
      "grad_norm": 3.4061026573181152,
      "learning_rate": 4.140268456375839e-05,
      "loss": 0.6374,
      "step": 13310
    },
    {
      "epoch": 0.5328,
      "grad_norm": 2.9198389053344727,
      "learning_rate": 4.139597315436242e-05,
      "loss": 0.6141,
      "step": 13320
    },
    {
      "epoch": 0.5332,
      "grad_norm": 2.8550968170166016,
      "learning_rate": 4.1389261744966446e-05,
      "loss": 0.7338,
      "step": 13330
    },
    {
      "epoch": 0.5336,
      "grad_norm": 3.233196496963501,
      "learning_rate": 4.138255033557047e-05,
      "loss": 0.6998,
      "step": 13340
    },
    {
      "epoch": 0.534,
      "grad_norm": 2.6822361946105957,
      "learning_rate": 4.13758389261745e-05,
      "loss": 0.6546,
      "step": 13350
    },
    {
      "epoch": 0.5344,
      "grad_norm": 3.8319461345672607,
      "learning_rate": 4.1369127516778525e-05,
      "loss": 0.609,
      "step": 13360
    },
    {
      "epoch": 0.5348,
      "grad_norm": 2.6695683002471924,
      "learning_rate": 4.1362416107382554e-05,
      "loss": 0.6393,
      "step": 13370
    },
    {
      "epoch": 0.5352,
      "grad_norm": 2.728753089904785,
      "learning_rate": 4.135570469798658e-05,
      "loss": 0.7138,
      "step": 13380
    },
    {
      "epoch": 0.5356,
      "grad_norm": 4.047537326812744,
      "learning_rate": 4.1348993288590604e-05,
      "loss": 0.6234,
      "step": 13390
    },
    {
      "epoch": 0.536,
      "grad_norm": 3.535611152648926,
      "learning_rate": 4.134228187919463e-05,
      "loss": 0.6373,
      "step": 13400
    },
    {
      "epoch": 0.5364,
      "grad_norm": 3.1522393226623535,
      "learning_rate": 4.133557046979866e-05,
      "loss": 0.7636,
      "step": 13410
    },
    {
      "epoch": 0.5368,
      "grad_norm": 2.059497117996216,
      "learning_rate": 4.132885906040268e-05,
      "loss": 0.6151,
      "step": 13420
    },
    {
      "epoch": 0.5372,
      "grad_norm": 2.889195680618286,
      "learning_rate": 4.132214765100672e-05,
      "loss": 0.6843,
      "step": 13430
    },
    {
      "epoch": 0.5376,
      "grad_norm": 3.1823596954345703,
      "learning_rate": 4.131543624161074e-05,
      "loss": 0.6906,
      "step": 13440
    },
    {
      "epoch": 0.538,
      "grad_norm": 3.273027181625366,
      "learning_rate": 4.130872483221477e-05,
      "loss": 0.689,
      "step": 13450
    },
    {
      "epoch": 0.5384,
      "grad_norm": 2.4551029205322266,
      "learning_rate": 4.130201342281879e-05,
      "loss": 0.6209,
      "step": 13460
    },
    {
      "epoch": 0.5388,
      "grad_norm": 2.7500712871551514,
      "learning_rate": 4.129530201342282e-05,
      "loss": 0.6844,
      "step": 13470
    },
    {
      "epoch": 0.5392,
      "grad_norm": 3.014403820037842,
      "learning_rate": 4.128859060402685e-05,
      "loss": 0.6159,
      "step": 13480
    },
    {
      "epoch": 0.5396,
      "grad_norm": 3.3393869400024414,
      "learning_rate": 4.1281879194630876e-05,
      "loss": 0.6825,
      "step": 13490
    },
    {
      "epoch": 0.54,
      "grad_norm": 3.0237796306610107,
      "learning_rate": 4.1275167785234905e-05,
      "loss": 0.5883,
      "step": 13500
    },
    {
      "epoch": 0.5404,
      "grad_norm": 2.8827602863311768,
      "learning_rate": 4.1268456375838926e-05,
      "loss": 0.6562,
      "step": 13510
    },
    {
      "epoch": 0.5408,
      "grad_norm": 2.6222169399261475,
      "learning_rate": 4.1261744966442955e-05,
      "loss": 0.7122,
      "step": 13520
    },
    {
      "epoch": 0.5412,
      "grad_norm": 2.674583911895752,
      "learning_rate": 4.125503355704698e-05,
      "loss": 0.6548,
      "step": 13530
    },
    {
      "epoch": 0.5416,
      "grad_norm": 2.9921023845672607,
      "learning_rate": 4.1248322147651005e-05,
      "loss": 0.6949,
      "step": 13540
    },
    {
      "epoch": 0.542,
      "grad_norm": 3.1699163913726807,
      "learning_rate": 4.124161073825504e-05,
      "loss": 0.6877,
      "step": 13550
    },
    {
      "epoch": 0.5424,
      "grad_norm": 3.255530595779419,
      "learning_rate": 4.123489932885906e-05,
      "loss": 0.7665,
      "step": 13560
    },
    {
      "epoch": 0.5428,
      "grad_norm": 3.5169150829315186,
      "learning_rate": 4.122818791946309e-05,
      "loss": 0.7444,
      "step": 13570
    },
    {
      "epoch": 0.5432,
      "grad_norm": 2.3808913230895996,
      "learning_rate": 4.122147651006711e-05,
      "loss": 0.6776,
      "step": 13580
    },
    {
      "epoch": 0.5436,
      "grad_norm": 2.521737813949585,
      "learning_rate": 4.121476510067114e-05,
      "loss": 0.7235,
      "step": 13590
    },
    {
      "epoch": 0.544,
      "grad_norm": 3.176992416381836,
      "learning_rate": 4.120805369127517e-05,
      "loss": 0.659,
      "step": 13600
    },
    {
      "epoch": 0.5444,
      "grad_norm": 3.3151967525482178,
      "learning_rate": 4.12013422818792e-05,
      "loss": 0.834,
      "step": 13610
    },
    {
      "epoch": 0.5448,
      "grad_norm": 2.2789411544799805,
      "learning_rate": 4.119463087248323e-05,
      "loss": 0.6998,
      "step": 13620
    },
    {
      "epoch": 0.5452,
      "grad_norm": 2.76491117477417,
      "learning_rate": 4.118791946308725e-05,
      "loss": 0.6447,
      "step": 13630
    },
    {
      "epoch": 0.5456,
      "grad_norm": 2.58713436126709,
      "learning_rate": 4.118120805369128e-05,
      "loss": 0.6544,
      "step": 13640
    },
    {
      "epoch": 0.546,
      "grad_norm": 2.975266218185425,
      "learning_rate": 4.11744966442953e-05,
      "loss": 0.6532,
      "step": 13650
    },
    {
      "epoch": 0.5464,
      "grad_norm": 2.922285795211792,
      "learning_rate": 4.1167785234899334e-05,
      "loss": 0.6513,
      "step": 13660
    },
    {
      "epoch": 0.5468,
      "grad_norm": 3.1076674461364746,
      "learning_rate": 4.116107382550336e-05,
      "loss": 0.7043,
      "step": 13670
    },
    {
      "epoch": 0.5472,
      "grad_norm": 2.990374803543091,
      "learning_rate": 4.1154362416107385e-05,
      "loss": 0.6504,
      "step": 13680
    },
    {
      "epoch": 0.5476,
      "grad_norm": 2.4491636753082275,
      "learning_rate": 4.114765100671141e-05,
      "loss": 0.6826,
      "step": 13690
    },
    {
      "epoch": 0.548,
      "grad_norm": 3.380791425704956,
      "learning_rate": 4.1140939597315435e-05,
      "loss": 0.7294,
      "step": 13700
    },
    {
      "epoch": 0.5484,
      "grad_norm": 2.9173474311828613,
      "learning_rate": 4.1134228187919463e-05,
      "loss": 0.7933,
      "step": 13710
    },
    {
      "epoch": 0.5488,
      "grad_norm": 2.7906644344329834,
      "learning_rate": 4.112751677852349e-05,
      "loss": 0.7568,
      "step": 13720
    },
    {
      "epoch": 0.5492,
      "grad_norm": 2.4770925045013428,
      "learning_rate": 4.112080536912752e-05,
      "loss": 0.5455,
      "step": 13730
    },
    {
      "epoch": 0.5496,
      "grad_norm": 3.250556707382202,
      "learning_rate": 4.111409395973155e-05,
      "loss": 0.7179,
      "step": 13740
    },
    {
      "epoch": 0.55,
      "grad_norm": 2.5584237575531006,
      "learning_rate": 4.110738255033557e-05,
      "loss": 0.6413,
      "step": 13750
    },
    {
      "epoch": 0.5504,
      "grad_norm": 2.6273722648620605,
      "learning_rate": 4.11006711409396e-05,
      "loss": 0.6547,
      "step": 13760
    },
    {
      "epoch": 0.5508,
      "grad_norm": 4.303055763244629,
      "learning_rate": 4.109395973154362e-05,
      "loss": 0.605,
      "step": 13770
    },
    {
      "epoch": 0.5512,
      "grad_norm": 3.1805620193481445,
      "learning_rate": 4.1087248322147656e-05,
      "loss": 0.5914,
      "step": 13780
    },
    {
      "epoch": 0.5516,
      "grad_norm": 2.825974702835083,
      "learning_rate": 4.1080536912751685e-05,
      "loss": 0.6243,
      "step": 13790
    },
    {
      "epoch": 0.552,
      "grad_norm": 2.764704704284668,
      "learning_rate": 4.107382550335571e-05,
      "loss": 0.6803,
      "step": 13800
    },
    {
      "epoch": 0.5524,
      "grad_norm": 3.6194310188293457,
      "learning_rate": 4.1067114093959735e-05,
      "loss": 0.6313,
      "step": 13810
    },
    {
      "epoch": 0.5528,
      "grad_norm": 3.1494579315185547,
      "learning_rate": 4.106040268456376e-05,
      "loss": 0.6681,
      "step": 13820
    },
    {
      "epoch": 0.5532,
      "grad_norm": 3.43070912361145,
      "learning_rate": 4.1053691275167786e-05,
      "loss": 0.5581,
      "step": 13830
    },
    {
      "epoch": 0.5536,
      "grad_norm": 2.569002866744995,
      "learning_rate": 4.1046979865771814e-05,
      "loss": 0.7177,
      "step": 13840
    },
    {
      "epoch": 0.554,
      "grad_norm": 3.5143954753875732,
      "learning_rate": 4.104026845637584e-05,
      "loss": 0.7117,
      "step": 13850
    },
    {
      "epoch": 0.5544,
      "grad_norm": 2.656062126159668,
      "learning_rate": 4.103355704697987e-05,
      "loss": 0.5505,
      "step": 13860
    },
    {
      "epoch": 0.5548,
      "grad_norm": 3.207396984100342,
      "learning_rate": 4.102684563758389e-05,
      "loss": 0.6395,
      "step": 13870
    },
    {
      "epoch": 0.5552,
      "grad_norm": 2.5921478271484375,
      "learning_rate": 4.102013422818792e-05,
      "loss": 0.6014,
      "step": 13880
    },
    {
      "epoch": 0.5556,
      "grad_norm": 2.906449556350708,
      "learning_rate": 4.101342281879194e-05,
      "loss": 0.6184,
      "step": 13890
    },
    {
      "epoch": 0.556,
      "grad_norm": 3.052870273590088,
      "learning_rate": 4.100671140939598e-05,
      "loss": 0.5997,
      "step": 13900
    },
    {
      "epoch": 0.5564,
      "grad_norm": 3.2944626808166504,
      "learning_rate": 4.1e-05,
      "loss": 0.5824,
      "step": 13910
    },
    {
      "epoch": 0.5568,
      "grad_norm": 2.1479732990264893,
      "learning_rate": 4.099328859060403e-05,
      "loss": 0.5643,
      "step": 13920
    },
    {
      "epoch": 0.5572,
      "grad_norm": 2.763780117034912,
      "learning_rate": 4.098657718120806e-05,
      "loss": 0.7068,
      "step": 13930
    },
    {
      "epoch": 0.5576,
      "grad_norm": 4.165106296539307,
      "learning_rate": 4.097986577181208e-05,
      "loss": 0.736,
      "step": 13940
    },
    {
      "epoch": 0.558,
      "grad_norm": 2.431734561920166,
      "learning_rate": 4.097315436241611e-05,
      "loss": 0.6061,
      "step": 13950
    },
    {
      "epoch": 0.5584,
      "grad_norm": 2.3083674907684326,
      "learning_rate": 4.0966442953020136e-05,
      "loss": 0.7311,
      "step": 13960
    },
    {
      "epoch": 0.5588,
      "grad_norm": 2.795103073120117,
      "learning_rate": 4.0959731543624165e-05,
      "loss": 0.7005,
      "step": 13970
    },
    {
      "epoch": 0.5592,
      "grad_norm": 3.2163963317871094,
      "learning_rate": 4.095302013422819e-05,
      "loss": 0.626,
      "step": 13980
    },
    {
      "epoch": 0.5596,
      "grad_norm": 2.5776302814483643,
      "learning_rate": 4.0946308724832215e-05,
      "loss": 0.6405,
      "step": 13990
    },
    {
      "epoch": 0.56,
      "grad_norm": 3.101755380630493,
      "learning_rate": 4.0939597315436244e-05,
      "loss": 0.6688,
      "step": 14000
    },
    {
      "epoch": 0.5604,
      "grad_norm": 2.105792760848999,
      "learning_rate": 4.093288590604027e-05,
      "loss": 0.6761,
      "step": 14010
    },
    {
      "epoch": 0.5608,
      "grad_norm": 2.7772622108459473,
      "learning_rate": 4.09261744966443e-05,
      "loss": 0.6296,
      "step": 14020
    },
    {
      "epoch": 0.5612,
      "grad_norm": 3.3605008125305176,
      "learning_rate": 4.091946308724832e-05,
      "loss": 0.6941,
      "step": 14030
    },
    {
      "epoch": 0.5616,
      "grad_norm": 2.1925148963928223,
      "learning_rate": 4.091275167785235e-05,
      "loss": 0.6709,
      "step": 14040
    },
    {
      "epoch": 0.562,
      "grad_norm": 2.5155112743377686,
      "learning_rate": 4.090604026845638e-05,
      "loss": 0.6256,
      "step": 14050
    },
    {
      "epoch": 0.5624,
      "grad_norm": 2.95902156829834,
      "learning_rate": 4.08993288590604e-05,
      "loss": 0.6802,
      "step": 14060
    },
    {
      "epoch": 0.5628,
      "grad_norm": 2.5211782455444336,
      "learning_rate": 4.089261744966443e-05,
      "loss": 0.5913,
      "step": 14070
    },
    {
      "epoch": 0.5632,
      "grad_norm": 2.4741547107696533,
      "learning_rate": 4.088590604026846e-05,
      "loss": 0.6572,
      "step": 14080
    },
    {
      "epoch": 0.5636,
      "grad_norm": 2.91072416305542,
      "learning_rate": 4.087919463087249e-05,
      "loss": 0.6385,
      "step": 14090
    },
    {
      "epoch": 0.564,
      "grad_norm": 2.607919931411743,
      "learning_rate": 4.087248322147651e-05,
      "loss": 0.6048,
      "step": 14100
    },
    {
      "epoch": 0.5644,
      "grad_norm": 2.823072910308838,
      "learning_rate": 4.086577181208054e-05,
      "loss": 0.7262,
      "step": 14110
    },
    {
      "epoch": 0.5648,
      "grad_norm": 2.803725004196167,
      "learning_rate": 4.0859060402684566e-05,
      "loss": 0.5479,
      "step": 14120
    },
    {
      "epoch": 0.5652,
      "grad_norm": 2.5832252502441406,
      "learning_rate": 4.0852348993288595e-05,
      "loss": 0.6582,
      "step": 14130
    },
    {
      "epoch": 0.5656,
      "grad_norm": 2.5881242752075195,
      "learning_rate": 4.084563758389262e-05,
      "loss": 0.6777,
      "step": 14140
    },
    {
      "epoch": 0.566,
      "grad_norm": 4.0962748527526855,
      "learning_rate": 4.0838926174496645e-05,
      "loss": 0.6185,
      "step": 14150
    },
    {
      "epoch": 0.5664,
      "grad_norm": 2.2584617137908936,
      "learning_rate": 4.0832214765100673e-05,
      "loss": 0.6938,
      "step": 14160
    },
    {
      "epoch": 0.5668,
      "grad_norm": 3.220797538757324,
      "learning_rate": 4.0825503355704695e-05,
      "loss": 0.6642,
      "step": 14170
    },
    {
      "epoch": 0.5672,
      "grad_norm": 3.1124861240386963,
      "learning_rate": 4.0818791946308724e-05,
      "loss": 0.6879,
      "step": 14180
    },
    {
      "epoch": 0.5676,
      "grad_norm": 3.4679458141326904,
      "learning_rate": 4.081208053691275e-05,
      "loss": 0.7243,
      "step": 14190
    },
    {
      "epoch": 0.568,
      "grad_norm": 2.2434709072113037,
      "learning_rate": 4.080536912751678e-05,
      "loss": 0.6013,
      "step": 14200
    },
    {
      "epoch": 0.5684,
      "grad_norm": 3.1839287281036377,
      "learning_rate": 4.079865771812081e-05,
      "loss": 0.7261,
      "step": 14210
    },
    {
      "epoch": 0.5688,
      "grad_norm": 3.696632146835327,
      "learning_rate": 4.079194630872483e-05,
      "loss": 0.6364,
      "step": 14220
    },
    {
      "epoch": 0.5692,
      "grad_norm": 2.817107677459717,
      "learning_rate": 4.078523489932886e-05,
      "loss": 0.6203,
      "step": 14230
    },
    {
      "epoch": 0.5696,
      "grad_norm": 3.6122305393218994,
      "learning_rate": 4.077852348993289e-05,
      "loss": 0.766,
      "step": 14240
    },
    {
      "epoch": 0.57,
      "grad_norm": 3.0136773586273193,
      "learning_rate": 4.077181208053692e-05,
      "loss": 0.6728,
      "step": 14250
    },
    {
      "epoch": 0.5704,
      "grad_norm": 3.0076546669006348,
      "learning_rate": 4.0765100671140945e-05,
      "loss": 0.715,
      "step": 14260
    },
    {
      "epoch": 0.5708,
      "grad_norm": 3.665771245956421,
      "learning_rate": 4.075838926174497e-05,
      "loss": 0.7001,
      "step": 14270
    },
    {
      "epoch": 0.5712,
      "grad_norm": 3.147658586502075,
      "learning_rate": 4.0751677852348996e-05,
      "loss": 0.7023,
      "step": 14280
    },
    {
      "epoch": 0.5716,
      "grad_norm": 2.860060214996338,
      "learning_rate": 4.074496644295302e-05,
      "loss": 0.6744,
      "step": 14290
    },
    {
      "epoch": 0.572,
      "grad_norm": 2.3043384552001953,
      "learning_rate": 4.0738255033557046e-05,
      "loss": 0.6542,
      "step": 14300
    },
    {
      "epoch": 0.5724,
      "grad_norm": 4.6020283699035645,
      "learning_rate": 4.073154362416108e-05,
      "loss": 0.746,
      "step": 14310
    },
    {
      "epoch": 0.5728,
      "grad_norm": 2.2124056816101074,
      "learning_rate": 4.07248322147651e-05,
      "loss": 0.7124,
      "step": 14320
    },
    {
      "epoch": 0.5732,
      "grad_norm": 3.354118824005127,
      "learning_rate": 4.071812080536913e-05,
      "loss": 0.6427,
      "step": 14330
    },
    {
      "epoch": 0.5736,
      "grad_norm": 2.897350549697876,
      "learning_rate": 4.0711409395973153e-05,
      "loss": 0.6179,
      "step": 14340
    },
    {
      "epoch": 0.574,
      "grad_norm": 3.1019058227539062,
      "learning_rate": 4.070469798657718e-05,
      "loss": 0.5878,
      "step": 14350
    },
    {
      "epoch": 0.5744,
      "grad_norm": 4.413414001464844,
      "learning_rate": 4.069798657718121e-05,
      "loss": 0.7403,
      "step": 14360
    },
    {
      "epoch": 0.5748,
      "grad_norm": 3.126195192337036,
      "learning_rate": 4.069127516778524e-05,
      "loss": 0.778,
      "step": 14370
    },
    {
      "epoch": 0.5752,
      "grad_norm": 2.7284648418426514,
      "learning_rate": 4.068456375838927e-05,
      "loss": 0.6558,
      "step": 14380
    },
    {
      "epoch": 0.5756,
      "grad_norm": 3.5280799865722656,
      "learning_rate": 4.067785234899329e-05,
      "loss": 0.7731,
      "step": 14390
    },
    {
      "epoch": 0.576,
      "grad_norm": 3.4363861083984375,
      "learning_rate": 4.067114093959732e-05,
      "loss": 0.6558,
      "step": 14400
    },
    {
      "epoch": 0.5764,
      "grad_norm": 2.724649429321289,
      "learning_rate": 4.066442953020134e-05,
      "loss": 0.5967,
      "step": 14410
    },
    {
      "epoch": 0.5768,
      "grad_norm": 3.0984976291656494,
      "learning_rate": 4.065771812080537e-05,
      "loss": 0.5925,
      "step": 14420
    },
    {
      "epoch": 0.5772,
      "grad_norm": 2.6386938095092773,
      "learning_rate": 4.0651006711409404e-05,
      "loss": 0.6789,
      "step": 14430
    },
    {
      "epoch": 0.5776,
      "grad_norm": 2.5459823608398438,
      "learning_rate": 4.0644295302013425e-05,
      "loss": 0.6069,
      "step": 14440
    },
    {
      "epoch": 0.578,
      "grad_norm": 3.0219788551330566,
      "learning_rate": 4.0637583892617454e-05,
      "loss": 0.6665,
      "step": 14450
    },
    {
      "epoch": 0.5784,
      "grad_norm": 2.7069296836853027,
      "learning_rate": 4.0630872483221476e-05,
      "loss": 0.6223,
      "step": 14460
    },
    {
      "epoch": 0.5788,
      "grad_norm": 7.41652250289917,
      "learning_rate": 4.0624161073825504e-05,
      "loss": 0.5831,
      "step": 14470
    },
    {
      "epoch": 0.5792,
      "grad_norm": 2.7770497798919678,
      "learning_rate": 4.061744966442953e-05,
      "loss": 0.6848,
      "step": 14480
    },
    {
      "epoch": 0.5796,
      "grad_norm": 2.0578744411468506,
      "learning_rate": 4.061073825503356e-05,
      "loss": 0.5737,
      "step": 14490
    },
    {
      "epoch": 0.58,
      "grad_norm": 1.9475806951522827,
      "learning_rate": 4.060402684563759e-05,
      "loss": 0.682,
      "step": 14500
    },
    {
      "epoch": 0.5804,
      "grad_norm": 3.034139394760132,
      "learning_rate": 4.059731543624161e-05,
      "loss": 0.6584,
      "step": 14510
    },
    {
      "epoch": 0.5808,
      "grad_norm": 3.184035062789917,
      "learning_rate": 4.059060402684564e-05,
      "loss": 0.6826,
      "step": 14520
    },
    {
      "epoch": 0.5812,
      "grad_norm": 2.291456699371338,
      "learning_rate": 4.058389261744966e-05,
      "loss": 0.7272,
      "step": 14530
    },
    {
      "epoch": 0.5816,
      "grad_norm": 3.3029966354370117,
      "learning_rate": 4.05771812080537e-05,
      "loss": 0.5524,
      "step": 14540
    },
    {
      "epoch": 0.582,
      "grad_norm": 3.0611732006073,
      "learning_rate": 4.057046979865772e-05,
      "loss": 0.6646,
      "step": 14550
    },
    {
      "epoch": 0.5824,
      "grad_norm": 2.673295021057129,
      "learning_rate": 4.056375838926175e-05,
      "loss": 0.6065,
      "step": 14560
    },
    {
      "epoch": 0.5828,
      "grad_norm": 2.1352438926696777,
      "learning_rate": 4.0557046979865776e-05,
      "loss": 0.5487,
      "step": 14570
    },
    {
      "epoch": 0.5832,
      "grad_norm": 2.7381556034088135,
      "learning_rate": 4.05503355704698e-05,
      "loss": 0.559,
      "step": 14580
    },
    {
      "epoch": 0.5836,
      "grad_norm": 2.6897683143615723,
      "learning_rate": 4.0543624161073826e-05,
      "loss": 0.6233,
      "step": 14590
    },
    {
      "epoch": 0.584,
      "grad_norm": 2.4214508533477783,
      "learning_rate": 4.0536912751677855e-05,
      "loss": 0.7041,
      "step": 14600
    },
    {
      "epoch": 0.5844,
      "grad_norm": 2.9491050243377686,
      "learning_rate": 4.0530201342281884e-05,
      "loss": 0.7543,
      "step": 14610
    },
    {
      "epoch": 0.5848,
      "grad_norm": 3.4605178833007812,
      "learning_rate": 4.0523489932885905e-05,
      "loss": 0.7426,
      "step": 14620
    },
    {
      "epoch": 0.5852,
      "grad_norm": 3.2755250930786133,
      "learning_rate": 4.0516778523489934e-05,
      "loss": 0.6486,
      "step": 14630
    },
    {
      "epoch": 0.5856,
      "grad_norm": 3.0776987075805664,
      "learning_rate": 4.051006711409396e-05,
      "loss": 0.7548,
      "step": 14640
    },
    {
      "epoch": 0.586,
      "grad_norm": 2.130636215209961,
      "learning_rate": 4.0503355704697984e-05,
      "loss": 0.5394,
      "step": 14650
    },
    {
      "epoch": 0.5864,
      "grad_norm": 2.825274705886841,
      "learning_rate": 4.049664429530202e-05,
      "loss": 0.641,
      "step": 14660
    },
    {
      "epoch": 0.5868,
      "grad_norm": 2.5961685180664062,
      "learning_rate": 4.048993288590604e-05,
      "loss": 0.7173,
      "step": 14670
    },
    {
      "epoch": 0.5872,
      "grad_norm": 2.8758902549743652,
      "learning_rate": 4.048322147651007e-05,
      "loss": 0.7188,
      "step": 14680
    },
    {
      "epoch": 0.5876,
      "grad_norm": 2.4247357845306396,
      "learning_rate": 4.04765100671141e-05,
      "loss": 0.6383,
      "step": 14690
    },
    {
      "epoch": 0.588,
      "grad_norm": 2.367893934249878,
      "learning_rate": 4.046979865771812e-05,
      "loss": 0.6637,
      "step": 14700
    },
    {
      "epoch": 0.5884,
      "grad_norm": 2.5715155601501465,
      "learning_rate": 4.046308724832215e-05,
      "loss": 0.6098,
      "step": 14710
    },
    {
      "epoch": 0.5888,
      "grad_norm": 2.5714304447174072,
      "learning_rate": 4.045637583892618e-05,
      "loss": 0.6994,
      "step": 14720
    },
    {
      "epoch": 0.5892,
      "grad_norm": 2.954469680786133,
      "learning_rate": 4.0449664429530206e-05,
      "loss": 0.6994,
      "step": 14730
    },
    {
      "epoch": 0.5896,
      "grad_norm": 3.8791444301605225,
      "learning_rate": 4.044295302013423e-05,
      "loss": 0.7802,
      "step": 14740
    },
    {
      "epoch": 0.59,
      "grad_norm": 2.8936562538146973,
      "learning_rate": 4.0436241610738256e-05,
      "loss": 0.6855,
      "step": 14750
    },
    {
      "epoch": 0.5904,
      "grad_norm": 2.90390944480896,
      "learning_rate": 4.0429530201342285e-05,
      "loss": 0.6605,
      "step": 14760
    },
    {
      "epoch": 0.5908,
      "grad_norm": 3.119464159011841,
      "learning_rate": 4.0422818791946306e-05,
      "loss": 0.6555,
      "step": 14770
    },
    {
      "epoch": 0.5912,
      "grad_norm": 3.157832384109497,
      "learning_rate": 4.041610738255034e-05,
      "loss": 0.6878,
      "step": 14780
    },
    {
      "epoch": 0.5916,
      "grad_norm": 2.5413689613342285,
      "learning_rate": 4.0409395973154364e-05,
      "loss": 0.6898,
      "step": 14790
    },
    {
      "epoch": 0.592,
      "grad_norm": 3.719078302383423,
      "learning_rate": 4.040268456375839e-05,
      "loss": 0.7114,
      "step": 14800
    },
    {
      "epoch": 0.5924,
      "grad_norm": 3.077950954437256,
      "learning_rate": 4.0395973154362414e-05,
      "loss": 0.6647,
      "step": 14810
    },
    {
      "epoch": 0.5928,
      "grad_norm": 2.630725145339966,
      "learning_rate": 4.038926174496644e-05,
      "loss": 0.6764,
      "step": 14820
    },
    {
      "epoch": 0.5932,
      "grad_norm": 2.465658187866211,
      "learning_rate": 4.038255033557047e-05,
      "loss": 0.725,
      "step": 14830
    },
    {
      "epoch": 0.5936,
      "grad_norm": 2.831110954284668,
      "learning_rate": 4.03758389261745e-05,
      "loss": 0.6222,
      "step": 14840
    },
    {
      "epoch": 0.594,
      "grad_norm": 2.34098219871521,
      "learning_rate": 4.036912751677853e-05,
      "loss": 0.7638,
      "step": 14850
    },
    {
      "epoch": 0.5944,
      "grad_norm": 3.084052562713623,
      "learning_rate": 4.036241610738255e-05,
      "loss": 0.7367,
      "step": 14860
    },
    {
      "epoch": 0.5948,
      "grad_norm": 2.5532188415527344,
      "learning_rate": 4.035570469798658e-05,
      "loss": 0.5495,
      "step": 14870
    },
    {
      "epoch": 0.5952,
      "grad_norm": 2.2977912425994873,
      "learning_rate": 4.034899328859061e-05,
      "loss": 0.6843,
      "step": 14880
    },
    {
      "epoch": 0.5956,
      "grad_norm": 2.6665477752685547,
      "learning_rate": 4.0342281879194635e-05,
      "loss": 0.6176,
      "step": 14890
    },
    {
      "epoch": 0.596,
      "grad_norm": 2.6073384284973145,
      "learning_rate": 4.0335570469798664e-05,
      "loss": 0.6717,
      "step": 14900
    },
    {
      "epoch": 0.5964,
      "grad_norm": 2.0586376190185547,
      "learning_rate": 4.0328859060402686e-05,
      "loss": 0.63,
      "step": 14910
    },
    {
      "epoch": 0.5968,
      "grad_norm": 3.194655179977417,
      "learning_rate": 4.0322147651006714e-05,
      "loss": 0.7049,
      "step": 14920
    },
    {
      "epoch": 0.5972,
      "grad_norm": 2.947901964187622,
      "learning_rate": 4.0315436241610736e-05,
      "loss": 0.698,
      "step": 14930
    },
    {
      "epoch": 0.5976,
      "grad_norm": 3.4358837604522705,
      "learning_rate": 4.0308724832214765e-05,
      "loss": 0.7348,
      "step": 14940
    },
    {
      "epoch": 0.598,
      "grad_norm": 2.8714492321014404,
      "learning_rate": 4.030201342281879e-05,
      "loss": 0.7087,
      "step": 14950
    },
    {
      "epoch": 0.5984,
      "grad_norm": 2.7398791313171387,
      "learning_rate": 4.029530201342282e-05,
      "loss": 0.6378,
      "step": 14960
    },
    {
      "epoch": 0.5988,
      "grad_norm": 2.3515636920928955,
      "learning_rate": 4.028859060402685e-05,
      "loss": 0.5907,
      "step": 14970
    },
    {
      "epoch": 0.5992,
      "grad_norm": 3.178678035736084,
      "learning_rate": 4.028187919463087e-05,
      "loss": 0.6023,
      "step": 14980
    },
    {
      "epoch": 0.5996,
      "grad_norm": 2.6966841220855713,
      "learning_rate": 4.02751677852349e-05,
      "loss": 0.6593,
      "step": 14990
    },
    {
      "epoch": 0.6,
      "grad_norm": 2.8310678005218506,
      "learning_rate": 4.026845637583892e-05,
      "loss": 0.6585,
      "step": 15000
    },
    {
      "epoch": 0.6004,
      "grad_norm": 2.90881609916687,
      "learning_rate": 4.026174496644296e-05,
      "loss": 0.6596,
      "step": 15010
    },
    {
      "epoch": 0.6008,
      "grad_norm": 3.3208155632019043,
      "learning_rate": 4.0255033557046986e-05,
      "loss": 0.6373,
      "step": 15020
    },
    {
      "epoch": 0.6012,
      "grad_norm": 3.0312399864196777,
      "learning_rate": 4.024832214765101e-05,
      "loss": 0.6732,
      "step": 15030
    },
    {
      "epoch": 0.6016,
      "grad_norm": 2.7437267303466797,
      "learning_rate": 4.0241610738255037e-05,
      "loss": 0.6625,
      "step": 15040
    },
    {
      "epoch": 0.602,
      "grad_norm": 2.512404441833496,
      "learning_rate": 4.023489932885906e-05,
      "loss": 0.6161,
      "step": 15050
    },
    {
      "epoch": 0.6024,
      "grad_norm": 3.027109384536743,
      "learning_rate": 4.022818791946309e-05,
      "loss": 0.6925,
      "step": 15060
    },
    {
      "epoch": 0.6028,
      "grad_norm": 2.532745599746704,
      "learning_rate": 4.0221476510067115e-05,
      "loss": 0.7472,
      "step": 15070
    },
    {
      "epoch": 0.6032,
      "grad_norm": 2.4091503620147705,
      "learning_rate": 4.0214765100671144e-05,
      "loss": 0.714,
      "step": 15080
    },
    {
      "epoch": 0.6036,
      "grad_norm": 2.9637610912323,
      "learning_rate": 4.020805369127517e-05,
      "loss": 0.7443,
      "step": 15090
    },
    {
      "epoch": 0.604,
      "grad_norm": 2.308908462524414,
      "learning_rate": 4.0201342281879194e-05,
      "loss": 0.7138,
      "step": 15100
    },
    {
      "epoch": 0.6044,
      "grad_norm": 3.2045955657958984,
      "learning_rate": 4.019463087248322e-05,
      "loss": 0.6478,
      "step": 15110
    },
    {
      "epoch": 0.6048,
      "grad_norm": 3.0916824340820312,
      "learning_rate": 4.0187919463087245e-05,
      "loss": 0.6161,
      "step": 15120
    },
    {
      "epoch": 0.6052,
      "grad_norm": 2.63867449760437,
      "learning_rate": 4.018120805369128e-05,
      "loss": 0.7299,
      "step": 15130
    },
    {
      "epoch": 0.6056,
      "grad_norm": 2.941654920578003,
      "learning_rate": 4.017449664429531e-05,
      "loss": 0.617,
      "step": 15140
    },
    {
      "epoch": 0.606,
      "grad_norm": 1.5423884391784668,
      "learning_rate": 4.016778523489933e-05,
      "loss": 0.6644,
      "step": 15150
    },
    {
      "epoch": 0.6064,
      "grad_norm": 3.322035789489746,
      "learning_rate": 4.016107382550336e-05,
      "loss": 0.7105,
      "step": 15160
    },
    {
      "epoch": 0.6068,
      "grad_norm": 3.174917459487915,
      "learning_rate": 4.015436241610738e-05,
      "loss": 0.6148,
      "step": 15170
    },
    {
      "epoch": 0.6072,
      "grad_norm": 2.6603407859802246,
      "learning_rate": 4.014765100671141e-05,
      "loss": 0.6949,
      "step": 15180
    },
    {
      "epoch": 0.6076,
      "grad_norm": 3.480567216873169,
      "learning_rate": 4.014093959731544e-05,
      "loss": 0.7236,
      "step": 15190
    },
    {
      "epoch": 0.608,
      "grad_norm": 2.8726019859313965,
      "learning_rate": 4.0134228187919466e-05,
      "loss": 0.6786,
      "step": 15200
    },
    {
      "epoch": 0.6084,
      "grad_norm": 3.6870195865631104,
      "learning_rate": 4.0127516778523495e-05,
      "loss": 0.7556,
      "step": 15210
    },
    {
      "epoch": 0.6088,
      "grad_norm": 2.6621816158294678,
      "learning_rate": 4.0120805369127517e-05,
      "loss": 0.5771,
      "step": 15220
    },
    {
      "epoch": 0.6092,
      "grad_norm": 3.057345151901245,
      "learning_rate": 4.0114093959731545e-05,
      "loss": 0.6871,
      "step": 15230
    },
    {
      "epoch": 0.6096,
      "grad_norm": 2.757213592529297,
      "learning_rate": 4.0107382550335574e-05,
      "loss": 0.6305,
      "step": 15240
    },
    {
      "epoch": 0.61,
      "grad_norm": 2.265141725540161,
      "learning_rate": 4.01006711409396e-05,
      "loss": 0.715,
      "step": 15250
    },
    {
      "epoch": 0.6104,
      "grad_norm": 1.9193347692489624,
      "learning_rate": 4.009395973154363e-05,
      "loss": 0.6453,
      "step": 15260
    },
    {
      "epoch": 0.6108,
      "grad_norm": 2.4169094562530518,
      "learning_rate": 4.008724832214765e-05,
      "loss": 0.5928,
      "step": 15270
    },
    {
      "epoch": 0.6112,
      "grad_norm": 2.5706028938293457,
      "learning_rate": 4.008053691275168e-05,
      "loss": 0.646,
      "step": 15280
    },
    {
      "epoch": 0.6116,
      "grad_norm": 2.7242908477783203,
      "learning_rate": 4.00738255033557e-05,
      "loss": 0.609,
      "step": 15290
    },
    {
      "epoch": 0.612,
      "grad_norm": 2.4970579147338867,
      "learning_rate": 4.006711409395973e-05,
      "loss": 0.6441,
      "step": 15300
    },
    {
      "epoch": 0.6124,
      "grad_norm": 3.3250279426574707,
      "learning_rate": 4.006040268456376e-05,
      "loss": 0.6929,
      "step": 15310
    },
    {
      "epoch": 0.6128,
      "grad_norm": 3.209737777709961,
      "learning_rate": 4.005369127516779e-05,
      "loss": 0.6081,
      "step": 15320
    },
    {
      "epoch": 0.6132,
      "grad_norm": 3.1696107387542725,
      "learning_rate": 4.004697986577182e-05,
      "loss": 0.6013,
      "step": 15330
    },
    {
      "epoch": 0.6136,
      "grad_norm": 2.3396100997924805,
      "learning_rate": 4.004026845637584e-05,
      "loss": 0.6693,
      "step": 15340
    },
    {
      "epoch": 0.614,
      "grad_norm": 2.354602336883545,
      "learning_rate": 4.003355704697987e-05,
      "loss": 0.6025,
      "step": 15350
    },
    {
      "epoch": 0.6144,
      "grad_norm": 2.623561382293701,
      "learning_rate": 4.0026845637583896e-05,
      "loss": 0.6924,
      "step": 15360
    },
    {
      "epoch": 0.6148,
      "grad_norm": 3.0428857803344727,
      "learning_rate": 4.0020134228187924e-05,
      "loss": 0.6199,
      "step": 15370
    },
    {
      "epoch": 0.6152,
      "grad_norm": 3.1244444847106934,
      "learning_rate": 4.0013422818791946e-05,
      "loss": 0.8096,
      "step": 15380
    },
    {
      "epoch": 0.6156,
      "grad_norm": 2.891183614730835,
      "learning_rate": 4.0006711409395975e-05,
      "loss": 0.745,
      "step": 15390
    },
    {
      "epoch": 0.616,
      "grad_norm": 2.6960840225219727,
      "learning_rate": 4e-05,
      "loss": 0.7027,
      "step": 15400
    },
    {
      "epoch": 0.6164,
      "grad_norm": 3.2454802989959717,
      "learning_rate": 3.9993288590604025e-05,
      "loss": 0.5933,
      "step": 15410
    },
    {
      "epoch": 0.6168,
      "grad_norm": 2.738192558288574,
      "learning_rate": 3.9986577181208054e-05,
      "loss": 0.7318,
      "step": 15420
    },
    {
      "epoch": 0.6172,
      "grad_norm": 2.9811534881591797,
      "learning_rate": 3.997986577181208e-05,
      "loss": 0.689,
      "step": 15430
    },
    {
      "epoch": 0.6176,
      "grad_norm": 1.932585597038269,
      "learning_rate": 3.997315436241611e-05,
      "loss": 0.5818,
      "step": 15440
    },
    {
      "epoch": 0.618,
      "grad_norm": 3.185991048812866,
      "learning_rate": 3.996644295302013e-05,
      "loss": 0.664,
      "step": 15450
    },
    {
      "epoch": 0.6184,
      "grad_norm": 2.0163228511810303,
      "learning_rate": 3.995973154362416e-05,
      "loss": 0.6476,
      "step": 15460
    },
    {
      "epoch": 0.6188,
      "grad_norm": 2.716287612915039,
      "learning_rate": 3.995302013422819e-05,
      "loss": 0.6504,
      "step": 15470
    },
    {
      "epoch": 0.6192,
      "grad_norm": 2.4616661071777344,
      "learning_rate": 3.994630872483222e-05,
      "loss": 0.651,
      "step": 15480
    },
    {
      "epoch": 0.6196,
      "grad_norm": 2.9781570434570312,
      "learning_rate": 3.993959731543625e-05,
      "loss": 0.6904,
      "step": 15490
    },
    {
      "epoch": 0.62,
      "grad_norm": 2.9832515716552734,
      "learning_rate": 3.993288590604027e-05,
      "loss": 0.6112,
      "step": 15500
    },
    {
      "epoch": 0.6204,
      "grad_norm": 2.1808359622955322,
      "learning_rate": 3.99261744966443e-05,
      "loss": 0.612,
      "step": 15510
    },
    {
      "epoch": 0.6208,
      "grad_norm": 2.6692512035369873,
      "learning_rate": 3.9919463087248326e-05,
      "loss": 0.619,
      "step": 15520
    },
    {
      "epoch": 0.6212,
      "grad_norm": 2.7151548862457275,
      "learning_rate": 3.991275167785235e-05,
      "loss": 0.7159,
      "step": 15530
    },
    {
      "epoch": 0.6216,
      "grad_norm": 2.266533136367798,
      "learning_rate": 3.990604026845638e-05,
      "loss": 0.6452,
      "step": 15540
    },
    {
      "epoch": 0.622,
      "grad_norm": 3.7735209465026855,
      "learning_rate": 3.9899328859060404e-05,
      "loss": 0.8066,
      "step": 15550
    },
    {
      "epoch": 0.6224,
      "grad_norm": 3.301746368408203,
      "learning_rate": 3.989261744966443e-05,
      "loss": 0.7748,
      "step": 15560
    },
    {
      "epoch": 0.6228,
      "grad_norm": 2.5517401695251465,
      "learning_rate": 3.9885906040268455e-05,
      "loss": 0.6262,
      "step": 15570
    },
    {
      "epoch": 0.6232,
      "grad_norm": 2.1647517681121826,
      "learning_rate": 3.987919463087248e-05,
      "loss": 0.5633,
      "step": 15580
    },
    {
      "epoch": 0.6236,
      "grad_norm": 2.6136279106140137,
      "learning_rate": 3.987248322147651e-05,
      "loss": 0.7095,
      "step": 15590
    },
    {
      "epoch": 0.624,
      "grad_norm": 3.1610562801361084,
      "learning_rate": 3.986577181208054e-05,
      "loss": 0.6298,
      "step": 15600
    },
    {
      "epoch": 0.6244,
      "grad_norm": 3.2614693641662598,
      "learning_rate": 3.985906040268457e-05,
      "loss": 0.6364,
      "step": 15610
    },
    {
      "epoch": 0.6248,
      "grad_norm": 2.0281076431274414,
      "learning_rate": 3.985234899328859e-05,
      "loss": 0.6092,
      "step": 15620
    },
    {
      "epoch": 0.6252,
      "grad_norm": 2.8400702476501465,
      "learning_rate": 3.984563758389262e-05,
      "loss": 0.6585,
      "step": 15630
    },
    {
      "epoch": 0.6256,
      "grad_norm": 2.6390230655670166,
      "learning_rate": 3.983892617449664e-05,
      "loss": 0.7177,
      "step": 15640
    },
    {
      "epoch": 0.626,
      "grad_norm": 2.4616949558258057,
      "learning_rate": 3.983221476510067e-05,
      "loss": 0.6761,
      "step": 15650
    },
    {
      "epoch": 0.6264,
      "grad_norm": 2.3749897480010986,
      "learning_rate": 3.9825503355704705e-05,
      "loss": 0.617,
      "step": 15660
    },
    {
      "epoch": 0.6268,
      "grad_norm": 2.197457790374756,
      "learning_rate": 3.9818791946308727e-05,
      "loss": 0.7031,
      "step": 15670
    },
    {
      "epoch": 0.6272,
      "grad_norm": 2.854912042617798,
      "learning_rate": 3.9812080536912755e-05,
      "loss": 0.6691,
      "step": 15680
    },
    {
      "epoch": 0.6276,
      "grad_norm": 2.439656972885132,
      "learning_rate": 3.980536912751678e-05,
      "loss": 0.594,
      "step": 15690
    },
    {
      "epoch": 0.628,
      "grad_norm": 2.774282693862915,
      "learning_rate": 3.9798657718120805e-05,
      "loss": 0.6525,
      "step": 15700
    },
    {
      "epoch": 0.6284,
      "grad_norm": 3.028691053390503,
      "learning_rate": 3.9791946308724834e-05,
      "loss": 0.627,
      "step": 15710
    },
    {
      "epoch": 0.6288,
      "grad_norm": 3.9521913528442383,
      "learning_rate": 3.978523489932886e-05,
      "loss": 0.7661,
      "step": 15720
    },
    {
      "epoch": 0.6292,
      "grad_norm": 3.9313011169433594,
      "learning_rate": 3.977852348993289e-05,
      "loss": 0.6153,
      "step": 15730
    },
    {
      "epoch": 0.6296,
      "grad_norm": 2.876455545425415,
      "learning_rate": 3.977181208053691e-05,
      "loss": 0.6666,
      "step": 15740
    },
    {
      "epoch": 0.63,
      "grad_norm": 2.2018702030181885,
      "learning_rate": 3.976510067114094e-05,
      "loss": 0.6036,
      "step": 15750
    },
    {
      "epoch": 0.6304,
      "grad_norm": 3.932279348373413,
      "learning_rate": 3.975838926174496e-05,
      "loss": 0.6552,
      "step": 15760
    },
    {
      "epoch": 0.6308,
      "grad_norm": 3.0360779762268066,
      "learning_rate": 3.9751677852349e-05,
      "loss": 0.619,
      "step": 15770
    },
    {
      "epoch": 0.6312,
      "grad_norm": 2.583052396774292,
      "learning_rate": 3.974496644295303e-05,
      "loss": 0.5603,
      "step": 15780
    },
    {
      "epoch": 0.6316,
      "grad_norm": 2.938479423522949,
      "learning_rate": 3.973825503355705e-05,
      "loss": 0.6941,
      "step": 15790
    },
    {
      "epoch": 0.632,
      "grad_norm": 3.0367674827575684,
      "learning_rate": 3.973154362416108e-05,
      "loss": 0.5698,
      "step": 15800
    },
    {
      "epoch": 0.6324,
      "grad_norm": 2.2833986282348633,
      "learning_rate": 3.97248322147651e-05,
      "loss": 0.5925,
      "step": 15810
    },
    {
      "epoch": 0.6328,
      "grad_norm": 2.7659029960632324,
      "learning_rate": 3.971812080536913e-05,
      "loss": 0.6582,
      "step": 15820
    },
    {
      "epoch": 0.6332,
      "grad_norm": 3.037580728530884,
      "learning_rate": 3.9711409395973156e-05,
      "loss": 0.6928,
      "step": 15830
    },
    {
      "epoch": 0.6336,
      "grad_norm": 3.1425228118896484,
      "learning_rate": 3.9704697986577185e-05,
      "loss": 0.6569,
      "step": 15840
    },
    {
      "epoch": 0.634,
      "grad_norm": 2.79679536819458,
      "learning_rate": 3.969798657718121e-05,
      "loss": 0.7633,
      "step": 15850
    },
    {
      "epoch": 0.6344,
      "grad_norm": 3.051236629486084,
      "learning_rate": 3.9691275167785235e-05,
      "loss": 0.6608,
      "step": 15860
    },
    {
      "epoch": 0.6348,
      "grad_norm": 1.9103388786315918,
      "learning_rate": 3.9684563758389264e-05,
      "loss": 0.631,
      "step": 15870
    },
    {
      "epoch": 0.6352,
      "grad_norm": 2.769672393798828,
      "learning_rate": 3.9677852348993285e-05,
      "loss": 0.6969,
      "step": 15880
    },
    {
      "epoch": 0.6356,
      "grad_norm": 3.5739617347717285,
      "learning_rate": 3.967114093959732e-05,
      "loss": 0.6131,
      "step": 15890
    },
    {
      "epoch": 0.636,
      "grad_norm": 2.9409701824188232,
      "learning_rate": 3.966442953020135e-05,
      "loss": 0.6307,
      "step": 15900
    },
    {
      "epoch": 0.6364,
      "grad_norm": 2.8866662979125977,
      "learning_rate": 3.965771812080537e-05,
      "loss": 0.6848,
      "step": 15910
    },
    {
      "epoch": 0.6368,
      "grad_norm": 2.6191256046295166,
      "learning_rate": 3.96510067114094e-05,
      "loss": 0.64,
      "step": 15920
    },
    {
      "epoch": 0.6372,
      "grad_norm": 2.5113556385040283,
      "learning_rate": 3.964429530201342e-05,
      "loss": 0.5808,
      "step": 15930
    },
    {
      "epoch": 0.6376,
      "grad_norm": 2.5078847408294678,
      "learning_rate": 3.963758389261745e-05,
      "loss": 0.6415,
      "step": 15940
    },
    {
      "epoch": 0.638,
      "grad_norm": 2.6233975887298584,
      "learning_rate": 3.963087248322148e-05,
      "loss": 0.5703,
      "step": 15950
    },
    {
      "epoch": 0.6384,
      "grad_norm": 2.4911444187164307,
      "learning_rate": 3.962416107382551e-05,
      "loss": 0.5941,
      "step": 15960
    },
    {
      "epoch": 0.6388,
      "grad_norm": 2.6405491828918457,
      "learning_rate": 3.9617449664429536e-05,
      "loss": 0.6952,
      "step": 15970
    },
    {
      "epoch": 0.6392,
      "grad_norm": 3.004049777984619,
      "learning_rate": 3.961073825503356e-05,
      "loss": 0.6357,
      "step": 15980
    },
    {
      "epoch": 0.6396,
      "grad_norm": 2.5081748962402344,
      "learning_rate": 3.9604026845637586e-05,
      "loss": 0.7413,
      "step": 15990
    },
    {
      "epoch": 0.64,
      "grad_norm": 2.6518468856811523,
      "learning_rate": 3.959731543624161e-05,
      "loss": 0.7626,
      "step": 16000
    },
    {
      "epoch": 0.6404,
      "grad_norm": 3.3208940029144287,
      "learning_rate": 3.959060402684564e-05,
      "loss": 0.5883,
      "step": 16010
    },
    {
      "epoch": 0.6408,
      "grad_norm": 2.4614245891571045,
      "learning_rate": 3.9583892617449665e-05,
      "loss": 0.7473,
      "step": 16020
    },
    {
      "epoch": 0.6412,
      "grad_norm": 3.079892158508301,
      "learning_rate": 3.957718120805369e-05,
      "loss": 0.7046,
      "step": 16030
    },
    {
      "epoch": 0.6416,
      "grad_norm": 3.1147894859313965,
      "learning_rate": 3.957046979865772e-05,
      "loss": 0.7196,
      "step": 16040
    },
    {
      "epoch": 0.642,
      "grad_norm": 2.9024529457092285,
      "learning_rate": 3.9563758389261744e-05,
      "loss": 0.5527,
      "step": 16050
    },
    {
      "epoch": 0.6424,
      "grad_norm": 2.465965986251831,
      "learning_rate": 3.955704697986577e-05,
      "loss": 0.6907,
      "step": 16060
    },
    {
      "epoch": 0.6428,
      "grad_norm": 2.864109754562378,
      "learning_rate": 3.95503355704698e-05,
      "loss": 0.5779,
      "step": 16070
    },
    {
      "epoch": 0.6432,
      "grad_norm": 2.6087985038757324,
      "learning_rate": 3.954362416107383e-05,
      "loss": 0.7629,
      "step": 16080
    },
    {
      "epoch": 0.6436,
      "grad_norm": 2.8370301723480225,
      "learning_rate": 3.953691275167785e-05,
      "loss": 0.6255,
      "step": 16090
    },
    {
      "epoch": 0.644,
      "grad_norm": 2.2473220825195312,
      "learning_rate": 3.953020134228188e-05,
      "loss": 0.6062,
      "step": 16100
    },
    {
      "epoch": 0.6444,
      "grad_norm": 3.9784224033355713,
      "learning_rate": 3.952348993288591e-05,
      "loss": 0.7584,
      "step": 16110
    },
    {
      "epoch": 0.6448,
      "grad_norm": 2.9230828285217285,
      "learning_rate": 3.951677852348994e-05,
      "loss": 0.6639,
      "step": 16120
    },
    {
      "epoch": 0.6452,
      "grad_norm": 2.971391439437866,
      "learning_rate": 3.9510067114093965e-05,
      "loss": 0.6716,
      "step": 16130
    },
    {
      "epoch": 0.6456,
      "grad_norm": 3.6617660522460938,
      "learning_rate": 3.950335570469799e-05,
      "loss": 0.7037,
      "step": 16140
    },
    {
      "epoch": 0.646,
      "grad_norm": 2.4304652214050293,
      "learning_rate": 3.9496644295302016e-05,
      "loss": 0.5311,
      "step": 16150
    },
    {
      "epoch": 0.6464,
      "grad_norm": 2.8079187870025635,
      "learning_rate": 3.9489932885906044e-05,
      "loss": 0.6199,
      "step": 16160
    },
    {
      "epoch": 0.6468,
      "grad_norm": 2.735928773880005,
      "learning_rate": 3.9483221476510066e-05,
      "loss": 0.7084,
      "step": 16170
    },
    {
      "epoch": 0.6472,
      "grad_norm": 3.049588680267334,
      "learning_rate": 3.9476510067114094e-05,
      "loss": 0.6094,
      "step": 16180
    },
    {
      "epoch": 0.6476,
      "grad_norm": 2.1536920070648193,
      "learning_rate": 3.946979865771812e-05,
      "loss": 0.5306,
      "step": 16190
    },
    {
      "epoch": 0.648,
      "grad_norm": 2.9033288955688477,
      "learning_rate": 3.946308724832215e-05,
      "loss": 0.6622,
      "step": 16200
    },
    {
      "epoch": 0.6484,
      "grad_norm": 2.2928202152252197,
      "learning_rate": 3.945637583892617e-05,
      "loss": 0.6785,
      "step": 16210
    },
    {
      "epoch": 0.6488,
      "grad_norm": 3.2940616607666016,
      "learning_rate": 3.94496644295302e-05,
      "loss": 0.645,
      "step": 16220
    },
    {
      "epoch": 0.6492,
      "grad_norm": 4.262206077575684,
      "learning_rate": 3.944295302013423e-05,
      "loss": 0.7025,
      "step": 16230
    },
    {
      "epoch": 0.6496,
      "grad_norm": 2.648700714111328,
      "learning_rate": 3.943624161073826e-05,
      "loss": 0.6872,
      "step": 16240
    },
    {
      "epoch": 0.65,
      "grad_norm": 3.3903422355651855,
      "learning_rate": 3.942953020134229e-05,
      "loss": 0.6896,
      "step": 16250
    },
    {
      "epoch": 0.6504,
      "grad_norm": 3.49595046043396,
      "learning_rate": 3.942281879194631e-05,
      "loss": 0.6161,
      "step": 16260
    },
    {
      "epoch": 0.6508,
      "grad_norm": 3.4042954444885254,
      "learning_rate": 3.941610738255034e-05,
      "loss": 0.6745,
      "step": 16270
    },
    {
      "epoch": 0.6512,
      "grad_norm": 2.2924716472625732,
      "learning_rate": 3.940939597315436e-05,
      "loss": 0.6076,
      "step": 16280
    },
    {
      "epoch": 0.6516,
      "grad_norm": 2.6406757831573486,
      "learning_rate": 3.940268456375839e-05,
      "loss": 0.6425,
      "step": 16290
    },
    {
      "epoch": 0.652,
      "grad_norm": 2.7034800052642822,
      "learning_rate": 3.939597315436242e-05,
      "loss": 0.6146,
      "step": 16300
    },
    {
      "epoch": 0.6524,
      "grad_norm": 3.1897318363189697,
      "learning_rate": 3.9389261744966445e-05,
      "loss": 0.6001,
      "step": 16310
    },
    {
      "epoch": 0.6528,
      "grad_norm": 2.7916510105133057,
      "learning_rate": 3.9382550335570474e-05,
      "loss": 0.6613,
      "step": 16320
    },
    {
      "epoch": 0.6532,
      "grad_norm": 2.9974026679992676,
      "learning_rate": 3.9375838926174496e-05,
      "loss": 0.6049,
      "step": 16330
    },
    {
      "epoch": 0.6536,
      "grad_norm": 3.0561230182647705,
      "learning_rate": 3.9369127516778524e-05,
      "loss": 0.6248,
      "step": 16340
    },
    {
      "epoch": 0.654,
      "grad_norm": 3.8648624420166016,
      "learning_rate": 3.936241610738255e-05,
      "loss": 0.7252,
      "step": 16350
    },
    {
      "epoch": 0.6544,
      "grad_norm": 2.825812816619873,
      "learning_rate": 3.935570469798658e-05,
      "loss": 0.702,
      "step": 16360
    },
    {
      "epoch": 0.6548,
      "grad_norm": 2.796757936477661,
      "learning_rate": 3.934899328859061e-05,
      "loss": 0.5684,
      "step": 16370
    },
    {
      "epoch": 0.6552,
      "grad_norm": 2.8536128997802734,
      "learning_rate": 3.934228187919463e-05,
      "loss": 0.692,
      "step": 16380
    },
    {
      "epoch": 0.6556,
      "grad_norm": 2.8434741497039795,
      "learning_rate": 3.933557046979866e-05,
      "loss": 0.6056,
      "step": 16390
    },
    {
      "epoch": 0.656,
      "grad_norm": 2.9845588207244873,
      "learning_rate": 3.932885906040268e-05,
      "loss": 0.6331,
      "step": 16400
    },
    {
      "epoch": 0.6564,
      "grad_norm": 2.9004201889038086,
      "learning_rate": 3.932214765100671e-05,
      "loss": 0.5699,
      "step": 16410
    },
    {
      "epoch": 0.6568,
      "grad_norm": 2.325535535812378,
      "learning_rate": 3.9315436241610746e-05,
      "loss": 0.6538,
      "step": 16420
    },
    {
      "epoch": 0.6572,
      "grad_norm": 2.4160962104797363,
      "learning_rate": 3.930872483221477e-05,
      "loss": 0.7568,
      "step": 16430
    },
    {
      "epoch": 0.6576,
      "grad_norm": 3.3030800819396973,
      "learning_rate": 3.9302013422818796e-05,
      "loss": 0.7013,
      "step": 16440
    },
    {
      "epoch": 0.658,
      "grad_norm": 2.4467904567718506,
      "learning_rate": 3.929530201342282e-05,
      "loss": 0.6181,
      "step": 16450
    },
    {
      "epoch": 0.6584,
      "grad_norm": 3.1068472862243652,
      "learning_rate": 3.9288590604026846e-05,
      "loss": 0.6158,
      "step": 16460
    },
    {
      "epoch": 0.6588,
      "grad_norm": 2.9256255626678467,
      "learning_rate": 3.9281879194630875e-05,
      "loss": 0.6038,
      "step": 16470
    },
    {
      "epoch": 0.6592,
      "grad_norm": 2.5895094871520996,
      "learning_rate": 3.9275167785234903e-05,
      "loss": 0.6147,
      "step": 16480
    },
    {
      "epoch": 0.6596,
      "grad_norm": 2.446857213973999,
      "learning_rate": 3.926845637583893e-05,
      "loss": 0.6502,
      "step": 16490
    },
    {
      "epoch": 0.66,
      "grad_norm": 2.414165735244751,
      "learning_rate": 3.9261744966442954e-05,
      "loss": 0.688,
      "step": 16500
    },
    {
      "epoch": 0.6604,
      "grad_norm": 3.4399590492248535,
      "learning_rate": 3.925503355704698e-05,
      "loss": 0.6578,
      "step": 16510
    },
    {
      "epoch": 0.6608,
      "grad_norm": 3.1444945335388184,
      "learning_rate": 3.9248322147651004e-05,
      "loss": 0.6173,
      "step": 16520
    },
    {
      "epoch": 0.6612,
      "grad_norm": 2.8786556720733643,
      "learning_rate": 3.924161073825503e-05,
      "loss": 0.661,
      "step": 16530
    },
    {
      "epoch": 0.6616,
      "grad_norm": 3.2467379570007324,
      "learning_rate": 3.923489932885907e-05,
      "loss": 0.6269,
      "step": 16540
    },
    {
      "epoch": 0.662,
      "grad_norm": 2.630397319793701,
      "learning_rate": 3.922818791946309e-05,
      "loss": 0.7213,
      "step": 16550
    },
    {
      "epoch": 0.6624,
      "grad_norm": 3.1746037006378174,
      "learning_rate": 3.922147651006712e-05,
      "loss": 0.7027,
      "step": 16560
    },
    {
      "epoch": 0.6628,
      "grad_norm": 2.4722402095794678,
      "learning_rate": 3.921476510067114e-05,
      "loss": 0.6256,
      "step": 16570
    },
    {
      "epoch": 0.6632,
      "grad_norm": 2.604987144470215,
      "learning_rate": 3.920805369127517e-05,
      "loss": 0.7037,
      "step": 16580
    },
    {
      "epoch": 0.6636,
      "grad_norm": 3.2738161087036133,
      "learning_rate": 3.92013422818792e-05,
      "loss": 0.7096,
      "step": 16590
    },
    {
      "epoch": 0.664,
      "grad_norm": 2.4159090518951416,
      "learning_rate": 3.9194630872483226e-05,
      "loss": 0.5736,
      "step": 16600
    },
    {
      "epoch": 0.6644,
      "grad_norm": 3.0978736877441406,
      "learning_rate": 3.9187919463087254e-05,
      "loss": 0.6892,
      "step": 16610
    },
    {
      "epoch": 0.6648,
      "grad_norm": 2.4957339763641357,
      "learning_rate": 3.9181208053691276e-05,
      "loss": 0.6112,
      "step": 16620
    },
    {
      "epoch": 0.6652,
      "grad_norm": 2.2683229446411133,
      "learning_rate": 3.9174496644295305e-05,
      "loss": 0.6132,
      "step": 16630
    },
    {
      "epoch": 0.6656,
      "grad_norm": 2.6586291790008545,
      "learning_rate": 3.9167785234899326e-05,
      "loss": 0.6951,
      "step": 16640
    },
    {
      "epoch": 0.666,
      "grad_norm": 2.891941785812378,
      "learning_rate": 3.916107382550336e-05,
      "loss": 0.7087,
      "step": 16650
    },
    {
      "epoch": 0.6664,
      "grad_norm": 2.129879951477051,
      "learning_rate": 3.915436241610738e-05,
      "loss": 0.6457,
      "step": 16660
    },
    {
      "epoch": 0.6668,
      "grad_norm": 2.7069215774536133,
      "learning_rate": 3.914765100671141e-05,
      "loss": 0.7243,
      "step": 16670
    },
    {
      "epoch": 0.6672,
      "grad_norm": 2.770142078399658,
      "learning_rate": 3.914093959731544e-05,
      "loss": 0.6708,
      "step": 16680
    },
    {
      "epoch": 0.6676,
      "grad_norm": 2.233508586883545,
      "learning_rate": 3.913422818791946e-05,
      "loss": 0.6256,
      "step": 16690
    },
    {
      "epoch": 0.668,
      "grad_norm": 3.0286409854888916,
      "learning_rate": 3.912751677852349e-05,
      "loss": 0.6529,
      "step": 16700
    },
    {
      "epoch": 0.6684,
      "grad_norm": 2.9890310764312744,
      "learning_rate": 3.912080536912752e-05,
      "loss": 0.6793,
      "step": 16710
    },
    {
      "epoch": 0.6688,
      "grad_norm": 2.2993505001068115,
      "learning_rate": 3.911409395973155e-05,
      "loss": 0.7156,
      "step": 16720
    },
    {
      "epoch": 0.6692,
      "grad_norm": 2.521427631378174,
      "learning_rate": 3.9107382550335576e-05,
      "loss": 0.594,
      "step": 16730
    },
    {
      "epoch": 0.6696,
      "grad_norm": 2.956130266189575,
      "learning_rate": 3.91006711409396e-05,
      "loss": 0.7603,
      "step": 16740
    },
    {
      "epoch": 0.67,
      "grad_norm": 2.4735944271087646,
      "learning_rate": 3.909395973154363e-05,
      "loss": 0.6344,
      "step": 16750
    },
    {
      "epoch": 0.6704,
      "grad_norm": 2.084751605987549,
      "learning_rate": 3.908724832214765e-05,
      "loss": 0.7404,
      "step": 16760
    },
    {
      "epoch": 0.6708,
      "grad_norm": 3.627316474914551,
      "learning_rate": 3.9080536912751684e-05,
      "loss": 0.6725,
      "step": 16770
    },
    {
      "epoch": 0.6712,
      "grad_norm": 2.3083643913269043,
      "learning_rate": 3.9073825503355706e-05,
      "loss": 0.612,
      "step": 16780
    },
    {
      "epoch": 0.6716,
      "grad_norm": 2.6350948810577393,
      "learning_rate": 3.9067114093959734e-05,
      "loss": 0.6151,
      "step": 16790
    },
    {
      "epoch": 0.672,
      "grad_norm": 3.4069998264312744,
      "learning_rate": 3.906040268456376e-05,
      "loss": 0.6302,
      "step": 16800
    },
    {
      "epoch": 0.6724,
      "grad_norm": 2.8082618713378906,
      "learning_rate": 3.9053691275167784e-05,
      "loss": 0.6084,
      "step": 16810
    },
    {
      "epoch": 0.6728,
      "grad_norm": 2.704960346221924,
      "learning_rate": 3.904697986577181e-05,
      "loss": 0.6369,
      "step": 16820
    },
    {
      "epoch": 0.6732,
      "grad_norm": 3.112938165664673,
      "learning_rate": 3.904026845637584e-05,
      "loss": 0.569,
      "step": 16830
    },
    {
      "epoch": 0.6736,
      "grad_norm": 2.7031657695770264,
      "learning_rate": 3.903355704697987e-05,
      "loss": 0.5422,
      "step": 16840
    },
    {
      "epoch": 0.674,
      "grad_norm": 4.248530864715576,
      "learning_rate": 3.902684563758389e-05,
      "loss": 0.6901,
      "step": 16850
    },
    {
      "epoch": 0.6744,
      "grad_norm": 2.9186441898345947,
      "learning_rate": 3.902013422818792e-05,
      "loss": 0.6061,
      "step": 16860
    },
    {
      "epoch": 0.6748,
      "grad_norm": 2.3909904956817627,
      "learning_rate": 3.901342281879195e-05,
      "loss": 0.5655,
      "step": 16870
    },
    {
      "epoch": 0.6752,
      "grad_norm": 3.2689261436462402,
      "learning_rate": 3.900671140939597e-05,
      "loss": 0.6776,
      "step": 16880
    },
    {
      "epoch": 0.6756,
      "grad_norm": 2.3777880668640137,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 0.6833,
      "step": 16890
    },
    {
      "epoch": 0.676,
      "grad_norm": 3.6601667404174805,
      "learning_rate": 3.899328859060403e-05,
      "loss": 0.6511,
      "step": 16900
    },
    {
      "epoch": 0.6764,
      "grad_norm": 2.8794591426849365,
      "learning_rate": 3.8986577181208056e-05,
      "loss": 0.686,
      "step": 16910
    },
    {
      "epoch": 0.6768,
      "grad_norm": 3.0541789531707764,
      "learning_rate": 3.897986577181208e-05,
      "loss": 0.6577,
      "step": 16920
    },
    {
      "epoch": 0.6772,
      "grad_norm": 3.78456974029541,
      "learning_rate": 3.897315436241611e-05,
      "loss": 0.7395,
      "step": 16930
    },
    {
      "epoch": 0.6776,
      "grad_norm": 2.5243794918060303,
      "learning_rate": 3.8966442953020135e-05,
      "loss": 0.5444,
      "step": 16940
    },
    {
      "epoch": 0.678,
      "grad_norm": 2.5961101055145264,
      "learning_rate": 3.8959731543624164e-05,
      "loss": 0.6713,
      "step": 16950
    },
    {
      "epoch": 0.6784,
      "grad_norm": 2.859834909439087,
      "learning_rate": 3.895302013422819e-05,
      "loss": 0.6482,
      "step": 16960
    },
    {
      "epoch": 0.6788,
      "grad_norm": 2.308690309524536,
      "learning_rate": 3.8946308724832214e-05,
      "loss": 0.722,
      "step": 16970
    },
    {
      "epoch": 0.6792,
      "grad_norm": 2.449629545211792,
      "learning_rate": 3.893959731543624e-05,
      "loss": 0.6095,
      "step": 16980
    },
    {
      "epoch": 0.6796,
      "grad_norm": 2.5356435775756836,
      "learning_rate": 3.893288590604027e-05,
      "loss": 0.6354,
      "step": 16990
    },
    {
      "epoch": 0.68,
      "grad_norm": 2.8770785331726074,
      "learning_rate": 3.89261744966443e-05,
      "loss": 0.6557,
      "step": 17000
    },
    {
      "epoch": 0.6804,
      "grad_norm": 3.231233596801758,
      "learning_rate": 3.891946308724833e-05,
      "loss": 0.7049,
      "step": 17010
    },
    {
      "epoch": 0.6808,
      "grad_norm": 2.9534451961517334,
      "learning_rate": 3.891275167785235e-05,
      "loss": 0.7156,
      "step": 17020
    },
    {
      "epoch": 0.6812,
      "grad_norm": 2.9440155029296875,
      "learning_rate": 3.890604026845638e-05,
      "loss": 0.6656,
      "step": 17030
    },
    {
      "epoch": 0.6816,
      "grad_norm": 3.7133140563964844,
      "learning_rate": 3.88993288590604e-05,
      "loss": 0.7146,
      "step": 17040
    },
    {
      "epoch": 0.682,
      "grad_norm": 2.2100026607513428,
      "learning_rate": 3.889261744966443e-05,
      "loss": 0.5971,
      "step": 17050
    },
    {
      "epoch": 0.6824,
      "grad_norm": 3.8702807426452637,
      "learning_rate": 3.888590604026846e-05,
      "loss": 0.6136,
      "step": 17060
    },
    {
      "epoch": 0.6828,
      "grad_norm": 3.362205743789673,
      "learning_rate": 3.8879194630872486e-05,
      "loss": 0.6379,
      "step": 17070
    },
    {
      "epoch": 0.6832,
      "grad_norm": 2.6337432861328125,
      "learning_rate": 3.8872483221476515e-05,
      "loss": 0.5604,
      "step": 17080
    },
    {
      "epoch": 0.6836,
      "grad_norm": 2.605609893798828,
      "learning_rate": 3.8865771812080536e-05,
      "loss": 0.7036,
      "step": 17090
    },
    {
      "epoch": 0.684,
      "grad_norm": 2.987057685852051,
      "learning_rate": 3.8859060402684565e-05,
      "loss": 0.6171,
      "step": 17100
    },
    {
      "epoch": 0.6844,
      "grad_norm": 2.3884315490722656,
      "learning_rate": 3.885234899328859e-05,
      "loss": 0.5803,
      "step": 17110
    },
    {
      "epoch": 0.6848,
      "grad_norm": 2.2108991146087646,
      "learning_rate": 3.884563758389262e-05,
      "loss": 0.6037,
      "step": 17120
    },
    {
      "epoch": 0.6852,
      "grad_norm": 3.5643489360809326,
      "learning_rate": 3.883892617449665e-05,
      "loss": 0.7703,
      "step": 17130
    },
    {
      "epoch": 0.6856,
      "grad_norm": 3.1288270950317383,
      "learning_rate": 3.883221476510067e-05,
      "loss": 0.5829,
      "step": 17140
    },
    {
      "epoch": 0.686,
      "grad_norm": 2.84372615814209,
      "learning_rate": 3.88255033557047e-05,
      "loss": 0.6617,
      "step": 17150
    },
    {
      "epoch": 0.6864,
      "grad_norm": 3.064120054244995,
      "learning_rate": 3.881879194630872e-05,
      "loss": 0.5572,
      "step": 17160
    },
    {
      "epoch": 0.6868,
      "grad_norm": 1.5904754400253296,
      "learning_rate": 3.881208053691275e-05,
      "loss": 0.4702,
      "step": 17170
    },
    {
      "epoch": 0.6872,
      "grad_norm": 2.6867165565490723,
      "learning_rate": 3.880536912751678e-05,
      "loss": 0.5387,
      "step": 17180
    },
    {
      "epoch": 0.6876,
      "grad_norm": 3.733205795288086,
      "learning_rate": 3.879865771812081e-05,
      "loss": 0.6551,
      "step": 17190
    },
    {
      "epoch": 0.688,
      "grad_norm": 2.7396199703216553,
      "learning_rate": 3.879194630872484e-05,
      "loss": 0.7276,
      "step": 17200
    },
    {
      "epoch": 0.6884,
      "grad_norm": 2.307896137237549,
      "learning_rate": 3.878523489932886e-05,
      "loss": 0.6734,
      "step": 17210
    },
    {
      "epoch": 0.6888,
      "grad_norm": 2.778264045715332,
      "learning_rate": 3.877852348993289e-05,
      "loss": 0.6624,
      "step": 17220
    },
    {
      "epoch": 0.6892,
      "grad_norm": 2.550386428833008,
      "learning_rate": 3.877181208053691e-05,
      "loss": 0.7409,
      "step": 17230
    },
    {
      "epoch": 0.6896,
      "grad_norm": 2.273405075073242,
      "learning_rate": 3.8765100671140944e-05,
      "loss": 0.6209,
      "step": 17240
    },
    {
      "epoch": 0.69,
      "grad_norm": 2.4345524311065674,
      "learning_rate": 3.875838926174497e-05,
      "loss": 0.5773,
      "step": 17250
    },
    {
      "epoch": 0.6904,
      "grad_norm": 2.9137847423553467,
      "learning_rate": 3.8751677852348995e-05,
      "loss": 0.6396,
      "step": 17260
    },
    {
      "epoch": 0.6908,
      "grad_norm": 2.3494386672973633,
      "learning_rate": 3.874496644295302e-05,
      "loss": 0.6616,
      "step": 17270
    },
    {
      "epoch": 0.6912,
      "grad_norm": 3.1524078845977783,
      "learning_rate": 3.8738255033557045e-05,
      "loss": 0.6584,
      "step": 17280
    },
    {
      "epoch": 0.6916,
      "grad_norm": 3.6666390895843506,
      "learning_rate": 3.8731543624161073e-05,
      "loss": 0.6832,
      "step": 17290
    },
    {
      "epoch": 0.692,
      "grad_norm": 3.01220440864563,
      "learning_rate": 3.87248322147651e-05,
      "loss": 0.705,
      "step": 17300
    },
    {
      "epoch": 0.6924,
      "grad_norm": 2.278951406478882,
      "learning_rate": 3.871812080536913e-05,
      "loss": 0.6298,
      "step": 17310
    },
    {
      "epoch": 0.6928,
      "grad_norm": 2.8980045318603516,
      "learning_rate": 3.871140939597316e-05,
      "loss": 0.4918,
      "step": 17320
    },
    {
      "epoch": 0.6932,
      "grad_norm": 2.747429132461548,
      "learning_rate": 3.870469798657718e-05,
      "loss": 0.7611,
      "step": 17330
    },
    {
      "epoch": 0.6936,
      "grad_norm": 2.7291746139526367,
      "learning_rate": 3.869798657718121e-05,
      "loss": 0.6793,
      "step": 17340
    },
    {
      "epoch": 0.694,
      "grad_norm": 2.3478140830993652,
      "learning_rate": 3.869127516778524e-05,
      "loss": 0.7283,
      "step": 17350
    },
    {
      "epoch": 0.6944,
      "grad_norm": 2.5554258823394775,
      "learning_rate": 3.8684563758389266e-05,
      "loss": 0.7338,
      "step": 17360
    },
    {
      "epoch": 0.6948,
      "grad_norm": 2.3910961151123047,
      "learning_rate": 3.8677852348993295e-05,
      "loss": 0.7003,
      "step": 17370
    },
    {
      "epoch": 0.6952,
      "grad_norm": 2.3748559951782227,
      "learning_rate": 3.867114093959732e-05,
      "loss": 0.6329,
      "step": 17380
    },
    {
      "epoch": 0.6956,
      "grad_norm": 2.66599178314209,
      "learning_rate": 3.8664429530201345e-05,
      "loss": 0.6576,
      "step": 17390
    },
    {
      "epoch": 0.696,
      "grad_norm": 2.885467052459717,
      "learning_rate": 3.865771812080537e-05,
      "loss": 0.6613,
      "step": 17400
    },
    {
      "epoch": 0.6964,
      "grad_norm": 3.3195526599884033,
      "learning_rate": 3.8651006711409396e-05,
      "loss": 0.7392,
      "step": 17410
    },
    {
      "epoch": 0.6968,
      "grad_norm": 2.886246681213379,
      "learning_rate": 3.8644295302013424e-05,
      "loss": 0.666,
      "step": 17420
    },
    {
      "epoch": 0.6972,
      "grad_norm": 2.7907216548919678,
      "learning_rate": 3.863758389261745e-05,
      "loss": 0.6095,
      "step": 17430
    },
    {
      "epoch": 0.6976,
      "grad_norm": 2.6040847301483154,
      "learning_rate": 3.863087248322148e-05,
      "loss": 0.6464,
      "step": 17440
    },
    {
      "epoch": 0.698,
      "grad_norm": 2.5672688484191895,
      "learning_rate": 3.86241610738255e-05,
      "loss": 0.6807,
      "step": 17450
    },
    {
      "epoch": 0.6984,
      "grad_norm": 3.732039451599121,
      "learning_rate": 3.861744966442953e-05,
      "loss": 0.761,
      "step": 17460
    },
    {
      "epoch": 0.6988,
      "grad_norm": 3.2355494499206543,
      "learning_rate": 3.861073825503356e-05,
      "loss": 0.7802,
      "step": 17470
    },
    {
      "epoch": 0.6992,
      "grad_norm": 2.732846975326538,
      "learning_rate": 3.860402684563759e-05,
      "loss": 0.5984,
      "step": 17480
    },
    {
      "epoch": 0.6996,
      "grad_norm": 3.2174887657165527,
      "learning_rate": 3.859731543624161e-05,
      "loss": 0.6009,
      "step": 17490
    },
    {
      "epoch": 0.7,
      "grad_norm": 3.165390729904175,
      "learning_rate": 3.859060402684564e-05,
      "loss": 0.6525,
      "step": 17500
    },
    {
      "epoch": 0.7004,
      "grad_norm": 3.0689074993133545,
      "learning_rate": 3.858389261744967e-05,
      "loss": 0.624,
      "step": 17510
    },
    {
      "epoch": 0.7008,
      "grad_norm": 3.493903160095215,
      "learning_rate": 3.857718120805369e-05,
      "loss": 0.5925,
      "step": 17520
    },
    {
      "epoch": 0.7012,
      "grad_norm": 3.003573417663574,
      "learning_rate": 3.857046979865772e-05,
      "loss": 0.6161,
      "step": 17530
    },
    {
      "epoch": 0.7016,
      "grad_norm": 3.2277753353118896,
      "learning_rate": 3.8563758389261746e-05,
      "loss": 0.6346,
      "step": 17540
    },
    {
      "epoch": 0.702,
      "grad_norm": 3.8999128341674805,
      "learning_rate": 3.8557046979865775e-05,
      "loss": 0.6273,
      "step": 17550
    },
    {
      "epoch": 0.7024,
      "grad_norm": 3.755049705505371,
      "learning_rate": 3.8550335570469804e-05,
      "loss": 0.6174,
      "step": 17560
    },
    {
      "epoch": 0.7028,
      "grad_norm": 3.0526247024536133,
      "learning_rate": 3.8543624161073825e-05,
      "loss": 0.7411,
      "step": 17570
    },
    {
      "epoch": 0.7032,
      "grad_norm": 2.9891536235809326,
      "learning_rate": 3.8536912751677854e-05,
      "loss": 0.6981,
      "step": 17580
    },
    {
      "epoch": 0.7036,
      "grad_norm": 3.5507020950317383,
      "learning_rate": 3.853020134228188e-05,
      "loss": 0.7209,
      "step": 17590
    },
    {
      "epoch": 0.704,
      "grad_norm": 3.3579845428466797,
      "learning_rate": 3.852348993288591e-05,
      "loss": 0.6196,
      "step": 17600
    },
    {
      "epoch": 0.7044,
      "grad_norm": 2.609142780303955,
      "learning_rate": 3.851677852348993e-05,
      "loss": 0.6409,
      "step": 17610
    },
    {
      "epoch": 0.7048,
      "grad_norm": 2.533576726913452,
      "learning_rate": 3.851006711409396e-05,
      "loss": 0.658,
      "step": 17620
    },
    {
      "epoch": 0.7052,
      "grad_norm": 2.3350071907043457,
      "learning_rate": 3.850335570469799e-05,
      "loss": 0.6615,
      "step": 17630
    },
    {
      "epoch": 0.7056,
      "grad_norm": 2.704578399658203,
      "learning_rate": 3.849664429530201e-05,
      "loss": 0.6374,
      "step": 17640
    },
    {
      "epoch": 0.706,
      "grad_norm": 2.756532669067383,
      "learning_rate": 3.848993288590605e-05,
      "loss": 0.5722,
      "step": 17650
    },
    {
      "epoch": 0.7064,
      "grad_norm": 3.075697898864746,
      "learning_rate": 3.848322147651007e-05,
      "loss": 0.6492,
      "step": 17660
    },
    {
      "epoch": 0.7068,
      "grad_norm": 2.743826389312744,
      "learning_rate": 3.84765100671141e-05,
      "loss": 0.6128,
      "step": 17670
    },
    {
      "epoch": 0.7072,
      "grad_norm": 3.3065569400787354,
      "learning_rate": 3.846979865771812e-05,
      "loss": 0.7086,
      "step": 17680
    },
    {
      "epoch": 0.7076,
      "grad_norm": 2.5805184841156006,
      "learning_rate": 3.846308724832215e-05,
      "loss": 0.6405,
      "step": 17690
    },
    {
      "epoch": 0.708,
      "grad_norm": 2.850447654724121,
      "learning_rate": 3.8456375838926176e-05,
      "loss": 0.715,
      "step": 17700
    },
    {
      "epoch": 0.7084,
      "grad_norm": 2.88507080078125,
      "learning_rate": 3.8449664429530205e-05,
      "loss": 0.6099,
      "step": 17710
    },
    {
      "epoch": 0.7088,
      "grad_norm": 2.813936471939087,
      "learning_rate": 3.844295302013423e-05,
      "loss": 0.6179,
      "step": 17720
    },
    {
      "epoch": 0.7092,
      "grad_norm": 3.1656992435455322,
      "learning_rate": 3.8436241610738255e-05,
      "loss": 0.6381,
      "step": 17730
    },
    {
      "epoch": 0.7096,
      "grad_norm": 2.640415668487549,
      "learning_rate": 3.8429530201342283e-05,
      "loss": 0.6302,
      "step": 17740
    },
    {
      "epoch": 0.71,
      "grad_norm": 2.8912603855133057,
      "learning_rate": 3.8422818791946305e-05,
      "loss": 0.607,
      "step": 17750
    },
    {
      "epoch": 0.7104,
      "grad_norm": 2.925459146499634,
      "learning_rate": 3.8416107382550334e-05,
      "loss": 0.7273,
      "step": 17760
    },
    {
      "epoch": 0.7108,
      "grad_norm": 3.287904977798462,
      "learning_rate": 3.840939597315437e-05,
      "loss": 0.7159,
      "step": 17770
    },
    {
      "epoch": 0.7112,
      "grad_norm": 3.071059226989746,
      "learning_rate": 3.840268456375839e-05,
      "loss": 0.7052,
      "step": 17780
    },
    {
      "epoch": 0.7116,
      "grad_norm": 2.4540932178497314,
      "learning_rate": 3.839597315436242e-05,
      "loss": 0.6702,
      "step": 17790
    },
    {
      "epoch": 0.712,
      "grad_norm": 2.9952239990234375,
      "learning_rate": 3.838926174496644e-05,
      "loss": 0.6773,
      "step": 17800
    },
    {
      "epoch": 0.7124,
      "grad_norm": 2.1060285568237305,
      "learning_rate": 3.838255033557047e-05,
      "loss": 0.5826,
      "step": 17810
    },
    {
      "epoch": 0.7128,
      "grad_norm": 2.586649179458618,
      "learning_rate": 3.83758389261745e-05,
      "loss": 0.5919,
      "step": 17820
    },
    {
      "epoch": 0.7132,
      "grad_norm": 3.3285775184631348,
      "learning_rate": 3.836912751677853e-05,
      "loss": 0.7479,
      "step": 17830
    },
    {
      "epoch": 0.7136,
      "grad_norm": 2.667691707611084,
      "learning_rate": 3.8362416107382555e-05,
      "loss": 0.5759,
      "step": 17840
    },
    {
      "epoch": 0.714,
      "grad_norm": 2.746779203414917,
      "learning_rate": 3.835570469798658e-05,
      "loss": 0.6411,
      "step": 17850
    },
    {
      "epoch": 0.7144,
      "grad_norm": 3.2178406715393066,
      "learning_rate": 3.8348993288590606e-05,
      "loss": 0.6291,
      "step": 17860
    },
    {
      "epoch": 0.7148,
      "grad_norm": 3.228527069091797,
      "learning_rate": 3.834228187919463e-05,
      "loss": 0.5766,
      "step": 17870
    },
    {
      "epoch": 0.7152,
      "grad_norm": 2.707022190093994,
      "learning_rate": 3.833557046979866e-05,
      "loss": 0.5735,
      "step": 17880
    },
    {
      "epoch": 0.7156,
      "grad_norm": 3.708523750305176,
      "learning_rate": 3.832885906040269e-05,
      "loss": 0.6678,
      "step": 17890
    },
    {
      "epoch": 0.716,
      "grad_norm": 2.6420671939849854,
      "learning_rate": 3.832214765100671e-05,
      "loss": 0.5523,
      "step": 17900
    },
    {
      "epoch": 0.7164,
      "grad_norm": 2.6153299808502197,
      "learning_rate": 3.831543624161074e-05,
      "loss": 0.6961,
      "step": 17910
    },
    {
      "epoch": 0.7168,
      "grad_norm": 2.417991876602173,
      "learning_rate": 3.8308724832214763e-05,
      "loss": 0.7167,
      "step": 17920
    },
    {
      "epoch": 0.7172,
      "grad_norm": 2.7902703285217285,
      "learning_rate": 3.830201342281879e-05,
      "loss": 0.655,
      "step": 17930
    },
    {
      "epoch": 0.7176,
      "grad_norm": 2.6775991916656494,
      "learning_rate": 3.829530201342282e-05,
      "loss": 0.6438,
      "step": 17940
    },
    {
      "epoch": 0.718,
      "grad_norm": 2.1523194313049316,
      "learning_rate": 3.828859060402685e-05,
      "loss": 0.5907,
      "step": 17950
    },
    {
      "epoch": 0.7184,
      "grad_norm": 2.4422736167907715,
      "learning_rate": 3.828187919463088e-05,
      "loss": 0.6117,
      "step": 17960
    },
    {
      "epoch": 0.7188,
      "grad_norm": 3.479269504547119,
      "learning_rate": 3.82751677852349e-05,
      "loss": 0.7513,
      "step": 17970
    },
    {
      "epoch": 0.7192,
      "grad_norm": 3.4245429039001465,
      "learning_rate": 3.826845637583893e-05,
      "loss": 0.7051,
      "step": 17980
    },
    {
      "epoch": 0.7196,
      "grad_norm": 3.1204349994659424,
      "learning_rate": 3.826174496644295e-05,
      "loss": 0.706,
      "step": 17990
    },
    {
      "epoch": 0.72,
      "grad_norm": 2.720520257949829,
      "learning_rate": 3.8255033557046985e-05,
      "loss": 0.6027,
      "step": 18000
    },
    {
      "epoch": 0.7204,
      "grad_norm": 2.917552947998047,
      "learning_rate": 3.8248322147651014e-05,
      "loss": 0.6433,
      "step": 18010
    },
    {
      "epoch": 0.7208,
      "grad_norm": 2.5124921798706055,
      "learning_rate": 3.8241610738255035e-05,
      "loss": 0.6234,
      "step": 18020
    },
    {
      "epoch": 0.7212,
      "grad_norm": 2.8396496772766113,
      "learning_rate": 3.8234899328859064e-05,
      "loss": 0.6377,
      "step": 18030
    },
    {
      "epoch": 0.7216,
      "grad_norm": 2.466649293899536,
      "learning_rate": 3.8228187919463086e-05,
      "loss": 0.687,
      "step": 18040
    },
    {
      "epoch": 0.722,
      "grad_norm": 3.0627634525299072,
      "learning_rate": 3.8221476510067114e-05,
      "loss": 0.6803,
      "step": 18050
    },
    {
      "epoch": 0.7224,
      "grad_norm": 3.442333698272705,
      "learning_rate": 3.821476510067114e-05,
      "loss": 0.6256,
      "step": 18060
    },
    {
      "epoch": 0.7228,
      "grad_norm": 3.0063469409942627,
      "learning_rate": 3.820805369127517e-05,
      "loss": 0.6363,
      "step": 18070
    },
    {
      "epoch": 0.7232,
      "grad_norm": 2.908412218093872,
      "learning_rate": 3.82013422818792e-05,
      "loss": 0.6894,
      "step": 18080
    },
    {
      "epoch": 0.7236,
      "grad_norm": 2.7849090099334717,
      "learning_rate": 3.819463087248322e-05,
      "loss": 0.6689,
      "step": 18090
    },
    {
      "epoch": 0.724,
      "grad_norm": 2.685570001602173,
      "learning_rate": 3.818791946308725e-05,
      "loss": 0.614,
      "step": 18100
    },
    {
      "epoch": 0.7244,
      "grad_norm": 2.407662868499756,
      "learning_rate": 3.818120805369127e-05,
      "loss": 0.5854,
      "step": 18110
    },
    {
      "epoch": 0.7248,
      "grad_norm": 3.44645094871521,
      "learning_rate": 3.817449664429531e-05,
      "loss": 0.6365,
      "step": 18120
    },
    {
      "epoch": 0.7252,
      "grad_norm": 2.0326900482177734,
      "learning_rate": 3.816778523489933e-05,
      "loss": 0.659,
      "step": 18130
    },
    {
      "epoch": 0.7256,
      "grad_norm": 2.731313467025757,
      "learning_rate": 3.816107382550336e-05,
      "loss": 0.6879,
      "step": 18140
    },
    {
      "epoch": 0.726,
      "grad_norm": 2.762331008911133,
      "learning_rate": 3.8154362416107386e-05,
      "loss": 0.6535,
      "step": 18150
    },
    {
      "epoch": 0.7264,
      "grad_norm": 3.430945873260498,
      "learning_rate": 3.814765100671141e-05,
      "loss": 0.7111,
      "step": 18160
    },
    {
      "epoch": 0.7268,
      "grad_norm": 2.5022284984588623,
      "learning_rate": 3.8140939597315436e-05,
      "loss": 0.6584,
      "step": 18170
    },
    {
      "epoch": 0.7272,
      "grad_norm": 4.103409290313721,
      "learning_rate": 3.8134228187919465e-05,
      "loss": 0.7021,
      "step": 18180
    },
    {
      "epoch": 0.7276,
      "grad_norm": 2.8356308937072754,
      "learning_rate": 3.8127516778523494e-05,
      "loss": 0.6316,
      "step": 18190
    },
    {
      "epoch": 0.728,
      "grad_norm": 3.119418144226074,
      "learning_rate": 3.812080536912752e-05,
      "loss": 0.6457,
      "step": 18200
    },
    {
      "epoch": 0.7284,
      "grad_norm": 2.8711800575256348,
      "learning_rate": 3.8114093959731544e-05,
      "loss": 0.6663,
      "step": 18210
    },
    {
      "epoch": 0.7288,
      "grad_norm": 2.0297155380249023,
      "learning_rate": 3.810738255033557e-05,
      "loss": 0.5709,
      "step": 18220
    },
    {
      "epoch": 0.7292,
      "grad_norm": 2.4113800525665283,
      "learning_rate": 3.81006711409396e-05,
      "loss": 0.5419,
      "step": 18230
    },
    {
      "epoch": 0.7296,
      "grad_norm": 3.096268653869629,
      "learning_rate": 3.809395973154363e-05,
      "loss": 0.6518,
      "step": 18240
    },
    {
      "epoch": 0.73,
      "grad_norm": 3.582397699356079,
      "learning_rate": 3.808724832214765e-05,
      "loss": 0.6977,
      "step": 18250
    },
    {
      "epoch": 0.7304,
      "grad_norm": 3.449704885482788,
      "learning_rate": 3.808053691275168e-05,
      "loss": 0.6694,
      "step": 18260
    },
    {
      "epoch": 0.7308,
      "grad_norm": 3.056062698364258,
      "learning_rate": 3.807382550335571e-05,
      "loss": 0.689,
      "step": 18270
    },
    {
      "epoch": 0.7312,
      "grad_norm": 3.2522695064544678,
      "learning_rate": 3.806711409395973e-05,
      "loss": 0.6108,
      "step": 18280
    },
    {
      "epoch": 0.7316,
      "grad_norm": 3.2113943099975586,
      "learning_rate": 3.806040268456376e-05,
      "loss": 0.5782,
      "step": 18290
    },
    {
      "epoch": 0.732,
      "grad_norm": 2.287821054458618,
      "learning_rate": 3.805369127516779e-05,
      "loss": 0.6627,
      "step": 18300
    },
    {
      "epoch": 0.7324,
      "grad_norm": 3.1780340671539307,
      "learning_rate": 3.8046979865771816e-05,
      "loss": 0.6753,
      "step": 18310
    },
    {
      "epoch": 0.7328,
      "grad_norm": 2.534308910369873,
      "learning_rate": 3.804026845637584e-05,
      "loss": 0.609,
      "step": 18320
    },
    {
      "epoch": 0.7332,
      "grad_norm": 3.177288770675659,
      "learning_rate": 3.8033557046979866e-05,
      "loss": 0.6767,
      "step": 18330
    },
    {
      "epoch": 0.7336,
      "grad_norm": 2.9344685077667236,
      "learning_rate": 3.8026845637583895e-05,
      "loss": 0.6256,
      "step": 18340
    },
    {
      "epoch": 0.734,
      "grad_norm": 2.8229310512542725,
      "learning_rate": 3.802013422818792e-05,
      "loss": 0.5483,
      "step": 18350
    },
    {
      "epoch": 0.7344,
      "grad_norm": 2.73649001121521,
      "learning_rate": 3.801342281879195e-05,
      "loss": 0.6712,
      "step": 18360
    },
    {
      "epoch": 0.7348,
      "grad_norm": 3.240237236022949,
      "learning_rate": 3.8006711409395974e-05,
      "loss": 0.6256,
      "step": 18370
    },
    {
      "epoch": 0.7352,
      "grad_norm": 3.2460389137268066,
      "learning_rate": 3.8e-05,
      "loss": 0.7432,
      "step": 18380
    },
    {
      "epoch": 0.7356,
      "grad_norm": 2.6687111854553223,
      "learning_rate": 3.7993288590604024e-05,
      "loss": 0.6611,
      "step": 18390
    },
    {
      "epoch": 0.736,
      "grad_norm": 3.4179494380950928,
      "learning_rate": 3.798657718120805e-05,
      "loss": 0.7229,
      "step": 18400
    },
    {
      "epoch": 0.7364,
      "grad_norm": 3.3402082920074463,
      "learning_rate": 3.797986577181208e-05,
      "loss": 0.7301,
      "step": 18410
    },
    {
      "epoch": 0.7368,
      "grad_norm": 2.510523557662964,
      "learning_rate": 3.797315436241611e-05,
      "loss": 0.6164,
      "step": 18420
    },
    {
      "epoch": 0.7372,
      "grad_norm": 2.609915256500244,
      "learning_rate": 3.796644295302014e-05,
      "loss": 0.5945,
      "step": 18430
    },
    {
      "epoch": 0.7376,
      "grad_norm": 2.0873756408691406,
      "learning_rate": 3.795973154362416e-05,
      "loss": 0.5804,
      "step": 18440
    },
    {
      "epoch": 0.738,
      "grad_norm": 3.023470640182495,
      "learning_rate": 3.795302013422819e-05,
      "loss": 0.6786,
      "step": 18450
    },
    {
      "epoch": 0.7384,
      "grad_norm": 3.551316976547241,
      "learning_rate": 3.794630872483222e-05,
      "loss": 0.7907,
      "step": 18460
    },
    {
      "epoch": 0.7388,
      "grad_norm": 2.180884838104248,
      "learning_rate": 3.7939597315436245e-05,
      "loss": 0.5828,
      "step": 18470
    },
    {
      "epoch": 0.7392,
      "grad_norm": 2.2210254669189453,
      "learning_rate": 3.7932885906040274e-05,
      "loss": 0.6051,
      "step": 18480
    },
    {
      "epoch": 0.7396,
      "grad_norm": 2.1385157108306885,
      "learning_rate": 3.7926174496644296e-05,
      "loss": 0.5995,
      "step": 18490
    },
    {
      "epoch": 0.74,
      "grad_norm": 4.4069294929504395,
      "learning_rate": 3.7919463087248324e-05,
      "loss": 0.5843,
      "step": 18500
    },
    {
      "epoch": 0.7404,
      "grad_norm": 2.124070405960083,
      "learning_rate": 3.7912751677852346e-05,
      "loss": 0.6165,
      "step": 18510
    },
    {
      "epoch": 0.7408,
      "grad_norm": 2.2968976497650146,
      "learning_rate": 3.7906040268456375e-05,
      "loss": 0.6302,
      "step": 18520
    },
    {
      "epoch": 0.7412,
      "grad_norm": 2.3413591384887695,
      "learning_rate": 3.789932885906041e-05,
      "loss": 0.6385,
      "step": 18530
    },
    {
      "epoch": 0.7416,
      "grad_norm": 2.739433526992798,
      "learning_rate": 3.789261744966443e-05,
      "loss": 0.6135,
      "step": 18540
    },
    {
      "epoch": 0.742,
      "grad_norm": 2.7085466384887695,
      "learning_rate": 3.788590604026846e-05,
      "loss": 0.7266,
      "step": 18550
    },
    {
      "epoch": 0.7424,
      "grad_norm": 2.546990156173706,
      "learning_rate": 3.787919463087248e-05,
      "loss": 0.6455,
      "step": 18560
    },
    {
      "epoch": 0.7428,
      "grad_norm": 2.5588059425354004,
      "learning_rate": 3.787248322147651e-05,
      "loss": 0.7666,
      "step": 18570
    },
    {
      "epoch": 0.7432,
      "grad_norm": 2.688906192779541,
      "learning_rate": 3.786577181208054e-05,
      "loss": 0.5891,
      "step": 18580
    },
    {
      "epoch": 0.7436,
      "grad_norm": 2.7010748386383057,
      "learning_rate": 3.785906040268457e-05,
      "loss": 0.5838,
      "step": 18590
    },
    {
      "epoch": 0.744,
      "grad_norm": 2.5975682735443115,
      "learning_rate": 3.7852348993288596e-05,
      "loss": 0.6307,
      "step": 18600
    },
    {
      "epoch": 0.7444,
      "grad_norm": 2.695477247238159,
      "learning_rate": 3.784563758389262e-05,
      "loss": 0.8019,
      "step": 18610
    },
    {
      "epoch": 0.7448,
      "grad_norm": 2.448190689086914,
      "learning_rate": 3.7838926174496647e-05,
      "loss": 0.7185,
      "step": 18620
    },
    {
      "epoch": 0.7452,
      "grad_norm": 2.764209508895874,
      "learning_rate": 3.783221476510067e-05,
      "loss": 0.6904,
      "step": 18630
    },
    {
      "epoch": 0.7456,
      "grad_norm": 3.1957240104675293,
      "learning_rate": 3.78255033557047e-05,
      "loss": 0.7227,
      "step": 18640
    },
    {
      "epoch": 0.746,
      "grad_norm": 2.911754608154297,
      "learning_rate": 3.781879194630873e-05,
      "loss": 0.6856,
      "step": 18650
    },
    {
      "epoch": 0.7464,
      "grad_norm": 2.817194938659668,
      "learning_rate": 3.7812080536912754e-05,
      "loss": 0.6813,
      "step": 18660
    },
    {
      "epoch": 0.7468,
      "grad_norm": 2.474632501602173,
      "learning_rate": 3.780536912751678e-05,
      "loss": 0.6189,
      "step": 18670
    },
    {
      "epoch": 0.7472,
      "grad_norm": 2.339911460876465,
      "learning_rate": 3.7798657718120804e-05,
      "loss": 0.635,
      "step": 18680
    },
    {
      "epoch": 0.7476,
      "grad_norm": 2.4786040782928467,
      "learning_rate": 3.779194630872483e-05,
      "loss": 0.6517,
      "step": 18690
    },
    {
      "epoch": 0.748,
      "grad_norm": 3.2406373023986816,
      "learning_rate": 3.778523489932886e-05,
      "loss": 0.6475,
      "step": 18700
    },
    {
      "epoch": 0.7484,
      "grad_norm": 3.701645851135254,
      "learning_rate": 3.777852348993289e-05,
      "loss": 0.7504,
      "step": 18710
    },
    {
      "epoch": 0.7488,
      "grad_norm": 3.1847729682922363,
      "learning_rate": 3.777181208053692e-05,
      "loss": 0.6993,
      "step": 18720
    },
    {
      "epoch": 0.7492,
      "grad_norm": 2.5921812057495117,
      "learning_rate": 3.776510067114094e-05,
      "loss": 0.7609,
      "step": 18730
    },
    {
      "epoch": 0.7496,
      "grad_norm": 2.7668397426605225,
      "learning_rate": 3.775838926174497e-05,
      "loss": 0.6194,
      "step": 18740
    },
    {
      "epoch": 0.75,
      "grad_norm": 2.199101448059082,
      "learning_rate": 3.775167785234899e-05,
      "loss": 0.6703,
      "step": 18750
    },
    {
      "epoch": 0.7504,
      "grad_norm": 2.5711121559143066,
      "learning_rate": 3.7744966442953026e-05,
      "loss": 0.6433,
      "step": 18760
    },
    {
      "epoch": 0.7508,
      "grad_norm": 2.298125982284546,
      "learning_rate": 3.773825503355705e-05,
      "loss": 0.5456,
      "step": 18770
    },
    {
      "epoch": 0.7512,
      "grad_norm": 2.2028284072875977,
      "learning_rate": 3.7731543624161076e-05,
      "loss": 0.4949,
      "step": 18780
    },
    {
      "epoch": 0.7516,
      "grad_norm": 3.45749568939209,
      "learning_rate": 3.7724832214765105e-05,
      "loss": 0.6852,
      "step": 18790
    },
    {
      "epoch": 0.752,
      "grad_norm": 3.182525396347046,
      "learning_rate": 3.7718120805369127e-05,
      "loss": 0.6499,
      "step": 18800
    },
    {
      "epoch": 0.7524,
      "grad_norm": 2.9674227237701416,
      "learning_rate": 3.7711409395973155e-05,
      "loss": 0.7581,
      "step": 18810
    },
    {
      "epoch": 0.7528,
      "grad_norm": 2.7631027698516846,
      "learning_rate": 3.7704697986577184e-05,
      "loss": 0.6458,
      "step": 18820
    },
    {
      "epoch": 0.7532,
      "grad_norm": 2.2563657760620117,
      "learning_rate": 3.769798657718121e-05,
      "loss": 0.6697,
      "step": 18830
    },
    {
      "epoch": 0.7536,
      "grad_norm": 3.081615924835205,
      "learning_rate": 3.769127516778524e-05,
      "loss": 0.7356,
      "step": 18840
    },
    {
      "epoch": 0.754,
      "grad_norm": 3.006171464920044,
      "learning_rate": 3.768456375838926e-05,
      "loss": 0.6273,
      "step": 18850
    },
    {
      "epoch": 0.7544,
      "grad_norm": 1.9384310245513916,
      "learning_rate": 3.767785234899329e-05,
      "loss": 0.6854,
      "step": 18860
    },
    {
      "epoch": 0.7548,
      "grad_norm": 3.1012024879455566,
      "learning_rate": 3.767114093959731e-05,
      "loss": 0.6696,
      "step": 18870
    },
    {
      "epoch": 0.7552,
      "grad_norm": 2.921243667602539,
      "learning_rate": 3.766442953020135e-05,
      "loss": 0.7366,
      "step": 18880
    },
    {
      "epoch": 0.7556,
      "grad_norm": 2.297517776489258,
      "learning_rate": 3.765771812080537e-05,
      "loss": 0.5127,
      "step": 18890
    },
    {
      "epoch": 0.756,
      "grad_norm": 3.5849504470825195,
      "learning_rate": 3.76510067114094e-05,
      "loss": 0.6697,
      "step": 18900
    },
    {
      "epoch": 0.7564,
      "grad_norm": 3.0715062618255615,
      "learning_rate": 3.764429530201343e-05,
      "loss": 0.7296,
      "step": 18910
    },
    {
      "epoch": 0.7568,
      "grad_norm": 2.770385503768921,
      "learning_rate": 3.763758389261745e-05,
      "loss": 0.598,
      "step": 18920
    },
    {
      "epoch": 0.7572,
      "grad_norm": 3.2590363025665283,
      "learning_rate": 3.763087248322148e-05,
      "loss": 0.6478,
      "step": 18930
    },
    {
      "epoch": 0.7576,
      "grad_norm": 2.3916175365448,
      "learning_rate": 3.7624161073825506e-05,
      "loss": 0.6635,
      "step": 18940
    },
    {
      "epoch": 0.758,
      "grad_norm": 3.2038934230804443,
      "learning_rate": 3.7617449664429534e-05,
      "loss": 0.6855,
      "step": 18950
    },
    {
      "epoch": 0.7584,
      "grad_norm": 2.924560546875,
      "learning_rate": 3.7610738255033556e-05,
      "loss": 0.5996,
      "step": 18960
    },
    {
      "epoch": 0.7588,
      "grad_norm": 2.6542670726776123,
      "learning_rate": 3.7604026845637585e-05,
      "loss": 0.601,
      "step": 18970
    },
    {
      "epoch": 0.7592,
      "grad_norm": 2.6980488300323486,
      "learning_rate": 3.759731543624161e-05,
      "loss": 0.6068,
      "step": 18980
    },
    {
      "epoch": 0.7596,
      "grad_norm": 2.675562858581543,
      "learning_rate": 3.7590604026845635e-05,
      "loss": 0.6478,
      "step": 18990
    },
    {
      "epoch": 0.76,
      "grad_norm": 2.6277289390563965,
      "learning_rate": 3.758389261744967e-05,
      "loss": 0.4995,
      "step": 19000
    },
    {
      "epoch": 0.7604,
      "grad_norm": 3.746654510498047,
      "learning_rate": 3.757718120805369e-05,
      "loss": 0.6882,
      "step": 19010
    },
    {
      "epoch": 0.7608,
      "grad_norm": 2.338688373565674,
      "learning_rate": 3.757046979865772e-05,
      "loss": 0.7288,
      "step": 19020
    },
    {
      "epoch": 0.7612,
      "grad_norm": 3.2009196281433105,
      "learning_rate": 3.756375838926175e-05,
      "loss": 0.7439,
      "step": 19030
    },
    {
      "epoch": 0.7616,
      "grad_norm": 2.798711061477661,
      "learning_rate": 3.755704697986577e-05,
      "loss": 0.6605,
      "step": 19040
    },
    {
      "epoch": 0.762,
      "grad_norm": 3.3391170501708984,
      "learning_rate": 3.75503355704698e-05,
      "loss": 0.7078,
      "step": 19050
    },
    {
      "epoch": 0.7624,
      "grad_norm": 3.0234334468841553,
      "learning_rate": 3.754362416107383e-05,
      "loss": 0.6648,
      "step": 19060
    },
    {
      "epoch": 0.7628,
      "grad_norm": 2.963672399520874,
      "learning_rate": 3.753691275167786e-05,
      "loss": 0.6097,
      "step": 19070
    },
    {
      "epoch": 0.7632,
      "grad_norm": 2.350733518600464,
      "learning_rate": 3.753020134228188e-05,
      "loss": 0.6023,
      "step": 19080
    },
    {
      "epoch": 0.7636,
      "grad_norm": 3.020902395248413,
      "learning_rate": 3.752348993288591e-05,
      "loss": 0.6271,
      "step": 19090
    },
    {
      "epoch": 0.764,
      "grad_norm": 2.7084834575653076,
      "learning_rate": 3.7516778523489936e-05,
      "loss": 0.5875,
      "step": 19100
    },
    {
      "epoch": 0.7644,
      "grad_norm": 4.015872478485107,
      "learning_rate": 3.7510067114093964e-05,
      "loss": 0.6543,
      "step": 19110
    },
    {
      "epoch": 0.7648,
      "grad_norm": 3.3391621112823486,
      "learning_rate": 3.750335570469799e-05,
      "loss": 0.5991,
      "step": 19120
    },
    {
      "epoch": 0.7652,
      "grad_norm": 2.861788272857666,
      "learning_rate": 3.7496644295302014e-05,
      "loss": 0.7211,
      "step": 19130
    },
    {
      "epoch": 0.7656,
      "grad_norm": 4.587080478668213,
      "learning_rate": 3.748993288590604e-05,
      "loss": 0.718,
      "step": 19140
    },
    {
      "epoch": 0.766,
      "grad_norm": 2.8821046352386475,
      "learning_rate": 3.7483221476510065e-05,
      "loss": 0.6104,
      "step": 19150
    },
    {
      "epoch": 0.7664,
      "grad_norm": 2.9002647399902344,
      "learning_rate": 3.747651006711409e-05,
      "loss": 0.6392,
      "step": 19160
    },
    {
      "epoch": 0.7668,
      "grad_norm": 2.1656806468963623,
      "learning_rate": 3.746979865771812e-05,
      "loss": 0.7,
      "step": 19170
    },
    {
      "epoch": 0.7672,
      "grad_norm": 2.59238600730896,
      "learning_rate": 3.746308724832215e-05,
      "loss": 0.6459,
      "step": 19180
    },
    {
      "epoch": 0.7676,
      "grad_norm": 3.394092321395874,
      "learning_rate": 3.745637583892618e-05,
      "loss": 0.7475,
      "step": 19190
    },
    {
      "epoch": 0.768,
      "grad_norm": 2.9153733253479004,
      "learning_rate": 3.74496644295302e-05,
      "loss": 0.6198,
      "step": 19200
    },
    {
      "epoch": 0.7684,
      "grad_norm": 2.8068785667419434,
      "learning_rate": 3.744295302013423e-05,
      "loss": 0.6984,
      "step": 19210
    },
    {
      "epoch": 0.7688,
      "grad_norm": 2.55497145652771,
      "learning_rate": 3.743624161073825e-05,
      "loss": 0.6382,
      "step": 19220
    },
    {
      "epoch": 0.7692,
      "grad_norm": 2.295140504837036,
      "learning_rate": 3.7429530201342286e-05,
      "loss": 0.6765,
      "step": 19230
    },
    {
      "epoch": 0.7696,
      "grad_norm": 3.0219101905822754,
      "learning_rate": 3.7422818791946315e-05,
      "loss": 0.6115,
      "step": 19240
    },
    {
      "epoch": 0.77,
      "grad_norm": 2.2858705520629883,
      "learning_rate": 3.741610738255034e-05,
      "loss": 0.5771,
      "step": 19250
    },
    {
      "epoch": 0.7704,
      "grad_norm": 2.9685208797454834,
      "learning_rate": 3.7409395973154365e-05,
      "loss": 0.567,
      "step": 19260
    },
    {
      "epoch": 0.7708,
      "grad_norm": 2.779207706451416,
      "learning_rate": 3.740268456375839e-05,
      "loss": 0.6105,
      "step": 19270
    },
    {
      "epoch": 0.7712,
      "grad_norm": 2.6382973194122314,
      "learning_rate": 3.7395973154362415e-05,
      "loss": 0.6673,
      "step": 19280
    },
    {
      "epoch": 0.7716,
      "grad_norm": 2.3124732971191406,
      "learning_rate": 3.7389261744966444e-05,
      "loss": 0.6488,
      "step": 19290
    },
    {
      "epoch": 0.772,
      "grad_norm": 1.9671870470046997,
      "learning_rate": 3.738255033557047e-05,
      "loss": 0.6691,
      "step": 19300
    },
    {
      "epoch": 0.7724,
      "grad_norm": 2.933844804763794,
      "learning_rate": 3.73758389261745e-05,
      "loss": 0.6552,
      "step": 19310
    },
    {
      "epoch": 0.7728,
      "grad_norm": 2.388317584991455,
      "learning_rate": 3.736912751677852e-05,
      "loss": 0.6773,
      "step": 19320
    },
    {
      "epoch": 0.7732,
      "grad_norm": 2.2352635860443115,
      "learning_rate": 3.736241610738255e-05,
      "loss": 0.561,
      "step": 19330
    },
    {
      "epoch": 0.7736,
      "grad_norm": 3.2965519428253174,
      "learning_rate": 3.735570469798657e-05,
      "loss": 0.6403,
      "step": 19340
    },
    {
      "epoch": 0.774,
      "grad_norm": 4.141732692718506,
      "learning_rate": 3.734899328859061e-05,
      "loss": 0.6205,
      "step": 19350
    },
    {
      "epoch": 0.7744,
      "grad_norm": 2.532724618911743,
      "learning_rate": 3.734228187919464e-05,
      "loss": 0.6378,
      "step": 19360
    },
    {
      "epoch": 0.7748,
      "grad_norm": 2.5048680305480957,
      "learning_rate": 3.733557046979866e-05,
      "loss": 0.6186,
      "step": 19370
    },
    {
      "epoch": 0.7752,
      "grad_norm": 2.3844215869903564,
      "learning_rate": 3.732885906040269e-05,
      "loss": 0.5599,
      "step": 19380
    },
    {
      "epoch": 0.7756,
      "grad_norm": 2.8930094242095947,
      "learning_rate": 3.732214765100671e-05,
      "loss": 0.7075,
      "step": 19390
    },
    {
      "epoch": 0.776,
      "grad_norm": 3.09838604927063,
      "learning_rate": 3.731543624161074e-05,
      "loss": 0.6223,
      "step": 19400
    },
    {
      "epoch": 0.7764,
      "grad_norm": 2.4066009521484375,
      "learning_rate": 3.7308724832214766e-05,
      "loss": 0.6296,
      "step": 19410
    },
    {
      "epoch": 0.7768,
      "grad_norm": 2.642413377761841,
      "learning_rate": 3.7302013422818795e-05,
      "loss": 0.581,
      "step": 19420
    },
    {
      "epoch": 0.7772,
      "grad_norm": 3.4222445487976074,
      "learning_rate": 3.729530201342282e-05,
      "loss": 0.654,
      "step": 19430
    },
    {
      "epoch": 0.7776,
      "grad_norm": 2.723402738571167,
      "learning_rate": 3.7288590604026845e-05,
      "loss": 0.5115,
      "step": 19440
    },
    {
      "epoch": 0.778,
      "grad_norm": 2.439492702484131,
      "learning_rate": 3.7281879194630874e-05,
      "loss": 0.6907,
      "step": 19450
    },
    {
      "epoch": 0.7784,
      "grad_norm": 3.355314016342163,
      "learning_rate": 3.72751677852349e-05,
      "loss": 0.7303,
      "step": 19460
    },
    {
      "epoch": 0.7788,
      "grad_norm": 3.1458475589752197,
      "learning_rate": 3.726845637583893e-05,
      "loss": 0.6973,
      "step": 19470
    },
    {
      "epoch": 0.7792,
      "grad_norm": 2.8816919326782227,
      "learning_rate": 3.726174496644296e-05,
      "loss": 0.6472,
      "step": 19480
    },
    {
      "epoch": 0.7796,
      "grad_norm": 2.4523909091949463,
      "learning_rate": 3.725503355704698e-05,
      "loss": 0.6216,
      "step": 19490
    },
    {
      "epoch": 0.78,
      "grad_norm": 3.534498453140259,
      "learning_rate": 3.724832214765101e-05,
      "loss": 0.812,
      "step": 19500
    },
    {
      "epoch": 0.7804,
      "grad_norm": 2.682774305343628,
      "learning_rate": 3.724161073825503e-05,
      "loss": 0.6101,
      "step": 19510
    },
    {
      "epoch": 0.7808,
      "grad_norm": 2.6352181434631348,
      "learning_rate": 3.723489932885906e-05,
      "loss": 0.6877,
      "step": 19520
    },
    {
      "epoch": 0.7812,
      "grad_norm": 2.697570323944092,
      "learning_rate": 3.722818791946309e-05,
      "loss": 0.6638,
      "step": 19530
    },
    {
      "epoch": 0.7816,
      "grad_norm": 3.7544989585876465,
      "learning_rate": 3.722147651006712e-05,
      "loss": 0.5866,
      "step": 19540
    },
    {
      "epoch": 0.782,
      "grad_norm": 2.301013946533203,
      "learning_rate": 3.7214765100671146e-05,
      "loss": 0.6381,
      "step": 19550
    },
    {
      "epoch": 0.7824,
      "grad_norm": 2.9056198596954346,
      "learning_rate": 3.720805369127517e-05,
      "loss": 0.6613,
      "step": 19560
    },
    {
      "epoch": 0.7828,
      "grad_norm": 2.9536783695220947,
      "learning_rate": 3.7201342281879196e-05,
      "loss": 0.6175,
      "step": 19570
    },
    {
      "epoch": 0.7832,
      "grad_norm": 2.685908794403076,
      "learning_rate": 3.7194630872483224e-05,
      "loss": 0.6108,
      "step": 19580
    },
    {
      "epoch": 0.7836,
      "grad_norm": 2.2140512466430664,
      "learning_rate": 3.718791946308725e-05,
      "loss": 0.6027,
      "step": 19590
    },
    {
      "epoch": 0.784,
      "grad_norm": 3.0471882820129395,
      "learning_rate": 3.7181208053691275e-05,
      "loss": 0.6693,
      "step": 19600
    },
    {
      "epoch": 0.7844,
      "grad_norm": 2.7148847579956055,
      "learning_rate": 3.71744966442953e-05,
      "loss": 0.6679,
      "step": 19610
    },
    {
      "epoch": 0.7848,
      "grad_norm": 2.6146299839019775,
      "learning_rate": 3.716778523489933e-05,
      "loss": 0.6723,
      "step": 19620
    },
    {
      "epoch": 0.7852,
      "grad_norm": 2.777719020843506,
      "learning_rate": 3.7161073825503354e-05,
      "loss": 0.7173,
      "step": 19630
    },
    {
      "epoch": 0.7856,
      "grad_norm": 2.9428417682647705,
      "learning_rate": 3.715436241610738e-05,
      "loss": 0.7381,
      "step": 19640
    },
    {
      "epoch": 0.786,
      "grad_norm": 3.763129234313965,
      "learning_rate": 3.714765100671141e-05,
      "loss": 0.7228,
      "step": 19650
    },
    {
      "epoch": 0.7864,
      "grad_norm": 3.8360466957092285,
      "learning_rate": 3.714093959731544e-05,
      "loss": 0.7993,
      "step": 19660
    },
    {
      "epoch": 0.7868,
      "grad_norm": 3.1702070236206055,
      "learning_rate": 3.713422818791947e-05,
      "loss": 0.6247,
      "step": 19670
    },
    {
      "epoch": 0.7872,
      "grad_norm": 2.4185564517974854,
      "learning_rate": 3.712751677852349e-05,
      "loss": 0.6469,
      "step": 19680
    },
    {
      "epoch": 0.7876,
      "grad_norm": 2.990539789199829,
      "learning_rate": 3.712080536912752e-05,
      "loss": 0.6598,
      "step": 19690
    },
    {
      "epoch": 0.788,
      "grad_norm": 2.24941086769104,
      "learning_rate": 3.711409395973155e-05,
      "loss": 0.5877,
      "step": 19700
    },
    {
      "epoch": 0.7884,
      "grad_norm": 3.2295143604278564,
      "learning_rate": 3.7107382550335575e-05,
      "loss": 0.679,
      "step": 19710
    },
    {
      "epoch": 0.7888,
      "grad_norm": 2.7994537353515625,
      "learning_rate": 3.71006711409396e-05,
      "loss": 0.5773,
      "step": 19720
    },
    {
      "epoch": 0.7892,
      "grad_norm": 2.6507816314697266,
      "learning_rate": 3.7093959731543626e-05,
      "loss": 0.6538,
      "step": 19730
    },
    {
      "epoch": 0.7896,
      "grad_norm": 3.7931036949157715,
      "learning_rate": 3.7087248322147654e-05,
      "loss": 0.6776,
      "step": 19740
    },
    {
      "epoch": 0.79,
      "grad_norm": 2.952317476272583,
      "learning_rate": 3.7080536912751676e-05,
      "loss": 0.6901,
      "step": 19750
    },
    {
      "epoch": 0.7904,
      "grad_norm": 2.8715200424194336,
      "learning_rate": 3.707382550335571e-05,
      "loss": 0.5762,
      "step": 19760
    },
    {
      "epoch": 0.7908,
      "grad_norm": 2.3567492961883545,
      "learning_rate": 3.706711409395973e-05,
      "loss": 0.6711,
      "step": 19770
    },
    {
      "epoch": 0.7912,
      "grad_norm": 2.717038631439209,
      "learning_rate": 3.706040268456376e-05,
      "loss": 0.6277,
      "step": 19780
    },
    {
      "epoch": 0.7916,
      "grad_norm": 2.804401159286499,
      "learning_rate": 3.705369127516778e-05,
      "loss": 0.6397,
      "step": 19790
    },
    {
      "epoch": 0.792,
      "grad_norm": 2.7457597255706787,
      "learning_rate": 3.704697986577181e-05,
      "loss": 0.7298,
      "step": 19800
    },
    {
      "epoch": 0.7924,
      "grad_norm": 2.7660553455352783,
      "learning_rate": 3.704026845637584e-05,
      "loss": 0.6567,
      "step": 19810
    },
    {
      "epoch": 0.7928,
      "grad_norm": 2.993800163269043,
      "learning_rate": 3.703355704697987e-05,
      "loss": 0.5781,
      "step": 19820
    },
    {
      "epoch": 0.7932,
      "grad_norm": 2.6249780654907227,
      "learning_rate": 3.70268456375839e-05,
      "loss": 0.5972,
      "step": 19830
    },
    {
      "epoch": 0.7936,
      "grad_norm": 2.4345431327819824,
      "learning_rate": 3.702013422818792e-05,
      "loss": 0.5548,
      "step": 19840
    },
    {
      "epoch": 0.794,
      "grad_norm": 2.385225772857666,
      "learning_rate": 3.701342281879195e-05,
      "loss": 0.6332,
      "step": 19850
    },
    {
      "epoch": 0.7944,
      "grad_norm": 2.17366361618042,
      "learning_rate": 3.7006711409395976e-05,
      "loss": 0.6833,
      "step": 19860
    },
    {
      "epoch": 0.7948,
      "grad_norm": 2.573822498321533,
      "learning_rate": 3.7e-05,
      "loss": 0.7243,
      "step": 19870
    },
    {
      "epoch": 0.7952,
      "grad_norm": 2.4837794303894043,
      "learning_rate": 3.6993288590604033e-05,
      "loss": 0.5953,
      "step": 19880
    },
    {
      "epoch": 0.7956,
      "grad_norm": 2.853266477584839,
      "learning_rate": 3.6986577181208055e-05,
      "loss": 0.6161,
      "step": 19890
    },
    {
      "epoch": 0.796,
      "grad_norm": 2.6179990768432617,
      "learning_rate": 3.6979865771812084e-05,
      "loss": 0.6697,
      "step": 19900
    },
    {
      "epoch": 0.7964,
      "grad_norm": 2.108604669570923,
      "learning_rate": 3.6973154362416106e-05,
      "loss": 0.5739,
      "step": 19910
    },
    {
      "epoch": 0.7968,
      "grad_norm": 2.7542152404785156,
      "learning_rate": 3.6966442953020134e-05,
      "loss": 0.6767,
      "step": 19920
    },
    {
      "epoch": 0.7972,
      "grad_norm": 2.695742607116699,
      "learning_rate": 3.695973154362416e-05,
      "loss": 0.581,
      "step": 19930
    },
    {
      "epoch": 0.7976,
      "grad_norm": 3.123898983001709,
      "learning_rate": 3.695302013422819e-05,
      "loss": 0.6727,
      "step": 19940
    },
    {
      "epoch": 0.798,
      "grad_norm": 2.5756301879882812,
      "learning_rate": 3.694630872483222e-05,
      "loss": 0.6166,
      "step": 19950
    },
    {
      "epoch": 0.7984,
      "grad_norm": 2.9848439693450928,
      "learning_rate": 3.693959731543624e-05,
      "loss": 0.6805,
      "step": 19960
    },
    {
      "epoch": 0.7988,
      "grad_norm": 3.01282000541687,
      "learning_rate": 3.693288590604027e-05,
      "loss": 0.6341,
      "step": 19970
    },
    {
      "epoch": 0.7992,
      "grad_norm": 2.334418773651123,
      "learning_rate": 3.692617449664429e-05,
      "loss": 0.5671,
      "step": 19980
    },
    {
      "epoch": 0.7996,
      "grad_norm": 3.1322286128997803,
      "learning_rate": 3.691946308724833e-05,
      "loss": 0.6146,
      "step": 19990
    },
    {
      "epoch": 0.8,
      "grad_norm": 2.5050268173217773,
      "learning_rate": 3.6912751677852356e-05,
      "loss": 0.7542,
      "step": 20000
    },
    {
      "epoch": 0.8004,
      "grad_norm": 2.287437677383423,
      "learning_rate": 3.690604026845638e-05,
      "loss": 0.6748,
      "step": 20010
    },
    {
      "epoch": 0.8008,
      "grad_norm": 3.0398106575012207,
      "learning_rate": 3.6899328859060406e-05,
      "loss": 0.5937,
      "step": 20020
    },
    {
      "epoch": 0.8012,
      "grad_norm": 2.720418930053711,
      "learning_rate": 3.689261744966443e-05,
      "loss": 0.6453,
      "step": 20030
    },
    {
      "epoch": 0.8016,
      "grad_norm": 1.8373079299926758,
      "learning_rate": 3.6885906040268456e-05,
      "loss": 0.7074,
      "step": 20040
    },
    {
      "epoch": 0.802,
      "grad_norm": 2.3694207668304443,
      "learning_rate": 3.6879194630872485e-05,
      "loss": 0.5857,
      "step": 20050
    },
    {
      "epoch": 0.8024,
      "grad_norm": 3.3307085037231445,
      "learning_rate": 3.6872483221476513e-05,
      "loss": 0.663,
      "step": 20060
    },
    {
      "epoch": 0.8028,
      "grad_norm": 2.9461309909820557,
      "learning_rate": 3.686577181208054e-05,
      "loss": 0.6321,
      "step": 20070
    },
    {
      "epoch": 0.8032,
      "grad_norm": 3.6145992279052734,
      "learning_rate": 3.6859060402684564e-05,
      "loss": 0.6549,
      "step": 20080
    },
    {
      "epoch": 0.8036,
      "grad_norm": 2.738481044769287,
      "learning_rate": 3.685234899328859e-05,
      "loss": 0.6095,
      "step": 20090
    },
    {
      "epoch": 0.804,
      "grad_norm": 2.249986171722412,
      "learning_rate": 3.6845637583892614e-05,
      "loss": 0.6176,
      "step": 20100
    },
    {
      "epoch": 0.8044,
      "grad_norm": 2.5465357303619385,
      "learning_rate": 3.683892617449665e-05,
      "loss": 0.6482,
      "step": 20110
    },
    {
      "epoch": 0.8048,
      "grad_norm": 2.9166269302368164,
      "learning_rate": 3.683221476510068e-05,
      "loss": 0.5976,
      "step": 20120
    },
    {
      "epoch": 0.8052,
      "grad_norm": 3.5691046714782715,
      "learning_rate": 3.68255033557047e-05,
      "loss": 0.6598,
      "step": 20130
    },
    {
      "epoch": 0.8056,
      "grad_norm": 2.835787773132324,
      "learning_rate": 3.681879194630873e-05,
      "loss": 0.6259,
      "step": 20140
    },
    {
      "epoch": 0.806,
      "grad_norm": 2.2415695190429688,
      "learning_rate": 3.681208053691275e-05,
      "loss": 0.6573,
      "step": 20150
    },
    {
      "epoch": 0.8064,
      "grad_norm": 2.6372005939483643,
      "learning_rate": 3.680536912751678e-05,
      "loss": 0.6167,
      "step": 20160
    },
    {
      "epoch": 0.8068,
      "grad_norm": 2.4542524814605713,
      "learning_rate": 3.679865771812081e-05,
      "loss": 0.6275,
      "step": 20170
    },
    {
      "epoch": 0.8072,
      "grad_norm": 2.9407553672790527,
      "learning_rate": 3.6791946308724836e-05,
      "loss": 0.667,
      "step": 20180
    },
    {
      "epoch": 0.8076,
      "grad_norm": 2.739750862121582,
      "learning_rate": 3.6785234899328864e-05,
      "loss": 0.6229,
      "step": 20190
    },
    {
      "epoch": 0.808,
      "grad_norm": 2.5658740997314453,
      "learning_rate": 3.6778523489932886e-05,
      "loss": 0.6557,
      "step": 20200
    },
    {
      "epoch": 0.8084,
      "grad_norm": 2.8965377807617188,
      "learning_rate": 3.6771812080536915e-05,
      "loss": 0.6063,
      "step": 20210
    },
    {
      "epoch": 0.8088,
      "grad_norm": 2.7066686153411865,
      "learning_rate": 3.6765100671140936e-05,
      "loss": 0.6817,
      "step": 20220
    },
    {
      "epoch": 0.8092,
      "grad_norm": 2.5364060401916504,
      "learning_rate": 3.675838926174497e-05,
      "loss": 0.5695,
      "step": 20230
    },
    {
      "epoch": 0.8096,
      "grad_norm": 2.7130749225616455,
      "learning_rate": 3.675167785234899e-05,
      "loss": 0.726,
      "step": 20240
    },
    {
      "epoch": 0.81,
      "grad_norm": 2.868290662765503,
      "learning_rate": 3.674496644295302e-05,
      "loss": 0.6683,
      "step": 20250
    },
    {
      "epoch": 0.8104,
      "grad_norm": 3.132082939147949,
      "learning_rate": 3.673825503355705e-05,
      "loss": 0.6709,
      "step": 20260
    },
    {
      "epoch": 0.8108,
      "grad_norm": 3.05066180229187,
      "learning_rate": 3.673154362416107e-05,
      "loss": 0.6541,
      "step": 20270
    },
    {
      "epoch": 0.8112,
      "grad_norm": 3.343435764312744,
      "learning_rate": 3.67248322147651e-05,
      "loss": 0.7181,
      "step": 20280
    },
    {
      "epoch": 0.8116,
      "grad_norm": 2.3377325534820557,
      "learning_rate": 3.671812080536913e-05,
      "loss": 0.5669,
      "step": 20290
    },
    {
      "epoch": 0.812,
      "grad_norm": 2.9997398853302,
      "learning_rate": 3.671140939597316e-05,
      "loss": 0.6086,
      "step": 20300
    },
    {
      "epoch": 0.8124,
      "grad_norm": 2.388828992843628,
      "learning_rate": 3.6704697986577186e-05,
      "loss": 0.6445,
      "step": 20310
    },
    {
      "epoch": 0.8128,
      "grad_norm": 3.2485222816467285,
      "learning_rate": 3.669798657718121e-05,
      "loss": 0.6439,
      "step": 20320
    },
    {
      "epoch": 0.8132,
      "grad_norm": 2.6188039779663086,
      "learning_rate": 3.669127516778524e-05,
      "loss": 0.6938,
      "step": 20330
    },
    {
      "epoch": 0.8136,
      "grad_norm": 3.6787800788879395,
      "learning_rate": 3.6684563758389265e-05,
      "loss": 0.7017,
      "step": 20340
    },
    {
      "epoch": 0.814,
      "grad_norm": 2.7237308025360107,
      "learning_rate": 3.6677852348993294e-05,
      "loss": 0.7031,
      "step": 20350
    },
    {
      "epoch": 0.8144,
      "grad_norm": 3.1904377937316895,
      "learning_rate": 3.6671140939597316e-05,
      "loss": 0.6437,
      "step": 20360
    },
    {
      "epoch": 0.8148,
      "grad_norm": 2.1352124214172363,
      "learning_rate": 3.6664429530201344e-05,
      "loss": 0.6343,
      "step": 20370
    },
    {
      "epoch": 0.8152,
      "grad_norm": 2.7999348640441895,
      "learning_rate": 3.665771812080537e-05,
      "loss": 0.5894,
      "step": 20380
    },
    {
      "epoch": 0.8156,
      "grad_norm": 3.053558588027954,
      "learning_rate": 3.6651006711409394e-05,
      "loss": 0.7726,
      "step": 20390
    },
    {
      "epoch": 0.816,
      "grad_norm": 2.919679880142212,
      "learning_rate": 3.664429530201342e-05,
      "loss": 0.6222,
      "step": 20400
    },
    {
      "epoch": 0.8164,
      "grad_norm": 2.276857614517212,
      "learning_rate": 3.663758389261745e-05,
      "loss": 0.7024,
      "step": 20410
    },
    {
      "epoch": 0.8168,
      "grad_norm": 3.535893201828003,
      "learning_rate": 3.663087248322148e-05,
      "loss": 0.7096,
      "step": 20420
    },
    {
      "epoch": 0.8172,
      "grad_norm": 3.165853500366211,
      "learning_rate": 3.66241610738255e-05,
      "loss": 0.6646,
      "step": 20430
    },
    {
      "epoch": 0.8176,
      "grad_norm": 2.533797025680542,
      "learning_rate": 3.661744966442953e-05,
      "loss": 0.6614,
      "step": 20440
    },
    {
      "epoch": 0.818,
      "grad_norm": 2.895681142807007,
      "learning_rate": 3.661073825503356e-05,
      "loss": 0.6377,
      "step": 20450
    },
    {
      "epoch": 0.8184,
      "grad_norm": 3.351382255554199,
      "learning_rate": 3.660402684563759e-05,
      "loss": 0.6563,
      "step": 20460
    },
    {
      "epoch": 0.8188,
      "grad_norm": 2.533212661743164,
      "learning_rate": 3.6597315436241616e-05,
      "loss": 0.6424,
      "step": 20470
    },
    {
      "epoch": 0.8192,
      "grad_norm": 1.8467493057250977,
      "learning_rate": 3.659060402684564e-05,
      "loss": 0.5424,
      "step": 20480
    },
    {
      "epoch": 0.8196,
      "grad_norm": 2.186194896697998,
      "learning_rate": 3.6583892617449666e-05,
      "loss": 0.5851,
      "step": 20490
    },
    {
      "epoch": 0.82,
      "grad_norm": 2.958665609359741,
      "learning_rate": 3.6577181208053695e-05,
      "loss": 0.5847,
      "step": 20500
    },
    {
      "epoch": 0.8204,
      "grad_norm": 3.6400725841522217,
      "learning_rate": 3.657046979865772e-05,
      "loss": 0.6398,
      "step": 20510
    },
    {
      "epoch": 0.8208,
      "grad_norm": 3.0532820224761963,
      "learning_rate": 3.6563758389261745e-05,
      "loss": 0.6857,
      "step": 20520
    },
    {
      "epoch": 0.8212,
      "grad_norm": 2.785339593887329,
      "learning_rate": 3.6557046979865774e-05,
      "loss": 0.6066,
      "step": 20530
    },
    {
      "epoch": 0.8216,
      "grad_norm": 3.3598029613494873,
      "learning_rate": 3.65503355704698e-05,
      "loss": 0.6295,
      "step": 20540
    },
    {
      "epoch": 0.822,
      "grad_norm": 2.433403730392456,
      "learning_rate": 3.6543624161073824e-05,
      "loss": 0.5237,
      "step": 20550
    },
    {
      "epoch": 0.8224,
      "grad_norm": 2.762821912765503,
      "learning_rate": 3.653691275167785e-05,
      "loss": 0.6335,
      "step": 20560
    },
    {
      "epoch": 0.8228,
      "grad_norm": 3.015998363494873,
      "learning_rate": 3.653020134228188e-05,
      "loss": 0.6352,
      "step": 20570
    },
    {
      "epoch": 0.8232,
      "grad_norm": 2.9748425483703613,
      "learning_rate": 3.652348993288591e-05,
      "loss": 0.7199,
      "step": 20580
    },
    {
      "epoch": 0.8236,
      "grad_norm": 2.7225160598754883,
      "learning_rate": 3.651677852348994e-05,
      "loss": 0.5965,
      "step": 20590
    },
    {
      "epoch": 0.824,
      "grad_norm": 2.5406248569488525,
      "learning_rate": 3.651006711409396e-05,
      "loss": 0.6887,
      "step": 20600
    },
    {
      "epoch": 0.8244,
      "grad_norm": 3.1695048809051514,
      "learning_rate": 3.650335570469799e-05,
      "loss": 0.6667,
      "step": 20610
    },
    {
      "epoch": 0.8248,
      "grad_norm": 3.4853973388671875,
      "learning_rate": 3.649664429530201e-05,
      "loss": 0.6405,
      "step": 20620
    },
    {
      "epoch": 0.8252,
      "grad_norm": 2.936939239501953,
      "learning_rate": 3.648993288590604e-05,
      "loss": 0.5677,
      "step": 20630
    },
    {
      "epoch": 0.8256,
      "grad_norm": 2.132577896118164,
      "learning_rate": 3.6483221476510074e-05,
      "loss": 0.6436,
      "step": 20640
    },
    {
      "epoch": 0.826,
      "grad_norm": 2.9105677604675293,
      "learning_rate": 3.6476510067114096e-05,
      "loss": 0.5659,
      "step": 20650
    },
    {
      "epoch": 0.8264,
      "grad_norm": 2.8525028228759766,
      "learning_rate": 3.6469798657718125e-05,
      "loss": 0.6791,
      "step": 20660
    },
    {
      "epoch": 0.8268,
      "grad_norm": 2.4015910625457764,
      "learning_rate": 3.6463087248322146e-05,
      "loss": 0.5825,
      "step": 20670
    },
    {
      "epoch": 0.8272,
      "grad_norm": 2.8911168575286865,
      "learning_rate": 3.6456375838926175e-05,
      "loss": 0.6169,
      "step": 20680
    },
    {
      "epoch": 0.8276,
      "grad_norm": 3.3714213371276855,
      "learning_rate": 3.6449664429530203e-05,
      "loss": 0.6979,
      "step": 20690
    },
    {
      "epoch": 0.828,
      "grad_norm": 2.5669617652893066,
      "learning_rate": 3.644295302013423e-05,
      "loss": 0.6964,
      "step": 20700
    },
    {
      "epoch": 0.8284,
      "grad_norm": 3.3203940391540527,
      "learning_rate": 3.643624161073826e-05,
      "loss": 0.7114,
      "step": 20710
    },
    {
      "epoch": 0.8288,
      "grad_norm": 2.7378451824188232,
      "learning_rate": 3.642953020134228e-05,
      "loss": 0.5923,
      "step": 20720
    },
    {
      "epoch": 0.8292,
      "grad_norm": 2.8036320209503174,
      "learning_rate": 3.642281879194631e-05,
      "loss": 0.7196,
      "step": 20730
    },
    {
      "epoch": 0.8296,
      "grad_norm": 2.8342957496643066,
      "learning_rate": 3.641610738255033e-05,
      "loss": 0.6672,
      "step": 20740
    },
    {
      "epoch": 0.83,
      "grad_norm": 2.611778736114502,
      "learning_rate": 3.640939597315436e-05,
      "loss": 0.623,
      "step": 20750
    },
    {
      "epoch": 0.8304,
      "grad_norm": 3.3803935050964355,
      "learning_rate": 3.6402684563758397e-05,
      "loss": 0.7531,
      "step": 20760
    },
    {
      "epoch": 0.8308,
      "grad_norm": 2.6229026317596436,
      "learning_rate": 3.639597315436242e-05,
      "loss": 0.7512,
      "step": 20770
    },
    {
      "epoch": 0.8312,
      "grad_norm": 2.9341235160827637,
      "learning_rate": 3.638926174496645e-05,
      "loss": 0.6554,
      "step": 20780
    },
    {
      "epoch": 0.8316,
      "grad_norm": 2.201152801513672,
      "learning_rate": 3.638255033557047e-05,
      "loss": 0.6623,
      "step": 20790
    },
    {
      "epoch": 0.832,
      "grad_norm": 2.997619390487671,
      "learning_rate": 3.63758389261745e-05,
      "loss": 0.5928,
      "step": 20800
    },
    {
      "epoch": 0.8324,
      "grad_norm": 3.6843819618225098,
      "learning_rate": 3.6369127516778526e-05,
      "loss": 0.7062,
      "step": 20810
    },
    {
      "epoch": 0.8328,
      "grad_norm": 3.3280749320983887,
      "learning_rate": 3.6362416107382554e-05,
      "loss": 0.5905,
      "step": 20820
    },
    {
      "epoch": 0.8332,
      "grad_norm": 2.614116907119751,
      "learning_rate": 3.635570469798658e-05,
      "loss": 0.6411,
      "step": 20830
    },
    {
      "epoch": 0.8336,
      "grad_norm": 4.94943904876709,
      "learning_rate": 3.6348993288590605e-05,
      "loss": 0.6421,
      "step": 20840
    },
    {
      "epoch": 0.834,
      "grad_norm": 3.179682493209839,
      "learning_rate": 3.634228187919463e-05,
      "loss": 0.7413,
      "step": 20850
    },
    {
      "epoch": 0.8344,
      "grad_norm": 2.2433056831359863,
      "learning_rate": 3.6335570469798655e-05,
      "loss": 0.6094,
      "step": 20860
    },
    {
      "epoch": 0.8348,
      "grad_norm": 2.686516523361206,
      "learning_rate": 3.632885906040269e-05,
      "loss": 0.6369,
      "step": 20870
    },
    {
      "epoch": 0.8352,
      "grad_norm": 2.3360345363616943,
      "learning_rate": 3.632214765100671e-05,
      "loss": 0.6346,
      "step": 20880
    },
    {
      "epoch": 0.8356,
      "grad_norm": 2.608717441558838,
      "learning_rate": 3.631543624161074e-05,
      "loss": 0.6403,
      "step": 20890
    },
    {
      "epoch": 0.836,
      "grad_norm": 3.1975553035736084,
      "learning_rate": 3.630872483221477e-05,
      "loss": 0.6759,
      "step": 20900
    },
    {
      "epoch": 0.8364,
      "grad_norm": 2.7851314544677734,
      "learning_rate": 3.630201342281879e-05,
      "loss": 0.6219,
      "step": 20910
    },
    {
      "epoch": 0.8368,
      "grad_norm": 2.5512900352478027,
      "learning_rate": 3.629530201342282e-05,
      "loss": 0.6768,
      "step": 20920
    },
    {
      "epoch": 0.8372,
      "grad_norm": 3.003229856491089,
      "learning_rate": 3.628859060402685e-05,
      "loss": 0.6456,
      "step": 20930
    },
    {
      "epoch": 0.8376,
      "grad_norm": 3.2285220623016357,
      "learning_rate": 3.6281879194630876e-05,
      "loss": 0.6083,
      "step": 20940
    },
    {
      "epoch": 0.838,
      "grad_norm": 3.328631639480591,
      "learning_rate": 3.6275167785234905e-05,
      "loss": 0.6658,
      "step": 20950
    },
    {
      "epoch": 0.8384,
      "grad_norm": 3.1262612342834473,
      "learning_rate": 3.626845637583893e-05,
      "loss": 0.6286,
      "step": 20960
    },
    {
      "epoch": 0.8388,
      "grad_norm": 2.9849467277526855,
      "learning_rate": 3.6261744966442955e-05,
      "loss": 0.6261,
      "step": 20970
    },
    {
      "epoch": 0.8392,
      "grad_norm": 2.229393720626831,
      "learning_rate": 3.625503355704698e-05,
      "loss": 0.6612,
      "step": 20980
    },
    {
      "epoch": 0.8396,
      "grad_norm": 2.0063700675964355,
      "learning_rate": 3.624832214765101e-05,
      "loss": 0.6623,
      "step": 20990
    },
    {
      "epoch": 0.84,
      "grad_norm": 2.8013064861297607,
      "learning_rate": 3.6241610738255034e-05,
      "loss": 0.7035,
      "step": 21000
    },
    {
      "epoch": 0.8404,
      "grad_norm": 4.085536003112793,
      "learning_rate": 3.623489932885906e-05,
      "loss": 0.6769,
      "step": 21010
    },
    {
      "epoch": 0.8408,
      "grad_norm": 2.9118518829345703,
      "learning_rate": 3.622818791946309e-05,
      "loss": 0.6279,
      "step": 21020
    },
    {
      "epoch": 0.8412,
      "grad_norm": 2.8541343212127686,
      "learning_rate": 3.622147651006711e-05,
      "loss": 0.6803,
      "step": 21030
    },
    {
      "epoch": 0.8416,
      "grad_norm": 3.5083584785461426,
      "learning_rate": 3.621476510067114e-05,
      "loss": 0.6525,
      "step": 21040
    },
    {
      "epoch": 0.842,
      "grad_norm": 2.6486237049102783,
      "learning_rate": 3.620805369127517e-05,
      "loss": 0.606,
      "step": 21050
    },
    {
      "epoch": 0.8424,
      "grad_norm": 2.6260600090026855,
      "learning_rate": 3.62013422818792e-05,
      "loss": 0.7148,
      "step": 21060
    },
    {
      "epoch": 0.8428,
      "grad_norm": 2.8877153396606445,
      "learning_rate": 3.619463087248322e-05,
      "loss": 0.6399,
      "step": 21070
    },
    {
      "epoch": 0.8432,
      "grad_norm": 2.7856266498565674,
      "learning_rate": 3.618791946308725e-05,
      "loss": 0.6174,
      "step": 21080
    },
    {
      "epoch": 0.8436,
      "grad_norm": 3.7148163318634033,
      "learning_rate": 3.618120805369128e-05,
      "loss": 0.7225,
      "step": 21090
    },
    {
      "epoch": 0.844,
      "grad_norm": 3.6815950870513916,
      "learning_rate": 3.61744966442953e-05,
      "loss": 0.7516,
      "step": 21100
    },
    {
      "epoch": 0.8444,
      "grad_norm": 2.4837021827697754,
      "learning_rate": 3.6167785234899335e-05,
      "loss": 0.6128,
      "step": 21110
    },
    {
      "epoch": 0.8448,
      "grad_norm": 3.4576144218444824,
      "learning_rate": 3.6161073825503356e-05,
      "loss": 0.6237,
      "step": 21120
    },
    {
      "epoch": 0.8452,
      "grad_norm": 2.6852328777313232,
      "learning_rate": 3.6154362416107385e-05,
      "loss": 0.6232,
      "step": 21130
    },
    {
      "epoch": 0.8456,
      "grad_norm": 3.054015874862671,
      "learning_rate": 3.6147651006711414e-05,
      "loss": 0.6965,
      "step": 21140
    },
    {
      "epoch": 0.846,
      "grad_norm": 2.2328381538391113,
      "learning_rate": 3.6140939597315435e-05,
      "loss": 0.5852,
      "step": 21150
    },
    {
      "epoch": 0.8464,
      "grad_norm": 2.6154625415802,
      "learning_rate": 3.6134228187919464e-05,
      "loss": 0.6003,
      "step": 21160
    },
    {
      "epoch": 0.8468,
      "grad_norm": 3.061631202697754,
      "learning_rate": 3.612751677852349e-05,
      "loss": 0.5663,
      "step": 21170
    },
    {
      "epoch": 0.8472,
      "grad_norm": 2.7267370223999023,
      "learning_rate": 3.612080536912752e-05,
      "loss": 0.6894,
      "step": 21180
    },
    {
      "epoch": 0.8476,
      "grad_norm": 3.4893720149993896,
      "learning_rate": 3.611409395973154e-05,
      "loss": 0.7483,
      "step": 21190
    },
    {
      "epoch": 0.848,
      "grad_norm": 2.8647053241729736,
      "learning_rate": 3.610738255033557e-05,
      "loss": 0.7426,
      "step": 21200
    },
    {
      "epoch": 0.8484,
      "grad_norm": 3.440636157989502,
      "learning_rate": 3.61006711409396e-05,
      "loss": 0.7127,
      "step": 21210
    },
    {
      "epoch": 0.8488,
      "grad_norm": 3.3624420166015625,
      "learning_rate": 3.609395973154363e-05,
      "loss": 0.5802,
      "step": 21220
    },
    {
      "epoch": 0.8492,
      "grad_norm": 2.4899001121520996,
      "learning_rate": 3.608724832214766e-05,
      "loss": 0.6352,
      "step": 21230
    },
    {
      "epoch": 0.8496,
      "grad_norm": 3.034518241882324,
      "learning_rate": 3.608053691275168e-05,
      "loss": 0.614,
      "step": 21240
    },
    {
      "epoch": 0.85,
      "grad_norm": 2.941889524459839,
      "learning_rate": 3.607382550335571e-05,
      "loss": 0.7088,
      "step": 21250
    },
    {
      "epoch": 0.8504,
      "grad_norm": 3.207120180130005,
      "learning_rate": 3.606711409395973e-05,
      "loss": 0.764,
      "step": 21260
    },
    {
      "epoch": 0.8508,
      "grad_norm": 3.091681718826294,
      "learning_rate": 3.606040268456376e-05,
      "loss": 0.694,
      "step": 21270
    },
    {
      "epoch": 0.8512,
      "grad_norm": 2.7920844554901123,
      "learning_rate": 3.6053691275167786e-05,
      "loss": 0.7085,
      "step": 21280
    },
    {
      "epoch": 0.8516,
      "grad_norm": 2.4143028259277344,
      "learning_rate": 3.6046979865771815e-05,
      "loss": 0.612,
      "step": 21290
    },
    {
      "epoch": 0.852,
      "grad_norm": 3.0630271434783936,
      "learning_rate": 3.604026845637584e-05,
      "loss": 0.6704,
      "step": 21300
    },
    {
      "epoch": 0.8524,
      "grad_norm": 2.2353522777557373,
      "learning_rate": 3.6033557046979865e-05,
      "loss": 0.6366,
      "step": 21310
    },
    {
      "epoch": 0.8528,
      "grad_norm": 2.7419111728668213,
      "learning_rate": 3.6026845637583893e-05,
      "loss": 0.6382,
      "step": 21320
    },
    {
      "epoch": 0.8532,
      "grad_norm": 3.2175421714782715,
      "learning_rate": 3.602013422818792e-05,
      "loss": 0.6262,
      "step": 21330
    },
    {
      "epoch": 0.8536,
      "grad_norm": 2.567720890045166,
      "learning_rate": 3.601342281879195e-05,
      "loss": 0.6626,
      "step": 21340
    },
    {
      "epoch": 0.854,
      "grad_norm": 3.099576711654663,
      "learning_rate": 3.600671140939598e-05,
      "loss": 0.6304,
      "step": 21350
    },
    {
      "epoch": 0.8544,
      "grad_norm": 3.9805891513824463,
      "learning_rate": 3.6e-05,
      "loss": 0.5889,
      "step": 21360
    },
    {
      "epoch": 0.8548,
      "grad_norm": 2.483046531677246,
      "learning_rate": 3.599328859060403e-05,
      "loss": 0.7639,
      "step": 21370
    },
    {
      "epoch": 0.8552,
      "grad_norm": 2.162038564682007,
      "learning_rate": 3.598657718120805e-05,
      "loss": 0.6644,
      "step": 21380
    },
    {
      "epoch": 0.8556,
      "grad_norm": 3.0042309761047363,
      "learning_rate": 3.597986577181208e-05,
      "loss": 0.521,
      "step": 21390
    },
    {
      "epoch": 0.856,
      "grad_norm": 2.2694578170776367,
      "learning_rate": 3.597315436241611e-05,
      "loss": 0.5997,
      "step": 21400
    },
    {
      "epoch": 0.8564,
      "grad_norm": 3.9843831062316895,
      "learning_rate": 3.596644295302014e-05,
      "loss": 0.6537,
      "step": 21410
    },
    {
      "epoch": 0.8568,
      "grad_norm": 3.126386880874634,
      "learning_rate": 3.5959731543624165e-05,
      "loss": 0.7509,
      "step": 21420
    },
    {
      "epoch": 0.8572,
      "grad_norm": 3.3724265098571777,
      "learning_rate": 3.595302013422819e-05,
      "loss": 0.7281,
      "step": 21430
    },
    {
      "epoch": 0.8576,
      "grad_norm": 3.3461639881134033,
      "learning_rate": 3.5946308724832216e-05,
      "loss": 0.7224,
      "step": 21440
    },
    {
      "epoch": 0.858,
      "grad_norm": 2.998181104660034,
      "learning_rate": 3.593959731543624e-05,
      "loss": 0.6529,
      "step": 21450
    },
    {
      "epoch": 0.8584,
      "grad_norm": 3.057715654373169,
      "learning_rate": 3.593288590604027e-05,
      "loss": 0.6583,
      "step": 21460
    },
    {
      "epoch": 0.8588,
      "grad_norm": 3.0065205097198486,
      "learning_rate": 3.59261744966443e-05,
      "loss": 0.5396,
      "step": 21470
    },
    {
      "epoch": 0.8592,
      "grad_norm": 3.011746406555176,
      "learning_rate": 3.591946308724832e-05,
      "loss": 0.6744,
      "step": 21480
    },
    {
      "epoch": 0.8596,
      "grad_norm": 3.2706851959228516,
      "learning_rate": 3.591275167785235e-05,
      "loss": 0.6967,
      "step": 21490
    },
    {
      "epoch": 0.86,
      "grad_norm": 2.7710883617401123,
      "learning_rate": 3.5906040268456373e-05,
      "loss": 0.6355,
      "step": 21500
    },
    {
      "epoch": 0.8604,
      "grad_norm": 2.564786195755005,
      "learning_rate": 3.58993288590604e-05,
      "loss": 0.6242,
      "step": 21510
    },
    {
      "epoch": 0.8608,
      "grad_norm": 2.109107255935669,
      "learning_rate": 3.589261744966443e-05,
      "loss": 0.5691,
      "step": 21520
    },
    {
      "epoch": 0.8612,
      "grad_norm": 2.4378323554992676,
      "learning_rate": 3.588590604026846e-05,
      "loss": 0.5543,
      "step": 21530
    },
    {
      "epoch": 0.8616,
      "grad_norm": 2.532006025314331,
      "learning_rate": 3.587919463087249e-05,
      "loss": 0.6581,
      "step": 21540
    },
    {
      "epoch": 0.862,
      "grad_norm": 2.833775043487549,
      "learning_rate": 3.587248322147651e-05,
      "loss": 0.6392,
      "step": 21550
    },
    {
      "epoch": 0.8624,
      "grad_norm": 3.474369525909424,
      "learning_rate": 3.586577181208054e-05,
      "loss": 0.6131,
      "step": 21560
    },
    {
      "epoch": 0.8628,
      "grad_norm": 3.0273354053497314,
      "learning_rate": 3.5859060402684567e-05,
      "loss": 0.6673,
      "step": 21570
    },
    {
      "epoch": 0.8632,
      "grad_norm": 2.879448175430298,
      "learning_rate": 3.5852348993288595e-05,
      "loss": 0.7087,
      "step": 21580
    },
    {
      "epoch": 0.8636,
      "grad_norm": 2.821503162384033,
      "learning_rate": 3.5845637583892624e-05,
      "loss": 0.6146,
      "step": 21590
    },
    {
      "epoch": 0.864,
      "grad_norm": 2.586021900177002,
      "learning_rate": 3.5838926174496645e-05,
      "loss": 0.6587,
      "step": 21600
    },
    {
      "epoch": 0.8644,
      "grad_norm": 3.489621877670288,
      "learning_rate": 3.5832214765100674e-05,
      "loss": 0.5919,
      "step": 21610
    },
    {
      "epoch": 0.8648,
      "grad_norm": 2.5228331089019775,
      "learning_rate": 3.5825503355704696e-05,
      "loss": 0.77,
      "step": 21620
    },
    {
      "epoch": 0.8652,
      "grad_norm": 2.2717933654785156,
      "learning_rate": 3.5818791946308724e-05,
      "loss": 0.6147,
      "step": 21630
    },
    {
      "epoch": 0.8656,
      "grad_norm": 3.6536362171173096,
      "learning_rate": 3.581208053691275e-05,
      "loss": 0.6961,
      "step": 21640
    },
    {
      "epoch": 0.866,
      "grad_norm": 2.652631998062134,
      "learning_rate": 3.580536912751678e-05,
      "loss": 0.6526,
      "step": 21650
    },
    {
      "epoch": 0.8664,
      "grad_norm": 2.5757479667663574,
      "learning_rate": 3.579865771812081e-05,
      "loss": 0.6239,
      "step": 21660
    },
    {
      "epoch": 0.8668,
      "grad_norm": 2.9498963356018066,
      "learning_rate": 3.579194630872483e-05,
      "loss": 0.6834,
      "step": 21670
    },
    {
      "epoch": 0.8672,
      "grad_norm": 2.456939697265625,
      "learning_rate": 3.578523489932886e-05,
      "loss": 0.531,
      "step": 21680
    },
    {
      "epoch": 0.8676,
      "grad_norm": 3.8619444370269775,
      "learning_rate": 3.577852348993289e-05,
      "loss": 0.8,
      "step": 21690
    },
    {
      "epoch": 0.868,
      "grad_norm": 2.6897470951080322,
      "learning_rate": 3.577181208053692e-05,
      "loss": 0.6554,
      "step": 21700
    },
    {
      "epoch": 0.8684,
      "grad_norm": 2.552377700805664,
      "learning_rate": 3.576510067114094e-05,
      "loss": 0.5278,
      "step": 21710
    },
    {
      "epoch": 0.8688,
      "grad_norm": 3.243558406829834,
      "learning_rate": 3.575838926174497e-05,
      "loss": 0.5736,
      "step": 21720
    },
    {
      "epoch": 0.8692,
      "grad_norm": 2.873225212097168,
      "learning_rate": 3.5751677852348996e-05,
      "loss": 0.6957,
      "step": 21730
    },
    {
      "epoch": 0.8696,
      "grad_norm": 2.72536563873291,
      "learning_rate": 3.574496644295302e-05,
      "loss": 0.5949,
      "step": 21740
    },
    {
      "epoch": 0.87,
      "grad_norm": 2.397379159927368,
      "learning_rate": 3.5738255033557046e-05,
      "loss": 0.6551,
      "step": 21750
    },
    {
      "epoch": 0.8704,
      "grad_norm": 3.4521965980529785,
      "learning_rate": 3.5731543624161075e-05,
      "loss": 0.7224,
      "step": 21760
    },
    {
      "epoch": 0.8708,
      "grad_norm": 2.077726364135742,
      "learning_rate": 3.5724832214765104e-05,
      "loss": 0.5696,
      "step": 21770
    },
    {
      "epoch": 0.8712,
      "grad_norm": 2.915966510772705,
      "learning_rate": 3.571812080536913e-05,
      "loss": 0.7064,
      "step": 21780
    },
    {
      "epoch": 0.8716,
      "grad_norm": 1.9741591215133667,
      "learning_rate": 3.5711409395973154e-05,
      "loss": 0.595,
      "step": 21790
    },
    {
      "epoch": 0.872,
      "grad_norm": 2.538238286972046,
      "learning_rate": 3.570469798657718e-05,
      "loss": 0.6403,
      "step": 21800
    },
    {
      "epoch": 0.8724,
      "grad_norm": 2.8352255821228027,
      "learning_rate": 3.569798657718121e-05,
      "loss": 0.7137,
      "step": 21810
    },
    {
      "epoch": 0.8728,
      "grad_norm": 3.153444766998291,
      "learning_rate": 3.569127516778524e-05,
      "loss": 0.61,
      "step": 21820
    },
    {
      "epoch": 0.8732,
      "grad_norm": 2.6691155433654785,
      "learning_rate": 3.568456375838926e-05,
      "loss": 0.6537,
      "step": 21830
    },
    {
      "epoch": 0.8736,
      "grad_norm": 2.4387383460998535,
      "learning_rate": 3.567785234899329e-05,
      "loss": 0.6674,
      "step": 21840
    },
    {
      "epoch": 0.874,
      "grad_norm": 4.013519763946533,
      "learning_rate": 3.567114093959732e-05,
      "loss": 0.6383,
      "step": 21850
    },
    {
      "epoch": 0.8744,
      "grad_norm": 6.241938591003418,
      "learning_rate": 3.566442953020134e-05,
      "loss": 0.6536,
      "step": 21860
    },
    {
      "epoch": 0.8748,
      "grad_norm": 3.7498104572296143,
      "learning_rate": 3.5657718120805375e-05,
      "loss": 0.7414,
      "step": 21870
    },
    {
      "epoch": 0.8752,
      "grad_norm": 2.9926652908325195,
      "learning_rate": 3.56510067114094e-05,
      "loss": 0.613,
      "step": 21880
    },
    {
      "epoch": 0.8756,
      "grad_norm": 3.6787540912628174,
      "learning_rate": 3.5644295302013426e-05,
      "loss": 0.6403,
      "step": 21890
    },
    {
      "epoch": 0.876,
      "grad_norm": 2.9747612476348877,
      "learning_rate": 3.563758389261745e-05,
      "loss": 0.6608,
      "step": 21900
    },
    {
      "epoch": 0.8764,
      "grad_norm": 3.3329832553863525,
      "learning_rate": 3.5630872483221476e-05,
      "loss": 0.62,
      "step": 21910
    },
    {
      "epoch": 0.8768,
      "grad_norm": 2.747211456298828,
      "learning_rate": 3.5624161073825505e-05,
      "loss": 0.6568,
      "step": 21920
    },
    {
      "epoch": 0.8772,
      "grad_norm": 2.6756083965301514,
      "learning_rate": 3.561744966442953e-05,
      "loss": 0.6738,
      "step": 21930
    },
    {
      "epoch": 0.8776,
      "grad_norm": 3.131394624710083,
      "learning_rate": 3.561073825503356e-05,
      "loss": 0.6781,
      "step": 21940
    },
    {
      "epoch": 0.878,
      "grad_norm": 2.649827241897583,
      "learning_rate": 3.5604026845637584e-05,
      "loss": 0.585,
      "step": 21950
    },
    {
      "epoch": 0.8784,
      "grad_norm": 2.43864369392395,
      "learning_rate": 3.559731543624161e-05,
      "loss": 0.5421,
      "step": 21960
    },
    {
      "epoch": 0.8788,
      "grad_norm": 2.605436086654663,
      "learning_rate": 3.559060402684564e-05,
      "loss": 0.7063,
      "step": 21970
    },
    {
      "epoch": 0.8792,
      "grad_norm": 2.6581408977508545,
      "learning_rate": 3.558389261744966e-05,
      "loss": 0.5557,
      "step": 21980
    },
    {
      "epoch": 0.8796,
      "grad_norm": 2.714548110961914,
      "learning_rate": 3.55771812080537e-05,
      "loss": 0.566,
      "step": 21990
    },
    {
      "epoch": 0.88,
      "grad_norm": 2.9480512142181396,
      "learning_rate": 3.557046979865772e-05,
      "loss": 0.6683,
      "step": 22000
    },
    {
      "epoch": 0.8804,
      "grad_norm": 2.4658548831939697,
      "learning_rate": 3.556375838926175e-05,
      "loss": 0.5436,
      "step": 22010
    },
    {
      "epoch": 0.8808,
      "grad_norm": 3.650261163711548,
      "learning_rate": 3.555704697986577e-05,
      "loss": 0.6726,
      "step": 22020
    },
    {
      "epoch": 0.8812,
      "grad_norm": 2.296212911605835,
      "learning_rate": 3.55503355704698e-05,
      "loss": 0.6129,
      "step": 22030
    },
    {
      "epoch": 0.8816,
      "grad_norm": 3.198395252227783,
      "learning_rate": 3.554362416107383e-05,
      "loss": 0.569,
      "step": 22040
    },
    {
      "epoch": 0.882,
      "grad_norm": 3.365407943725586,
      "learning_rate": 3.5536912751677855e-05,
      "loss": 0.6191,
      "step": 22050
    },
    {
      "epoch": 0.8824,
      "grad_norm": 3.3283774852752686,
      "learning_rate": 3.5530201342281884e-05,
      "loss": 0.7207,
      "step": 22060
    },
    {
      "epoch": 0.8828,
      "grad_norm": 2.385096788406372,
      "learning_rate": 3.5523489932885906e-05,
      "loss": 0.6559,
      "step": 22070
    },
    {
      "epoch": 0.8832,
      "grad_norm": 2.2189857959747314,
      "learning_rate": 3.5516778523489934e-05,
      "loss": 0.5609,
      "step": 22080
    },
    {
      "epoch": 0.8836,
      "grad_norm": 2.4988551139831543,
      "learning_rate": 3.5510067114093956e-05,
      "loss": 0.6264,
      "step": 22090
    },
    {
      "epoch": 0.884,
      "grad_norm": 1.8189507722854614,
      "learning_rate": 3.550335570469799e-05,
      "loss": 0.6642,
      "step": 22100
    },
    {
      "epoch": 0.8844,
      "grad_norm": 2.658963680267334,
      "learning_rate": 3.549664429530202e-05,
      "loss": 0.5653,
      "step": 22110
    },
    {
      "epoch": 0.8848,
      "grad_norm": 2.536247730255127,
      "learning_rate": 3.548993288590604e-05,
      "loss": 0.7547,
      "step": 22120
    },
    {
      "epoch": 0.8852,
      "grad_norm": 3.2113397121429443,
      "learning_rate": 3.548322147651007e-05,
      "loss": 0.6842,
      "step": 22130
    },
    {
      "epoch": 0.8856,
      "grad_norm": 3.2078773975372314,
      "learning_rate": 3.547651006711409e-05,
      "loss": 0.6367,
      "step": 22140
    },
    {
      "epoch": 0.886,
      "grad_norm": 2.6000471115112305,
      "learning_rate": 3.546979865771812e-05,
      "loss": 0.6123,
      "step": 22150
    },
    {
      "epoch": 0.8864,
      "grad_norm": 3.7032711505889893,
      "learning_rate": 3.546308724832215e-05,
      "loss": 0.6286,
      "step": 22160
    },
    {
      "epoch": 0.8868,
      "grad_norm": 4.020802974700928,
      "learning_rate": 3.545637583892618e-05,
      "loss": 0.7124,
      "step": 22170
    },
    {
      "epoch": 0.8872,
      "grad_norm": 2.5556070804595947,
      "learning_rate": 3.5449664429530206e-05,
      "loss": 0.6712,
      "step": 22180
    },
    {
      "epoch": 0.8876,
      "grad_norm": 2.4694292545318604,
      "learning_rate": 3.544295302013423e-05,
      "loss": 0.6763,
      "step": 22190
    },
    {
      "epoch": 0.888,
      "grad_norm": 2.83347487449646,
      "learning_rate": 3.5436241610738257e-05,
      "loss": 0.5922,
      "step": 22200
    },
    {
      "epoch": 0.8884,
      "grad_norm": 2.932614803314209,
      "learning_rate": 3.542953020134228e-05,
      "loss": 0.6136,
      "step": 22210
    },
    {
      "epoch": 0.8888,
      "grad_norm": 2.769073247909546,
      "learning_rate": 3.5422818791946314e-05,
      "loss": 0.7616,
      "step": 22220
    },
    {
      "epoch": 0.8892,
      "grad_norm": 2.7955167293548584,
      "learning_rate": 3.541610738255034e-05,
      "loss": 0.616,
      "step": 22230
    },
    {
      "epoch": 0.8896,
      "grad_norm": 2.512054681777954,
      "learning_rate": 3.5409395973154364e-05,
      "loss": 0.6703,
      "step": 22240
    },
    {
      "epoch": 0.89,
      "grad_norm": 2.6935629844665527,
      "learning_rate": 3.540268456375839e-05,
      "loss": 0.5948,
      "step": 22250
    },
    {
      "epoch": 0.8904,
      "grad_norm": 2.2587084770202637,
      "learning_rate": 3.5395973154362414e-05,
      "loss": 0.6633,
      "step": 22260
    },
    {
      "epoch": 0.8908,
      "grad_norm": 3.0564498901367188,
      "learning_rate": 3.538926174496644e-05,
      "loss": 0.7038,
      "step": 22270
    },
    {
      "epoch": 0.8912,
      "grad_norm": 3.236745595932007,
      "learning_rate": 3.538255033557047e-05,
      "loss": 0.6816,
      "step": 22280
    },
    {
      "epoch": 0.8916,
      "grad_norm": 2.5916147232055664,
      "learning_rate": 3.53758389261745e-05,
      "loss": 0.6092,
      "step": 22290
    },
    {
      "epoch": 0.892,
      "grad_norm": 3.282597541809082,
      "learning_rate": 3.536912751677853e-05,
      "loss": 0.7496,
      "step": 22300
    },
    {
      "epoch": 0.8924,
      "grad_norm": 3.6948702335357666,
      "learning_rate": 3.536241610738255e-05,
      "loss": 0.7628,
      "step": 22310
    },
    {
      "epoch": 0.8928,
      "grad_norm": 2.1524970531463623,
      "learning_rate": 3.535570469798658e-05,
      "loss": 0.6137,
      "step": 22320
    },
    {
      "epoch": 0.8932,
      "grad_norm": 2.3902993202209473,
      "learning_rate": 3.53489932885906e-05,
      "loss": 0.6524,
      "step": 22330
    },
    {
      "epoch": 0.8936,
      "grad_norm": 3.1359822750091553,
      "learning_rate": 3.5342281879194636e-05,
      "loss": 0.6382,
      "step": 22340
    },
    {
      "epoch": 0.894,
      "grad_norm": 2.5131406784057617,
      "learning_rate": 3.533557046979866e-05,
      "loss": 0.5589,
      "step": 22350
    },
    {
      "epoch": 0.8944,
      "grad_norm": 2.9243369102478027,
      "learning_rate": 3.5328859060402686e-05,
      "loss": 0.6106,
      "step": 22360
    },
    {
      "epoch": 0.8948,
      "grad_norm": 2.874424695968628,
      "learning_rate": 3.5322147651006715e-05,
      "loss": 0.6486,
      "step": 22370
    },
    {
      "epoch": 0.8952,
      "grad_norm": 2.5867888927459717,
      "learning_rate": 3.5315436241610737e-05,
      "loss": 0.6598,
      "step": 22380
    },
    {
      "epoch": 0.8956,
      "grad_norm": 3.094146251678467,
      "learning_rate": 3.5308724832214765e-05,
      "loss": 0.6175,
      "step": 22390
    },
    {
      "epoch": 0.896,
      "grad_norm": 2.5371482372283936,
      "learning_rate": 3.5302013422818794e-05,
      "loss": 0.5686,
      "step": 22400
    },
    {
      "epoch": 0.8964,
      "grad_norm": 2.386951446533203,
      "learning_rate": 3.529530201342282e-05,
      "loss": 0.6899,
      "step": 22410
    },
    {
      "epoch": 0.8968,
      "grad_norm": 2.760913133621216,
      "learning_rate": 3.528859060402685e-05,
      "loss": 0.6226,
      "step": 22420
    },
    {
      "epoch": 0.8972,
      "grad_norm": 3.102088689804077,
      "learning_rate": 3.528187919463087e-05,
      "loss": 0.6673,
      "step": 22430
    },
    {
      "epoch": 0.8976,
      "grad_norm": 2.7096738815307617,
      "learning_rate": 3.52751677852349e-05,
      "loss": 0.6336,
      "step": 22440
    },
    {
      "epoch": 0.898,
      "grad_norm": 3.495880365371704,
      "learning_rate": 3.526845637583893e-05,
      "loss": 0.6422,
      "step": 22450
    },
    {
      "epoch": 0.8984,
      "grad_norm": 2.862619400024414,
      "learning_rate": 3.526174496644296e-05,
      "loss": 0.6119,
      "step": 22460
    },
    {
      "epoch": 0.8988,
      "grad_norm": 3.0719521045684814,
      "learning_rate": 3.525503355704698e-05,
      "loss": 0.6258,
      "step": 22470
    },
    {
      "epoch": 0.8992,
      "grad_norm": 2.512887954711914,
      "learning_rate": 3.524832214765101e-05,
      "loss": 0.6796,
      "step": 22480
    },
    {
      "epoch": 0.8996,
      "grad_norm": 3.2813565731048584,
      "learning_rate": 3.524161073825504e-05,
      "loss": 0.6892,
      "step": 22490
    },
    {
      "epoch": 0.9,
      "grad_norm": 2.6065776348114014,
      "learning_rate": 3.523489932885906e-05,
      "loss": 0.721,
      "step": 22500
    },
    {
      "epoch": 0.9004,
      "grad_norm": 2.76696515083313,
      "learning_rate": 3.522818791946309e-05,
      "loss": 0.6725,
      "step": 22510
    },
    {
      "epoch": 0.9008,
      "grad_norm": 2.3018453121185303,
      "learning_rate": 3.5221476510067116e-05,
      "loss": 0.6218,
      "step": 22520
    },
    {
      "epoch": 0.9012,
      "grad_norm": 2.6126811504364014,
      "learning_rate": 3.5214765100671144e-05,
      "loss": 0.7508,
      "step": 22530
    },
    {
      "epoch": 0.9016,
      "grad_norm": 2.614501476287842,
      "learning_rate": 3.5208053691275166e-05,
      "loss": 0.632,
      "step": 22540
    },
    {
      "epoch": 0.902,
      "grad_norm": 2.845632314682007,
      "learning_rate": 3.5201342281879195e-05,
      "loss": 0.5829,
      "step": 22550
    },
    {
      "epoch": 0.9024,
      "grad_norm": 2.4399378299713135,
      "learning_rate": 3.519463087248322e-05,
      "loss": 0.6149,
      "step": 22560
    },
    {
      "epoch": 0.9028,
      "grad_norm": 3.183302164077759,
      "learning_rate": 3.518791946308725e-05,
      "loss": 0.6208,
      "step": 22570
    },
    {
      "epoch": 0.9032,
      "grad_norm": 2.658010721206665,
      "learning_rate": 3.518120805369128e-05,
      "loss": 0.6034,
      "step": 22580
    },
    {
      "epoch": 0.9036,
      "grad_norm": 2.4047088623046875,
      "learning_rate": 3.51744966442953e-05,
      "loss": 0.6872,
      "step": 22590
    },
    {
      "epoch": 0.904,
      "grad_norm": 3.033576250076294,
      "learning_rate": 3.516778523489933e-05,
      "loss": 0.6567,
      "step": 22600
    },
    {
      "epoch": 0.9044,
      "grad_norm": 2.720743179321289,
      "learning_rate": 3.516107382550336e-05,
      "loss": 0.555,
      "step": 22610
    },
    {
      "epoch": 0.9048,
      "grad_norm": 3.146355628967285,
      "learning_rate": 3.515436241610738e-05,
      "loss": 0.7434,
      "step": 22620
    },
    {
      "epoch": 0.9052,
      "grad_norm": 3.1976447105407715,
      "learning_rate": 3.514765100671141e-05,
      "loss": 0.6328,
      "step": 22630
    },
    {
      "epoch": 0.9056,
      "grad_norm": 2.535156726837158,
      "learning_rate": 3.514093959731544e-05,
      "loss": 0.671,
      "step": 22640
    },
    {
      "epoch": 0.906,
      "grad_norm": 3.6091716289520264,
      "learning_rate": 3.513422818791947e-05,
      "loss": 0.637,
      "step": 22650
    },
    {
      "epoch": 0.9064,
      "grad_norm": 3.1599111557006836,
      "learning_rate": 3.512751677852349e-05,
      "loss": 0.6135,
      "step": 22660
    },
    {
      "epoch": 0.9068,
      "grad_norm": 3.345064401626587,
      "learning_rate": 3.512080536912752e-05,
      "loss": 0.6164,
      "step": 22670
    },
    {
      "epoch": 0.9072,
      "grad_norm": 2.3423871994018555,
      "learning_rate": 3.5114093959731546e-05,
      "loss": 0.6845,
      "step": 22680
    },
    {
      "epoch": 0.9076,
      "grad_norm": 2.925123691558838,
      "learning_rate": 3.5107382550335574e-05,
      "loss": 0.6371,
      "step": 22690
    },
    {
      "epoch": 0.908,
      "grad_norm": 3.31846022605896,
      "learning_rate": 3.51006711409396e-05,
      "loss": 0.5046,
      "step": 22700
    },
    {
      "epoch": 0.9084,
      "grad_norm": 2.745043992996216,
      "learning_rate": 3.5093959731543624e-05,
      "loss": 0.5652,
      "step": 22710
    },
    {
      "epoch": 0.9088,
      "grad_norm": 2.846129894256592,
      "learning_rate": 3.508724832214765e-05,
      "loss": 0.703,
      "step": 22720
    },
    {
      "epoch": 0.9092,
      "grad_norm": 2.7878971099853516,
      "learning_rate": 3.5080536912751675e-05,
      "loss": 0.6828,
      "step": 22730
    },
    {
      "epoch": 0.9096,
      "grad_norm": 3.0585646629333496,
      "learning_rate": 3.50738255033557e-05,
      "loss": 0.6036,
      "step": 22740
    },
    {
      "epoch": 0.91,
      "grad_norm": 2.5112664699554443,
      "learning_rate": 3.506711409395974e-05,
      "loss": 0.5481,
      "step": 22750
    },
    {
      "epoch": 0.9104,
      "grad_norm": 2.991410732269287,
      "learning_rate": 3.506040268456376e-05,
      "loss": 0.6214,
      "step": 22760
    },
    {
      "epoch": 0.9108,
      "grad_norm": 3.435396671295166,
      "learning_rate": 3.505369127516779e-05,
      "loss": 0.6857,
      "step": 22770
    },
    {
      "epoch": 0.9112,
      "grad_norm": 2.3725783824920654,
      "learning_rate": 3.504697986577181e-05,
      "loss": 0.6715,
      "step": 22780
    },
    {
      "epoch": 0.9116,
      "grad_norm": 2.5316622257232666,
      "learning_rate": 3.504026845637584e-05,
      "loss": 0.8148,
      "step": 22790
    },
    {
      "epoch": 0.912,
      "grad_norm": 2.8128325939178467,
      "learning_rate": 3.503355704697987e-05,
      "loss": 0.6568,
      "step": 22800
    },
    {
      "epoch": 0.9124,
      "grad_norm": 3.0848476886749268,
      "learning_rate": 3.5026845637583896e-05,
      "loss": 0.6763,
      "step": 22810
    },
    {
      "epoch": 0.9128,
      "grad_norm": 2.6950247287750244,
      "learning_rate": 3.5020134228187925e-05,
      "loss": 0.6712,
      "step": 22820
    },
    {
      "epoch": 0.9132,
      "grad_norm": 2.9944491386413574,
      "learning_rate": 3.501342281879195e-05,
      "loss": 0.603,
      "step": 22830
    },
    {
      "epoch": 0.9136,
      "grad_norm": 2.368638038635254,
      "learning_rate": 3.5006711409395975e-05,
      "loss": 0.7382,
      "step": 22840
    },
    {
      "epoch": 0.914,
      "grad_norm": 2.3465585708618164,
      "learning_rate": 3.5e-05,
      "loss": 0.5979,
      "step": 22850
    },
    {
      "epoch": 0.9144,
      "grad_norm": 3.527831792831421,
      "learning_rate": 3.4993288590604025e-05,
      "loss": 0.6764,
      "step": 22860
    },
    {
      "epoch": 0.9148,
      "grad_norm": 2.7417221069335938,
      "learning_rate": 3.498657718120806e-05,
      "loss": 0.6091,
      "step": 22870
    },
    {
      "epoch": 0.9152,
      "grad_norm": 3.8646461963653564,
      "learning_rate": 3.497986577181208e-05,
      "loss": 0.5744,
      "step": 22880
    },
    {
      "epoch": 0.9156,
      "grad_norm": 3.2070789337158203,
      "learning_rate": 3.497315436241611e-05,
      "loss": 0.7079,
      "step": 22890
    },
    {
      "epoch": 0.916,
      "grad_norm": 2.489180326461792,
      "learning_rate": 3.496644295302013e-05,
      "loss": 0.6228,
      "step": 22900
    },
    {
      "epoch": 0.9164,
      "grad_norm": 3.4307432174682617,
      "learning_rate": 3.495973154362416e-05,
      "loss": 0.6229,
      "step": 22910
    },
    {
      "epoch": 0.9168,
      "grad_norm": 2.4075958728790283,
      "learning_rate": 3.495302013422819e-05,
      "loss": 0.6447,
      "step": 22920
    },
    {
      "epoch": 0.9172,
      "grad_norm": 2.7582619190216064,
      "learning_rate": 3.494630872483222e-05,
      "loss": 0.6944,
      "step": 22930
    },
    {
      "epoch": 0.9176,
      "grad_norm": 2.8218495845794678,
      "learning_rate": 3.493959731543625e-05,
      "loss": 0.6083,
      "step": 22940
    },
    {
      "epoch": 0.918,
      "grad_norm": 2.7906322479248047,
      "learning_rate": 3.493288590604027e-05,
      "loss": 0.6389,
      "step": 22950
    },
    {
      "epoch": 0.9184,
      "grad_norm": 2.6337690353393555,
      "learning_rate": 3.49261744966443e-05,
      "loss": 0.54,
      "step": 22960
    },
    {
      "epoch": 0.9188,
      "grad_norm": 3.0463666915893555,
      "learning_rate": 3.491946308724832e-05,
      "loss": 0.6807,
      "step": 22970
    },
    {
      "epoch": 0.9192,
      "grad_norm": 2.9650049209594727,
      "learning_rate": 3.4912751677852354e-05,
      "loss": 0.6714,
      "step": 22980
    },
    {
      "epoch": 0.9196,
      "grad_norm": 2.5922908782958984,
      "learning_rate": 3.4906040268456376e-05,
      "loss": 0.6615,
      "step": 22990
    },
    {
      "epoch": 0.92,
      "grad_norm": 2.741194725036621,
      "learning_rate": 3.4899328859060405e-05,
      "loss": 0.609,
      "step": 23000
    },
    {
      "epoch": 0.9204,
      "grad_norm": 3.1642675399780273,
      "learning_rate": 3.489261744966443e-05,
      "loss": 0.6997,
      "step": 23010
    },
    {
      "epoch": 0.9208,
      "grad_norm": 2.933170795440674,
      "learning_rate": 3.4885906040268455e-05,
      "loss": 0.6448,
      "step": 23020
    },
    {
      "epoch": 0.9212,
      "grad_norm": 3.5911970138549805,
      "learning_rate": 3.4879194630872484e-05,
      "loss": 0.6237,
      "step": 23030
    },
    {
      "epoch": 0.9216,
      "grad_norm": 2.054105281829834,
      "learning_rate": 3.487248322147651e-05,
      "loss": 0.6309,
      "step": 23040
    },
    {
      "epoch": 0.922,
      "grad_norm": 1.8927556276321411,
      "learning_rate": 3.486577181208054e-05,
      "loss": 0.5832,
      "step": 23050
    },
    {
      "epoch": 0.9224,
      "grad_norm": 2.2110376358032227,
      "learning_rate": 3.485906040268457e-05,
      "loss": 0.6497,
      "step": 23060
    },
    {
      "epoch": 0.9228,
      "grad_norm": 2.7118115425109863,
      "learning_rate": 3.485234899328859e-05,
      "loss": 0.5912,
      "step": 23070
    },
    {
      "epoch": 0.9232,
      "grad_norm": 2.9721550941467285,
      "learning_rate": 3.484563758389262e-05,
      "loss": 0.6797,
      "step": 23080
    },
    {
      "epoch": 0.9236,
      "grad_norm": 2.1525590419769287,
      "learning_rate": 3.483892617449664e-05,
      "loss": 0.6108,
      "step": 23090
    },
    {
      "epoch": 0.924,
      "grad_norm": 3.316413640975952,
      "learning_rate": 3.483221476510068e-05,
      "loss": 0.6962,
      "step": 23100
    },
    {
      "epoch": 0.9244,
      "grad_norm": 2.903064250946045,
      "learning_rate": 3.48255033557047e-05,
      "loss": 0.6262,
      "step": 23110
    },
    {
      "epoch": 0.9248,
      "grad_norm": 2.697641134262085,
      "learning_rate": 3.481879194630873e-05,
      "loss": 0.6126,
      "step": 23120
    },
    {
      "epoch": 0.9252,
      "grad_norm": 2.996253252029419,
      "learning_rate": 3.4812080536912756e-05,
      "loss": 0.6291,
      "step": 23130
    },
    {
      "epoch": 0.9256,
      "grad_norm": 2.9148929119110107,
      "learning_rate": 3.480536912751678e-05,
      "loss": 0.5785,
      "step": 23140
    },
    {
      "epoch": 0.926,
      "grad_norm": 2.4443840980529785,
      "learning_rate": 3.4798657718120806e-05,
      "loss": 0.598,
      "step": 23150
    },
    {
      "epoch": 0.9264,
      "grad_norm": 2.6524722576141357,
      "learning_rate": 3.4791946308724834e-05,
      "loss": 0.5653,
      "step": 23160
    },
    {
      "epoch": 0.9268,
      "grad_norm": 2.1885030269622803,
      "learning_rate": 3.478523489932886e-05,
      "loss": 0.6927,
      "step": 23170
    },
    {
      "epoch": 0.9272,
      "grad_norm": 3.1629838943481445,
      "learning_rate": 3.4778523489932885e-05,
      "loss": 0.5982,
      "step": 23180
    },
    {
      "epoch": 0.9276,
      "grad_norm": 3.1165125370025635,
      "learning_rate": 3.477181208053691e-05,
      "loss": 0.6927,
      "step": 23190
    },
    {
      "epoch": 0.928,
      "grad_norm": 3.3101227283477783,
      "learning_rate": 3.476510067114094e-05,
      "loss": 0.7354,
      "step": 23200
    },
    {
      "epoch": 0.9284,
      "grad_norm": 2.2818424701690674,
      "learning_rate": 3.4758389261744964e-05,
      "loss": 0.7101,
      "step": 23210
    },
    {
      "epoch": 0.9288,
      "grad_norm": 4.413806915283203,
      "learning_rate": 3.4751677852349e-05,
      "loss": 0.6525,
      "step": 23220
    },
    {
      "epoch": 0.9292,
      "grad_norm": 2.099142551422119,
      "learning_rate": 3.474496644295302e-05,
      "loss": 0.5596,
      "step": 23230
    },
    {
      "epoch": 0.9296,
      "grad_norm": 2.758840799331665,
      "learning_rate": 3.473825503355705e-05,
      "loss": 0.6848,
      "step": 23240
    },
    {
      "epoch": 0.93,
      "grad_norm": 2.846252918243408,
      "learning_rate": 3.473154362416108e-05,
      "loss": 0.7092,
      "step": 23250
    },
    {
      "epoch": 0.9304,
      "grad_norm": 3.172677993774414,
      "learning_rate": 3.47248322147651e-05,
      "loss": 0.7527,
      "step": 23260
    },
    {
      "epoch": 0.9308,
      "grad_norm": 3.4264864921569824,
      "learning_rate": 3.471812080536913e-05,
      "loss": 0.6582,
      "step": 23270
    },
    {
      "epoch": 0.9312,
      "grad_norm": 3.0054450035095215,
      "learning_rate": 3.471140939597316e-05,
      "loss": 0.731,
      "step": 23280
    },
    {
      "epoch": 0.9316,
      "grad_norm": 2.8531527519226074,
      "learning_rate": 3.4704697986577185e-05,
      "loss": 0.6486,
      "step": 23290
    },
    {
      "epoch": 0.932,
      "grad_norm": 3.4025661945343018,
      "learning_rate": 3.469798657718121e-05,
      "loss": 0.6533,
      "step": 23300
    },
    {
      "epoch": 0.9324,
      "grad_norm": 4.051945209503174,
      "learning_rate": 3.4691275167785236e-05,
      "loss": 0.653,
      "step": 23310
    },
    {
      "epoch": 0.9328,
      "grad_norm": 2.820908308029175,
      "learning_rate": 3.4684563758389264e-05,
      "loss": 0.6173,
      "step": 23320
    },
    {
      "epoch": 0.9332,
      "grad_norm": 2.604328155517578,
      "learning_rate": 3.467785234899329e-05,
      "loss": 0.6679,
      "step": 23330
    },
    {
      "epoch": 0.9336,
      "grad_norm": 3.438282012939453,
      "learning_rate": 3.467114093959732e-05,
      "loss": 0.7951,
      "step": 23340
    },
    {
      "epoch": 0.934,
      "grad_norm": 2.437666654586792,
      "learning_rate": 3.466442953020134e-05,
      "loss": 0.6107,
      "step": 23350
    },
    {
      "epoch": 0.9344,
      "grad_norm": 3.1798222064971924,
      "learning_rate": 3.465771812080537e-05,
      "loss": 0.7468,
      "step": 23360
    },
    {
      "epoch": 0.9348,
      "grad_norm": 3.2226645946502686,
      "learning_rate": 3.465100671140939e-05,
      "loss": 0.635,
      "step": 23370
    },
    {
      "epoch": 0.9352,
      "grad_norm": 2.671015977859497,
      "learning_rate": 3.464429530201342e-05,
      "loss": 0.616,
      "step": 23380
    },
    {
      "epoch": 0.9356,
      "grad_norm": 3.259403944015503,
      "learning_rate": 3.463758389261745e-05,
      "loss": 0.69,
      "step": 23390
    },
    {
      "epoch": 0.936,
      "grad_norm": 2.179821729660034,
      "learning_rate": 3.463087248322148e-05,
      "loss": 0.7359,
      "step": 23400
    },
    {
      "epoch": 0.9364,
      "grad_norm": 3.058227777481079,
      "learning_rate": 3.462416107382551e-05,
      "loss": 0.5834,
      "step": 23410
    },
    {
      "epoch": 0.9368,
      "grad_norm": 2.2977681159973145,
      "learning_rate": 3.461744966442953e-05,
      "loss": 0.6656,
      "step": 23420
    },
    {
      "epoch": 0.9372,
      "grad_norm": 2.656359910964966,
      "learning_rate": 3.461073825503356e-05,
      "loss": 0.7056,
      "step": 23430
    },
    {
      "epoch": 0.9376,
      "grad_norm": 2.8509583473205566,
      "learning_rate": 3.4604026845637586e-05,
      "loss": 0.6111,
      "step": 23440
    },
    {
      "epoch": 0.938,
      "grad_norm": 2.5167980194091797,
      "learning_rate": 3.4597315436241615e-05,
      "loss": 0.7048,
      "step": 23450
    },
    {
      "epoch": 0.9384,
      "grad_norm": 2.1380457878112793,
      "learning_rate": 3.4590604026845643e-05,
      "loss": 0.6308,
      "step": 23460
    },
    {
      "epoch": 0.9388,
      "grad_norm": 2.871922492980957,
      "learning_rate": 3.4583892617449665e-05,
      "loss": 0.5899,
      "step": 23470
    },
    {
      "epoch": 0.9392,
      "grad_norm": 2.1386654376983643,
      "learning_rate": 3.4577181208053694e-05,
      "loss": 0.5766,
      "step": 23480
    },
    {
      "epoch": 0.9396,
      "grad_norm": 2.6905550956726074,
      "learning_rate": 3.4570469798657716e-05,
      "loss": 0.6234,
      "step": 23490
    },
    {
      "epoch": 0.94,
      "grad_norm": 2.4078333377838135,
      "learning_rate": 3.4563758389261744e-05,
      "loss": 0.5932,
      "step": 23500
    },
    {
      "epoch": 0.9404,
      "grad_norm": 3.4906299114227295,
      "learning_rate": 3.455704697986577e-05,
      "loss": 0.6859,
      "step": 23510
    },
    {
      "epoch": 0.9408,
      "grad_norm": 2.2726893424987793,
      "learning_rate": 3.45503355704698e-05,
      "loss": 0.6811,
      "step": 23520
    },
    {
      "epoch": 0.9412,
      "grad_norm": 2.7418317794799805,
      "learning_rate": 3.454362416107383e-05,
      "loss": 0.5942,
      "step": 23530
    },
    {
      "epoch": 0.9416,
      "grad_norm": 2.6601500511169434,
      "learning_rate": 3.453691275167785e-05,
      "loss": 0.6723,
      "step": 23540
    },
    {
      "epoch": 0.942,
      "grad_norm": 4.010611534118652,
      "learning_rate": 3.453020134228188e-05,
      "loss": 0.7679,
      "step": 23550
    },
    {
      "epoch": 0.9424,
      "grad_norm": 2.7944257259368896,
      "learning_rate": 3.45234899328859e-05,
      "loss": 0.5892,
      "step": 23560
    },
    {
      "epoch": 0.9428,
      "grad_norm": 3.975606679916382,
      "learning_rate": 3.451677852348994e-05,
      "loss": 0.731,
      "step": 23570
    },
    {
      "epoch": 0.9432,
      "grad_norm": 3.0459039211273193,
      "learning_rate": 3.4510067114093966e-05,
      "loss": 0.5801,
      "step": 23580
    },
    {
      "epoch": 0.9436,
      "grad_norm": 2.789994955062866,
      "learning_rate": 3.450335570469799e-05,
      "loss": 0.671,
      "step": 23590
    },
    {
      "epoch": 0.944,
      "grad_norm": 3.193769931793213,
      "learning_rate": 3.4496644295302016e-05,
      "loss": 0.6695,
      "step": 23600
    },
    {
      "epoch": 0.9444,
      "grad_norm": 2.697413921356201,
      "learning_rate": 3.448993288590604e-05,
      "loss": 0.7389,
      "step": 23610
    },
    {
      "epoch": 0.9448,
      "grad_norm": 2.6668710708618164,
      "learning_rate": 3.4483221476510066e-05,
      "loss": 0.71,
      "step": 23620
    },
    {
      "epoch": 0.9452,
      "grad_norm": 2.8705406188964844,
      "learning_rate": 3.44765100671141e-05,
      "loss": 0.6488,
      "step": 23630
    },
    {
      "epoch": 0.9456,
      "grad_norm": 3.102778911590576,
      "learning_rate": 3.4469798657718123e-05,
      "loss": 0.6467,
      "step": 23640
    },
    {
      "epoch": 0.946,
      "grad_norm": 3.1551082134246826,
      "learning_rate": 3.446308724832215e-05,
      "loss": 0.6254,
      "step": 23650
    },
    {
      "epoch": 0.9464,
      "grad_norm": 2.6926088333129883,
      "learning_rate": 3.4456375838926174e-05,
      "loss": 0.5631,
      "step": 23660
    },
    {
      "epoch": 0.9468,
      "grad_norm": 2.375680446624756,
      "learning_rate": 3.44496644295302e-05,
      "loss": 0.5794,
      "step": 23670
    },
    {
      "epoch": 0.9472,
      "grad_norm": 3.3953590393066406,
      "learning_rate": 3.444295302013423e-05,
      "loss": 0.6691,
      "step": 23680
    },
    {
      "epoch": 0.9476,
      "grad_norm": 2.8174452781677246,
      "learning_rate": 3.443624161073826e-05,
      "loss": 0.5797,
      "step": 23690
    },
    {
      "epoch": 0.948,
      "grad_norm": 2.6679623126983643,
      "learning_rate": 3.442953020134229e-05,
      "loss": 0.652,
      "step": 23700
    },
    {
      "epoch": 0.9484,
      "grad_norm": 3.1248743534088135,
      "learning_rate": 3.442281879194631e-05,
      "loss": 0.6038,
      "step": 23710
    },
    {
      "epoch": 0.9488,
      "grad_norm": 2.247781991958618,
      "learning_rate": 3.441610738255034e-05,
      "loss": 0.6832,
      "step": 23720
    },
    {
      "epoch": 0.9492,
      "grad_norm": 2.631268262863159,
      "learning_rate": 3.440939597315436e-05,
      "loss": 0.663,
      "step": 23730
    },
    {
      "epoch": 0.9496,
      "grad_norm": 3.004281520843506,
      "learning_rate": 3.440268456375839e-05,
      "loss": 0.613,
      "step": 23740
    },
    {
      "epoch": 0.95,
      "grad_norm": 2.86214542388916,
      "learning_rate": 3.439597315436242e-05,
      "loss": 0.6403,
      "step": 23750
    },
    {
      "epoch": 0.9504,
      "grad_norm": 2.1745193004608154,
      "learning_rate": 3.4389261744966446e-05,
      "loss": 0.6073,
      "step": 23760
    },
    {
      "epoch": 0.9508,
      "grad_norm": 2.1228792667388916,
      "learning_rate": 3.4382550335570474e-05,
      "loss": 0.6491,
      "step": 23770
    },
    {
      "epoch": 0.9512,
      "grad_norm": 2.8455188274383545,
      "learning_rate": 3.4375838926174496e-05,
      "loss": 0.6203,
      "step": 23780
    },
    {
      "epoch": 0.9516,
      "grad_norm": 2.534259796142578,
      "learning_rate": 3.4369127516778525e-05,
      "loss": 0.6477,
      "step": 23790
    },
    {
      "epoch": 0.952,
      "grad_norm": 3.0509815216064453,
      "learning_rate": 3.436241610738255e-05,
      "loss": 0.6657,
      "step": 23800
    },
    {
      "epoch": 0.9524,
      "grad_norm": 4.026450157165527,
      "learning_rate": 3.435570469798658e-05,
      "loss": 0.4961,
      "step": 23810
    },
    {
      "epoch": 0.9528,
      "grad_norm": 2.5301623344421387,
      "learning_rate": 3.43489932885906e-05,
      "loss": 0.5029,
      "step": 23820
    },
    {
      "epoch": 0.9532,
      "grad_norm": 3.1369924545288086,
      "learning_rate": 3.434228187919463e-05,
      "loss": 0.6843,
      "step": 23830
    },
    {
      "epoch": 0.9536,
      "grad_norm": 2.9306161403656006,
      "learning_rate": 3.433557046979866e-05,
      "loss": 0.7177,
      "step": 23840
    },
    {
      "epoch": 0.954,
      "grad_norm": 3.1413979530334473,
      "learning_rate": 3.432885906040268e-05,
      "loss": 0.7021,
      "step": 23850
    },
    {
      "epoch": 0.9544,
      "grad_norm": 3.5003037452697754,
      "learning_rate": 3.432214765100671e-05,
      "loss": 0.7292,
      "step": 23860
    },
    {
      "epoch": 0.9548,
      "grad_norm": 3.225750207901001,
      "learning_rate": 3.431543624161074e-05,
      "loss": 0.7703,
      "step": 23870
    },
    {
      "epoch": 0.9552,
      "grad_norm": 3.2354907989501953,
      "learning_rate": 3.430872483221477e-05,
      "loss": 0.6343,
      "step": 23880
    },
    {
      "epoch": 0.9556,
      "grad_norm": 3.542510747909546,
      "learning_rate": 3.4302013422818796e-05,
      "loss": 0.6933,
      "step": 23890
    },
    {
      "epoch": 0.956,
      "grad_norm": 2.70084810256958,
      "learning_rate": 3.429530201342282e-05,
      "loss": 0.5978,
      "step": 23900
    },
    {
      "epoch": 0.9564,
      "grad_norm": 2.427915096282959,
      "learning_rate": 3.428859060402685e-05,
      "loss": 0.7283,
      "step": 23910
    },
    {
      "epoch": 0.9568,
      "grad_norm": 2.7815773487091064,
      "learning_rate": 3.4281879194630875e-05,
      "loss": 0.6571,
      "step": 23920
    },
    {
      "epoch": 0.9572,
      "grad_norm": 3.120884895324707,
      "learning_rate": 3.4275167785234904e-05,
      "loss": 0.6711,
      "step": 23930
    },
    {
      "epoch": 0.9576,
      "grad_norm": 2.434499979019165,
      "learning_rate": 3.4268456375838926e-05,
      "loss": 0.5759,
      "step": 23940
    },
    {
      "epoch": 0.958,
      "grad_norm": 2.1989080905914307,
      "learning_rate": 3.4261744966442954e-05,
      "loss": 0.6935,
      "step": 23950
    },
    {
      "epoch": 0.9584,
      "grad_norm": 3.2044246196746826,
      "learning_rate": 3.425503355704698e-05,
      "loss": 0.6762,
      "step": 23960
    },
    {
      "epoch": 0.9588,
      "grad_norm": 2.4083173274993896,
      "learning_rate": 3.4248322147651004e-05,
      "loss": 0.7087,
      "step": 23970
    },
    {
      "epoch": 0.9592,
      "grad_norm": 2.8289108276367188,
      "learning_rate": 3.424161073825504e-05,
      "loss": 0.6739,
      "step": 23980
    },
    {
      "epoch": 0.9596,
      "grad_norm": 2.870875597000122,
      "learning_rate": 3.423489932885906e-05,
      "loss": 0.6309,
      "step": 23990
    },
    {
      "epoch": 0.96,
      "grad_norm": 3.30214262008667,
      "learning_rate": 3.422818791946309e-05,
      "loss": 0.7044,
      "step": 24000
    },
    {
      "epoch": 0.9604,
      "grad_norm": 2.384659767150879,
      "learning_rate": 3.422147651006711e-05,
      "loss": 0.7308,
      "step": 24010
    },
    {
      "epoch": 0.9608,
      "grad_norm": 2.385859251022339,
      "learning_rate": 3.421476510067114e-05,
      "loss": 0.686,
      "step": 24020
    },
    {
      "epoch": 0.9612,
      "grad_norm": 3.5814642906188965,
      "learning_rate": 3.420805369127517e-05,
      "loss": 0.6893,
      "step": 24030
    },
    {
      "epoch": 0.9616,
      "grad_norm": 3.4132649898529053,
      "learning_rate": 3.42013422818792e-05,
      "loss": 0.6499,
      "step": 24040
    },
    {
      "epoch": 0.962,
      "grad_norm": 2.8778440952301025,
      "learning_rate": 3.4194630872483226e-05,
      "loss": 0.6532,
      "step": 24050
    },
    {
      "epoch": 0.9624,
      "grad_norm": 2.654895782470703,
      "learning_rate": 3.418791946308725e-05,
      "loss": 0.6926,
      "step": 24060
    },
    {
      "epoch": 0.9628,
      "grad_norm": 2.650794744491577,
      "learning_rate": 3.4181208053691276e-05,
      "loss": 0.6646,
      "step": 24070
    },
    {
      "epoch": 0.9632,
      "grad_norm": 3.0593035221099854,
      "learning_rate": 3.4174496644295305e-05,
      "loss": 0.6846,
      "step": 24080
    },
    {
      "epoch": 0.9636,
      "grad_norm": 2.805060625076294,
      "learning_rate": 3.416778523489933e-05,
      "loss": 0.5383,
      "step": 24090
    },
    {
      "epoch": 0.964,
      "grad_norm": 3.367892026901245,
      "learning_rate": 3.416107382550336e-05,
      "loss": 0.6662,
      "step": 24100
    },
    {
      "epoch": 0.9644,
      "grad_norm": 3.432666063308716,
      "learning_rate": 3.4154362416107384e-05,
      "loss": 0.6092,
      "step": 24110
    },
    {
      "epoch": 0.9648,
      "grad_norm": 2.8456006050109863,
      "learning_rate": 3.414765100671141e-05,
      "loss": 0.5529,
      "step": 24120
    },
    {
      "epoch": 0.9652,
      "grad_norm": 2.5629022121429443,
      "learning_rate": 3.4140939597315434e-05,
      "loss": 0.7192,
      "step": 24130
    },
    {
      "epoch": 0.9656,
      "grad_norm": 2.38545298576355,
      "learning_rate": 3.413422818791946e-05,
      "loss": 0.6546,
      "step": 24140
    },
    {
      "epoch": 0.966,
      "grad_norm": 3.3142545223236084,
      "learning_rate": 3.412751677852349e-05,
      "loss": 0.6836,
      "step": 24150
    },
    {
      "epoch": 0.9664,
      "grad_norm": 2.2749786376953125,
      "learning_rate": 3.412080536912752e-05,
      "loss": 0.5746,
      "step": 24160
    },
    {
      "epoch": 0.9668,
      "grad_norm": 2.8844943046569824,
      "learning_rate": 3.411409395973155e-05,
      "loss": 0.5901,
      "step": 24170
    },
    {
      "epoch": 0.9672,
      "grad_norm": 3.587669849395752,
      "learning_rate": 3.410738255033557e-05,
      "loss": 0.5905,
      "step": 24180
    },
    {
      "epoch": 0.9676,
      "grad_norm": 3.3477187156677246,
      "learning_rate": 3.41006711409396e-05,
      "loss": 0.6935,
      "step": 24190
    },
    {
      "epoch": 0.968,
      "grad_norm": 2.8045923709869385,
      "learning_rate": 3.409395973154362e-05,
      "loss": 0.6916,
      "step": 24200
    },
    {
      "epoch": 0.9684,
      "grad_norm": 3.448389768600464,
      "learning_rate": 3.4087248322147656e-05,
      "loss": 0.6508,
      "step": 24210
    },
    {
      "epoch": 0.9688,
      "grad_norm": 3.3156583309173584,
      "learning_rate": 3.4080536912751684e-05,
      "loss": 0.6366,
      "step": 24220
    },
    {
      "epoch": 0.9692,
      "grad_norm": 2.959561347961426,
      "learning_rate": 3.4073825503355706e-05,
      "loss": 0.6371,
      "step": 24230
    },
    {
      "epoch": 0.9696,
      "grad_norm": 2.3718369007110596,
      "learning_rate": 3.4067114093959735e-05,
      "loss": 0.6719,
      "step": 24240
    },
    {
      "epoch": 0.97,
      "grad_norm": 2.846212387084961,
      "learning_rate": 3.4060402684563756e-05,
      "loss": 0.6565,
      "step": 24250
    },
    {
      "epoch": 0.9704,
      "grad_norm": 2.3316657543182373,
      "learning_rate": 3.4053691275167785e-05,
      "loss": 0.6579,
      "step": 24260
    },
    {
      "epoch": 0.9708,
      "grad_norm": 2.5045337677001953,
      "learning_rate": 3.4046979865771813e-05,
      "loss": 0.6378,
      "step": 24270
    },
    {
      "epoch": 0.9712,
      "grad_norm": 2.177309989929199,
      "learning_rate": 3.404026845637584e-05,
      "loss": 0.5992,
      "step": 24280
    },
    {
      "epoch": 0.9716,
      "grad_norm": 2.6128830909729004,
      "learning_rate": 3.403355704697987e-05,
      "loss": 0.5813,
      "step": 24290
    },
    {
      "epoch": 0.972,
      "grad_norm": 3.0998618602752686,
      "learning_rate": 3.402684563758389e-05,
      "loss": 0.63,
      "step": 24300
    },
    {
      "epoch": 0.9724,
      "grad_norm": 2.4489095211029053,
      "learning_rate": 3.402013422818792e-05,
      "loss": 0.6546,
      "step": 24310
    },
    {
      "epoch": 0.9728,
      "grad_norm": 2.502791404724121,
      "learning_rate": 3.401342281879194e-05,
      "loss": 0.5713,
      "step": 24320
    },
    {
      "epoch": 0.9732,
      "grad_norm": 2.729304790496826,
      "learning_rate": 3.400671140939598e-05,
      "loss": 0.5473,
      "step": 24330
    },
    {
      "epoch": 0.9736,
      "grad_norm": 2.980849266052246,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.7037,
      "step": 24340
    },
    {
      "epoch": 0.974,
      "grad_norm": 2.686516046524048,
      "learning_rate": 3.399328859060403e-05,
      "loss": 0.5806,
      "step": 24350
    },
    {
      "epoch": 0.9744,
      "grad_norm": 3.1655118465423584,
      "learning_rate": 3.398657718120806e-05,
      "loss": 0.5731,
      "step": 24360
    },
    {
      "epoch": 0.9748,
      "grad_norm": 3.5922954082489014,
      "learning_rate": 3.397986577181208e-05,
      "loss": 0.7433,
      "step": 24370
    },
    {
      "epoch": 0.9752,
      "grad_norm": 3.0289571285247803,
      "learning_rate": 3.397315436241611e-05,
      "loss": 0.6125,
      "step": 24380
    },
    {
      "epoch": 0.9756,
      "grad_norm": 2.5844709873199463,
      "learning_rate": 3.3966442953020136e-05,
      "loss": 0.6174,
      "step": 24390
    },
    {
      "epoch": 0.976,
      "grad_norm": 2.567650556564331,
      "learning_rate": 3.3959731543624164e-05,
      "loss": 0.5672,
      "step": 24400
    },
    {
      "epoch": 0.9764,
      "grad_norm": 2.2999637126922607,
      "learning_rate": 3.395302013422819e-05,
      "loss": 0.5717,
      "step": 24410
    },
    {
      "epoch": 0.9768,
      "grad_norm": 3.143195867538452,
      "learning_rate": 3.3946308724832215e-05,
      "loss": 0.6211,
      "step": 24420
    },
    {
      "epoch": 0.9772,
      "grad_norm": 2.911008596420288,
      "learning_rate": 3.393959731543624e-05,
      "loss": 0.6665,
      "step": 24430
    },
    {
      "epoch": 0.9776,
      "grad_norm": 2.32985782623291,
      "learning_rate": 3.3932885906040265e-05,
      "loss": 0.637,
      "step": 24440
    },
    {
      "epoch": 0.978,
      "grad_norm": 3.169396162033081,
      "learning_rate": 3.39261744966443e-05,
      "loss": 0.6624,
      "step": 24450
    },
    {
      "epoch": 0.9784,
      "grad_norm": 2.2996087074279785,
      "learning_rate": 3.391946308724832e-05,
      "loss": 0.6822,
      "step": 24460
    },
    {
      "epoch": 0.9788,
      "grad_norm": 3.1528165340423584,
      "learning_rate": 3.391275167785235e-05,
      "loss": 0.6735,
      "step": 24470
    },
    {
      "epoch": 0.9792,
      "grad_norm": 3.248906135559082,
      "learning_rate": 3.390604026845638e-05,
      "loss": 0.6768,
      "step": 24480
    },
    {
      "epoch": 0.9796,
      "grad_norm": 3.166490316390991,
      "learning_rate": 3.38993288590604e-05,
      "loss": 0.5906,
      "step": 24490
    },
    {
      "epoch": 0.98,
      "grad_norm": 3.021224021911621,
      "learning_rate": 3.389261744966443e-05,
      "loss": 0.775,
      "step": 24500
    },
    {
      "epoch": 0.9804,
      "grad_norm": 1.9524716138839722,
      "learning_rate": 3.388590604026846e-05,
      "loss": 0.5767,
      "step": 24510
    },
    {
      "epoch": 0.9808,
      "grad_norm": 3.0304019451141357,
      "learning_rate": 3.3879194630872486e-05,
      "loss": 0.6233,
      "step": 24520
    },
    {
      "epoch": 0.9812,
      "grad_norm": 2.9882726669311523,
      "learning_rate": 3.3872483221476515e-05,
      "loss": 0.7062,
      "step": 24530
    },
    {
      "epoch": 0.9816,
      "grad_norm": 2.0800392627716064,
      "learning_rate": 3.386577181208054e-05,
      "loss": 0.6303,
      "step": 24540
    },
    {
      "epoch": 0.982,
      "grad_norm": 2.7413501739501953,
      "learning_rate": 3.3859060402684565e-05,
      "loss": 0.68,
      "step": 24550
    },
    {
      "epoch": 0.9824,
      "grad_norm": 3.2381744384765625,
      "learning_rate": 3.3852348993288594e-05,
      "loss": 0.5671,
      "step": 24560
    },
    {
      "epoch": 0.9828,
      "grad_norm": 2.5674824714660645,
      "learning_rate": 3.384563758389262e-05,
      "loss": 0.6032,
      "step": 24570
    },
    {
      "epoch": 0.9832,
      "grad_norm": 2.742704153060913,
      "learning_rate": 3.3838926174496644e-05,
      "loss": 0.6456,
      "step": 24580
    },
    {
      "epoch": 0.9836,
      "grad_norm": 3.4055850505828857,
      "learning_rate": 3.383221476510067e-05,
      "loss": 0.6278,
      "step": 24590
    },
    {
      "epoch": 0.984,
      "grad_norm": 3.0211474895477295,
      "learning_rate": 3.38255033557047e-05,
      "loss": 0.6078,
      "step": 24600
    },
    {
      "epoch": 0.9844,
      "grad_norm": 2.441741704940796,
      "learning_rate": 3.381879194630872e-05,
      "loss": 0.5374,
      "step": 24610
    },
    {
      "epoch": 0.9848,
      "grad_norm": 2.8879570960998535,
      "learning_rate": 3.381208053691275e-05,
      "loss": 0.7379,
      "step": 24620
    },
    {
      "epoch": 0.9852,
      "grad_norm": 3.3201122283935547,
      "learning_rate": 3.380536912751678e-05,
      "loss": 0.777,
      "step": 24630
    },
    {
      "epoch": 0.9856,
      "grad_norm": 2.2203636169433594,
      "learning_rate": 3.379865771812081e-05,
      "loss": 0.5266,
      "step": 24640
    },
    {
      "epoch": 0.986,
      "grad_norm": 2.1624605655670166,
      "learning_rate": 3.379194630872483e-05,
      "loss": 0.6749,
      "step": 24650
    },
    {
      "epoch": 0.9864,
      "grad_norm": 2.6583428382873535,
      "learning_rate": 3.378523489932886e-05,
      "loss": 0.6635,
      "step": 24660
    },
    {
      "epoch": 0.9868,
      "grad_norm": 1.9244006872177124,
      "learning_rate": 3.377852348993289e-05,
      "loss": 0.626,
      "step": 24670
    },
    {
      "epoch": 0.9872,
      "grad_norm": 2.551121234893799,
      "learning_rate": 3.3771812080536916e-05,
      "loss": 0.6396,
      "step": 24680
    },
    {
      "epoch": 0.9876,
      "grad_norm": 2.4843697547912598,
      "learning_rate": 3.3765100671140945e-05,
      "loss": 0.5955,
      "step": 24690
    },
    {
      "epoch": 0.988,
      "grad_norm": 2.578678607940674,
      "learning_rate": 3.3758389261744966e-05,
      "loss": 0.6809,
      "step": 24700
    },
    {
      "epoch": 0.9884,
      "grad_norm": 2.6742210388183594,
      "learning_rate": 3.3751677852348995e-05,
      "loss": 0.7158,
      "step": 24710
    },
    {
      "epoch": 0.9888,
      "grad_norm": 2.6259899139404297,
      "learning_rate": 3.3744966442953024e-05,
      "loss": 0.679,
      "step": 24720
    },
    {
      "epoch": 0.9892,
      "grad_norm": 3.026787757873535,
      "learning_rate": 3.3738255033557045e-05,
      "loss": 0.6361,
      "step": 24730
    },
    {
      "epoch": 0.9896,
      "grad_norm": 2.5753116607666016,
      "learning_rate": 3.3731543624161074e-05,
      "loss": 0.6687,
      "step": 24740
    },
    {
      "epoch": 0.99,
      "grad_norm": 2.927006244659424,
      "learning_rate": 3.37248322147651e-05,
      "loss": 0.6212,
      "step": 24750
    },
    {
      "epoch": 0.9904,
      "grad_norm": 3.015146255493164,
      "learning_rate": 3.371812080536913e-05,
      "loss": 0.7673,
      "step": 24760
    },
    {
      "epoch": 0.9908,
      "grad_norm": 2.5367417335510254,
      "learning_rate": 3.371140939597315e-05,
      "loss": 0.7583,
      "step": 24770
    },
    {
      "epoch": 0.9912,
      "grad_norm": 2.8953945636749268,
      "learning_rate": 3.370469798657718e-05,
      "loss": 0.6122,
      "step": 24780
    },
    {
      "epoch": 0.9916,
      "grad_norm": 2.9593968391418457,
      "learning_rate": 3.369798657718121e-05,
      "loss": 0.6716,
      "step": 24790
    },
    {
      "epoch": 0.992,
      "grad_norm": 2.5431649684906006,
      "learning_rate": 3.369127516778524e-05,
      "loss": 0.6519,
      "step": 24800
    },
    {
      "epoch": 0.9924,
      "grad_norm": 3.210615873336792,
      "learning_rate": 3.368456375838927e-05,
      "loss": 0.6451,
      "step": 24810
    },
    {
      "epoch": 0.9928,
      "grad_norm": 2.4136033058166504,
      "learning_rate": 3.367785234899329e-05,
      "loss": 0.6337,
      "step": 24820
    },
    {
      "epoch": 0.9932,
      "grad_norm": 3.02496600151062,
      "learning_rate": 3.367114093959732e-05,
      "loss": 0.6379,
      "step": 24830
    },
    {
      "epoch": 0.9936,
      "grad_norm": 2.4142374992370605,
      "learning_rate": 3.366442953020134e-05,
      "loss": 0.5919,
      "step": 24840
    },
    {
      "epoch": 0.994,
      "grad_norm": 2.794318675994873,
      "learning_rate": 3.365771812080537e-05,
      "loss": 0.5828,
      "step": 24850
    },
    {
      "epoch": 0.9944,
      "grad_norm": 1.9463094472885132,
      "learning_rate": 3.36510067114094e-05,
      "loss": 0.6215,
      "step": 24860
    },
    {
      "epoch": 0.9948,
      "grad_norm": 2.4752161502838135,
      "learning_rate": 3.3644295302013425e-05,
      "loss": 0.6155,
      "step": 24870
    },
    {
      "epoch": 0.9952,
      "grad_norm": 3.349655866622925,
      "learning_rate": 3.363758389261745e-05,
      "loss": 0.6776,
      "step": 24880
    },
    {
      "epoch": 0.9956,
      "grad_norm": 2.2851407527923584,
      "learning_rate": 3.3630872483221475e-05,
      "loss": 0.6762,
      "step": 24890
    },
    {
      "epoch": 0.996,
      "grad_norm": 2.7551541328430176,
      "learning_rate": 3.3624161073825504e-05,
      "loss": 0.7277,
      "step": 24900
    },
    {
      "epoch": 0.9964,
      "grad_norm": 2.952103614807129,
      "learning_rate": 3.361744966442953e-05,
      "loss": 0.5474,
      "step": 24910
    },
    {
      "epoch": 0.9968,
      "grad_norm": 3.2048983573913574,
      "learning_rate": 3.361073825503356e-05,
      "loss": 0.6443,
      "step": 24920
    },
    {
      "epoch": 0.9972,
      "grad_norm": 2.9364664554595947,
      "learning_rate": 3.360402684563759e-05,
      "loss": 0.6297,
      "step": 24930
    },
    {
      "epoch": 0.9976,
      "grad_norm": 2.8946073055267334,
      "learning_rate": 3.359731543624161e-05,
      "loss": 0.7211,
      "step": 24940
    },
    {
      "epoch": 0.998,
      "grad_norm": 2.2430801391601562,
      "learning_rate": 3.359060402684564e-05,
      "loss": 0.5762,
      "step": 24950
    },
    {
      "epoch": 0.9984,
      "grad_norm": 2.795145273208618,
      "learning_rate": 3.358389261744966e-05,
      "loss": 0.5987,
      "step": 24960
    },
    {
      "epoch": 0.9988,
      "grad_norm": 2.506633758544922,
      "learning_rate": 3.357718120805369e-05,
      "loss": 0.5795,
      "step": 24970
    },
    {
      "epoch": 0.9992,
      "grad_norm": 2.6779398918151855,
      "learning_rate": 3.3570469798657725e-05,
      "loss": 0.6424,
      "step": 24980
    },
    {
      "epoch": 0.9996,
      "grad_norm": 3.1734914779663086,
      "learning_rate": 3.356375838926175e-05,
      "loss": 0.5915,
      "step": 24990
    },
    {
      "epoch": 1.0,
      "grad_norm": 2.1701467037200928,
      "learning_rate": 3.3557046979865775e-05,
      "loss": 0.6344,
      "step": 25000
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.6501377820968628,
      "eval_runtime": 0.6057,
      "eval_samples_per_second": 3301.823,
      "eval_steps_per_second": 52.829,
      "step": 25000
    },
    {
      "epoch": 1.0004,
      "grad_norm": 2.6681067943573,
      "learning_rate": 3.35503355704698e-05,
      "loss": 0.6552,
      "step": 25010
    },
    {
      "epoch": 1.0008,
      "grad_norm": 2.844752788543701,
      "learning_rate": 3.3543624161073826e-05,
      "loss": 0.5896,
      "step": 25020
    },
    {
      "epoch": 1.0012,
      "grad_norm": 3.0433547496795654,
      "learning_rate": 3.3536912751677854e-05,
      "loss": 0.5372,
      "step": 25030
    },
    {
      "epoch": 1.0016,
      "grad_norm": 3.2985925674438477,
      "learning_rate": 3.353020134228188e-05,
      "loss": 0.5405,
      "step": 25040
    },
    {
      "epoch": 1.002,
      "grad_norm": 3.049551248550415,
      "learning_rate": 3.352348993288591e-05,
      "loss": 0.6084,
      "step": 25050
    },
    {
      "epoch": 1.0024,
      "grad_norm": 2.922562837600708,
      "learning_rate": 3.351677852348993e-05,
      "loss": 0.5753,
      "step": 25060
    },
    {
      "epoch": 1.0028,
      "grad_norm": 2.688703775405884,
      "learning_rate": 3.351006711409396e-05,
      "loss": 0.501,
      "step": 25070
    },
    {
      "epoch": 1.0032,
      "grad_norm": 3.400390863418579,
      "learning_rate": 3.3503355704697983e-05,
      "loss": 0.5298,
      "step": 25080
    },
    {
      "epoch": 1.0036,
      "grad_norm": 2.5236217975616455,
      "learning_rate": 3.349664429530202e-05,
      "loss": 0.6291,
      "step": 25090
    },
    {
      "epoch": 1.004,
      "grad_norm": 2.5811736583709717,
      "learning_rate": 3.348993288590605e-05,
      "loss": 0.5684,
      "step": 25100
    },
    {
      "epoch": 1.0044,
      "grad_norm": 2.5528838634490967,
      "learning_rate": 3.348322147651007e-05,
      "loss": 0.5961,
      "step": 25110
    },
    {
      "epoch": 1.0048,
      "grad_norm": 3.069091320037842,
      "learning_rate": 3.34765100671141e-05,
      "loss": 0.5936,
      "step": 25120
    },
    {
      "epoch": 1.0052,
      "grad_norm": 3.58849835395813,
      "learning_rate": 3.346979865771812e-05,
      "loss": 0.6573,
      "step": 25130
    },
    {
      "epoch": 1.0056,
      "grad_norm": 3.6214163303375244,
      "learning_rate": 3.346308724832215e-05,
      "loss": 0.5382,
      "step": 25140
    },
    {
      "epoch": 1.006,
      "grad_norm": 2.3496835231781006,
      "learning_rate": 3.3456375838926177e-05,
      "loss": 0.6065,
      "step": 25150
    },
    {
      "epoch": 1.0064,
      "grad_norm": 2.2588796615600586,
      "learning_rate": 3.3449664429530205e-05,
      "loss": 0.5339,
      "step": 25160
    },
    {
      "epoch": 1.0068,
      "grad_norm": 2.4820590019226074,
      "learning_rate": 3.3442953020134234e-05,
      "loss": 0.5512,
      "step": 25170
    },
    {
      "epoch": 1.0072,
      "grad_norm": 3.2525198459625244,
      "learning_rate": 3.3436241610738255e-05,
      "loss": 0.6071,
      "step": 25180
    },
    {
      "epoch": 1.0076,
      "grad_norm": 2.282285451889038,
      "learning_rate": 3.3429530201342284e-05,
      "loss": 0.5758,
      "step": 25190
    },
    {
      "epoch": 1.008,
      "grad_norm": 2.382826328277588,
      "learning_rate": 3.3422818791946306e-05,
      "loss": 0.5811,
      "step": 25200
    },
    {
      "epoch": 1.0084,
      "grad_norm": 2.404653310775757,
      "learning_rate": 3.341610738255034e-05,
      "loss": 0.6088,
      "step": 25210
    },
    {
      "epoch": 1.0088,
      "grad_norm": 2.444904327392578,
      "learning_rate": 3.340939597315436e-05,
      "loss": 0.5309,
      "step": 25220
    },
    {
      "epoch": 1.0092,
      "grad_norm": 3.018251419067383,
      "learning_rate": 3.340268456375839e-05,
      "loss": 0.6127,
      "step": 25230
    },
    {
      "epoch": 1.0096,
      "grad_norm": 2.509671926498413,
      "learning_rate": 3.339597315436242e-05,
      "loss": 0.5285,
      "step": 25240
    },
    {
      "epoch": 1.01,
      "grad_norm": 2.4562344551086426,
      "learning_rate": 3.338926174496644e-05,
      "loss": 0.5154,
      "step": 25250
    },
    {
      "epoch": 1.0104,
      "grad_norm": 3.1599767208099365,
      "learning_rate": 3.338255033557047e-05,
      "loss": 0.5848,
      "step": 25260
    },
    {
      "epoch": 1.0108,
      "grad_norm": 2.4227471351623535,
      "learning_rate": 3.33758389261745e-05,
      "loss": 0.518,
      "step": 25270
    },
    {
      "epoch": 1.0112,
      "grad_norm": 2.864992141723633,
      "learning_rate": 3.336912751677853e-05,
      "loss": 0.5341,
      "step": 25280
    },
    {
      "epoch": 1.0116,
      "grad_norm": 2.8180365562438965,
      "learning_rate": 3.336241610738255e-05,
      "loss": 0.5399,
      "step": 25290
    },
    {
      "epoch": 1.012,
      "grad_norm": 2.445948600769043,
      "learning_rate": 3.335570469798658e-05,
      "loss": 0.4896,
      "step": 25300
    },
    {
      "epoch": 1.0124,
      "grad_norm": 2.2914133071899414,
      "learning_rate": 3.3348993288590606e-05,
      "loss": 0.5156,
      "step": 25310
    },
    {
      "epoch": 1.0128,
      "grad_norm": 2.7182021141052246,
      "learning_rate": 3.334228187919463e-05,
      "loss": 0.612,
      "step": 25320
    },
    {
      "epoch": 1.0132,
      "grad_norm": 3.20866060256958,
      "learning_rate": 3.333557046979866e-05,
      "loss": 0.5955,
      "step": 25330
    },
    {
      "epoch": 1.0136,
      "grad_norm": 2.528397560119629,
      "learning_rate": 3.3328859060402685e-05,
      "loss": 0.6254,
      "step": 25340
    },
    {
      "epoch": 1.014,
      "grad_norm": 3.525009870529175,
      "learning_rate": 3.3322147651006714e-05,
      "loss": 0.6313,
      "step": 25350
    },
    {
      "epoch": 1.0144,
      "grad_norm": 1.9566153287887573,
      "learning_rate": 3.331543624161074e-05,
      "loss": 0.5349,
      "step": 25360
    },
    {
      "epoch": 1.0148,
      "grad_norm": 2.9507110118865967,
      "learning_rate": 3.3308724832214764e-05,
      "loss": 0.5818,
      "step": 25370
    },
    {
      "epoch": 1.0152,
      "grad_norm": 2.3440287113189697,
      "learning_rate": 3.330201342281879e-05,
      "loss": 0.6233,
      "step": 25380
    },
    {
      "epoch": 1.0156,
      "grad_norm": 2.712064743041992,
      "learning_rate": 3.329530201342282e-05,
      "loss": 0.5075,
      "step": 25390
    },
    {
      "epoch": 1.016,
      "grad_norm": 2.4811086654663086,
      "learning_rate": 3.328859060402685e-05,
      "loss": 0.6063,
      "step": 25400
    },
    {
      "epoch": 1.0164,
      "grad_norm": 2.2981481552124023,
      "learning_rate": 3.328187919463087e-05,
      "loss": 0.4962,
      "step": 25410
    },
    {
      "epoch": 1.0168,
      "grad_norm": 2.8369393348693848,
      "learning_rate": 3.32751677852349e-05,
      "loss": 0.6892,
      "step": 25420
    },
    {
      "epoch": 1.0172,
      "grad_norm": 2.185253620147705,
      "learning_rate": 3.326845637583893e-05,
      "loss": 0.5919,
      "step": 25430
    },
    {
      "epoch": 1.0176,
      "grad_norm": 2.2731621265411377,
      "learning_rate": 3.326174496644296e-05,
      "loss": 0.5586,
      "step": 25440
    },
    {
      "epoch": 1.018,
      "grad_norm": 2.4338982105255127,
      "learning_rate": 3.3255033557046986e-05,
      "loss": 0.5762,
      "step": 25450
    },
    {
      "epoch": 1.0184,
      "grad_norm": 1.8464679718017578,
      "learning_rate": 3.324832214765101e-05,
      "loss": 0.5105,
      "step": 25460
    },
    {
      "epoch": 1.0188,
      "grad_norm": 2.4614245891571045,
      "learning_rate": 3.3241610738255036e-05,
      "loss": 0.5785,
      "step": 25470
    },
    {
      "epoch": 1.0192,
      "grad_norm": 2.16603946685791,
      "learning_rate": 3.323489932885906e-05,
      "loss": 0.5619,
      "step": 25480
    },
    {
      "epoch": 1.0196,
      "grad_norm": 2.5986156463623047,
      "learning_rate": 3.3228187919463086e-05,
      "loss": 0.6817,
      "step": 25490
    },
    {
      "epoch": 1.02,
      "grad_norm": 2.298004388809204,
      "learning_rate": 3.3221476510067115e-05,
      "loss": 0.4864,
      "step": 25500
    },
    {
      "epoch": 1.0204,
      "grad_norm": 3.456921100616455,
      "learning_rate": 3.321476510067114e-05,
      "loss": 0.6209,
      "step": 25510
    },
    {
      "epoch": 1.0208,
      "grad_norm": 3.5158016681671143,
      "learning_rate": 3.320805369127517e-05,
      "loss": 0.5812,
      "step": 25520
    },
    {
      "epoch": 1.0212,
      "grad_norm": 2.3226478099823,
      "learning_rate": 3.3201342281879194e-05,
      "loss": 0.5701,
      "step": 25530
    },
    {
      "epoch": 1.0216,
      "grad_norm": 2.237541675567627,
      "learning_rate": 3.319463087248322e-05,
      "loss": 0.5105,
      "step": 25540
    },
    {
      "epoch": 1.022,
      "grad_norm": 2.4240052700042725,
      "learning_rate": 3.318791946308725e-05,
      "loss": 0.4945,
      "step": 25550
    },
    {
      "epoch": 1.0224,
      "grad_norm": 3.27217698097229,
      "learning_rate": 3.318120805369128e-05,
      "loss": 0.5943,
      "step": 25560
    },
    {
      "epoch": 1.0228,
      "grad_norm": 2.727756977081299,
      "learning_rate": 3.317449664429531e-05,
      "loss": 0.5349,
      "step": 25570
    },
    {
      "epoch": 1.0232,
      "grad_norm": 2.9807870388031006,
      "learning_rate": 3.316778523489933e-05,
      "loss": 0.5344,
      "step": 25580
    },
    {
      "epoch": 1.0236,
      "grad_norm": 3.3863584995269775,
      "learning_rate": 3.316107382550336e-05,
      "loss": 0.6173,
      "step": 25590
    },
    {
      "epoch": 1.024,
      "grad_norm": 2.049929141998291,
      "learning_rate": 3.315436241610738e-05,
      "loss": 0.4634,
      "step": 25600
    },
    {
      "epoch": 1.0244,
      "grad_norm": 2.6815829277038574,
      "learning_rate": 3.314765100671141e-05,
      "loss": 0.6751,
      "step": 25610
    },
    {
      "epoch": 1.0248,
      "grad_norm": 2.2904374599456787,
      "learning_rate": 3.314093959731544e-05,
      "loss": 0.5862,
      "step": 25620
    },
    {
      "epoch": 1.0252,
      "grad_norm": 2.417165756225586,
      "learning_rate": 3.3134228187919465e-05,
      "loss": 0.5871,
      "step": 25630
    },
    {
      "epoch": 1.0256,
      "grad_norm": 2.960521936416626,
      "learning_rate": 3.3127516778523494e-05,
      "loss": 0.6002,
      "step": 25640
    },
    {
      "epoch": 1.026,
      "grad_norm": 2.5161426067352295,
      "learning_rate": 3.3120805369127516e-05,
      "loss": 0.6489,
      "step": 25650
    },
    {
      "epoch": 1.0264,
      "grad_norm": 2.9121012687683105,
      "learning_rate": 3.3114093959731544e-05,
      "loss": 0.6054,
      "step": 25660
    },
    {
      "epoch": 1.0268,
      "grad_norm": 3.292731523513794,
      "learning_rate": 3.3107382550335566e-05,
      "loss": 0.5428,
      "step": 25670
    },
    {
      "epoch": 1.0272,
      "grad_norm": 2.9858431816101074,
      "learning_rate": 3.31006711409396e-05,
      "loss": 0.5956,
      "step": 25680
    },
    {
      "epoch": 1.0276,
      "grad_norm": 2.9913642406463623,
      "learning_rate": 3.309395973154363e-05,
      "loss": 0.5671,
      "step": 25690
    },
    {
      "epoch": 1.028,
      "grad_norm": 2.6013972759246826,
      "learning_rate": 3.308724832214765e-05,
      "loss": 0.5196,
      "step": 25700
    },
    {
      "epoch": 1.0284,
      "grad_norm": 2.7959773540496826,
      "learning_rate": 3.308053691275168e-05,
      "loss": 0.556,
      "step": 25710
    },
    {
      "epoch": 1.0288,
      "grad_norm": 3.1716790199279785,
      "learning_rate": 3.30738255033557e-05,
      "loss": 0.5636,
      "step": 25720
    },
    {
      "epoch": 1.0292,
      "grad_norm": 2.6028523445129395,
      "learning_rate": 3.306711409395973e-05,
      "loss": 0.4796,
      "step": 25730
    },
    {
      "epoch": 1.0296,
      "grad_norm": 1.872520089149475,
      "learning_rate": 3.3060402684563766e-05,
      "loss": 0.5481,
      "step": 25740
    },
    {
      "epoch": 1.03,
      "grad_norm": 2.498922109603882,
      "learning_rate": 3.305369127516779e-05,
      "loss": 0.5253,
      "step": 25750
    },
    {
      "epoch": 1.0304,
      "grad_norm": 3.0528228282928467,
      "learning_rate": 3.3046979865771816e-05,
      "loss": 0.5769,
      "step": 25760
    },
    {
      "epoch": 1.0308,
      "grad_norm": 2.9432640075683594,
      "learning_rate": 3.304026845637584e-05,
      "loss": 0.5696,
      "step": 25770
    },
    {
      "epoch": 1.0312,
      "grad_norm": 2.572280168533325,
      "learning_rate": 3.3033557046979867e-05,
      "loss": 0.5909,
      "step": 25780
    },
    {
      "epoch": 1.0316,
      "grad_norm": 2.5282857418060303,
      "learning_rate": 3.3026845637583895e-05,
      "loss": 0.5429,
      "step": 25790
    },
    {
      "epoch": 1.032,
      "grad_norm": 2.4034359455108643,
      "learning_rate": 3.3020134228187924e-05,
      "loss": 0.5953,
      "step": 25800
    },
    {
      "epoch": 1.0324,
      "grad_norm": 2.8323936462402344,
      "learning_rate": 3.301342281879195e-05,
      "loss": 0.5092,
      "step": 25810
    },
    {
      "epoch": 1.0328,
      "grad_norm": 3.026520252227783,
      "learning_rate": 3.3006711409395974e-05,
      "loss": 0.5664,
      "step": 25820
    },
    {
      "epoch": 1.0332,
      "grad_norm": 2.574962854385376,
      "learning_rate": 3.3e-05,
      "loss": 0.6011,
      "step": 25830
    },
    {
      "epoch": 1.0336,
      "grad_norm": 2.428966999053955,
      "learning_rate": 3.2993288590604024e-05,
      "loss": 0.5408,
      "step": 25840
    },
    {
      "epoch": 1.034,
      "grad_norm": 3.2730653285980225,
      "learning_rate": 3.298657718120805e-05,
      "loss": 0.6087,
      "step": 25850
    },
    {
      "epoch": 1.0344,
      "grad_norm": 3.018148422241211,
      "learning_rate": 3.297986577181208e-05,
      "loss": 0.6072,
      "step": 25860
    },
    {
      "epoch": 1.0348,
      "grad_norm": 2.524212598800659,
      "learning_rate": 3.297315436241611e-05,
      "loss": 0.5622,
      "step": 25870
    },
    {
      "epoch": 1.0352,
      "grad_norm": 2.5634613037109375,
      "learning_rate": 3.296644295302014e-05,
      "loss": 0.5972,
      "step": 25880
    },
    {
      "epoch": 1.0356,
      "grad_norm": 2.583712100982666,
      "learning_rate": 3.295973154362416e-05,
      "loss": 0.4625,
      "step": 25890
    },
    {
      "epoch": 1.036,
      "grad_norm": 2.92690110206604,
      "learning_rate": 3.295302013422819e-05,
      "loss": 0.604,
      "step": 25900
    },
    {
      "epoch": 1.0364,
      "grad_norm": 2.8448069095611572,
      "learning_rate": 3.294630872483222e-05,
      "loss": 0.5683,
      "step": 25910
    },
    {
      "epoch": 1.0368,
      "grad_norm": 2.354928493499756,
      "learning_rate": 3.2939597315436246e-05,
      "loss": 0.6459,
      "step": 25920
    },
    {
      "epoch": 1.0372,
      "grad_norm": 3.0253024101257324,
      "learning_rate": 3.2932885906040274e-05,
      "loss": 0.5881,
      "step": 25930
    },
    {
      "epoch": 1.0376,
      "grad_norm": 2.2853615283966064,
      "learning_rate": 3.2926174496644296e-05,
      "loss": 0.5657,
      "step": 25940
    },
    {
      "epoch": 1.038,
      "grad_norm": 2.3009893894195557,
      "learning_rate": 3.2919463087248325e-05,
      "loss": 0.5889,
      "step": 25950
    },
    {
      "epoch": 1.0384,
      "grad_norm": 2.2796976566314697,
      "learning_rate": 3.2912751677852347e-05,
      "loss": 0.5197,
      "step": 25960
    },
    {
      "epoch": 1.0388,
      "grad_norm": 2.748962640762329,
      "learning_rate": 3.2906040268456375e-05,
      "loss": 0.5338,
      "step": 25970
    },
    {
      "epoch": 1.0392,
      "grad_norm": 3.919386386871338,
      "learning_rate": 3.2899328859060404e-05,
      "loss": 0.5726,
      "step": 25980
    },
    {
      "epoch": 1.0396,
      "grad_norm": 2.4018197059631348,
      "learning_rate": 3.289261744966443e-05,
      "loss": 0.5399,
      "step": 25990
    },
    {
      "epoch": 1.04,
      "grad_norm": 2.8678550720214844,
      "learning_rate": 3.288590604026846e-05,
      "loss": 0.5909,
      "step": 26000
    },
    {
      "epoch": 1.0404,
      "grad_norm": 3.237858295440674,
      "learning_rate": 3.287919463087248e-05,
      "loss": 0.6449,
      "step": 26010
    },
    {
      "epoch": 1.0408,
      "grad_norm": 2.6730716228485107,
      "learning_rate": 3.287248322147651e-05,
      "loss": 0.5347,
      "step": 26020
    },
    {
      "epoch": 1.0412,
      "grad_norm": 2.8408098220825195,
      "learning_rate": 3.286577181208054e-05,
      "loss": 0.5554,
      "step": 26030
    },
    {
      "epoch": 1.0416,
      "grad_norm": 2.948417901992798,
      "learning_rate": 3.285906040268457e-05,
      "loss": 0.5983,
      "step": 26040
    },
    {
      "epoch": 1.042,
      "grad_norm": 2.7021305561065674,
      "learning_rate": 3.285234899328859e-05,
      "loss": 0.618,
      "step": 26050
    },
    {
      "epoch": 1.0424,
      "grad_norm": 1.8971513509750366,
      "learning_rate": 3.284563758389262e-05,
      "loss": 0.5517,
      "step": 26060
    },
    {
      "epoch": 1.0428,
      "grad_norm": 2.557196617126465,
      "learning_rate": 3.283892617449665e-05,
      "loss": 0.5815,
      "step": 26070
    },
    {
      "epoch": 1.0432,
      "grad_norm": 2.7724316120147705,
      "learning_rate": 3.283221476510067e-05,
      "loss": 0.4847,
      "step": 26080
    },
    {
      "epoch": 1.0436,
      "grad_norm": 2.2768051624298096,
      "learning_rate": 3.2825503355704704e-05,
      "loss": 0.6268,
      "step": 26090
    },
    {
      "epoch": 1.044,
      "grad_norm": 2.8662376403808594,
      "learning_rate": 3.2818791946308726e-05,
      "loss": 0.6764,
      "step": 26100
    },
    {
      "epoch": 1.0444,
      "grad_norm": 3.1099653244018555,
      "learning_rate": 3.2812080536912754e-05,
      "loss": 0.5692,
      "step": 26110
    },
    {
      "epoch": 1.0448,
      "grad_norm": 2.0652897357940674,
      "learning_rate": 3.2805369127516776e-05,
      "loss": 0.6133,
      "step": 26120
    },
    {
      "epoch": 1.0452,
      "grad_norm": 2.5276288986206055,
      "learning_rate": 3.2798657718120805e-05,
      "loss": 0.6059,
      "step": 26130
    },
    {
      "epoch": 1.0456,
      "grad_norm": 2.839688539505005,
      "learning_rate": 3.279194630872483e-05,
      "loss": 0.5822,
      "step": 26140
    },
    {
      "epoch": 1.046,
      "grad_norm": 3.526944398880005,
      "learning_rate": 3.278523489932886e-05,
      "loss": 0.6281,
      "step": 26150
    },
    {
      "epoch": 1.0464,
      "grad_norm": 2.9170751571655273,
      "learning_rate": 3.277852348993289e-05,
      "loss": 0.5702,
      "step": 26160
    },
    {
      "epoch": 1.0468,
      "grad_norm": 3.0152852535247803,
      "learning_rate": 3.277181208053691e-05,
      "loss": 0.4844,
      "step": 26170
    },
    {
      "epoch": 1.0472,
      "grad_norm": 2.3493919372558594,
      "learning_rate": 3.276510067114094e-05,
      "loss": 0.5264,
      "step": 26180
    },
    {
      "epoch": 1.0476,
      "grad_norm": 2.010509967803955,
      "learning_rate": 3.275838926174497e-05,
      "loss": 0.561,
      "step": 26190
    },
    {
      "epoch": 1.048,
      "grad_norm": 2.7942070960998535,
      "learning_rate": 3.275167785234899e-05,
      "loss": 0.5497,
      "step": 26200
    },
    {
      "epoch": 1.0484,
      "grad_norm": 3.041640281677246,
      "learning_rate": 3.2744966442953026e-05,
      "loss": 0.6082,
      "step": 26210
    },
    {
      "epoch": 1.0488,
      "grad_norm": 3.1304361820220947,
      "learning_rate": 3.273825503355705e-05,
      "loss": 0.5292,
      "step": 26220
    },
    {
      "epoch": 1.0492,
      "grad_norm": 2.932493209838867,
      "learning_rate": 3.273154362416108e-05,
      "loss": 0.5396,
      "step": 26230
    },
    {
      "epoch": 1.0496,
      "grad_norm": 2.298793077468872,
      "learning_rate": 3.27248322147651e-05,
      "loss": 0.6189,
      "step": 26240
    },
    {
      "epoch": 1.05,
      "grad_norm": 2.8317766189575195,
      "learning_rate": 3.271812080536913e-05,
      "loss": 0.5818,
      "step": 26250
    },
    {
      "epoch": 1.0504,
      "grad_norm": 2.4039313793182373,
      "learning_rate": 3.2711409395973156e-05,
      "loss": 0.6074,
      "step": 26260
    },
    {
      "epoch": 1.0508,
      "grad_norm": 2.5321545600891113,
      "learning_rate": 3.2704697986577184e-05,
      "loss": 0.6191,
      "step": 26270
    },
    {
      "epoch": 1.0512,
      "grad_norm": 1.8066209554672241,
      "learning_rate": 3.269798657718121e-05,
      "loss": 0.5713,
      "step": 26280
    },
    {
      "epoch": 1.0516,
      "grad_norm": 4.133465766906738,
      "learning_rate": 3.2691275167785234e-05,
      "loss": 0.6227,
      "step": 26290
    },
    {
      "epoch": 1.052,
      "grad_norm": 2.637587308883667,
      "learning_rate": 3.268456375838926e-05,
      "loss": 0.515,
      "step": 26300
    },
    {
      "epoch": 1.0524,
      "grad_norm": 2.8858888149261475,
      "learning_rate": 3.2677852348993285e-05,
      "loss": 0.6637,
      "step": 26310
    },
    {
      "epoch": 1.0528,
      "grad_norm": 2.761113405227661,
      "learning_rate": 3.267114093959732e-05,
      "loss": 0.5978,
      "step": 26320
    },
    {
      "epoch": 1.0532,
      "grad_norm": 3.009395122528076,
      "learning_rate": 3.266442953020135e-05,
      "loss": 0.5382,
      "step": 26330
    },
    {
      "epoch": 1.0536,
      "grad_norm": 2.689883232116699,
      "learning_rate": 3.265771812080537e-05,
      "loss": 0.5072,
      "step": 26340
    },
    {
      "epoch": 1.054,
      "grad_norm": 2.8492348194122314,
      "learning_rate": 3.26510067114094e-05,
      "loss": 0.5983,
      "step": 26350
    },
    {
      "epoch": 1.0544,
      "grad_norm": 2.896495819091797,
      "learning_rate": 3.264429530201342e-05,
      "loss": 0.6564,
      "step": 26360
    },
    {
      "epoch": 1.0548,
      "grad_norm": 3.0630345344543457,
      "learning_rate": 3.263758389261745e-05,
      "loss": 0.6383,
      "step": 26370
    },
    {
      "epoch": 1.0552,
      "grad_norm": 3.012504816055298,
      "learning_rate": 3.263087248322148e-05,
      "loss": 0.6264,
      "step": 26380
    },
    {
      "epoch": 1.0556,
      "grad_norm": 2.986374855041504,
      "learning_rate": 3.2624161073825506e-05,
      "loss": 0.4999,
      "step": 26390
    },
    {
      "epoch": 1.056,
      "grad_norm": 2.729726791381836,
      "learning_rate": 3.2617449664429535e-05,
      "loss": 0.6149,
      "step": 26400
    },
    {
      "epoch": 1.0564,
      "grad_norm": 2.4008514881134033,
      "learning_rate": 3.261073825503356e-05,
      "loss": 0.5995,
      "step": 26410
    },
    {
      "epoch": 1.0568,
      "grad_norm": 2.446702718734741,
      "learning_rate": 3.2604026845637585e-05,
      "loss": 0.5296,
      "step": 26420
    },
    {
      "epoch": 1.0572,
      "grad_norm": 2.5998849868774414,
      "learning_rate": 3.259731543624161e-05,
      "loss": 0.5597,
      "step": 26430
    },
    {
      "epoch": 1.0576,
      "grad_norm": 2.2730679512023926,
      "learning_rate": 3.259060402684564e-05,
      "loss": 0.5427,
      "step": 26440
    },
    {
      "epoch": 1.058,
      "grad_norm": 3.033034324645996,
      "learning_rate": 3.258389261744967e-05,
      "loss": 0.669,
      "step": 26450
    },
    {
      "epoch": 1.0584,
      "grad_norm": 2.1780426502227783,
      "learning_rate": 3.257718120805369e-05,
      "loss": 0.5627,
      "step": 26460
    },
    {
      "epoch": 1.0588,
      "grad_norm": 2.578359603881836,
      "learning_rate": 3.257046979865772e-05,
      "loss": 0.5354,
      "step": 26470
    },
    {
      "epoch": 1.0592,
      "grad_norm": 3.5885939598083496,
      "learning_rate": 3.256375838926174e-05,
      "loss": 0.5585,
      "step": 26480
    },
    {
      "epoch": 1.0596,
      "grad_norm": 2.638366222381592,
      "learning_rate": 3.255704697986577e-05,
      "loss": 0.4469,
      "step": 26490
    },
    {
      "epoch": 1.06,
      "grad_norm": 2.8591644763946533,
      "learning_rate": 3.25503355704698e-05,
      "loss": 0.5228,
      "step": 26500
    },
    {
      "epoch": 1.0604,
      "grad_norm": 2.302518367767334,
      "learning_rate": 3.254362416107383e-05,
      "loss": 0.5372,
      "step": 26510
    },
    {
      "epoch": 1.0608,
      "grad_norm": 2.5060489177703857,
      "learning_rate": 3.253691275167786e-05,
      "loss": 0.5094,
      "step": 26520
    },
    {
      "epoch": 1.0612,
      "grad_norm": 2.691075086593628,
      "learning_rate": 3.253020134228188e-05,
      "loss": 0.6067,
      "step": 26530
    },
    {
      "epoch": 1.0616,
      "grad_norm": 2.606501579284668,
      "learning_rate": 3.252348993288591e-05,
      "loss": 0.5177,
      "step": 26540
    },
    {
      "epoch": 1.062,
      "grad_norm": 3.5218005180358887,
      "learning_rate": 3.251677852348993e-05,
      "loss": 0.54,
      "step": 26550
    },
    {
      "epoch": 1.0624,
      "grad_norm": 2.6761038303375244,
      "learning_rate": 3.2510067114093964e-05,
      "loss": 0.4749,
      "step": 26560
    },
    {
      "epoch": 1.0628,
      "grad_norm": 2.49526309967041,
      "learning_rate": 3.250335570469799e-05,
      "loss": 0.5708,
      "step": 26570
    },
    {
      "epoch": 1.0632,
      "grad_norm": 3.0320992469787598,
      "learning_rate": 3.2496644295302015e-05,
      "loss": 0.5868,
      "step": 26580
    },
    {
      "epoch": 1.0636,
      "grad_norm": 3.0079073905944824,
      "learning_rate": 3.248993288590604e-05,
      "loss": 0.6351,
      "step": 26590
    },
    {
      "epoch": 1.064,
      "grad_norm": 2.9735631942749023,
      "learning_rate": 3.2483221476510065e-05,
      "loss": 0.5718,
      "step": 26600
    },
    {
      "epoch": 1.0644,
      "grad_norm": 3.137864351272583,
      "learning_rate": 3.2476510067114094e-05,
      "loss": 0.555,
      "step": 26610
    },
    {
      "epoch": 1.0648,
      "grad_norm": 2.9297688007354736,
      "learning_rate": 3.246979865771812e-05,
      "loss": 0.5676,
      "step": 26620
    },
    {
      "epoch": 1.0652,
      "grad_norm": 2.4227373600006104,
      "learning_rate": 3.246308724832215e-05,
      "loss": 0.5671,
      "step": 26630
    },
    {
      "epoch": 1.0656,
      "grad_norm": 2.4680263996124268,
      "learning_rate": 3.245637583892618e-05,
      "loss": 0.5729,
      "step": 26640
    },
    {
      "epoch": 1.066,
      "grad_norm": 2.1855692863464355,
      "learning_rate": 3.24496644295302e-05,
      "loss": 0.5193,
      "step": 26650
    },
    {
      "epoch": 1.0664,
      "grad_norm": 2.362400770187378,
      "learning_rate": 3.244295302013423e-05,
      "loss": 0.5409,
      "step": 26660
    },
    {
      "epoch": 1.0668,
      "grad_norm": 3.3995144367218018,
      "learning_rate": 3.243624161073826e-05,
      "loss": 0.5918,
      "step": 26670
    },
    {
      "epoch": 1.0672,
      "grad_norm": 3.972227096557617,
      "learning_rate": 3.242953020134229e-05,
      "loss": 0.5537,
      "step": 26680
    },
    {
      "epoch": 1.0676,
      "grad_norm": 2.4619836807250977,
      "learning_rate": 3.242281879194631e-05,
      "loss": 0.5088,
      "step": 26690
    },
    {
      "epoch": 1.068,
      "grad_norm": 2.2907748222351074,
      "learning_rate": 3.241610738255034e-05,
      "loss": 0.5837,
      "step": 26700
    },
    {
      "epoch": 1.0684,
      "grad_norm": 2.701281785964966,
      "learning_rate": 3.2409395973154366e-05,
      "loss": 0.5321,
      "step": 26710
    },
    {
      "epoch": 1.0688,
      "grad_norm": 2.6670303344726562,
      "learning_rate": 3.240268456375839e-05,
      "loss": 0.5798,
      "step": 26720
    },
    {
      "epoch": 1.0692,
      "grad_norm": 2.9788460731506348,
      "learning_rate": 3.2395973154362416e-05,
      "loss": 0.5341,
      "step": 26730
    },
    {
      "epoch": 1.0695999999999999,
      "grad_norm": 3.155662775039673,
      "learning_rate": 3.2389261744966444e-05,
      "loss": 0.5746,
      "step": 26740
    },
    {
      "epoch": 1.07,
      "grad_norm": 2.4707603454589844,
      "learning_rate": 3.238255033557047e-05,
      "loss": 0.5657,
      "step": 26750
    },
    {
      "epoch": 1.0704,
      "grad_norm": 3.4774696826934814,
      "learning_rate": 3.2375838926174495e-05,
      "loss": 0.7061,
      "step": 26760
    },
    {
      "epoch": 1.0708,
      "grad_norm": 2.8070220947265625,
      "learning_rate": 3.236912751677852e-05,
      "loss": 0.4987,
      "step": 26770
    },
    {
      "epoch": 1.0712,
      "grad_norm": 2.666937828063965,
      "learning_rate": 3.236241610738255e-05,
      "loss": 0.6208,
      "step": 26780
    },
    {
      "epoch": 1.0716,
      "grad_norm": 3.2352020740509033,
      "learning_rate": 3.235570469798658e-05,
      "loss": 0.5034,
      "step": 26790
    },
    {
      "epoch": 1.072,
      "grad_norm": 2.865037679672241,
      "learning_rate": 3.234899328859061e-05,
      "loss": 0.5777,
      "step": 26800
    },
    {
      "epoch": 1.0724,
      "grad_norm": 2.7279863357543945,
      "learning_rate": 3.234228187919463e-05,
      "loss": 0.5414,
      "step": 26810
    },
    {
      "epoch": 1.0728,
      "grad_norm": 2.6822845935821533,
      "learning_rate": 3.233557046979866e-05,
      "loss": 0.5272,
      "step": 26820
    },
    {
      "epoch": 1.0732,
      "grad_norm": 2.3687398433685303,
      "learning_rate": 3.232885906040269e-05,
      "loss": 0.5198,
      "step": 26830
    },
    {
      "epoch": 1.0735999999999999,
      "grad_norm": 3.3711533546447754,
      "learning_rate": 3.232214765100671e-05,
      "loss": 0.5828,
      "step": 26840
    },
    {
      "epoch": 1.074,
      "grad_norm": 3.1747334003448486,
      "learning_rate": 3.231543624161074e-05,
      "loss": 0.6624,
      "step": 26850
    },
    {
      "epoch": 1.0744,
      "grad_norm": 2.31327223777771,
      "learning_rate": 3.230872483221477e-05,
      "loss": 0.6431,
      "step": 26860
    },
    {
      "epoch": 1.0748,
      "grad_norm": 2.929917573928833,
      "learning_rate": 3.2302013422818795e-05,
      "loss": 0.5667,
      "step": 26870
    },
    {
      "epoch": 1.0752,
      "grad_norm": 2.125523090362549,
      "learning_rate": 3.229530201342282e-05,
      "loss": 0.5459,
      "step": 26880
    },
    {
      "epoch": 1.0756000000000001,
      "grad_norm": 2.0937869548797607,
      "learning_rate": 3.2288590604026846e-05,
      "loss": 0.4667,
      "step": 26890
    },
    {
      "epoch": 1.076,
      "grad_norm": 3.7944729328155518,
      "learning_rate": 3.2281879194630874e-05,
      "loss": 0.5344,
      "step": 26900
    },
    {
      "epoch": 1.0764,
      "grad_norm": 2.5829877853393555,
      "learning_rate": 3.22751677852349e-05,
      "loss": 0.5867,
      "step": 26910
    },
    {
      "epoch": 1.0768,
      "grad_norm": 3.133430004119873,
      "learning_rate": 3.226845637583893e-05,
      "loss": 0.5554,
      "step": 26920
    },
    {
      "epoch": 1.0772,
      "grad_norm": 2.033942461013794,
      "learning_rate": 3.226174496644295e-05,
      "loss": 0.5625,
      "step": 26930
    },
    {
      "epoch": 1.0776,
      "grad_norm": 2.271867036819458,
      "learning_rate": 3.225503355704698e-05,
      "loss": 0.5668,
      "step": 26940
    },
    {
      "epoch": 1.078,
      "grad_norm": 2.468493938446045,
      "learning_rate": 3.2248322147651e-05,
      "loss": 0.5321,
      "step": 26950
    },
    {
      "epoch": 1.0784,
      "grad_norm": 2.654057741165161,
      "learning_rate": 3.224161073825503e-05,
      "loss": 0.5391,
      "step": 26960
    },
    {
      "epoch": 1.0788,
      "grad_norm": 2.7238810062408447,
      "learning_rate": 3.223489932885907e-05,
      "loss": 0.5264,
      "step": 26970
    },
    {
      "epoch": 1.0792,
      "grad_norm": 2.638526439666748,
      "learning_rate": 3.222818791946309e-05,
      "loss": 0.5483,
      "step": 26980
    },
    {
      "epoch": 1.0796000000000001,
      "grad_norm": 2.704834222793579,
      "learning_rate": 3.222147651006712e-05,
      "loss": 0.5638,
      "step": 26990
    },
    {
      "epoch": 1.08,
      "grad_norm": 2.601025104522705,
      "learning_rate": 3.221476510067114e-05,
      "loss": 0.5339,
      "step": 27000
    },
    {
      "epoch": 1.0804,
      "grad_norm": 3.2042713165283203,
      "learning_rate": 3.220805369127517e-05,
      "loss": 0.7456,
      "step": 27010
    },
    {
      "epoch": 1.0808,
      "grad_norm": 2.655366897583008,
      "learning_rate": 3.2201342281879196e-05,
      "loss": 0.5028,
      "step": 27020
    },
    {
      "epoch": 1.0812,
      "grad_norm": 2.4316749572753906,
      "learning_rate": 3.2194630872483225e-05,
      "loss": 0.6263,
      "step": 27030
    },
    {
      "epoch": 1.0816,
      "grad_norm": 2.6666579246520996,
      "learning_rate": 3.2187919463087253e-05,
      "loss": 0.5765,
      "step": 27040
    },
    {
      "epoch": 1.082,
      "grad_norm": 2.3394601345062256,
      "learning_rate": 3.2181208053691275e-05,
      "loss": 0.6111,
      "step": 27050
    },
    {
      "epoch": 1.0824,
      "grad_norm": 2.470240831375122,
      "learning_rate": 3.2174496644295304e-05,
      "loss": 0.5079,
      "step": 27060
    },
    {
      "epoch": 1.0828,
      "grad_norm": 2.799340009689331,
      "learning_rate": 3.2167785234899326e-05,
      "loss": 0.5187,
      "step": 27070
    },
    {
      "epoch": 1.0832,
      "grad_norm": 2.450610876083374,
      "learning_rate": 3.2161073825503354e-05,
      "loss": 0.6745,
      "step": 27080
    },
    {
      "epoch": 1.0836,
      "grad_norm": 2.328165292739868,
      "learning_rate": 3.215436241610739e-05,
      "loss": 0.4933,
      "step": 27090
    },
    {
      "epoch": 1.084,
      "grad_norm": 2.7551534175872803,
      "learning_rate": 3.214765100671141e-05,
      "loss": 0.5937,
      "step": 27100
    },
    {
      "epoch": 1.0844,
      "grad_norm": 2.679450035095215,
      "learning_rate": 3.214093959731544e-05,
      "loss": 0.6281,
      "step": 27110
    },
    {
      "epoch": 1.0848,
      "grad_norm": 2.7573351860046387,
      "learning_rate": 3.213422818791946e-05,
      "loss": 0.6513,
      "step": 27120
    },
    {
      "epoch": 1.0852,
      "grad_norm": 3.3684210777282715,
      "learning_rate": 3.212751677852349e-05,
      "loss": 0.5816,
      "step": 27130
    },
    {
      "epoch": 1.0856,
      "grad_norm": 3.2368924617767334,
      "learning_rate": 3.212080536912752e-05,
      "loss": 0.6722,
      "step": 27140
    },
    {
      "epoch": 1.086,
      "grad_norm": 2.3751840591430664,
      "learning_rate": 3.211409395973155e-05,
      "loss": 0.5832,
      "step": 27150
    },
    {
      "epoch": 1.0864,
      "grad_norm": 2.4752888679504395,
      "learning_rate": 3.2107382550335576e-05,
      "loss": 0.5824,
      "step": 27160
    },
    {
      "epoch": 1.0868,
      "grad_norm": 2.2487826347351074,
      "learning_rate": 3.21006711409396e-05,
      "loss": 0.529,
      "step": 27170
    },
    {
      "epoch": 1.0872,
      "grad_norm": 2.1760332584381104,
      "learning_rate": 3.2093959731543626e-05,
      "loss": 0.5399,
      "step": 27180
    },
    {
      "epoch": 1.0876,
      "grad_norm": 2.102583885192871,
      "learning_rate": 3.208724832214765e-05,
      "loss": 0.5524,
      "step": 27190
    },
    {
      "epoch": 1.088,
      "grad_norm": 3.1830060482025146,
      "learning_rate": 3.208053691275168e-05,
      "loss": 0.6279,
      "step": 27200
    },
    {
      "epoch": 1.0884,
      "grad_norm": 2.856316089630127,
      "learning_rate": 3.207382550335571e-05,
      "loss": 0.6463,
      "step": 27210
    },
    {
      "epoch": 1.0888,
      "grad_norm": 2.2168359756469727,
      "learning_rate": 3.2067114093959733e-05,
      "loss": 0.4942,
      "step": 27220
    },
    {
      "epoch": 1.0892,
      "grad_norm": 3.2997233867645264,
      "learning_rate": 3.206040268456376e-05,
      "loss": 0.5954,
      "step": 27230
    },
    {
      "epoch": 1.0896,
      "grad_norm": 2.927036762237549,
      "learning_rate": 3.2053691275167784e-05,
      "loss": 0.5856,
      "step": 27240
    },
    {
      "epoch": 1.09,
      "grad_norm": 2.279008150100708,
      "learning_rate": 3.204697986577181e-05,
      "loss": 0.5802,
      "step": 27250
    },
    {
      "epoch": 1.0904,
      "grad_norm": 3.099576234817505,
      "learning_rate": 3.204026845637584e-05,
      "loss": 0.5466,
      "step": 27260
    },
    {
      "epoch": 1.0908,
      "grad_norm": 2.7019495964050293,
      "learning_rate": 3.203355704697987e-05,
      "loss": 0.5074,
      "step": 27270
    },
    {
      "epoch": 1.0912,
      "grad_norm": 2.3351495265960693,
      "learning_rate": 3.20268456375839e-05,
      "loss": 0.4906,
      "step": 27280
    },
    {
      "epoch": 1.0916,
      "grad_norm": 3.3746533393859863,
      "learning_rate": 3.202013422818792e-05,
      "loss": 0.6212,
      "step": 27290
    },
    {
      "epoch": 1.092,
      "grad_norm": 2.8466310501098633,
      "learning_rate": 3.201342281879195e-05,
      "loss": 0.549,
      "step": 27300
    },
    {
      "epoch": 1.0924,
      "grad_norm": 2.0039474964141846,
      "learning_rate": 3.200671140939597e-05,
      "loss": 0.5861,
      "step": 27310
    },
    {
      "epoch": 1.0928,
      "grad_norm": 2.8605239391326904,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 0.583,
      "step": 27320
    },
    {
      "epoch": 1.0932,
      "grad_norm": 3.186762809753418,
      "learning_rate": 3.199328859060403e-05,
      "loss": 0.565,
      "step": 27330
    },
    {
      "epoch": 1.0936,
      "grad_norm": 2.9300620555877686,
      "learning_rate": 3.1986577181208056e-05,
      "loss": 0.5756,
      "step": 27340
    },
    {
      "epoch": 1.094,
      "grad_norm": 2.6984877586364746,
      "learning_rate": 3.1979865771812084e-05,
      "loss": 0.6475,
      "step": 27350
    },
    {
      "epoch": 1.0944,
      "grad_norm": 2.3073935508728027,
      "learning_rate": 3.1973154362416106e-05,
      "loss": 0.5669,
      "step": 27360
    },
    {
      "epoch": 1.0948,
      "grad_norm": 2.80859637260437,
      "learning_rate": 3.1966442953020135e-05,
      "loss": 0.5964,
      "step": 27370
    },
    {
      "epoch": 1.0952,
      "grad_norm": 2.2330574989318848,
      "learning_rate": 3.195973154362416e-05,
      "loss": 0.5816,
      "step": 27380
    },
    {
      "epoch": 1.0956,
      "grad_norm": 2.475471019744873,
      "learning_rate": 3.195302013422819e-05,
      "loss": 0.5994,
      "step": 27390
    },
    {
      "epoch": 1.096,
      "grad_norm": 2.3842148780822754,
      "learning_rate": 3.194630872483222e-05,
      "loss": 0.5349,
      "step": 27400
    },
    {
      "epoch": 1.0964,
      "grad_norm": 2.4490654468536377,
      "learning_rate": 3.193959731543624e-05,
      "loss": 0.5579,
      "step": 27410
    },
    {
      "epoch": 1.0968,
      "grad_norm": 2.2918460369110107,
      "learning_rate": 3.193288590604027e-05,
      "loss": 0.5721,
      "step": 27420
    },
    {
      "epoch": 1.0972,
      "grad_norm": 2.8877573013305664,
      "learning_rate": 3.192617449664429e-05,
      "loss": 0.604,
      "step": 27430
    },
    {
      "epoch": 1.0976,
      "grad_norm": 2.3091881275177,
      "learning_rate": 3.191946308724833e-05,
      "loss": 0.6047,
      "step": 27440
    },
    {
      "epoch": 1.098,
      "grad_norm": 2.6382198333740234,
      "learning_rate": 3.191275167785235e-05,
      "loss": 0.5265,
      "step": 27450
    },
    {
      "epoch": 1.0984,
      "grad_norm": 2.612816572189331,
      "learning_rate": 3.190604026845638e-05,
      "loss": 0.5362,
      "step": 27460
    },
    {
      "epoch": 1.0988,
      "grad_norm": 3.049402952194214,
      "learning_rate": 3.1899328859060406e-05,
      "loss": 0.5633,
      "step": 27470
    },
    {
      "epoch": 1.0992,
      "grad_norm": 2.746443271636963,
      "learning_rate": 3.189261744966443e-05,
      "loss": 0.52,
      "step": 27480
    },
    {
      "epoch": 1.0996,
      "grad_norm": 2.948211669921875,
      "learning_rate": 3.188590604026846e-05,
      "loss": 0.5036,
      "step": 27490
    },
    {
      "epoch": 1.1,
      "grad_norm": 3.5980405807495117,
      "learning_rate": 3.1879194630872485e-05,
      "loss": 0.5694,
      "step": 27500
    },
    {
      "epoch": 1.1004,
      "grad_norm": 2.702807903289795,
      "learning_rate": 3.1872483221476514e-05,
      "loss": 0.5861,
      "step": 27510
    },
    {
      "epoch": 1.1008,
      "grad_norm": 3.289835214614868,
      "learning_rate": 3.1865771812080536e-05,
      "loss": 0.6254,
      "step": 27520
    },
    {
      "epoch": 1.1012,
      "grad_norm": 2.400815486907959,
      "learning_rate": 3.1859060402684564e-05,
      "loss": 0.5897,
      "step": 27530
    },
    {
      "epoch": 1.1016,
      "grad_norm": 2.230991840362549,
      "learning_rate": 3.185234899328859e-05,
      "loss": 0.551,
      "step": 27540
    },
    {
      "epoch": 1.102,
      "grad_norm": 2.8455421924591064,
      "learning_rate": 3.184563758389262e-05,
      "loss": 0.5617,
      "step": 27550
    },
    {
      "epoch": 1.1024,
      "grad_norm": 2.7378854751586914,
      "learning_rate": 3.183892617449665e-05,
      "loss": 0.4651,
      "step": 27560
    },
    {
      "epoch": 1.1028,
      "grad_norm": 2.769663095474243,
      "learning_rate": 3.183221476510067e-05,
      "loss": 0.5242,
      "step": 27570
    },
    {
      "epoch": 1.1032,
      "grad_norm": 2.7103734016418457,
      "learning_rate": 3.18255033557047e-05,
      "loss": 0.5528,
      "step": 27580
    },
    {
      "epoch": 1.1036,
      "grad_norm": 2.596937417984009,
      "learning_rate": 3.181879194630872e-05,
      "loss": 0.5415,
      "step": 27590
    },
    {
      "epoch": 1.104,
      "grad_norm": 2.1865856647491455,
      "learning_rate": 3.181208053691275e-05,
      "loss": 0.5045,
      "step": 27600
    },
    {
      "epoch": 1.1044,
      "grad_norm": 2.769365072250366,
      "learning_rate": 3.180536912751678e-05,
      "loss": 0.5123,
      "step": 27610
    },
    {
      "epoch": 1.1048,
      "grad_norm": 2.2729578018188477,
      "learning_rate": 3.179865771812081e-05,
      "loss": 0.5586,
      "step": 27620
    },
    {
      "epoch": 1.1052,
      "grad_norm": 2.089266300201416,
      "learning_rate": 3.1791946308724836e-05,
      "loss": 0.595,
      "step": 27630
    },
    {
      "epoch": 1.1056,
      "grad_norm": 2.6856930255889893,
      "learning_rate": 3.178523489932886e-05,
      "loss": 0.5656,
      "step": 27640
    },
    {
      "epoch": 1.106,
      "grad_norm": 2.7769148349761963,
      "learning_rate": 3.1778523489932886e-05,
      "loss": 0.6432,
      "step": 27650
    },
    {
      "epoch": 1.1064,
      "grad_norm": 3.251607656478882,
      "learning_rate": 3.1771812080536915e-05,
      "loss": 0.5846,
      "step": 27660
    },
    {
      "epoch": 1.1068,
      "grad_norm": 3.0843892097473145,
      "learning_rate": 3.1765100671140943e-05,
      "loss": 0.5774,
      "step": 27670
    },
    {
      "epoch": 1.1072,
      "grad_norm": 2.634700298309326,
      "learning_rate": 3.175838926174497e-05,
      "loss": 0.5675,
      "step": 27680
    },
    {
      "epoch": 1.1076,
      "grad_norm": 3.131293296813965,
      "learning_rate": 3.1751677852348994e-05,
      "loss": 0.6026,
      "step": 27690
    },
    {
      "epoch": 1.108,
      "grad_norm": 2.8194899559020996,
      "learning_rate": 3.174496644295302e-05,
      "loss": 0.5351,
      "step": 27700
    },
    {
      "epoch": 1.1084,
      "grad_norm": 2.632948398590088,
      "learning_rate": 3.1738255033557044e-05,
      "loss": 0.5385,
      "step": 27710
    },
    {
      "epoch": 1.1088,
      "grad_norm": 3.6939210891723633,
      "learning_rate": 3.173154362416107e-05,
      "loss": 0.5823,
      "step": 27720
    },
    {
      "epoch": 1.1092,
      "grad_norm": 2.247758388519287,
      "learning_rate": 3.17248322147651e-05,
      "loss": 0.529,
      "step": 27730
    },
    {
      "epoch": 1.1096,
      "grad_norm": 2.6196258068084717,
      "learning_rate": 3.171812080536913e-05,
      "loss": 0.5534,
      "step": 27740
    },
    {
      "epoch": 1.11,
      "grad_norm": 3.030431032180786,
      "learning_rate": 3.171140939597316e-05,
      "loss": 0.4916,
      "step": 27750
    },
    {
      "epoch": 1.1104,
      "grad_norm": 2.587653636932373,
      "learning_rate": 3.170469798657718e-05,
      "loss": 0.5651,
      "step": 27760
    },
    {
      "epoch": 1.1108,
      "grad_norm": 1.8669618368148804,
      "learning_rate": 3.169798657718121e-05,
      "loss": 0.5434,
      "step": 27770
    },
    {
      "epoch": 1.1112,
      "grad_norm": 3.2605321407318115,
      "learning_rate": 3.169127516778523e-05,
      "loss": 0.6439,
      "step": 27780
    },
    {
      "epoch": 1.1116,
      "grad_norm": 2.2672173976898193,
      "learning_rate": 3.1684563758389266e-05,
      "loss": 0.611,
      "step": 27790
    },
    {
      "epoch": 1.112,
      "grad_norm": 2.6711292266845703,
      "learning_rate": 3.1677852348993294e-05,
      "loss": 0.6787,
      "step": 27800
    },
    {
      "epoch": 1.1124,
      "grad_norm": 3.5552573204040527,
      "learning_rate": 3.1671140939597316e-05,
      "loss": 0.463,
      "step": 27810
    },
    {
      "epoch": 1.1128,
      "grad_norm": 1.9540714025497437,
      "learning_rate": 3.1664429530201345e-05,
      "loss": 0.5867,
      "step": 27820
    },
    {
      "epoch": 1.1132,
      "grad_norm": 2.1917219161987305,
      "learning_rate": 3.1657718120805366e-05,
      "loss": 0.5433,
      "step": 27830
    },
    {
      "epoch": 1.1136,
      "grad_norm": 2.120494842529297,
      "learning_rate": 3.1651006711409395e-05,
      "loss": 0.5168,
      "step": 27840
    },
    {
      "epoch": 1.114,
      "grad_norm": 2.3262999057769775,
      "learning_rate": 3.164429530201343e-05,
      "loss": 0.5518,
      "step": 27850
    },
    {
      "epoch": 1.1144,
      "grad_norm": 2.763251543045044,
      "learning_rate": 3.163758389261745e-05,
      "loss": 0.5164,
      "step": 27860
    },
    {
      "epoch": 1.1148,
      "grad_norm": 3.040184736251831,
      "learning_rate": 3.163087248322148e-05,
      "loss": 0.6831,
      "step": 27870
    },
    {
      "epoch": 1.1152,
      "grad_norm": 3.1600100994110107,
      "learning_rate": 3.16241610738255e-05,
      "loss": 0.5304,
      "step": 27880
    },
    {
      "epoch": 1.1156,
      "grad_norm": 2.272757053375244,
      "learning_rate": 3.161744966442953e-05,
      "loss": 0.5446,
      "step": 27890
    },
    {
      "epoch": 1.116,
      "grad_norm": 2.480262517929077,
      "learning_rate": 3.161073825503356e-05,
      "loss": 0.5363,
      "step": 27900
    },
    {
      "epoch": 1.1164,
      "grad_norm": 1.983237862586975,
      "learning_rate": 3.160402684563759e-05,
      "loss": 0.4663,
      "step": 27910
    },
    {
      "epoch": 1.1168,
      "grad_norm": 2.9942784309387207,
      "learning_rate": 3.1597315436241617e-05,
      "loss": 0.6218,
      "step": 27920
    },
    {
      "epoch": 1.1172,
      "grad_norm": 2.5523202419281006,
      "learning_rate": 3.159060402684564e-05,
      "loss": 0.5523,
      "step": 27930
    },
    {
      "epoch": 1.1176,
      "grad_norm": 2.259385347366333,
      "learning_rate": 3.158389261744967e-05,
      "loss": 0.5286,
      "step": 27940
    },
    {
      "epoch": 1.1179999999999999,
      "grad_norm": 2.588259220123291,
      "learning_rate": 3.157718120805369e-05,
      "loss": 0.4916,
      "step": 27950
    },
    {
      "epoch": 1.1184,
      "grad_norm": 2.77231764793396,
      "learning_rate": 3.157046979865772e-05,
      "loss": 0.6013,
      "step": 27960
    },
    {
      "epoch": 1.1188,
      "grad_norm": 2.499253749847412,
      "learning_rate": 3.1563758389261746e-05,
      "loss": 0.5378,
      "step": 27970
    },
    {
      "epoch": 1.1192,
      "grad_norm": 2.7323107719421387,
      "learning_rate": 3.1557046979865774e-05,
      "loss": 0.5363,
      "step": 27980
    },
    {
      "epoch": 1.1196,
      "grad_norm": 2.5480363368988037,
      "learning_rate": 3.15503355704698e-05,
      "loss": 0.5104,
      "step": 27990
    },
    {
      "epoch": 1.12,
      "grad_norm": 2.9022696018218994,
      "learning_rate": 3.1543624161073825e-05,
      "loss": 0.5586,
      "step": 28000
    },
    {
      "epoch": 1.1204,
      "grad_norm": 3.196261405944824,
      "learning_rate": 3.153691275167785e-05,
      "loss": 0.6218,
      "step": 28010
    },
    {
      "epoch": 1.1208,
      "grad_norm": 2.8795247077941895,
      "learning_rate": 3.153020134228188e-05,
      "loss": 0.5865,
      "step": 28020
    },
    {
      "epoch": 1.1212,
      "grad_norm": 3.0462918281555176,
      "learning_rate": 3.152348993288591e-05,
      "loss": 0.499,
      "step": 28030
    },
    {
      "epoch": 1.1216,
      "grad_norm": 2.7704038619995117,
      "learning_rate": 3.151677852348994e-05,
      "loss": 0.6033,
      "step": 28040
    },
    {
      "epoch": 1.1219999999999999,
      "grad_norm": 2.8696000576019287,
      "learning_rate": 3.151006711409396e-05,
      "loss": 0.5681,
      "step": 28050
    },
    {
      "epoch": 1.1224,
      "grad_norm": 2.818981170654297,
      "learning_rate": 3.150335570469799e-05,
      "loss": 0.6253,
      "step": 28060
    },
    {
      "epoch": 1.1228,
      "grad_norm": 3.050593852996826,
      "learning_rate": 3.149664429530201e-05,
      "loss": 0.5216,
      "step": 28070
    },
    {
      "epoch": 1.1232,
      "grad_norm": 2.4522979259490967,
      "learning_rate": 3.148993288590604e-05,
      "loss": 0.5659,
      "step": 28080
    },
    {
      "epoch": 1.1236,
      "grad_norm": 3.0620713233947754,
      "learning_rate": 3.148322147651007e-05,
      "loss": 0.6498,
      "step": 28090
    },
    {
      "epoch": 1.124,
      "grad_norm": 2.9869494438171387,
      "learning_rate": 3.1476510067114096e-05,
      "loss": 0.5332,
      "step": 28100
    },
    {
      "epoch": 1.1244,
      "grad_norm": 2.665067672729492,
      "learning_rate": 3.1469798657718125e-05,
      "loss": 0.491,
      "step": 28110
    },
    {
      "epoch": 1.1248,
      "grad_norm": 3.205244541168213,
      "learning_rate": 3.146308724832215e-05,
      "loss": 0.5916,
      "step": 28120
    },
    {
      "epoch": 1.1252,
      "grad_norm": 2.009387493133545,
      "learning_rate": 3.1456375838926175e-05,
      "loss": 0.5818,
      "step": 28130
    },
    {
      "epoch": 1.1256,
      "grad_norm": 2.8738834857940674,
      "learning_rate": 3.1449664429530204e-05,
      "loss": 0.4932,
      "step": 28140
    },
    {
      "epoch": 1.126,
      "grad_norm": 3.1511855125427246,
      "learning_rate": 3.144295302013423e-05,
      "loss": 0.5596,
      "step": 28150
    },
    {
      "epoch": 1.1264,
      "grad_norm": 3.1120216846466064,
      "learning_rate": 3.1436241610738254e-05,
      "loss": 0.5979,
      "step": 28160
    },
    {
      "epoch": 1.1268,
      "grad_norm": 2.747636556625366,
      "learning_rate": 3.142953020134228e-05,
      "loss": 0.5509,
      "step": 28170
    },
    {
      "epoch": 1.1272,
      "grad_norm": 3.1795973777770996,
      "learning_rate": 3.142281879194631e-05,
      "loss": 0.6696,
      "step": 28180
    },
    {
      "epoch": 1.1276,
      "grad_norm": 2.659562587738037,
      "learning_rate": 3.141610738255033e-05,
      "loss": 0.5369,
      "step": 28190
    },
    {
      "epoch": 1.1280000000000001,
      "grad_norm": 2.9605863094329834,
      "learning_rate": 3.140939597315437e-05,
      "loss": 0.4914,
      "step": 28200
    },
    {
      "epoch": 1.1284,
      "grad_norm": 2.5203278064727783,
      "learning_rate": 3.140268456375839e-05,
      "loss": 0.5487,
      "step": 28210
    },
    {
      "epoch": 1.1288,
      "grad_norm": 2.47701096534729,
      "learning_rate": 3.139597315436242e-05,
      "loss": 0.4259,
      "step": 28220
    },
    {
      "epoch": 1.1292,
      "grad_norm": 2.7197651863098145,
      "learning_rate": 3.138926174496645e-05,
      "loss": 0.6388,
      "step": 28230
    },
    {
      "epoch": 1.1296,
      "grad_norm": 3.0793960094451904,
      "learning_rate": 3.138255033557047e-05,
      "loss": 0.5843,
      "step": 28240
    },
    {
      "epoch": 1.13,
      "grad_norm": 2.671632766723633,
      "learning_rate": 3.13758389261745e-05,
      "loss": 0.5162,
      "step": 28250
    },
    {
      "epoch": 1.1304,
      "grad_norm": 3.1955959796905518,
      "learning_rate": 3.1369127516778526e-05,
      "loss": 0.5939,
      "step": 28260
    },
    {
      "epoch": 1.1308,
      "grad_norm": 3.172588348388672,
      "learning_rate": 3.1362416107382555e-05,
      "loss": 0.5428,
      "step": 28270
    },
    {
      "epoch": 1.1312,
      "grad_norm": 2.0077812671661377,
      "learning_rate": 3.1355704697986576e-05,
      "loss": 0.4765,
      "step": 28280
    },
    {
      "epoch": 1.1316,
      "grad_norm": 2.6554548740386963,
      "learning_rate": 3.1348993288590605e-05,
      "loss": 0.6649,
      "step": 28290
    },
    {
      "epoch": 1.1320000000000001,
      "grad_norm": 2.802067279815674,
      "learning_rate": 3.1342281879194634e-05,
      "loss": 0.566,
      "step": 28300
    },
    {
      "epoch": 1.1324,
      "grad_norm": 2.405040740966797,
      "learning_rate": 3.1335570469798655e-05,
      "loss": 0.5216,
      "step": 28310
    },
    {
      "epoch": 1.1328,
      "grad_norm": 2.3778774738311768,
      "learning_rate": 3.132885906040269e-05,
      "loss": 0.5985,
      "step": 28320
    },
    {
      "epoch": 1.1332,
      "grad_norm": 3.7086856365203857,
      "learning_rate": 3.132214765100671e-05,
      "loss": 0.5841,
      "step": 28330
    },
    {
      "epoch": 1.1336,
      "grad_norm": 2.6315226554870605,
      "learning_rate": 3.131543624161074e-05,
      "loss": 0.512,
      "step": 28340
    },
    {
      "epoch": 1.134,
      "grad_norm": 2.6424179077148438,
      "learning_rate": 3.130872483221476e-05,
      "loss": 0.5273,
      "step": 28350
    },
    {
      "epoch": 1.1344,
      "grad_norm": 2.9681546688079834,
      "learning_rate": 3.130201342281879e-05,
      "loss": 0.5337,
      "step": 28360
    },
    {
      "epoch": 1.1348,
      "grad_norm": 2.910259485244751,
      "learning_rate": 3.129530201342282e-05,
      "loss": 0.6066,
      "step": 28370
    },
    {
      "epoch": 1.1352,
      "grad_norm": 2.1451501846313477,
      "learning_rate": 3.128859060402685e-05,
      "loss": 0.5143,
      "step": 28380
    },
    {
      "epoch": 1.1356,
      "grad_norm": 2.491337776184082,
      "learning_rate": 3.128187919463088e-05,
      "loss": 0.5653,
      "step": 28390
    },
    {
      "epoch": 1.1360000000000001,
      "grad_norm": 2.617091417312622,
      "learning_rate": 3.12751677852349e-05,
      "loss": 0.4946,
      "step": 28400
    },
    {
      "epoch": 1.1364,
      "grad_norm": 2.4806060791015625,
      "learning_rate": 3.126845637583893e-05,
      "loss": 0.5649,
      "step": 28410
    },
    {
      "epoch": 1.1368,
      "grad_norm": 3.19565486907959,
      "learning_rate": 3.126174496644295e-05,
      "loss": 0.5873,
      "step": 28420
    },
    {
      "epoch": 1.1372,
      "grad_norm": 2.6998844146728516,
      "learning_rate": 3.1255033557046984e-05,
      "loss": 0.5032,
      "step": 28430
    },
    {
      "epoch": 1.1376,
      "grad_norm": 3.072960138320923,
      "learning_rate": 3.124832214765101e-05,
      "loss": 0.5529,
      "step": 28440
    },
    {
      "epoch": 1.138,
      "grad_norm": 2.2081708908081055,
      "learning_rate": 3.1241610738255035e-05,
      "loss": 0.5501,
      "step": 28450
    },
    {
      "epoch": 1.1384,
      "grad_norm": 2.4630420207977295,
      "learning_rate": 3.123489932885906e-05,
      "loss": 0.5572,
      "step": 28460
    },
    {
      "epoch": 1.1388,
      "grad_norm": 3.4067325592041016,
      "learning_rate": 3.1228187919463085e-05,
      "loss": 0.6291,
      "step": 28470
    },
    {
      "epoch": 1.1392,
      "grad_norm": 2.441612958908081,
      "learning_rate": 3.1221476510067114e-05,
      "loss": 0.5466,
      "step": 28480
    },
    {
      "epoch": 1.1396,
      "grad_norm": 2.9545376300811768,
      "learning_rate": 3.121476510067114e-05,
      "loss": 0.5514,
      "step": 28490
    },
    {
      "epoch": 1.1400000000000001,
      "grad_norm": 2.4423272609710693,
      "learning_rate": 3.120805369127517e-05,
      "loss": 0.5765,
      "step": 28500
    },
    {
      "epoch": 1.1404,
      "grad_norm": 2.8226120471954346,
      "learning_rate": 3.12013422818792e-05,
      "loss": 0.5418,
      "step": 28510
    },
    {
      "epoch": 1.1408,
      "grad_norm": 2.3566579818725586,
      "learning_rate": 3.119463087248322e-05,
      "loss": 0.5906,
      "step": 28520
    },
    {
      "epoch": 1.1412,
      "grad_norm": 3.194507122039795,
      "learning_rate": 3.118791946308725e-05,
      "loss": 0.5447,
      "step": 28530
    },
    {
      "epoch": 1.1416,
      "grad_norm": 2.4124608039855957,
      "learning_rate": 3.118120805369127e-05,
      "loss": 0.5479,
      "step": 28540
    },
    {
      "epoch": 1.142,
      "grad_norm": 3.138850450515747,
      "learning_rate": 3.1174496644295307e-05,
      "loss": 0.5667,
      "step": 28550
    },
    {
      "epoch": 1.1424,
      "grad_norm": 2.520623207092285,
      "learning_rate": 3.1167785234899335e-05,
      "loss": 0.5053,
      "step": 28560
    },
    {
      "epoch": 1.1428,
      "grad_norm": 3.3073322772979736,
      "learning_rate": 3.116107382550336e-05,
      "loss": 0.535,
      "step": 28570
    },
    {
      "epoch": 1.1432,
      "grad_norm": 3.1384646892547607,
      "learning_rate": 3.1154362416107385e-05,
      "loss": 0.5782,
      "step": 28580
    },
    {
      "epoch": 1.1436,
      "grad_norm": 2.411987781524658,
      "learning_rate": 3.114765100671141e-05,
      "loss": 0.479,
      "step": 28590
    },
    {
      "epoch": 1.144,
      "grad_norm": 2.6330978870391846,
      "learning_rate": 3.1140939597315436e-05,
      "loss": 0.6234,
      "step": 28600
    },
    {
      "epoch": 1.1444,
      "grad_norm": 2.8414244651794434,
      "learning_rate": 3.1134228187919464e-05,
      "loss": 0.5583,
      "step": 28610
    },
    {
      "epoch": 1.1448,
      "grad_norm": 2.089231014251709,
      "learning_rate": 3.112751677852349e-05,
      "loss": 0.4632,
      "step": 28620
    },
    {
      "epoch": 1.1452,
      "grad_norm": 2.6132776737213135,
      "learning_rate": 3.112080536912752e-05,
      "loss": 0.55,
      "step": 28630
    },
    {
      "epoch": 1.1456,
      "grad_norm": 2.6550285816192627,
      "learning_rate": 3.111409395973154e-05,
      "loss": 0.5526,
      "step": 28640
    },
    {
      "epoch": 1.146,
      "grad_norm": 3.750453233718872,
      "learning_rate": 3.110738255033557e-05,
      "loss": 0.5861,
      "step": 28650
    },
    {
      "epoch": 1.1464,
      "grad_norm": 2.7361249923706055,
      "learning_rate": 3.1100671140939593e-05,
      "loss": 0.6401,
      "step": 28660
    },
    {
      "epoch": 1.1468,
      "grad_norm": 2.6583399772644043,
      "learning_rate": 3.109395973154363e-05,
      "loss": 0.6633,
      "step": 28670
    },
    {
      "epoch": 1.1472,
      "grad_norm": 2.268024206161499,
      "learning_rate": 3.108724832214766e-05,
      "loss": 0.5579,
      "step": 28680
    },
    {
      "epoch": 1.1476,
      "grad_norm": 3.1798477172851562,
      "learning_rate": 3.108053691275168e-05,
      "loss": 0.6088,
      "step": 28690
    },
    {
      "epoch": 1.148,
      "grad_norm": 2.4262990951538086,
      "learning_rate": 3.107382550335571e-05,
      "loss": 0.5629,
      "step": 28700
    },
    {
      "epoch": 1.1484,
      "grad_norm": 2.19705867767334,
      "learning_rate": 3.106711409395973e-05,
      "loss": 0.5236,
      "step": 28710
    },
    {
      "epoch": 1.1488,
      "grad_norm": 2.8742668628692627,
      "learning_rate": 3.106040268456376e-05,
      "loss": 0.6275,
      "step": 28720
    },
    {
      "epoch": 1.1492,
      "grad_norm": 3.8786747455596924,
      "learning_rate": 3.1053691275167787e-05,
      "loss": 0.522,
      "step": 28730
    },
    {
      "epoch": 1.1496,
      "grad_norm": 3.334754228591919,
      "learning_rate": 3.1046979865771815e-05,
      "loss": 0.5463,
      "step": 28740
    },
    {
      "epoch": 1.15,
      "grad_norm": 2.899134635925293,
      "learning_rate": 3.1040268456375844e-05,
      "loss": 0.5497,
      "step": 28750
    },
    {
      "epoch": 1.1504,
      "grad_norm": 2.54093337059021,
      "learning_rate": 3.1033557046979865e-05,
      "loss": 0.5829,
      "step": 28760
    },
    {
      "epoch": 1.1508,
      "grad_norm": 3.2243738174438477,
      "learning_rate": 3.1026845637583894e-05,
      "loss": 0.5168,
      "step": 28770
    },
    {
      "epoch": 1.1512,
      "grad_norm": 2.4418084621429443,
      "learning_rate": 3.102013422818792e-05,
      "loss": 0.5938,
      "step": 28780
    },
    {
      "epoch": 1.1516,
      "grad_norm": 3.1576201915740967,
      "learning_rate": 3.101342281879195e-05,
      "loss": 0.5793,
      "step": 28790
    },
    {
      "epoch": 1.152,
      "grad_norm": 2.1011385917663574,
      "learning_rate": 3.100671140939597e-05,
      "loss": 0.5282,
      "step": 28800
    },
    {
      "epoch": 1.1524,
      "grad_norm": 2.4305331707000732,
      "learning_rate": 3.1e-05,
      "loss": 0.5418,
      "step": 28810
    },
    {
      "epoch": 1.1528,
      "grad_norm": 3.5677404403686523,
      "learning_rate": 3.099328859060403e-05,
      "loss": 0.652,
      "step": 28820
    },
    {
      "epoch": 1.1532,
      "grad_norm": 2.476165771484375,
      "learning_rate": 3.098657718120805e-05,
      "loss": 0.5555,
      "step": 28830
    },
    {
      "epoch": 1.1536,
      "grad_norm": 2.8193798065185547,
      "learning_rate": 3.097986577181208e-05,
      "loss": 0.6307,
      "step": 28840
    },
    {
      "epoch": 1.154,
      "grad_norm": 2.6899008750915527,
      "learning_rate": 3.097315436241611e-05,
      "loss": 0.6472,
      "step": 28850
    },
    {
      "epoch": 1.1544,
      "grad_norm": 1.638950228691101,
      "learning_rate": 3.096644295302014e-05,
      "loss": 0.5398,
      "step": 28860
    },
    {
      "epoch": 1.1548,
      "grad_norm": 2.897627353668213,
      "learning_rate": 3.0959731543624166e-05,
      "loss": 0.5791,
      "step": 28870
    },
    {
      "epoch": 1.1552,
      "grad_norm": 2.514442205429077,
      "learning_rate": 3.095302013422819e-05,
      "loss": 0.5758,
      "step": 28880
    },
    {
      "epoch": 1.1556,
      "grad_norm": 2.5724918842315674,
      "learning_rate": 3.0946308724832216e-05,
      "loss": 0.5572,
      "step": 28890
    },
    {
      "epoch": 1.156,
      "grad_norm": 2.8783721923828125,
      "learning_rate": 3.0939597315436245e-05,
      "loss": 0.4844,
      "step": 28900
    },
    {
      "epoch": 1.1564,
      "grad_norm": 2.9633896350860596,
      "learning_rate": 3.093288590604027e-05,
      "loss": 0.6305,
      "step": 28910
    },
    {
      "epoch": 1.1568,
      "grad_norm": 2.016840934753418,
      "learning_rate": 3.0926174496644295e-05,
      "loss": 0.4863,
      "step": 28920
    },
    {
      "epoch": 1.1572,
      "grad_norm": 2.1637158393859863,
      "learning_rate": 3.0919463087248324e-05,
      "loss": 0.4895,
      "step": 28930
    },
    {
      "epoch": 1.1576,
      "grad_norm": 2.669706106185913,
      "learning_rate": 3.091275167785235e-05,
      "loss": 0.5843,
      "step": 28940
    },
    {
      "epoch": 1.158,
      "grad_norm": 3.575420618057251,
      "learning_rate": 3.0906040268456374e-05,
      "loss": 0.5368,
      "step": 28950
    },
    {
      "epoch": 1.1584,
      "grad_norm": 3.1233866214752197,
      "learning_rate": 3.08993288590604e-05,
      "loss": 0.5134,
      "step": 28960
    },
    {
      "epoch": 1.1588,
      "grad_norm": 2.866537570953369,
      "learning_rate": 3.089261744966443e-05,
      "loss": 0.5924,
      "step": 28970
    },
    {
      "epoch": 1.1592,
      "grad_norm": 3.240797996520996,
      "learning_rate": 3.088590604026846e-05,
      "loss": 0.6915,
      "step": 28980
    },
    {
      "epoch": 1.1596,
      "grad_norm": 2.51733660697937,
      "learning_rate": 3.087919463087248e-05,
      "loss": 0.5612,
      "step": 28990
    },
    {
      "epoch": 1.16,
      "grad_norm": 2.9808475971221924,
      "learning_rate": 3.087248322147651e-05,
      "loss": 0.6135,
      "step": 29000
    },
    {
      "epoch": 1.1604,
      "grad_norm": 2.7964489459991455,
      "learning_rate": 3.086577181208054e-05,
      "loss": 0.5977,
      "step": 29010
    },
    {
      "epoch": 1.1608,
      "grad_norm": 2.6819474697113037,
      "learning_rate": 3.085906040268457e-05,
      "loss": 0.5176,
      "step": 29020
    },
    {
      "epoch": 1.1612,
      "grad_norm": 3.6161022186279297,
      "learning_rate": 3.0852348993288596e-05,
      "loss": 0.7046,
      "step": 29030
    },
    {
      "epoch": 1.1616,
      "grad_norm": 3.341790199279785,
      "learning_rate": 3.084563758389262e-05,
      "loss": 0.5038,
      "step": 29040
    },
    {
      "epoch": 1.162,
      "grad_norm": 3.147334098815918,
      "learning_rate": 3.0838926174496646e-05,
      "loss": 0.5551,
      "step": 29050
    },
    {
      "epoch": 1.1623999999999999,
      "grad_norm": 3.1292474269866943,
      "learning_rate": 3.083221476510067e-05,
      "loss": 0.5871,
      "step": 29060
    },
    {
      "epoch": 1.1628,
      "grad_norm": 2.8428754806518555,
      "learning_rate": 3.0825503355704696e-05,
      "loss": 0.4901,
      "step": 29070
    },
    {
      "epoch": 1.1632,
      "grad_norm": 3.333993911743164,
      "learning_rate": 3.081879194630873e-05,
      "loss": 0.6338,
      "step": 29080
    },
    {
      "epoch": 1.1636,
      "grad_norm": 2.351393699645996,
      "learning_rate": 3.081208053691275e-05,
      "loss": 0.6449,
      "step": 29090
    },
    {
      "epoch": 1.164,
      "grad_norm": 2.778488874435425,
      "learning_rate": 3.080536912751678e-05,
      "loss": 0.593,
      "step": 29100
    },
    {
      "epoch": 1.1644,
      "grad_norm": 3.1898460388183594,
      "learning_rate": 3.0798657718120804e-05,
      "loss": 0.5619,
      "step": 29110
    },
    {
      "epoch": 1.1648,
      "grad_norm": 2.2338995933532715,
      "learning_rate": 3.079194630872483e-05,
      "loss": 0.5806,
      "step": 29120
    },
    {
      "epoch": 1.1652,
      "grad_norm": 3.574159622192383,
      "learning_rate": 3.078523489932886e-05,
      "loss": 0.6006,
      "step": 29130
    },
    {
      "epoch": 1.1656,
      "grad_norm": 2.5712051391601562,
      "learning_rate": 3.077852348993289e-05,
      "loss": 0.6139,
      "step": 29140
    },
    {
      "epoch": 1.166,
      "grad_norm": 2.7324132919311523,
      "learning_rate": 3.077181208053692e-05,
      "loss": 0.6425,
      "step": 29150
    },
    {
      "epoch": 1.1663999999999999,
      "grad_norm": 2.5806751251220703,
      "learning_rate": 3.076510067114094e-05,
      "loss": 0.4913,
      "step": 29160
    },
    {
      "epoch": 1.1668,
      "grad_norm": 2.798236608505249,
      "learning_rate": 3.075838926174497e-05,
      "loss": 0.6602,
      "step": 29170
    },
    {
      "epoch": 1.1672,
      "grad_norm": 2.411698818206787,
      "learning_rate": 3.075167785234899e-05,
      "loss": 0.5251,
      "step": 29180
    },
    {
      "epoch": 1.1676,
      "grad_norm": 3.2034802436828613,
      "learning_rate": 3.074496644295302e-05,
      "loss": 0.5722,
      "step": 29190
    },
    {
      "epoch": 1.168,
      "grad_norm": 1.999997615814209,
      "learning_rate": 3.0738255033557054e-05,
      "loss": 0.5747,
      "step": 29200
    },
    {
      "epoch": 1.1684,
      "grad_norm": 3.121180772781372,
      "learning_rate": 3.0731543624161075e-05,
      "loss": 0.5872,
      "step": 29210
    },
    {
      "epoch": 1.1688,
      "grad_norm": 2.3871848583221436,
      "learning_rate": 3.0724832214765104e-05,
      "loss": 0.5186,
      "step": 29220
    },
    {
      "epoch": 1.1692,
      "grad_norm": 3.4890196323394775,
      "learning_rate": 3.0718120805369126e-05,
      "loss": 0.5622,
      "step": 29230
    },
    {
      "epoch": 1.1696,
      "grad_norm": 2.5541841983795166,
      "learning_rate": 3.0711409395973154e-05,
      "loss": 0.5892,
      "step": 29240
    },
    {
      "epoch": 1.17,
      "grad_norm": 3.030374526977539,
      "learning_rate": 3.070469798657718e-05,
      "loss": 0.6131,
      "step": 29250
    },
    {
      "epoch": 1.1703999999999999,
      "grad_norm": 2.957075834274292,
      "learning_rate": 3.069798657718121e-05,
      "loss": 0.5473,
      "step": 29260
    },
    {
      "epoch": 1.1708,
      "grad_norm": 3.44457745552063,
      "learning_rate": 3.069127516778524e-05,
      "loss": 0.606,
      "step": 29270
    },
    {
      "epoch": 1.1712,
      "grad_norm": 3.1187350749969482,
      "learning_rate": 3.068456375838926e-05,
      "loss": 0.5977,
      "step": 29280
    },
    {
      "epoch": 1.1716,
      "grad_norm": 3.260183572769165,
      "learning_rate": 3.067785234899329e-05,
      "loss": 0.6208,
      "step": 29290
    },
    {
      "epoch": 1.172,
      "grad_norm": 2.367997646331787,
      "learning_rate": 3.067114093959731e-05,
      "loss": 0.5782,
      "step": 29300
    },
    {
      "epoch": 1.1724,
      "grad_norm": 2.8130664825439453,
      "learning_rate": 3.066442953020135e-05,
      "loss": 0.5263,
      "step": 29310
    },
    {
      "epoch": 1.1728,
      "grad_norm": 2.694734573364258,
      "learning_rate": 3.0657718120805376e-05,
      "loss": 0.5931,
      "step": 29320
    },
    {
      "epoch": 1.1732,
      "grad_norm": 1.6595991849899292,
      "learning_rate": 3.06510067114094e-05,
      "loss": 0.5271,
      "step": 29330
    },
    {
      "epoch": 1.1736,
      "grad_norm": 3.231452465057373,
      "learning_rate": 3.0644295302013426e-05,
      "loss": 0.614,
      "step": 29340
    },
    {
      "epoch": 1.174,
      "grad_norm": 2.764230966567993,
      "learning_rate": 3.063758389261745e-05,
      "loss": 0.6919,
      "step": 29350
    },
    {
      "epoch": 1.1743999999999999,
      "grad_norm": 3.373854875564575,
      "learning_rate": 3.0630872483221477e-05,
      "loss": 0.6805,
      "step": 29360
    },
    {
      "epoch": 1.1748,
      "grad_norm": 2.9538512229919434,
      "learning_rate": 3.0624161073825505e-05,
      "loss": 0.5803,
      "step": 29370
    },
    {
      "epoch": 1.1752,
      "grad_norm": 2.5143845081329346,
      "learning_rate": 3.0617449664429534e-05,
      "loss": 0.6756,
      "step": 29380
    },
    {
      "epoch": 1.1756,
      "grad_norm": 2.9240338802337646,
      "learning_rate": 3.061073825503356e-05,
      "loss": 0.5605,
      "step": 29390
    },
    {
      "epoch": 1.176,
      "grad_norm": 2.278923749923706,
      "learning_rate": 3.0604026845637584e-05,
      "loss": 0.5188,
      "step": 29400
    },
    {
      "epoch": 1.1764000000000001,
      "grad_norm": 2.2261643409729004,
      "learning_rate": 3.059731543624161e-05,
      "loss": 0.5711,
      "step": 29410
    },
    {
      "epoch": 1.1768,
      "grad_norm": 2.61130952835083,
      "learning_rate": 3.0590604026845634e-05,
      "loss": 0.575,
      "step": 29420
    },
    {
      "epoch": 1.1772,
      "grad_norm": 2.9551258087158203,
      "learning_rate": 3.058389261744967e-05,
      "loss": 0.5414,
      "step": 29430
    },
    {
      "epoch": 1.1776,
      "grad_norm": 2.7280354499816895,
      "learning_rate": 3.057718120805369e-05,
      "loss": 0.6151,
      "step": 29440
    },
    {
      "epoch": 1.178,
      "grad_norm": 2.444359540939331,
      "learning_rate": 3.057046979865772e-05,
      "loss": 0.5204,
      "step": 29450
    },
    {
      "epoch": 1.1784,
      "grad_norm": 2.2046329975128174,
      "learning_rate": 3.056375838926175e-05,
      "loss": 0.5933,
      "step": 29460
    },
    {
      "epoch": 1.1788,
      "grad_norm": 2.786102294921875,
      "learning_rate": 3.055704697986577e-05,
      "loss": 0.4694,
      "step": 29470
    },
    {
      "epoch": 1.1792,
      "grad_norm": 2.160698652267456,
      "learning_rate": 3.05503355704698e-05,
      "loss": 0.5886,
      "step": 29480
    },
    {
      "epoch": 1.1796,
      "grad_norm": 3.176285743713379,
      "learning_rate": 3.054362416107383e-05,
      "loss": 0.4902,
      "step": 29490
    },
    {
      "epoch": 1.18,
      "grad_norm": 3.138507604598999,
      "learning_rate": 3.0536912751677856e-05,
      "loss": 0.5702,
      "step": 29500
    },
    {
      "epoch": 1.1804000000000001,
      "grad_norm": 3.363960027694702,
      "learning_rate": 3.0530201342281884e-05,
      "loss": 0.6007,
      "step": 29510
    },
    {
      "epoch": 1.1808,
      "grad_norm": 2.4401721954345703,
      "learning_rate": 3.0523489932885906e-05,
      "loss": 0.5584,
      "step": 29520
    },
    {
      "epoch": 1.1812,
      "grad_norm": 3.2089803218841553,
      "learning_rate": 3.0516778523489935e-05,
      "loss": 0.5562,
      "step": 29530
    },
    {
      "epoch": 1.1816,
      "grad_norm": 2.456482172012329,
      "learning_rate": 3.051006711409396e-05,
      "loss": 0.5141,
      "step": 29540
    },
    {
      "epoch": 1.182,
      "grad_norm": 2.7932093143463135,
      "learning_rate": 3.050335570469799e-05,
      "loss": 0.6238,
      "step": 29550
    },
    {
      "epoch": 1.1824,
      "grad_norm": 3.02156925201416,
      "learning_rate": 3.0496644295302014e-05,
      "loss": 0.5459,
      "step": 29560
    },
    {
      "epoch": 1.1828,
      "grad_norm": 2.487112283706665,
      "learning_rate": 3.0489932885906042e-05,
      "loss": 0.5001,
      "step": 29570
    },
    {
      "epoch": 1.1832,
      "grad_norm": 4.631263256072998,
      "learning_rate": 3.048322147651007e-05,
      "loss": 0.6279,
      "step": 29580
    },
    {
      "epoch": 1.1836,
      "grad_norm": 2.9596691131591797,
      "learning_rate": 3.0476510067114096e-05,
      "loss": 0.6551,
      "step": 29590
    },
    {
      "epoch": 1.184,
      "grad_norm": 3.0701537132263184,
      "learning_rate": 3.0469798657718124e-05,
      "loss": 0.5655,
      "step": 29600
    },
    {
      "epoch": 1.1844000000000001,
      "grad_norm": 2.729858636856079,
      "learning_rate": 3.0463087248322146e-05,
      "loss": 0.5762,
      "step": 29610
    },
    {
      "epoch": 1.1848,
      "grad_norm": 2.850048303604126,
      "learning_rate": 3.0456375838926178e-05,
      "loss": 0.571,
      "step": 29620
    },
    {
      "epoch": 1.1852,
      "grad_norm": 2.750767230987549,
      "learning_rate": 3.04496644295302e-05,
      "loss": 0.5734,
      "step": 29630
    },
    {
      "epoch": 1.1856,
      "grad_norm": 3.000922441482544,
      "learning_rate": 3.044295302013423e-05,
      "loss": 0.6349,
      "step": 29640
    },
    {
      "epoch": 1.186,
      "grad_norm": 2.674556016921997,
      "learning_rate": 3.0436241610738257e-05,
      "loss": 0.6417,
      "step": 29650
    },
    {
      "epoch": 1.1864,
      "grad_norm": 2.852374792098999,
      "learning_rate": 3.0429530201342282e-05,
      "loss": 0.5375,
      "step": 29660
    },
    {
      "epoch": 1.1868,
      "grad_norm": 2.973972797393799,
      "learning_rate": 3.042281879194631e-05,
      "loss": 0.5363,
      "step": 29670
    },
    {
      "epoch": 1.1872,
      "grad_norm": 2.5759806632995605,
      "learning_rate": 3.0416107382550336e-05,
      "loss": 0.5753,
      "step": 29680
    },
    {
      "epoch": 1.1876,
      "grad_norm": 2.3921353816986084,
      "learning_rate": 3.0409395973154364e-05,
      "loss": 0.5668,
      "step": 29690
    },
    {
      "epoch": 1.188,
      "grad_norm": 3.0388448238372803,
      "learning_rate": 3.0402684563758393e-05,
      "loss": 0.6412,
      "step": 29700
    },
    {
      "epoch": 1.1884000000000001,
      "grad_norm": 2.866696834564209,
      "learning_rate": 3.0395973154362418e-05,
      "loss": 0.5557,
      "step": 29710
    },
    {
      "epoch": 1.1888,
      "grad_norm": 2.5185093879699707,
      "learning_rate": 3.0389261744966447e-05,
      "loss": 0.6021,
      "step": 29720
    },
    {
      "epoch": 1.1892,
      "grad_norm": 2.4021477699279785,
      "learning_rate": 3.038255033557047e-05,
      "loss": 0.574,
      "step": 29730
    },
    {
      "epoch": 1.1896,
      "grad_norm": 3.1400814056396484,
      "learning_rate": 3.03758389261745e-05,
      "loss": 0.5582,
      "step": 29740
    },
    {
      "epoch": 1.19,
      "grad_norm": 3.0480878353118896,
      "learning_rate": 3.0369127516778522e-05,
      "loss": 0.5472,
      "step": 29750
    },
    {
      "epoch": 1.1904,
      "grad_norm": 3.127910852432251,
      "learning_rate": 3.036241610738255e-05,
      "loss": 0.6111,
      "step": 29760
    },
    {
      "epoch": 1.1908,
      "grad_norm": 2.2115795612335205,
      "learning_rate": 3.0355704697986583e-05,
      "loss": 0.5811,
      "step": 29770
    },
    {
      "epoch": 1.1912,
      "grad_norm": 2.343622922897339,
      "learning_rate": 3.0348993288590604e-05,
      "loss": 0.4613,
      "step": 29780
    },
    {
      "epoch": 1.1916,
      "grad_norm": 2.238291025161743,
      "learning_rate": 3.0342281879194633e-05,
      "loss": 0.4725,
      "step": 29790
    },
    {
      "epoch": 1.192,
      "grad_norm": 2.513249158859253,
      "learning_rate": 3.0335570469798658e-05,
      "loss": 0.4955,
      "step": 29800
    },
    {
      "epoch": 1.1924,
      "grad_norm": 1.734350562095642,
      "learning_rate": 3.0328859060402687e-05,
      "loss": 0.5124,
      "step": 29810
    },
    {
      "epoch": 1.1928,
      "grad_norm": 2.8358004093170166,
      "learning_rate": 3.0322147651006712e-05,
      "loss": 0.6087,
      "step": 29820
    },
    {
      "epoch": 1.1932,
      "grad_norm": 2.6493141651153564,
      "learning_rate": 3.031543624161074e-05,
      "loss": 0.6153,
      "step": 29830
    },
    {
      "epoch": 1.1936,
      "grad_norm": 3.3049426078796387,
      "learning_rate": 3.030872483221477e-05,
      "loss": 0.5763,
      "step": 29840
    },
    {
      "epoch": 1.194,
      "grad_norm": 2.1210691928863525,
      "learning_rate": 3.0302013422818794e-05,
      "loss": 0.5546,
      "step": 29850
    },
    {
      "epoch": 1.1944,
      "grad_norm": 3.7620301246643066,
      "learning_rate": 3.0295302013422823e-05,
      "loss": 0.5814,
      "step": 29860
    },
    {
      "epoch": 1.1948,
      "grad_norm": 3.4458072185516357,
      "learning_rate": 3.0288590604026844e-05,
      "loss": 0.5869,
      "step": 29870
    },
    {
      "epoch": 1.1952,
      "grad_norm": 2.354334831237793,
      "learning_rate": 3.0281879194630873e-05,
      "loss": 0.6779,
      "step": 29880
    },
    {
      "epoch": 1.1956,
      "grad_norm": 2.1339945793151855,
      "learning_rate": 3.0275167785234898e-05,
      "loss": 0.52,
      "step": 29890
    },
    {
      "epoch": 1.196,
      "grad_norm": 3.1748170852661133,
      "learning_rate": 3.0268456375838927e-05,
      "loss": 0.5982,
      "step": 29900
    },
    {
      "epoch": 1.1964,
      "grad_norm": 1.543513536453247,
      "learning_rate": 3.0261744966442955e-05,
      "loss": 0.6224,
      "step": 29910
    },
    {
      "epoch": 1.1968,
      "grad_norm": 2.9142255783081055,
      "learning_rate": 3.025503355704698e-05,
      "loss": 0.5286,
      "step": 29920
    },
    {
      "epoch": 1.1972,
      "grad_norm": 2.600059986114502,
      "learning_rate": 3.024832214765101e-05,
      "loss": 0.542,
      "step": 29930
    },
    {
      "epoch": 1.1976,
      "grad_norm": 2.446451425552368,
      "learning_rate": 3.0241610738255034e-05,
      "loss": 0.5978,
      "step": 29940
    },
    {
      "epoch": 1.198,
      "grad_norm": 2.7244536876678467,
      "learning_rate": 3.0234899328859063e-05,
      "loss": 0.5166,
      "step": 29950
    },
    {
      "epoch": 1.1984,
      "grad_norm": 2.260843276977539,
      "learning_rate": 3.022818791946309e-05,
      "loss": 0.6111,
      "step": 29960
    },
    {
      "epoch": 1.1988,
      "grad_norm": 2.7217485904693604,
      "learning_rate": 3.0221476510067116e-05,
      "loss": 0.5462,
      "step": 29970
    },
    {
      "epoch": 1.1992,
      "grad_norm": 2.539304256439209,
      "learning_rate": 3.0214765100671145e-05,
      "loss": 0.5423,
      "step": 29980
    },
    {
      "epoch": 1.1996,
      "grad_norm": 3.1896421909332275,
      "learning_rate": 3.0208053691275167e-05,
      "loss": 0.5824,
      "step": 29990
    },
    {
      "epoch": 1.2,
      "grad_norm": 2.573455810546875,
      "learning_rate": 3.02013422818792e-05,
      "loss": 0.5995,
      "step": 30000
    },
    {
      "epoch": 1.2004,
      "grad_norm": 2.6947336196899414,
      "learning_rate": 3.019463087248322e-05,
      "loss": 0.5672,
      "step": 30010
    },
    {
      "epoch": 1.2008,
      "grad_norm": 3.043132781982422,
      "learning_rate": 3.018791946308725e-05,
      "loss": 0.5243,
      "step": 30020
    },
    {
      "epoch": 1.2012,
      "grad_norm": 2.852795362472534,
      "learning_rate": 3.0181208053691277e-05,
      "loss": 0.562,
      "step": 30030
    },
    {
      "epoch": 1.2016,
      "grad_norm": 3.222790241241455,
      "learning_rate": 3.0174496644295303e-05,
      "loss": 0.5499,
      "step": 30040
    },
    {
      "epoch": 1.202,
      "grad_norm": 2.493802785873413,
      "learning_rate": 3.016778523489933e-05,
      "loss": 0.556,
      "step": 30050
    },
    {
      "epoch": 1.2024,
      "grad_norm": 2.8053386211395264,
      "learning_rate": 3.0161073825503356e-05,
      "loss": 0.6414,
      "step": 30060
    },
    {
      "epoch": 1.2028,
      "grad_norm": 2.210927724838257,
      "learning_rate": 3.0154362416107385e-05,
      "loss": 0.5367,
      "step": 30070
    },
    {
      "epoch": 1.2032,
      "grad_norm": 3.2059645652770996,
      "learning_rate": 3.0147651006711407e-05,
      "loss": 0.5941,
      "step": 30080
    },
    {
      "epoch": 1.2036,
      "grad_norm": 2.9661285877227783,
      "learning_rate": 3.014093959731544e-05,
      "loss": 0.5739,
      "step": 30090
    },
    {
      "epoch": 1.204,
      "grad_norm": 2.452096462249756,
      "learning_rate": 3.0134228187919467e-05,
      "loss": 0.5078,
      "step": 30100
    },
    {
      "epoch": 1.2044,
      "grad_norm": 3.1927363872528076,
      "learning_rate": 3.012751677852349e-05,
      "loss": 0.5841,
      "step": 30110
    },
    {
      "epoch": 1.2048,
      "grad_norm": 2.4899401664733887,
      "learning_rate": 3.012080536912752e-05,
      "loss": 0.5371,
      "step": 30120
    },
    {
      "epoch": 1.2052,
      "grad_norm": 3.054600954055786,
      "learning_rate": 3.0114093959731543e-05,
      "loss": 0.6331,
      "step": 30130
    },
    {
      "epoch": 1.2056,
      "grad_norm": 2.0861523151397705,
      "learning_rate": 3.010738255033557e-05,
      "loss": 0.5564,
      "step": 30140
    },
    {
      "epoch": 1.206,
      "grad_norm": 3.0206165313720703,
      "learning_rate": 3.0100671140939603e-05,
      "loss": 0.6064,
      "step": 30150
    },
    {
      "epoch": 1.2064,
      "grad_norm": 2.6122591495513916,
      "learning_rate": 3.0093959731543625e-05,
      "loss": 0.535,
      "step": 30160
    },
    {
      "epoch": 1.2068,
      "grad_norm": 2.7976653575897217,
      "learning_rate": 3.0087248322147653e-05,
      "loss": 0.5873,
      "step": 30170
    },
    {
      "epoch": 1.2072,
      "grad_norm": 3.0926625728607178,
      "learning_rate": 3.008053691275168e-05,
      "loss": 0.6736,
      "step": 30180
    },
    {
      "epoch": 1.2076,
      "grad_norm": 2.7024073600769043,
      "learning_rate": 3.0073825503355707e-05,
      "loss": 0.5659,
      "step": 30190
    },
    {
      "epoch": 1.208,
      "grad_norm": 3.002330780029297,
      "learning_rate": 3.0067114093959732e-05,
      "loss": 0.6469,
      "step": 30200
    },
    {
      "epoch": 1.2084,
      "grad_norm": 3.433100938796997,
      "learning_rate": 3.006040268456376e-05,
      "loss": 0.6461,
      "step": 30210
    },
    {
      "epoch": 1.2088,
      "grad_norm": 2.5183398723602295,
      "learning_rate": 3.005369127516779e-05,
      "loss": 0.5846,
      "step": 30220
    },
    {
      "epoch": 1.2092,
      "grad_norm": 3.5542984008789062,
      "learning_rate": 3.004697986577181e-05,
      "loss": 0.6724,
      "step": 30230
    },
    {
      "epoch": 1.2096,
      "grad_norm": 2.9661195278167725,
      "learning_rate": 3.0040268456375843e-05,
      "loss": 0.5085,
      "step": 30240
    },
    {
      "epoch": 1.21,
      "grad_norm": 2.885443687438965,
      "learning_rate": 3.0033557046979865e-05,
      "loss": 0.5572,
      "step": 30250
    },
    {
      "epoch": 1.2104,
      "grad_norm": 3.340237855911255,
      "learning_rate": 3.0026845637583893e-05,
      "loss": 0.6538,
      "step": 30260
    },
    {
      "epoch": 1.2107999999999999,
      "grad_norm": 2.445878028869629,
      "learning_rate": 3.002013422818792e-05,
      "loss": 0.5637,
      "step": 30270
    },
    {
      "epoch": 1.2112,
      "grad_norm": 2.51545786857605,
      "learning_rate": 3.0013422818791947e-05,
      "loss": 0.5654,
      "step": 30280
    },
    {
      "epoch": 1.2116,
      "grad_norm": 3.0447099208831787,
      "learning_rate": 3.0006711409395976e-05,
      "loss": 0.5481,
      "step": 30290
    },
    {
      "epoch": 1.212,
      "grad_norm": 2.836122751235962,
      "learning_rate": 3e-05,
      "loss": 0.6017,
      "step": 30300
    },
    {
      "epoch": 1.2124,
      "grad_norm": 2.2346630096435547,
      "learning_rate": 2.999328859060403e-05,
      "loss": 0.5579,
      "step": 30310
    },
    {
      "epoch": 1.2128,
      "grad_norm": 2.9907915592193604,
      "learning_rate": 2.9986577181208054e-05,
      "loss": 0.5089,
      "step": 30320
    },
    {
      "epoch": 1.2132,
      "grad_norm": 2.84352970123291,
      "learning_rate": 2.9979865771812083e-05,
      "loss": 0.525,
      "step": 30330
    },
    {
      "epoch": 1.2136,
      "grad_norm": 2.8248488903045654,
      "learning_rate": 2.997315436241611e-05,
      "loss": 0.5654,
      "step": 30340
    },
    {
      "epoch": 1.214,
      "grad_norm": 2.840541124343872,
      "learning_rate": 2.9966442953020137e-05,
      "loss": 0.5951,
      "step": 30350
    },
    {
      "epoch": 1.2144,
      "grad_norm": 2.85986328125,
      "learning_rate": 2.9959731543624165e-05,
      "loss": 0.5944,
      "step": 30360
    },
    {
      "epoch": 1.2147999999999999,
      "grad_norm": 3.3588967323303223,
      "learning_rate": 2.9953020134228187e-05,
      "loss": 0.5951,
      "step": 30370
    },
    {
      "epoch": 1.2152,
      "grad_norm": 3.0355818271636963,
      "learning_rate": 2.9946308724832216e-05,
      "loss": 0.5576,
      "step": 30380
    },
    {
      "epoch": 1.2156,
      "grad_norm": 3.1595213413238525,
      "learning_rate": 2.993959731543624e-05,
      "loss": 0.5477,
      "step": 30390
    },
    {
      "epoch": 1.216,
      "grad_norm": 2.5539963245391846,
      "learning_rate": 2.993288590604027e-05,
      "loss": 0.613,
      "step": 30400
    },
    {
      "epoch": 1.2164,
      "grad_norm": 2.9326865673065186,
      "learning_rate": 2.9926174496644298e-05,
      "loss": 0.5787,
      "step": 30410
    },
    {
      "epoch": 1.2168,
      "grad_norm": 3.3227622509002686,
      "learning_rate": 2.9919463087248323e-05,
      "loss": 0.5689,
      "step": 30420
    },
    {
      "epoch": 1.2172,
      "grad_norm": 2.288140058517456,
      "learning_rate": 2.991275167785235e-05,
      "loss": 0.557,
      "step": 30430
    },
    {
      "epoch": 1.2176,
      "grad_norm": 2.720353841781616,
      "learning_rate": 2.9906040268456377e-05,
      "loss": 0.5992,
      "step": 30440
    },
    {
      "epoch": 1.218,
      "grad_norm": 3.0469038486480713,
      "learning_rate": 2.9899328859060405e-05,
      "loss": 0.5668,
      "step": 30450
    },
    {
      "epoch": 1.2184,
      "grad_norm": 3.139357089996338,
      "learning_rate": 2.9892617449664427e-05,
      "loss": 0.684,
      "step": 30460
    },
    {
      "epoch": 1.2187999999999999,
      "grad_norm": 2.595228910446167,
      "learning_rate": 2.988590604026846e-05,
      "loss": 0.5659,
      "step": 30470
    },
    {
      "epoch": 1.2192,
      "grad_norm": 2.324509382247925,
      "learning_rate": 2.9879194630872488e-05,
      "loss": 0.6074,
      "step": 30480
    },
    {
      "epoch": 1.2196,
      "grad_norm": 3.1484389305114746,
      "learning_rate": 2.987248322147651e-05,
      "loss": 0.5399,
      "step": 30490
    },
    {
      "epoch": 1.22,
      "grad_norm": 2.7712888717651367,
      "learning_rate": 2.986577181208054e-05,
      "loss": 0.6039,
      "step": 30500
    },
    {
      "epoch": 1.2204,
      "grad_norm": 2.2378954887390137,
      "learning_rate": 2.9859060402684563e-05,
      "loss": 0.5502,
      "step": 30510
    },
    {
      "epoch": 1.2208,
      "grad_norm": 3.114192008972168,
      "learning_rate": 2.985234899328859e-05,
      "loss": 0.5736,
      "step": 30520
    },
    {
      "epoch": 1.2212,
      "grad_norm": 2.434357166290283,
      "learning_rate": 2.9845637583892617e-05,
      "loss": 0.5222,
      "step": 30530
    },
    {
      "epoch": 1.2216,
      "grad_norm": 2.1876487731933594,
      "learning_rate": 2.9838926174496645e-05,
      "loss": 0.6185,
      "step": 30540
    },
    {
      "epoch": 1.222,
      "grad_norm": 2.8514041900634766,
      "learning_rate": 2.9832214765100674e-05,
      "loss": 0.5288,
      "step": 30550
    },
    {
      "epoch": 1.2224,
      "grad_norm": 2.3977932929992676,
      "learning_rate": 2.98255033557047e-05,
      "loss": 0.6544,
      "step": 30560
    },
    {
      "epoch": 1.2227999999999999,
      "grad_norm": 3.2343976497650146,
      "learning_rate": 2.9818791946308727e-05,
      "loss": 0.5958,
      "step": 30570
    },
    {
      "epoch": 1.2232,
      "grad_norm": 2.744143486022949,
      "learning_rate": 2.981208053691275e-05,
      "loss": 0.5712,
      "step": 30580
    },
    {
      "epoch": 1.2236,
      "grad_norm": 2.790008068084717,
      "learning_rate": 2.980536912751678e-05,
      "loss": 0.5229,
      "step": 30590
    },
    {
      "epoch": 1.224,
      "grad_norm": 3.0835988521575928,
      "learning_rate": 2.979865771812081e-05,
      "loss": 0.5435,
      "step": 30600
    },
    {
      "epoch": 1.2244,
      "grad_norm": 2.172715902328491,
      "learning_rate": 2.979194630872483e-05,
      "loss": 0.4675,
      "step": 30610
    },
    {
      "epoch": 1.2248,
      "grad_norm": 3.189446210861206,
      "learning_rate": 2.9785234899328863e-05,
      "loss": 0.5561,
      "step": 30620
    },
    {
      "epoch": 1.2252,
      "grad_norm": 3.788771867752075,
      "learning_rate": 2.9778523489932885e-05,
      "loss": 0.578,
      "step": 30630
    },
    {
      "epoch": 1.2256,
      "grad_norm": 3.312558650970459,
      "learning_rate": 2.9771812080536914e-05,
      "loss": 0.6163,
      "step": 30640
    },
    {
      "epoch": 1.226,
      "grad_norm": 2.6270663738250732,
      "learning_rate": 2.976510067114094e-05,
      "loss": 0.59,
      "step": 30650
    },
    {
      "epoch": 1.2264,
      "grad_norm": 2.158541679382324,
      "learning_rate": 2.9758389261744967e-05,
      "loss": 0.5558,
      "step": 30660
    },
    {
      "epoch": 1.2268,
      "grad_norm": 2.642613410949707,
      "learning_rate": 2.9751677852348996e-05,
      "loss": 0.5898,
      "step": 30670
    },
    {
      "epoch": 1.2272,
      "grad_norm": 2.6585400104522705,
      "learning_rate": 2.974496644295302e-05,
      "loss": 0.542,
      "step": 30680
    },
    {
      "epoch": 1.2276,
      "grad_norm": 3.073800563812256,
      "learning_rate": 2.973825503355705e-05,
      "loss": 0.5736,
      "step": 30690
    },
    {
      "epoch": 1.228,
      "grad_norm": 3.2078447341918945,
      "learning_rate": 2.9731543624161075e-05,
      "loss": 0.5587,
      "step": 30700
    },
    {
      "epoch": 1.2284,
      "grad_norm": 2.9604146480560303,
      "learning_rate": 2.9724832214765103e-05,
      "loss": 0.5136,
      "step": 30710
    },
    {
      "epoch": 1.2288000000000001,
      "grad_norm": 2.7427122592926025,
      "learning_rate": 2.9718120805369125e-05,
      "loss": 0.5727,
      "step": 30720
    },
    {
      "epoch": 1.2292,
      "grad_norm": 2.5395023822784424,
      "learning_rate": 2.9711409395973154e-05,
      "loss": 0.5799,
      "step": 30730
    },
    {
      "epoch": 1.2296,
      "grad_norm": 2.478080987930298,
      "learning_rate": 2.9704697986577186e-05,
      "loss": 0.65,
      "step": 30740
    },
    {
      "epoch": 1.23,
      "grad_norm": 3.0659866333007812,
      "learning_rate": 2.9697986577181207e-05,
      "loss": 0.607,
      "step": 30750
    },
    {
      "epoch": 1.2304,
      "grad_norm": 2.3908193111419678,
      "learning_rate": 2.9691275167785236e-05,
      "loss": 0.5908,
      "step": 30760
    },
    {
      "epoch": 1.2308,
      "grad_norm": 3.049292802810669,
      "learning_rate": 2.968456375838926e-05,
      "loss": 0.4194,
      "step": 30770
    },
    {
      "epoch": 1.2312,
      "grad_norm": 2.268719434738159,
      "learning_rate": 2.967785234899329e-05,
      "loss": 0.5538,
      "step": 30780
    },
    {
      "epoch": 1.2316,
      "grad_norm": 3.212834596633911,
      "learning_rate": 2.9671140939597318e-05,
      "loss": 0.5544,
      "step": 30790
    },
    {
      "epoch": 1.232,
      "grad_norm": 3.0673882961273193,
      "learning_rate": 2.9664429530201343e-05,
      "loss": 0.6639,
      "step": 30800
    },
    {
      "epoch": 1.2324,
      "grad_norm": 2.5395069122314453,
      "learning_rate": 2.9657718120805372e-05,
      "loss": 0.6635,
      "step": 30810
    },
    {
      "epoch": 1.2328000000000001,
      "grad_norm": 3.215008497238159,
      "learning_rate": 2.9651006711409397e-05,
      "loss": 0.5791,
      "step": 30820
    },
    {
      "epoch": 1.2332,
      "grad_norm": 2.235689163208008,
      "learning_rate": 2.9644295302013426e-05,
      "loss": 0.6682,
      "step": 30830
    },
    {
      "epoch": 1.2336,
      "grad_norm": 2.7943341732025146,
      "learning_rate": 2.9637583892617447e-05,
      "loss": 0.6081,
      "step": 30840
    },
    {
      "epoch": 1.234,
      "grad_norm": 2.6691761016845703,
      "learning_rate": 2.963087248322148e-05,
      "loss": 0.4746,
      "step": 30850
    },
    {
      "epoch": 1.2344,
      "grad_norm": 3.060544490814209,
      "learning_rate": 2.9624161073825508e-05,
      "loss": 0.6093,
      "step": 30860
    },
    {
      "epoch": 1.2348,
      "grad_norm": 2.867414951324463,
      "learning_rate": 2.961744966442953e-05,
      "loss": 0.4974,
      "step": 30870
    },
    {
      "epoch": 1.2352,
      "grad_norm": 2.55251407623291,
      "learning_rate": 2.9610738255033558e-05,
      "loss": 0.4218,
      "step": 30880
    },
    {
      "epoch": 1.2356,
      "grad_norm": 4.049835681915283,
      "learning_rate": 2.9604026845637583e-05,
      "loss": 0.5796,
      "step": 30890
    },
    {
      "epoch": 1.236,
      "grad_norm": 2.9977867603302,
      "learning_rate": 2.9597315436241612e-05,
      "loss": 0.552,
      "step": 30900
    },
    {
      "epoch": 1.2364,
      "grad_norm": 2.2067553997039795,
      "learning_rate": 2.9590604026845637e-05,
      "loss": 0.5008,
      "step": 30910
    },
    {
      "epoch": 1.2368000000000001,
      "grad_norm": 3.8342342376708984,
      "learning_rate": 2.9583892617449666e-05,
      "loss": 0.5288,
      "step": 30920
    },
    {
      "epoch": 1.2372,
      "grad_norm": 2.4760379791259766,
      "learning_rate": 2.9577181208053694e-05,
      "loss": 0.5006,
      "step": 30930
    },
    {
      "epoch": 1.2376,
      "grad_norm": 2.4016966819763184,
      "learning_rate": 2.957046979865772e-05,
      "loss": 0.5794,
      "step": 30940
    },
    {
      "epoch": 1.238,
      "grad_norm": 3.0164356231689453,
      "learning_rate": 2.9563758389261748e-05,
      "loss": 0.5853,
      "step": 30950
    },
    {
      "epoch": 1.2384,
      "grad_norm": 2.357196092605591,
      "learning_rate": 2.955704697986577e-05,
      "loss": 0.5126,
      "step": 30960
    },
    {
      "epoch": 1.2388,
      "grad_norm": 2.771996259689331,
      "learning_rate": 2.95503355704698e-05,
      "loss": 0.5929,
      "step": 30970
    },
    {
      "epoch": 1.2392,
      "grad_norm": 2.265852689743042,
      "learning_rate": 2.954362416107383e-05,
      "loss": 0.6375,
      "step": 30980
    },
    {
      "epoch": 1.2396,
      "grad_norm": 3.0157675743103027,
      "learning_rate": 2.9536912751677852e-05,
      "loss": 0.5274,
      "step": 30990
    },
    {
      "epoch": 1.24,
      "grad_norm": 2.4567084312438965,
      "learning_rate": 2.9530201342281884e-05,
      "loss": 0.5711,
      "step": 31000
    },
    {
      "epoch": 1.2404,
      "grad_norm": 2.7308104038238525,
      "learning_rate": 2.9523489932885906e-05,
      "loss": 0.5643,
      "step": 31010
    },
    {
      "epoch": 1.2408,
      "grad_norm": 2.8591485023498535,
      "learning_rate": 2.9516778523489934e-05,
      "loss": 0.5766,
      "step": 31020
    },
    {
      "epoch": 1.2412,
      "grad_norm": 2.8467633724212646,
      "learning_rate": 2.951006711409396e-05,
      "loss": 0.5628,
      "step": 31030
    },
    {
      "epoch": 1.2416,
      "grad_norm": 2.9162726402282715,
      "learning_rate": 2.9503355704697988e-05,
      "loss": 0.5434,
      "step": 31040
    },
    {
      "epoch": 1.242,
      "grad_norm": 2.697767496109009,
      "learning_rate": 2.9496644295302016e-05,
      "loss": 0.5589,
      "step": 31050
    },
    {
      "epoch": 1.2424,
      "grad_norm": 3.736710548400879,
      "learning_rate": 2.948993288590604e-05,
      "loss": 0.6353,
      "step": 31060
    },
    {
      "epoch": 1.2428,
      "grad_norm": 2.3451225757598877,
      "learning_rate": 2.948322147651007e-05,
      "loss": 0.4837,
      "step": 31070
    },
    {
      "epoch": 1.2432,
      "grad_norm": 2.5859334468841553,
      "learning_rate": 2.9476510067114095e-05,
      "loss": 0.54,
      "step": 31080
    },
    {
      "epoch": 1.2436,
      "grad_norm": 4.107954502105713,
      "learning_rate": 2.9469798657718124e-05,
      "loss": 0.6546,
      "step": 31090
    },
    {
      "epoch": 1.244,
      "grad_norm": 2.6423702239990234,
      "learning_rate": 2.9463087248322146e-05,
      "loss": 0.5583,
      "step": 31100
    },
    {
      "epoch": 1.2444,
      "grad_norm": 2.376357078552246,
      "learning_rate": 2.9456375838926174e-05,
      "loss": 0.5993,
      "step": 31110
    },
    {
      "epoch": 1.2448,
      "grad_norm": 2.9426209926605225,
      "learning_rate": 2.9449664429530206e-05,
      "loss": 0.5032,
      "step": 31120
    },
    {
      "epoch": 1.2452,
      "grad_norm": 2.880704164505005,
      "learning_rate": 2.9442953020134228e-05,
      "loss": 0.5302,
      "step": 31130
    },
    {
      "epoch": 1.2456,
      "grad_norm": 2.6739370822906494,
      "learning_rate": 2.9436241610738256e-05,
      "loss": 0.5448,
      "step": 31140
    },
    {
      "epoch": 1.246,
      "grad_norm": 2.8993544578552246,
      "learning_rate": 2.942953020134228e-05,
      "loss": 0.4983,
      "step": 31150
    },
    {
      "epoch": 1.2464,
      "grad_norm": 2.344623327255249,
      "learning_rate": 2.942281879194631e-05,
      "loss": 0.5406,
      "step": 31160
    },
    {
      "epoch": 1.2468,
      "grad_norm": 3.7284412384033203,
      "learning_rate": 2.941610738255034e-05,
      "loss": 0.6361,
      "step": 31170
    },
    {
      "epoch": 1.2472,
      "grad_norm": 2.5269408226013184,
      "learning_rate": 2.9409395973154364e-05,
      "loss": 0.52,
      "step": 31180
    },
    {
      "epoch": 1.2476,
      "grad_norm": 2.889009475708008,
      "learning_rate": 2.9402684563758392e-05,
      "loss": 0.5642,
      "step": 31190
    },
    {
      "epoch": 1.248,
      "grad_norm": 2.4695019721984863,
      "learning_rate": 2.9395973154362418e-05,
      "loss": 0.5187,
      "step": 31200
    },
    {
      "epoch": 1.2484,
      "grad_norm": 3.4072554111480713,
      "learning_rate": 2.9389261744966446e-05,
      "loss": 0.6311,
      "step": 31210
    },
    {
      "epoch": 1.2488,
      "grad_norm": 1.83438241481781,
      "learning_rate": 2.9382550335570468e-05,
      "loss": 0.5197,
      "step": 31220
    },
    {
      "epoch": 1.2492,
      "grad_norm": 3.152726411819458,
      "learning_rate": 2.93758389261745e-05,
      "loss": 0.5508,
      "step": 31230
    },
    {
      "epoch": 1.2496,
      "grad_norm": 1.849825382232666,
      "learning_rate": 2.936912751677853e-05,
      "loss": 0.502,
      "step": 31240
    },
    {
      "epoch": 1.25,
      "grad_norm": 2.4086110591888428,
      "learning_rate": 2.936241610738255e-05,
      "loss": 0.5251,
      "step": 31250
    },
    {
      "epoch": 1.2504,
      "grad_norm": 2.727262020111084,
      "learning_rate": 2.935570469798658e-05,
      "loss": 0.5584,
      "step": 31260
    },
    {
      "epoch": 1.2508,
      "grad_norm": 2.620661735534668,
      "learning_rate": 2.9348993288590604e-05,
      "loss": 0.6359,
      "step": 31270
    },
    {
      "epoch": 1.2511999999999999,
      "grad_norm": 2.1319167613983154,
      "learning_rate": 2.9342281879194632e-05,
      "loss": 0.5693,
      "step": 31280
    },
    {
      "epoch": 1.2516,
      "grad_norm": 2.5812511444091797,
      "learning_rate": 2.9335570469798658e-05,
      "loss": 0.4676,
      "step": 31290
    },
    {
      "epoch": 1.252,
      "grad_norm": 3.1233396530151367,
      "learning_rate": 2.9328859060402686e-05,
      "loss": 0.6337,
      "step": 31300
    },
    {
      "epoch": 1.2524,
      "grad_norm": 2.56827449798584,
      "learning_rate": 2.9322147651006715e-05,
      "loss": 0.586,
      "step": 31310
    },
    {
      "epoch": 1.2528000000000001,
      "grad_norm": 2.217341899871826,
      "learning_rate": 2.931543624161074e-05,
      "loss": 0.5316,
      "step": 31320
    },
    {
      "epoch": 1.2532,
      "grad_norm": 2.227855920791626,
      "learning_rate": 2.930872483221477e-05,
      "loss": 0.5359,
      "step": 31330
    },
    {
      "epoch": 1.2536,
      "grad_norm": 3.6092112064361572,
      "learning_rate": 2.930201342281879e-05,
      "loss": 0.5301,
      "step": 31340
    },
    {
      "epoch": 1.254,
      "grad_norm": 2.5754425525665283,
      "learning_rate": 2.9295302013422822e-05,
      "loss": 0.5431,
      "step": 31350
    },
    {
      "epoch": 1.2544,
      "grad_norm": 3.1000232696533203,
      "learning_rate": 2.9288590604026844e-05,
      "loss": 0.6228,
      "step": 31360
    },
    {
      "epoch": 1.2548,
      "grad_norm": 2.568722724914551,
      "learning_rate": 2.9281879194630872e-05,
      "loss": 0.4966,
      "step": 31370
    },
    {
      "epoch": 1.2551999999999999,
      "grad_norm": 2.884185791015625,
      "learning_rate": 2.9275167785234904e-05,
      "loss": 0.6778,
      "step": 31380
    },
    {
      "epoch": 1.2556,
      "grad_norm": 3.3314716815948486,
      "learning_rate": 2.9268456375838926e-05,
      "loss": 0.6422,
      "step": 31390
    },
    {
      "epoch": 1.256,
      "grad_norm": 2.438701629638672,
      "learning_rate": 2.9261744966442955e-05,
      "loss": 0.5726,
      "step": 31400
    },
    {
      "epoch": 1.2564,
      "grad_norm": 2.9101223945617676,
      "learning_rate": 2.925503355704698e-05,
      "loss": 0.6313,
      "step": 31410
    },
    {
      "epoch": 1.2568,
      "grad_norm": 2.423977851867676,
      "learning_rate": 2.9248322147651008e-05,
      "loss": 0.5648,
      "step": 31420
    },
    {
      "epoch": 1.2572,
      "grad_norm": 2.682811737060547,
      "learning_rate": 2.9241610738255037e-05,
      "loss": 0.5072,
      "step": 31430
    },
    {
      "epoch": 1.2576,
      "grad_norm": 3.033623218536377,
      "learning_rate": 2.9234899328859062e-05,
      "loss": 0.6044,
      "step": 31440
    },
    {
      "epoch": 1.258,
      "grad_norm": 3.1836252212524414,
      "learning_rate": 2.922818791946309e-05,
      "loss": 0.5868,
      "step": 31450
    },
    {
      "epoch": 1.2584,
      "grad_norm": 2.3270115852355957,
      "learning_rate": 2.9221476510067112e-05,
      "loss": 0.5727,
      "step": 31460
    },
    {
      "epoch": 1.2588,
      "grad_norm": 2.119997262954712,
      "learning_rate": 2.9214765100671144e-05,
      "loss": 0.5882,
      "step": 31470
    },
    {
      "epoch": 1.2591999999999999,
      "grad_norm": 2.647688865661621,
      "learning_rate": 2.9208053691275166e-05,
      "loss": 0.6122,
      "step": 31480
    },
    {
      "epoch": 1.2596,
      "grad_norm": 2.7559564113616943,
      "learning_rate": 2.9201342281879195e-05,
      "loss": 0.5452,
      "step": 31490
    },
    {
      "epoch": 1.26,
      "grad_norm": 2.7726058959960938,
      "learning_rate": 2.9194630872483227e-05,
      "loss": 0.6356,
      "step": 31500
    },
    {
      "epoch": 1.2604,
      "grad_norm": 2.9100983142852783,
      "learning_rate": 2.9187919463087248e-05,
      "loss": 0.5942,
      "step": 31510
    },
    {
      "epoch": 1.2608,
      "grad_norm": 2.149721622467041,
      "learning_rate": 2.9181208053691277e-05,
      "loss": 0.54,
      "step": 31520
    },
    {
      "epoch": 1.2612,
      "grad_norm": 1.9351533651351929,
      "learning_rate": 2.9174496644295302e-05,
      "loss": 0.5829,
      "step": 31530
    },
    {
      "epoch": 1.2616,
      "grad_norm": 2.9802916049957275,
      "learning_rate": 2.916778523489933e-05,
      "loss": 0.6394,
      "step": 31540
    },
    {
      "epoch": 1.262,
      "grad_norm": 3.1005733013153076,
      "learning_rate": 2.9161073825503356e-05,
      "loss": 0.5867,
      "step": 31550
    },
    {
      "epoch": 1.2624,
      "grad_norm": 2.889617681503296,
      "learning_rate": 2.9154362416107384e-05,
      "loss": 0.6072,
      "step": 31560
    },
    {
      "epoch": 1.2628,
      "grad_norm": 2.4258408546447754,
      "learning_rate": 2.9147651006711413e-05,
      "loss": 0.545,
      "step": 31570
    },
    {
      "epoch": 1.2631999999999999,
      "grad_norm": 3.489466428756714,
      "learning_rate": 2.9140939597315438e-05,
      "loss": 0.6121,
      "step": 31580
    },
    {
      "epoch": 1.2636,
      "grad_norm": 2.4583137035369873,
      "learning_rate": 2.9134228187919466e-05,
      "loss": 0.4963,
      "step": 31590
    },
    {
      "epoch": 1.264,
      "grad_norm": 2.6136465072631836,
      "learning_rate": 2.9127516778523488e-05,
      "loss": 0.6106,
      "step": 31600
    },
    {
      "epoch": 1.2644,
      "grad_norm": 3.40822172164917,
      "learning_rate": 2.9120805369127517e-05,
      "loss": 0.6682,
      "step": 31610
    },
    {
      "epoch": 1.2648,
      "grad_norm": 3.622487783432007,
      "learning_rate": 2.911409395973155e-05,
      "loss": 0.6659,
      "step": 31620
    },
    {
      "epoch": 1.2652,
      "grad_norm": 3.2177960872650146,
      "learning_rate": 2.910738255033557e-05,
      "loss": 0.5816,
      "step": 31630
    },
    {
      "epoch": 1.2656,
      "grad_norm": 3.262375831604004,
      "learning_rate": 2.91006711409396e-05,
      "loss": 0.5678,
      "step": 31640
    },
    {
      "epoch": 1.266,
      "grad_norm": 2.64638090133667,
      "learning_rate": 2.9093959731543624e-05,
      "loss": 0.6542,
      "step": 31650
    },
    {
      "epoch": 1.2664,
      "grad_norm": 2.454834461212158,
      "learning_rate": 2.9087248322147653e-05,
      "loss": 0.5292,
      "step": 31660
    },
    {
      "epoch": 1.2668,
      "grad_norm": 2.387162446975708,
      "learning_rate": 2.9080536912751678e-05,
      "loss": 0.5427,
      "step": 31670
    },
    {
      "epoch": 1.2671999999999999,
      "grad_norm": 2.27305269241333,
      "learning_rate": 2.9073825503355706e-05,
      "loss": 0.5764,
      "step": 31680
    },
    {
      "epoch": 1.2676,
      "grad_norm": 2.5255184173583984,
      "learning_rate": 2.9067114093959735e-05,
      "loss": 0.6124,
      "step": 31690
    },
    {
      "epoch": 1.268,
      "grad_norm": 3.2399067878723145,
      "learning_rate": 2.906040268456376e-05,
      "loss": 0.5303,
      "step": 31700
    },
    {
      "epoch": 1.2684,
      "grad_norm": 2.653526782989502,
      "learning_rate": 2.905369127516779e-05,
      "loss": 0.576,
      "step": 31710
    },
    {
      "epoch": 1.2688,
      "grad_norm": 2.2700347900390625,
      "learning_rate": 2.904697986577181e-05,
      "loss": 0.491,
      "step": 31720
    },
    {
      "epoch": 1.2692,
      "grad_norm": 2.3717496395111084,
      "learning_rate": 2.9040268456375842e-05,
      "loss": 0.5886,
      "step": 31730
    },
    {
      "epoch": 1.2696,
      "grad_norm": 2.4285426139831543,
      "learning_rate": 2.9033557046979864e-05,
      "loss": 0.5253,
      "step": 31740
    },
    {
      "epoch": 1.27,
      "grad_norm": 2.5474252700805664,
      "learning_rate": 2.9026845637583893e-05,
      "loss": 0.6509,
      "step": 31750
    },
    {
      "epoch": 1.2704,
      "grad_norm": 2.7361552715301514,
      "learning_rate": 2.902013422818792e-05,
      "loss": 0.5853,
      "step": 31760
    },
    {
      "epoch": 1.2708,
      "grad_norm": 2.7413198947906494,
      "learning_rate": 2.9013422818791946e-05,
      "loss": 0.5615,
      "step": 31770
    },
    {
      "epoch": 1.2711999999999999,
      "grad_norm": 3.0923736095428467,
      "learning_rate": 2.9006711409395975e-05,
      "loss": 0.5027,
      "step": 31780
    },
    {
      "epoch": 1.2716,
      "grad_norm": 2.623171091079712,
      "learning_rate": 2.9e-05,
      "loss": 0.5582,
      "step": 31790
    },
    {
      "epoch": 1.272,
      "grad_norm": 2.648512363433838,
      "learning_rate": 2.899328859060403e-05,
      "loss": 0.5124,
      "step": 31800
    },
    {
      "epoch": 1.2724,
      "grad_norm": 2.9958062171936035,
      "learning_rate": 2.8986577181208057e-05,
      "loss": 0.5506,
      "step": 31810
    },
    {
      "epoch": 1.2728,
      "grad_norm": 3.159156560897827,
      "learning_rate": 2.8979865771812082e-05,
      "loss": 0.6497,
      "step": 31820
    },
    {
      "epoch": 1.2732,
      "grad_norm": 2.846482515335083,
      "learning_rate": 2.897315436241611e-05,
      "loss": 0.5479,
      "step": 31830
    },
    {
      "epoch": 1.2736,
      "grad_norm": 3.3781285285949707,
      "learning_rate": 2.8966442953020133e-05,
      "loss": 0.5375,
      "step": 31840
    },
    {
      "epoch": 1.274,
      "grad_norm": 3.0555179119110107,
      "learning_rate": 2.8959731543624165e-05,
      "loss": 0.5772,
      "step": 31850
    },
    {
      "epoch": 1.2744,
      "grad_norm": 2.653123140335083,
      "learning_rate": 2.8953020134228186e-05,
      "loss": 0.5685,
      "step": 31860
    },
    {
      "epoch": 1.2748,
      "grad_norm": 2.5228474140167236,
      "learning_rate": 2.8946308724832215e-05,
      "loss": 0.5432,
      "step": 31870
    },
    {
      "epoch": 1.2752,
      "grad_norm": 2.579584836959839,
      "learning_rate": 2.8939597315436247e-05,
      "loss": 0.5224,
      "step": 31880
    },
    {
      "epoch": 1.2756,
      "grad_norm": 3.1672422885894775,
      "learning_rate": 2.893288590604027e-05,
      "loss": 0.6025,
      "step": 31890
    },
    {
      "epoch": 1.276,
      "grad_norm": 2.507539749145508,
      "learning_rate": 2.8926174496644297e-05,
      "loss": 0.5114,
      "step": 31900
    },
    {
      "epoch": 1.2764,
      "grad_norm": 2.8620831966400146,
      "learning_rate": 2.8919463087248322e-05,
      "loss": 0.5826,
      "step": 31910
    },
    {
      "epoch": 1.2768,
      "grad_norm": 3.271267890930176,
      "learning_rate": 2.891275167785235e-05,
      "loss": 0.6055,
      "step": 31920
    },
    {
      "epoch": 1.2772000000000001,
      "grad_norm": 2.548161745071411,
      "learning_rate": 2.8906040268456376e-05,
      "loss": 0.6217,
      "step": 31930
    },
    {
      "epoch": 1.2776,
      "grad_norm": 2.539935827255249,
      "learning_rate": 2.8899328859060405e-05,
      "loss": 0.6353,
      "step": 31940
    },
    {
      "epoch": 1.278,
      "grad_norm": 2.1979901790618896,
      "learning_rate": 2.8892617449664433e-05,
      "loss": 0.5438,
      "step": 31950
    },
    {
      "epoch": 1.2784,
      "grad_norm": 2.544682502746582,
      "learning_rate": 2.888590604026846e-05,
      "loss": 0.5728,
      "step": 31960
    },
    {
      "epoch": 1.2788,
      "grad_norm": 3.338914155960083,
      "learning_rate": 2.8879194630872487e-05,
      "loss": 0.6209,
      "step": 31970
    },
    {
      "epoch": 1.2792,
      "grad_norm": 2.3211658000946045,
      "learning_rate": 2.887248322147651e-05,
      "loss": 0.5511,
      "step": 31980
    },
    {
      "epoch": 1.2796,
      "grad_norm": 2.6260759830474854,
      "learning_rate": 2.8865771812080537e-05,
      "loss": 0.5508,
      "step": 31990
    },
    {
      "epoch": 1.28,
      "grad_norm": 2.774238348007202,
      "learning_rate": 2.885906040268457e-05,
      "loss": 0.5501,
      "step": 32000
    },
    {
      "epoch": 1.2804,
      "grad_norm": 3.63059401512146,
      "learning_rate": 2.885234899328859e-05,
      "loss": 0.6576,
      "step": 32010
    },
    {
      "epoch": 1.2808,
      "grad_norm": 2.5076868534088135,
      "learning_rate": 2.884563758389262e-05,
      "loss": 0.5238,
      "step": 32020
    },
    {
      "epoch": 1.2812000000000001,
      "grad_norm": 2.181440830230713,
      "learning_rate": 2.8838926174496645e-05,
      "loss": 0.544,
      "step": 32030
    },
    {
      "epoch": 1.2816,
      "grad_norm": 2.5257503986358643,
      "learning_rate": 2.8832214765100673e-05,
      "loss": 0.6763,
      "step": 32040
    },
    {
      "epoch": 1.282,
      "grad_norm": 2.6392030715942383,
      "learning_rate": 2.88255033557047e-05,
      "loss": 0.5348,
      "step": 32050
    },
    {
      "epoch": 1.2824,
      "grad_norm": 2.0325446128845215,
      "learning_rate": 2.8818791946308727e-05,
      "loss": 0.6159,
      "step": 32060
    },
    {
      "epoch": 1.2828,
      "grad_norm": 2.8061861991882324,
      "learning_rate": 2.8812080536912755e-05,
      "loss": 0.5215,
      "step": 32070
    },
    {
      "epoch": 1.2832,
      "grad_norm": 2.5338103771209717,
      "learning_rate": 2.880536912751678e-05,
      "loss": 0.5628,
      "step": 32080
    },
    {
      "epoch": 1.2836,
      "grad_norm": 2.8123722076416016,
      "learning_rate": 2.879865771812081e-05,
      "loss": 0.5965,
      "step": 32090
    },
    {
      "epoch": 1.284,
      "grad_norm": 2.979733943939209,
      "learning_rate": 2.879194630872483e-05,
      "loss": 0.5908,
      "step": 32100
    },
    {
      "epoch": 1.2844,
      "grad_norm": 2.868299961090088,
      "learning_rate": 2.8785234899328863e-05,
      "loss": 0.6301,
      "step": 32110
    },
    {
      "epoch": 1.2848,
      "grad_norm": 2.7694480419158936,
      "learning_rate": 2.8778523489932885e-05,
      "loss": 0.599,
      "step": 32120
    },
    {
      "epoch": 1.2852000000000001,
      "grad_norm": 1.9788999557495117,
      "learning_rate": 2.8771812080536913e-05,
      "loss": 0.5038,
      "step": 32130
    },
    {
      "epoch": 1.2856,
      "grad_norm": 3.031327962875366,
      "learning_rate": 2.8765100671140942e-05,
      "loss": 0.5647,
      "step": 32140
    },
    {
      "epoch": 1.286,
      "grad_norm": 2.936068296432495,
      "learning_rate": 2.8758389261744967e-05,
      "loss": 0.4556,
      "step": 32150
    },
    {
      "epoch": 1.2864,
      "grad_norm": 3.0612375736236572,
      "learning_rate": 2.8751677852348995e-05,
      "loss": 0.6626,
      "step": 32160
    },
    {
      "epoch": 1.2868,
      "grad_norm": 3.1444382667541504,
      "learning_rate": 2.874496644295302e-05,
      "loss": 0.5965,
      "step": 32170
    },
    {
      "epoch": 1.2872,
      "grad_norm": 2.34159779548645,
      "learning_rate": 2.873825503355705e-05,
      "loss": 0.5736,
      "step": 32180
    },
    {
      "epoch": 1.2876,
      "grad_norm": 2.707267999649048,
      "learning_rate": 2.873154362416107e-05,
      "loss": 0.5983,
      "step": 32190
    },
    {
      "epoch": 1.288,
      "grad_norm": 3.1428236961364746,
      "learning_rate": 2.8724832214765103e-05,
      "loss": 0.5827,
      "step": 32200
    },
    {
      "epoch": 1.2884,
      "grad_norm": 2.379206895828247,
      "learning_rate": 2.871812080536913e-05,
      "loss": 0.4961,
      "step": 32210
    },
    {
      "epoch": 1.2888,
      "grad_norm": 3.001157760620117,
      "learning_rate": 2.8711409395973153e-05,
      "loss": 0.5848,
      "step": 32220
    },
    {
      "epoch": 1.2892000000000001,
      "grad_norm": 2.559298515319824,
      "learning_rate": 2.8704697986577185e-05,
      "loss": 0.5001,
      "step": 32230
    },
    {
      "epoch": 1.2896,
      "grad_norm": 2.2747251987457275,
      "learning_rate": 2.8697986577181207e-05,
      "loss": 0.5848,
      "step": 32240
    },
    {
      "epoch": 1.29,
      "grad_norm": 2.8413450717926025,
      "learning_rate": 2.8691275167785235e-05,
      "loss": 0.5611,
      "step": 32250
    },
    {
      "epoch": 1.2904,
      "grad_norm": 3.2788777351379395,
      "learning_rate": 2.8684563758389267e-05,
      "loss": 0.6087,
      "step": 32260
    },
    {
      "epoch": 1.2908,
      "grad_norm": 2.6618399620056152,
      "learning_rate": 2.867785234899329e-05,
      "loss": 0.5448,
      "step": 32270
    },
    {
      "epoch": 1.2912,
      "grad_norm": 3.3691649436950684,
      "learning_rate": 2.8671140939597318e-05,
      "loss": 0.5328,
      "step": 32280
    },
    {
      "epoch": 1.2916,
      "grad_norm": 2.4543094635009766,
      "learning_rate": 2.8664429530201343e-05,
      "loss": 0.6009,
      "step": 32290
    },
    {
      "epoch": 1.292,
      "grad_norm": 3.395908832550049,
      "learning_rate": 2.865771812080537e-05,
      "loss": 0.5798,
      "step": 32300
    },
    {
      "epoch": 1.2924,
      "grad_norm": 2.48317551612854,
      "learning_rate": 2.8651006711409397e-05,
      "loss": 0.53,
      "step": 32310
    },
    {
      "epoch": 1.2928,
      "grad_norm": 2.7564923763275146,
      "learning_rate": 2.8644295302013425e-05,
      "loss": 0.595,
      "step": 32320
    },
    {
      "epoch": 1.2932000000000001,
      "grad_norm": 2.1909379959106445,
      "learning_rate": 2.8637583892617454e-05,
      "loss": 0.5376,
      "step": 32330
    },
    {
      "epoch": 1.2936,
      "grad_norm": 2.9684784412384033,
      "learning_rate": 2.8630872483221475e-05,
      "loss": 0.6262,
      "step": 32340
    },
    {
      "epoch": 1.294,
      "grad_norm": 2.204071521759033,
      "learning_rate": 2.8624161073825507e-05,
      "loss": 0.5738,
      "step": 32350
    },
    {
      "epoch": 1.2944,
      "grad_norm": 2.924665689468384,
      "learning_rate": 2.861744966442953e-05,
      "loss": 0.5122,
      "step": 32360
    },
    {
      "epoch": 1.2948,
      "grad_norm": 3.153275728225708,
      "learning_rate": 2.8610738255033558e-05,
      "loss": 0.6047,
      "step": 32370
    },
    {
      "epoch": 1.2952,
      "grad_norm": 2.533406972885132,
      "learning_rate": 2.8604026845637583e-05,
      "loss": 0.5787,
      "step": 32380
    },
    {
      "epoch": 1.2955999999999999,
      "grad_norm": 2.446681499481201,
      "learning_rate": 2.859731543624161e-05,
      "loss": 0.5349,
      "step": 32390
    },
    {
      "epoch": 1.296,
      "grad_norm": 2.254500389099121,
      "learning_rate": 2.859060402684564e-05,
      "loss": 0.5395,
      "step": 32400
    },
    {
      "epoch": 1.2964,
      "grad_norm": 2.761232852935791,
      "learning_rate": 2.8583892617449665e-05,
      "loss": 0.5942,
      "step": 32410
    },
    {
      "epoch": 1.2968,
      "grad_norm": 3.373697280883789,
      "learning_rate": 2.8577181208053694e-05,
      "loss": 0.5389,
      "step": 32420
    },
    {
      "epoch": 1.2972000000000001,
      "grad_norm": 1.531486988067627,
      "learning_rate": 2.857046979865772e-05,
      "loss": 0.5648,
      "step": 32430
    },
    {
      "epoch": 1.2976,
      "grad_norm": 2.2358686923980713,
      "learning_rate": 2.8563758389261747e-05,
      "loss": 0.5673,
      "step": 32440
    },
    {
      "epoch": 1.298,
      "grad_norm": 2.984868049621582,
      "learning_rate": 2.8557046979865776e-05,
      "loss": 0.5825,
      "step": 32450
    },
    {
      "epoch": 1.2984,
      "grad_norm": 2.5741379261016846,
      "learning_rate": 2.85503355704698e-05,
      "loss": 0.5142,
      "step": 32460
    },
    {
      "epoch": 1.2988,
      "grad_norm": 3.23129940032959,
      "learning_rate": 2.854362416107383e-05,
      "loss": 0.5374,
      "step": 32470
    },
    {
      "epoch": 1.2992,
      "grad_norm": 2.2275381088256836,
      "learning_rate": 2.853691275167785e-05,
      "loss": 0.5045,
      "step": 32480
    },
    {
      "epoch": 1.2995999999999999,
      "grad_norm": 3.1398439407348633,
      "learning_rate": 2.853020134228188e-05,
      "loss": 0.5616,
      "step": 32490
    },
    {
      "epoch": 1.3,
      "grad_norm": 3.9578030109405518,
      "learning_rate": 2.8523489932885905e-05,
      "loss": 0.5691,
      "step": 32500
    },
    {
      "epoch": 1.3004,
      "grad_norm": 2.8349497318267822,
      "learning_rate": 2.8516778523489934e-05,
      "loss": 0.5759,
      "step": 32510
    },
    {
      "epoch": 1.3008,
      "grad_norm": 3.154015302658081,
      "learning_rate": 2.8510067114093962e-05,
      "loss": 0.5896,
      "step": 32520
    },
    {
      "epoch": 1.3012000000000001,
      "grad_norm": 2.7470648288726807,
      "learning_rate": 2.8503355704697987e-05,
      "loss": 0.5855,
      "step": 32530
    },
    {
      "epoch": 1.3016,
      "grad_norm": 2.8652782440185547,
      "learning_rate": 2.8496644295302016e-05,
      "loss": 0.5063,
      "step": 32540
    },
    {
      "epoch": 1.302,
      "grad_norm": 2.845433235168457,
      "learning_rate": 2.848993288590604e-05,
      "loss": 0.5729,
      "step": 32550
    },
    {
      "epoch": 1.3024,
      "grad_norm": 2.4842159748077393,
      "learning_rate": 2.848322147651007e-05,
      "loss": 0.631,
      "step": 32560
    },
    {
      "epoch": 1.3028,
      "grad_norm": 2.7998223304748535,
      "learning_rate": 2.847651006711409e-05,
      "loss": 0.6177,
      "step": 32570
    },
    {
      "epoch": 1.3032,
      "grad_norm": 2.833840847015381,
      "learning_rate": 2.8469798657718123e-05,
      "loss": 0.5785,
      "step": 32580
    },
    {
      "epoch": 1.3035999999999999,
      "grad_norm": 2.375558376312256,
      "learning_rate": 2.8463087248322152e-05,
      "loss": 0.4881,
      "step": 32590
    },
    {
      "epoch": 1.304,
      "grad_norm": 2.5357329845428467,
      "learning_rate": 2.8456375838926174e-05,
      "loss": 0.5585,
      "step": 32600
    },
    {
      "epoch": 1.3044,
      "grad_norm": 3.068636655807495,
      "learning_rate": 2.8449664429530206e-05,
      "loss": 0.5844,
      "step": 32610
    },
    {
      "epoch": 1.3048,
      "grad_norm": 2.7584946155548096,
      "learning_rate": 2.8442953020134227e-05,
      "loss": 0.619,
      "step": 32620
    },
    {
      "epoch": 1.3052000000000001,
      "grad_norm": 3.3332104682922363,
      "learning_rate": 2.8436241610738256e-05,
      "loss": 0.5798,
      "step": 32630
    },
    {
      "epoch": 1.3056,
      "grad_norm": 2.084059476852417,
      "learning_rate": 2.8429530201342284e-05,
      "loss": 0.5481,
      "step": 32640
    },
    {
      "epoch": 1.306,
      "grad_norm": 3.0927727222442627,
      "learning_rate": 2.842281879194631e-05,
      "loss": 0.6069,
      "step": 32650
    },
    {
      "epoch": 1.3064,
      "grad_norm": 2.432905912399292,
      "learning_rate": 2.8416107382550338e-05,
      "loss": 0.4598,
      "step": 32660
    },
    {
      "epoch": 1.3068,
      "grad_norm": 2.816911458969116,
      "learning_rate": 2.8409395973154363e-05,
      "loss": 0.5693,
      "step": 32670
    },
    {
      "epoch": 1.3072,
      "grad_norm": 2.3693878650665283,
      "learning_rate": 2.8402684563758392e-05,
      "loss": 0.5647,
      "step": 32680
    },
    {
      "epoch": 1.3075999999999999,
      "grad_norm": 2.268235921859741,
      "learning_rate": 2.8395973154362414e-05,
      "loss": 0.5159,
      "step": 32690
    },
    {
      "epoch": 1.308,
      "grad_norm": 2.724985361099243,
      "learning_rate": 2.8389261744966445e-05,
      "loss": 0.5825,
      "step": 32700
    },
    {
      "epoch": 1.3084,
      "grad_norm": 2.5987331867218018,
      "learning_rate": 2.8382550335570474e-05,
      "loss": 0.6305,
      "step": 32710
    },
    {
      "epoch": 1.3088,
      "grad_norm": 2.631415843963623,
      "learning_rate": 2.8375838926174496e-05,
      "loss": 0.5573,
      "step": 32720
    },
    {
      "epoch": 1.3092,
      "grad_norm": 2.6733763217926025,
      "learning_rate": 2.8369127516778528e-05,
      "loss": 0.5537,
      "step": 32730
    },
    {
      "epoch": 1.3096,
      "grad_norm": 2.248295783996582,
      "learning_rate": 2.836241610738255e-05,
      "loss": 0.5231,
      "step": 32740
    },
    {
      "epoch": 1.31,
      "grad_norm": 2.372180461883545,
      "learning_rate": 2.8355704697986578e-05,
      "loss": 0.5427,
      "step": 32750
    },
    {
      "epoch": 1.3104,
      "grad_norm": 2.494823455810547,
      "learning_rate": 2.8348993288590603e-05,
      "loss": 0.6303,
      "step": 32760
    },
    {
      "epoch": 1.3108,
      "grad_norm": 2.7257769107818604,
      "learning_rate": 2.8342281879194632e-05,
      "loss": 0.4864,
      "step": 32770
    },
    {
      "epoch": 1.3112,
      "grad_norm": 2.8924732208251953,
      "learning_rate": 2.833557046979866e-05,
      "loss": 0.5778,
      "step": 32780
    },
    {
      "epoch": 1.3115999999999999,
      "grad_norm": 2.8016350269317627,
      "learning_rate": 2.8328859060402685e-05,
      "loss": 0.5405,
      "step": 32790
    },
    {
      "epoch": 1.312,
      "grad_norm": 2.265369176864624,
      "learning_rate": 2.8322147651006714e-05,
      "loss": 0.4749,
      "step": 32800
    },
    {
      "epoch": 1.3124,
      "grad_norm": 3.2060248851776123,
      "learning_rate": 2.831543624161074e-05,
      "loss": 0.5464,
      "step": 32810
    },
    {
      "epoch": 1.3128,
      "grad_norm": 2.1939504146575928,
      "learning_rate": 2.8308724832214768e-05,
      "loss": 0.5867,
      "step": 32820
    },
    {
      "epoch": 1.3132,
      "grad_norm": 2.457106113433838,
      "learning_rate": 2.830201342281879e-05,
      "loss": 0.5842,
      "step": 32830
    },
    {
      "epoch": 1.3136,
      "grad_norm": 2.832024574279785,
      "learning_rate": 2.8295302013422818e-05,
      "loss": 0.5675,
      "step": 32840
    },
    {
      "epoch": 1.314,
      "grad_norm": 2.749641180038452,
      "learning_rate": 2.828859060402685e-05,
      "loss": 0.5929,
      "step": 32850
    },
    {
      "epoch": 1.3144,
      "grad_norm": 2.7644271850585938,
      "learning_rate": 2.8281879194630872e-05,
      "loss": 0.489,
      "step": 32860
    },
    {
      "epoch": 1.3148,
      "grad_norm": 1.8721497058868408,
      "learning_rate": 2.82751677852349e-05,
      "loss": 0.543,
      "step": 32870
    },
    {
      "epoch": 1.3152,
      "grad_norm": 2.6551239490509033,
      "learning_rate": 2.8268456375838925e-05,
      "loss": 0.5661,
      "step": 32880
    },
    {
      "epoch": 1.3155999999999999,
      "grad_norm": 2.834566593170166,
      "learning_rate": 2.8261744966442954e-05,
      "loss": 0.5575,
      "step": 32890
    },
    {
      "epoch": 1.316,
      "grad_norm": 3.063337802886963,
      "learning_rate": 2.8255033557046983e-05,
      "loss": 0.5293,
      "step": 32900
    },
    {
      "epoch": 1.3164,
      "grad_norm": 2.7622263431549072,
      "learning_rate": 2.8248322147651008e-05,
      "loss": 0.6451,
      "step": 32910
    },
    {
      "epoch": 1.3168,
      "grad_norm": 2.7624144554138184,
      "learning_rate": 2.8241610738255036e-05,
      "loss": 0.5636,
      "step": 32920
    },
    {
      "epoch": 1.3172,
      "grad_norm": 2.719447374343872,
      "learning_rate": 2.823489932885906e-05,
      "loss": 0.5599,
      "step": 32930
    },
    {
      "epoch": 1.3176,
      "grad_norm": 2.784823417663574,
      "learning_rate": 2.822818791946309e-05,
      "loss": 0.5833,
      "step": 32940
    },
    {
      "epoch": 1.318,
      "grad_norm": 2.351830005645752,
      "learning_rate": 2.8221476510067112e-05,
      "loss": 0.5276,
      "step": 32950
    },
    {
      "epoch": 1.3184,
      "grad_norm": 3.3187761306762695,
      "learning_rate": 2.8214765100671144e-05,
      "loss": 0.6421,
      "step": 32960
    },
    {
      "epoch": 1.3188,
      "grad_norm": 2.301279067993164,
      "learning_rate": 2.8208053691275172e-05,
      "loss": 0.5269,
      "step": 32970
    },
    {
      "epoch": 1.3192,
      "grad_norm": 2.5828514099121094,
      "learning_rate": 2.8201342281879194e-05,
      "loss": 0.497,
      "step": 32980
    },
    {
      "epoch": 1.3195999999999999,
      "grad_norm": 2.2222957611083984,
      "learning_rate": 2.8194630872483223e-05,
      "loss": 0.645,
      "step": 32990
    },
    {
      "epoch": 1.32,
      "grad_norm": 1.917177677154541,
      "learning_rate": 2.8187919463087248e-05,
      "loss": 0.5491,
      "step": 33000
    },
    {
      "epoch": 1.3204,
      "grad_norm": 2.4054808616638184,
      "learning_rate": 2.8181208053691276e-05,
      "loss": 0.5903,
      "step": 33010
    },
    {
      "epoch": 1.3208,
      "grad_norm": 2.893033742904663,
      "learning_rate": 2.81744966442953e-05,
      "loss": 0.5534,
      "step": 33020
    },
    {
      "epoch": 1.3212,
      "grad_norm": 2.6714515686035156,
      "learning_rate": 2.816778523489933e-05,
      "loss": 0.5341,
      "step": 33030
    },
    {
      "epoch": 1.3216,
      "grad_norm": 3.2031571865081787,
      "learning_rate": 2.816107382550336e-05,
      "loss": 0.6316,
      "step": 33040
    },
    {
      "epoch": 1.322,
      "grad_norm": 2.52872896194458,
      "learning_rate": 2.8154362416107384e-05,
      "loss": 0.5104,
      "step": 33050
    },
    {
      "epoch": 1.3224,
      "grad_norm": 2.8710999488830566,
      "learning_rate": 2.8147651006711412e-05,
      "loss": 0.5,
      "step": 33060
    },
    {
      "epoch": 1.3228,
      "grad_norm": 3.06146502494812,
      "learning_rate": 2.8140939597315434e-05,
      "loss": 0.6015,
      "step": 33070
    },
    {
      "epoch": 1.3232,
      "grad_norm": 2.6067941188812256,
      "learning_rate": 2.8134228187919466e-05,
      "loss": 0.5183,
      "step": 33080
    },
    {
      "epoch": 1.3235999999999999,
      "grad_norm": 2.0643420219421387,
      "learning_rate": 2.8127516778523494e-05,
      "loss": 0.4905,
      "step": 33090
    },
    {
      "epoch": 1.324,
      "grad_norm": 2.8214046955108643,
      "learning_rate": 2.8120805369127516e-05,
      "loss": 0.5291,
      "step": 33100
    },
    {
      "epoch": 1.3244,
      "grad_norm": 1.813257098197937,
      "learning_rate": 2.8114093959731548e-05,
      "loss": 0.5328,
      "step": 33110
    },
    {
      "epoch": 1.3248,
      "grad_norm": 2.506535291671753,
      "learning_rate": 2.810738255033557e-05,
      "loss": 0.5105,
      "step": 33120
    },
    {
      "epoch": 1.3252,
      "grad_norm": 3.015389919281006,
      "learning_rate": 2.81006711409396e-05,
      "loss": 0.5759,
      "step": 33130
    },
    {
      "epoch": 1.3256000000000001,
      "grad_norm": 3.26168155670166,
      "learning_rate": 2.8093959731543624e-05,
      "loss": 0.627,
      "step": 33140
    },
    {
      "epoch": 1.326,
      "grad_norm": 1.8495360612869263,
      "learning_rate": 2.8087248322147652e-05,
      "loss": 0.6023,
      "step": 33150
    },
    {
      "epoch": 1.3264,
      "grad_norm": 2.9181525707244873,
      "learning_rate": 2.808053691275168e-05,
      "loss": 0.596,
      "step": 33160
    },
    {
      "epoch": 1.3268,
      "grad_norm": 2.3658711910247803,
      "learning_rate": 2.8073825503355706e-05,
      "loss": 0.5352,
      "step": 33170
    },
    {
      "epoch": 1.3272,
      "grad_norm": 2.9261810779571533,
      "learning_rate": 2.8067114093959734e-05,
      "loss": 0.4947,
      "step": 33180
    },
    {
      "epoch": 1.3276,
      "grad_norm": 2.7899999618530273,
      "learning_rate": 2.806040268456376e-05,
      "loss": 0.5673,
      "step": 33190
    },
    {
      "epoch": 1.328,
      "grad_norm": 2.0598108768463135,
      "learning_rate": 2.8053691275167788e-05,
      "loss": 0.5538,
      "step": 33200
    },
    {
      "epoch": 1.3284,
      "grad_norm": 2.854095697402954,
      "learning_rate": 2.804697986577181e-05,
      "loss": 0.5892,
      "step": 33210
    },
    {
      "epoch": 1.3288,
      "grad_norm": 3.489910364151001,
      "learning_rate": 2.804026845637584e-05,
      "loss": 0.6475,
      "step": 33220
    },
    {
      "epoch": 1.3292,
      "grad_norm": 3.4217071533203125,
      "learning_rate": 2.803355704697987e-05,
      "loss": 0.6061,
      "step": 33230
    },
    {
      "epoch": 1.3296000000000001,
      "grad_norm": 4.280336856842041,
      "learning_rate": 2.8026845637583892e-05,
      "loss": 0.6311,
      "step": 33240
    },
    {
      "epoch": 1.33,
      "grad_norm": 2.508399486541748,
      "learning_rate": 2.802013422818792e-05,
      "loss": 0.6816,
      "step": 33250
    },
    {
      "epoch": 1.3304,
      "grad_norm": 2.4827065467834473,
      "learning_rate": 2.8013422818791946e-05,
      "loss": 0.6002,
      "step": 33260
    },
    {
      "epoch": 1.3308,
      "grad_norm": 2.6620981693267822,
      "learning_rate": 2.8006711409395974e-05,
      "loss": 0.5808,
      "step": 33270
    },
    {
      "epoch": 1.3312,
      "grad_norm": 2.520970582962036,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.5362,
      "step": 33280
    },
    {
      "epoch": 1.3316,
      "grad_norm": 2.5137710571289062,
      "learning_rate": 2.7993288590604028e-05,
      "loss": 0.5699,
      "step": 33290
    },
    {
      "epoch": 1.332,
      "grad_norm": 3.306593894958496,
      "learning_rate": 2.7986577181208057e-05,
      "loss": 0.5571,
      "step": 33300
    },
    {
      "epoch": 1.3324,
      "grad_norm": 2.523463487625122,
      "learning_rate": 2.7979865771812082e-05,
      "loss": 0.5796,
      "step": 33310
    },
    {
      "epoch": 1.3328,
      "grad_norm": 2.7195487022399902,
      "learning_rate": 2.797315436241611e-05,
      "loss": 0.6915,
      "step": 33320
    },
    {
      "epoch": 1.3332,
      "grad_norm": 2.975843906402588,
      "learning_rate": 2.7966442953020132e-05,
      "loss": 0.6335,
      "step": 33330
    },
    {
      "epoch": 1.3336000000000001,
      "grad_norm": 3.130964994430542,
      "learning_rate": 2.7959731543624164e-05,
      "loss": 0.5752,
      "step": 33340
    },
    {
      "epoch": 1.334,
      "grad_norm": 2.2460174560546875,
      "learning_rate": 2.7953020134228193e-05,
      "loss": 0.5896,
      "step": 33350
    },
    {
      "epoch": 1.3344,
      "grad_norm": 2.4203178882598877,
      "learning_rate": 2.7946308724832214e-05,
      "loss": 0.5646,
      "step": 33360
    },
    {
      "epoch": 1.3348,
      "grad_norm": 2.2725307941436768,
      "learning_rate": 2.7939597315436243e-05,
      "loss": 0.5236,
      "step": 33370
    },
    {
      "epoch": 1.3352,
      "grad_norm": 2.368285894393921,
      "learning_rate": 2.7932885906040268e-05,
      "loss": 0.5342,
      "step": 33380
    },
    {
      "epoch": 1.3356,
      "grad_norm": 2.350029706954956,
      "learning_rate": 2.7926174496644297e-05,
      "loss": 0.5129,
      "step": 33390
    },
    {
      "epoch": 1.336,
      "grad_norm": 2.2192883491516113,
      "learning_rate": 2.7919463087248322e-05,
      "loss": 0.5546,
      "step": 33400
    },
    {
      "epoch": 1.3364,
      "grad_norm": 3.1991539001464844,
      "learning_rate": 2.791275167785235e-05,
      "loss": 0.5334,
      "step": 33410
    },
    {
      "epoch": 1.3368,
      "grad_norm": 2.54958176612854,
      "learning_rate": 2.790604026845638e-05,
      "loss": 0.4787,
      "step": 33420
    },
    {
      "epoch": 1.3372,
      "grad_norm": 2.504538059234619,
      "learning_rate": 2.7899328859060404e-05,
      "loss": 0.5741,
      "step": 33430
    },
    {
      "epoch": 1.3376000000000001,
      "grad_norm": 2.6566264629364014,
      "learning_rate": 2.7892617449664433e-05,
      "loss": 0.5902,
      "step": 33440
    },
    {
      "epoch": 1.338,
      "grad_norm": 2.7783331871032715,
      "learning_rate": 2.7885906040268454e-05,
      "loss": 0.5632,
      "step": 33450
    },
    {
      "epoch": 1.3384,
      "grad_norm": 2.479461908340454,
      "learning_rate": 2.7879194630872486e-05,
      "loss": 0.5445,
      "step": 33460
    },
    {
      "epoch": 1.3388,
      "grad_norm": 3.04885196685791,
      "learning_rate": 2.7872483221476515e-05,
      "loss": 0.5959,
      "step": 33470
    },
    {
      "epoch": 1.3392,
      "grad_norm": 2.4816930294036865,
      "learning_rate": 2.7865771812080537e-05,
      "loss": 0.5591,
      "step": 33480
    },
    {
      "epoch": 1.3396,
      "grad_norm": 3.1970839500427246,
      "learning_rate": 2.785906040268457e-05,
      "loss": 0.5246,
      "step": 33490
    },
    {
      "epoch": 1.34,
      "grad_norm": 2.5216517448425293,
      "learning_rate": 2.785234899328859e-05,
      "loss": 0.5627,
      "step": 33500
    },
    {
      "epoch": 1.3404,
      "grad_norm": 2.89290189743042,
      "learning_rate": 2.784563758389262e-05,
      "loss": 0.6496,
      "step": 33510
    },
    {
      "epoch": 1.3408,
      "grad_norm": 2.5073282718658447,
      "learning_rate": 2.7838926174496644e-05,
      "loss": 0.5406,
      "step": 33520
    },
    {
      "epoch": 1.3412,
      "grad_norm": 2.106245279312134,
      "learning_rate": 2.7832214765100673e-05,
      "loss": 0.5566,
      "step": 33530
    },
    {
      "epoch": 1.3416000000000001,
      "grad_norm": 2.8688042163848877,
      "learning_rate": 2.78255033557047e-05,
      "loss": 0.5979,
      "step": 33540
    },
    {
      "epoch": 1.342,
      "grad_norm": 2.6036033630371094,
      "learning_rate": 2.7818791946308726e-05,
      "loss": 0.5636,
      "step": 33550
    },
    {
      "epoch": 1.3424,
      "grad_norm": 2.635723829269409,
      "learning_rate": 2.7812080536912755e-05,
      "loss": 0.5878,
      "step": 33560
    },
    {
      "epoch": 1.3428,
      "grad_norm": 2.999521017074585,
      "learning_rate": 2.7805369127516777e-05,
      "loss": 0.5831,
      "step": 33570
    },
    {
      "epoch": 1.3432,
      "grad_norm": 2.272702217102051,
      "learning_rate": 2.779865771812081e-05,
      "loss": 0.528,
      "step": 33580
    },
    {
      "epoch": 1.3436,
      "grad_norm": 2.8222031593322754,
      "learning_rate": 2.779194630872483e-05,
      "loss": 0.477,
      "step": 33590
    },
    {
      "epoch": 1.3439999999999999,
      "grad_norm": 2.301314353942871,
      "learning_rate": 2.778523489932886e-05,
      "loss": 0.5518,
      "step": 33600
    },
    {
      "epoch": 1.3444,
      "grad_norm": 2.334089994430542,
      "learning_rate": 2.777852348993289e-05,
      "loss": 0.5282,
      "step": 33610
    },
    {
      "epoch": 1.3448,
      "grad_norm": 2.983482599258423,
      "learning_rate": 2.7771812080536913e-05,
      "loss": 0.5515,
      "step": 33620
    },
    {
      "epoch": 1.3452,
      "grad_norm": 1.8080083131790161,
      "learning_rate": 2.776510067114094e-05,
      "loss": 0.495,
      "step": 33630
    },
    {
      "epoch": 1.3456000000000001,
      "grad_norm": 2.4134864807128906,
      "learning_rate": 2.7758389261744966e-05,
      "loss": 0.5096,
      "step": 33640
    },
    {
      "epoch": 1.346,
      "grad_norm": 2.5020623207092285,
      "learning_rate": 2.7751677852348995e-05,
      "loss": 0.5372,
      "step": 33650
    },
    {
      "epoch": 1.3464,
      "grad_norm": 3.045367956161499,
      "learning_rate": 2.774496644295302e-05,
      "loss": 0.5857,
      "step": 33660
    },
    {
      "epoch": 1.3468,
      "grad_norm": 3.248783826828003,
      "learning_rate": 2.773825503355705e-05,
      "loss": 0.5091,
      "step": 33670
    },
    {
      "epoch": 1.3472,
      "grad_norm": 2.6999495029449463,
      "learning_rate": 2.7731543624161077e-05,
      "loss": 0.5422,
      "step": 33680
    },
    {
      "epoch": 1.3476,
      "grad_norm": 3.7507174015045166,
      "learning_rate": 2.7724832214765102e-05,
      "loss": 0.7021,
      "step": 33690
    },
    {
      "epoch": 1.3479999999999999,
      "grad_norm": 2.143763542175293,
      "learning_rate": 2.771812080536913e-05,
      "loss": 0.6285,
      "step": 33700
    },
    {
      "epoch": 1.3484,
      "grad_norm": 2.482365131378174,
      "learning_rate": 2.7711409395973153e-05,
      "loss": 0.6231,
      "step": 33710
    },
    {
      "epoch": 1.3488,
      "grad_norm": 3.406599283218384,
      "learning_rate": 2.770469798657718e-05,
      "loss": 0.5825,
      "step": 33720
    },
    {
      "epoch": 1.3492,
      "grad_norm": 2.90816593170166,
      "learning_rate": 2.7697986577181213e-05,
      "loss": 0.5438,
      "step": 33730
    },
    {
      "epoch": 1.3496000000000001,
      "grad_norm": 2.5353827476501465,
      "learning_rate": 2.7691275167785235e-05,
      "loss": 0.5386,
      "step": 33740
    },
    {
      "epoch": 1.35,
      "grad_norm": 3.0548558235168457,
      "learning_rate": 2.7684563758389263e-05,
      "loss": 0.542,
      "step": 33750
    },
    {
      "epoch": 1.3504,
      "grad_norm": 2.833969831466675,
      "learning_rate": 2.767785234899329e-05,
      "loss": 0.4739,
      "step": 33760
    },
    {
      "epoch": 1.3508,
      "grad_norm": 3.7458250522613525,
      "learning_rate": 2.7671140939597317e-05,
      "loss": 0.5793,
      "step": 33770
    },
    {
      "epoch": 1.3512,
      "grad_norm": 2.198897361755371,
      "learning_rate": 2.7664429530201342e-05,
      "loss": 0.5036,
      "step": 33780
    },
    {
      "epoch": 1.3516,
      "grad_norm": 2.394404649734497,
      "learning_rate": 2.765771812080537e-05,
      "loss": 0.5466,
      "step": 33790
    },
    {
      "epoch": 1.3519999999999999,
      "grad_norm": 2.453261375427246,
      "learning_rate": 2.76510067114094e-05,
      "loss": 0.5419,
      "step": 33800
    },
    {
      "epoch": 1.3524,
      "grad_norm": 2.879086971282959,
      "learning_rate": 2.7644295302013424e-05,
      "loss": 0.6163,
      "step": 33810
    },
    {
      "epoch": 1.3528,
      "grad_norm": 2.133444309234619,
      "learning_rate": 2.7637583892617453e-05,
      "loss": 0.5183,
      "step": 33820
    },
    {
      "epoch": 1.3532,
      "grad_norm": 2.237916946411133,
      "learning_rate": 2.7630872483221475e-05,
      "loss": 0.552,
      "step": 33830
    },
    {
      "epoch": 1.3536000000000001,
      "grad_norm": 2.4639947414398193,
      "learning_rate": 2.7624161073825507e-05,
      "loss": 0.5662,
      "step": 33840
    },
    {
      "epoch": 1.354,
      "grad_norm": 2.1038455963134766,
      "learning_rate": 2.761744966442953e-05,
      "loss": 0.5794,
      "step": 33850
    },
    {
      "epoch": 1.3544,
      "grad_norm": 2.9292473793029785,
      "learning_rate": 2.7610738255033557e-05,
      "loss": 0.5247,
      "step": 33860
    },
    {
      "epoch": 1.3548,
      "grad_norm": 2.7716498374938965,
      "learning_rate": 2.7604026845637586e-05,
      "loss": 0.5468,
      "step": 33870
    },
    {
      "epoch": 1.3552,
      "grad_norm": 3.3925607204437256,
      "learning_rate": 2.759731543624161e-05,
      "loss": 0.5716,
      "step": 33880
    },
    {
      "epoch": 1.3556,
      "grad_norm": 2.88586688041687,
      "learning_rate": 2.759060402684564e-05,
      "loss": 0.545,
      "step": 33890
    },
    {
      "epoch": 1.3559999999999999,
      "grad_norm": 3.303598165512085,
      "learning_rate": 2.7583892617449664e-05,
      "loss": 0.5096,
      "step": 33900
    },
    {
      "epoch": 1.3564,
      "grad_norm": 2.6161949634552,
      "learning_rate": 2.7577181208053693e-05,
      "loss": 0.6163,
      "step": 33910
    },
    {
      "epoch": 1.3568,
      "grad_norm": 3.3219025135040283,
      "learning_rate": 2.757046979865772e-05,
      "loss": 0.5328,
      "step": 33920
    },
    {
      "epoch": 1.3572,
      "grad_norm": 2.20650315284729,
      "learning_rate": 2.7563758389261747e-05,
      "loss": 0.577,
      "step": 33930
    },
    {
      "epoch": 1.3576,
      "grad_norm": 2.8382561206817627,
      "learning_rate": 2.7557046979865775e-05,
      "loss": 0.5349,
      "step": 33940
    },
    {
      "epoch": 1.358,
      "grad_norm": 2.7537107467651367,
      "learning_rate": 2.7550335570469797e-05,
      "loss": 0.5918,
      "step": 33950
    },
    {
      "epoch": 1.3584,
      "grad_norm": 2.620140552520752,
      "learning_rate": 2.754362416107383e-05,
      "loss": 0.6011,
      "step": 33960
    },
    {
      "epoch": 1.3588,
      "grad_norm": 2.256430149078369,
      "learning_rate": 2.753691275167785e-05,
      "loss": 0.5606,
      "step": 33970
    },
    {
      "epoch": 1.3592,
      "grad_norm": 4.296530723571777,
      "learning_rate": 2.753020134228188e-05,
      "loss": 0.5493,
      "step": 33980
    },
    {
      "epoch": 1.3596,
      "grad_norm": 2.562670946121216,
      "learning_rate": 2.752348993288591e-05,
      "loss": 0.5368,
      "step": 33990
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 3.427175521850586,
      "learning_rate": 2.7516778523489933e-05,
      "loss": 0.5893,
      "step": 34000
    },
    {
      "epoch": 1.3604,
      "grad_norm": 1.8964054584503174,
      "learning_rate": 2.751006711409396e-05,
      "loss": 0.5149,
      "step": 34010
    },
    {
      "epoch": 1.3608,
      "grad_norm": 2.7252116203308105,
      "learning_rate": 2.7503355704697987e-05,
      "loss": 0.6044,
      "step": 34020
    },
    {
      "epoch": 1.3612,
      "grad_norm": 2.6571617126464844,
      "learning_rate": 2.7496644295302015e-05,
      "loss": 0.561,
      "step": 34030
    },
    {
      "epoch": 1.3616,
      "grad_norm": 2.0289862155914307,
      "learning_rate": 2.748993288590604e-05,
      "loss": 0.4468,
      "step": 34040
    },
    {
      "epoch": 1.362,
      "grad_norm": 2.4076285362243652,
      "learning_rate": 2.748322147651007e-05,
      "loss": 0.633,
      "step": 34050
    },
    {
      "epoch": 1.3624,
      "grad_norm": 2.1247782707214355,
      "learning_rate": 2.7476510067114098e-05,
      "loss": 0.5846,
      "step": 34060
    },
    {
      "epoch": 1.3628,
      "grad_norm": 2.9208688735961914,
      "learning_rate": 2.7469798657718123e-05,
      "loss": 0.5441,
      "step": 34070
    },
    {
      "epoch": 1.3632,
      "grad_norm": 3.744939088821411,
      "learning_rate": 2.746308724832215e-05,
      "loss": 0.6396,
      "step": 34080
    },
    {
      "epoch": 1.3636,
      "grad_norm": 3.095282793045044,
      "learning_rate": 2.7456375838926173e-05,
      "loss": 0.5487,
      "step": 34090
    },
    {
      "epoch": 1.3639999999999999,
      "grad_norm": 2.1411592960357666,
      "learning_rate": 2.74496644295302e-05,
      "loss": 0.5115,
      "step": 34100
    },
    {
      "epoch": 1.3644,
      "grad_norm": 4.450193881988525,
      "learning_rate": 2.7442953020134233e-05,
      "loss": 0.5473,
      "step": 34110
    },
    {
      "epoch": 1.3648,
      "grad_norm": 2.7116005420684814,
      "learning_rate": 2.7436241610738255e-05,
      "loss": 0.6052,
      "step": 34120
    },
    {
      "epoch": 1.3652,
      "grad_norm": 2.433441162109375,
      "learning_rate": 2.7429530201342284e-05,
      "loss": 0.4634,
      "step": 34130
    },
    {
      "epoch": 1.3656,
      "grad_norm": 2.544055700302124,
      "learning_rate": 2.742281879194631e-05,
      "loss": 0.5259,
      "step": 34140
    },
    {
      "epoch": 1.366,
      "grad_norm": 2.55401873588562,
      "learning_rate": 2.7416107382550337e-05,
      "loss": 0.5018,
      "step": 34150
    },
    {
      "epoch": 1.3664,
      "grad_norm": 3.066987991333008,
      "learning_rate": 2.7409395973154363e-05,
      "loss": 0.6068,
      "step": 34160
    },
    {
      "epoch": 1.3668,
      "grad_norm": 2.761402130126953,
      "learning_rate": 2.740268456375839e-05,
      "loss": 0.5786,
      "step": 34170
    },
    {
      "epoch": 1.3672,
      "grad_norm": 3.2938590049743652,
      "learning_rate": 2.739597315436242e-05,
      "loss": 0.6124,
      "step": 34180
    },
    {
      "epoch": 1.3676,
      "grad_norm": 2.4784650802612305,
      "learning_rate": 2.7389261744966445e-05,
      "loss": 0.4894,
      "step": 34190
    },
    {
      "epoch": 1.3679999999999999,
      "grad_norm": 2.746943712234497,
      "learning_rate": 2.7382550335570473e-05,
      "loss": 0.5396,
      "step": 34200
    },
    {
      "epoch": 1.3684,
      "grad_norm": 3.5667545795440674,
      "learning_rate": 2.7375838926174495e-05,
      "loss": 0.6339,
      "step": 34210
    },
    {
      "epoch": 1.3688,
      "grad_norm": 2.067272186279297,
      "learning_rate": 2.7369127516778527e-05,
      "loss": 0.5159,
      "step": 34220
    },
    {
      "epoch": 1.3692,
      "grad_norm": 2.542479991912842,
      "learning_rate": 2.736241610738255e-05,
      "loss": 0.501,
      "step": 34230
    },
    {
      "epoch": 1.3696,
      "grad_norm": 2.6586503982543945,
      "learning_rate": 2.7355704697986577e-05,
      "loss": 0.6291,
      "step": 34240
    },
    {
      "epoch": 1.37,
      "grad_norm": 2.2247142791748047,
      "learning_rate": 2.7348993288590606e-05,
      "loss": 0.5212,
      "step": 34250
    },
    {
      "epoch": 1.3704,
      "grad_norm": 2.5753769874572754,
      "learning_rate": 2.734228187919463e-05,
      "loss": 0.57,
      "step": 34260
    },
    {
      "epoch": 1.3708,
      "grad_norm": 3.1933529376983643,
      "learning_rate": 2.733557046979866e-05,
      "loss": 0.6253,
      "step": 34270
    },
    {
      "epoch": 1.3712,
      "grad_norm": 3.0543813705444336,
      "learning_rate": 2.7328859060402685e-05,
      "loss": 0.5647,
      "step": 34280
    },
    {
      "epoch": 1.3716,
      "grad_norm": 2.9078171253204346,
      "learning_rate": 2.7322147651006713e-05,
      "loss": 0.533,
      "step": 34290
    },
    {
      "epoch": 1.3719999999999999,
      "grad_norm": 2.7421698570251465,
      "learning_rate": 2.7315436241610742e-05,
      "loss": 0.5833,
      "step": 34300
    },
    {
      "epoch": 1.3724,
      "grad_norm": 2.446889638900757,
      "learning_rate": 2.7308724832214767e-05,
      "loss": 0.5697,
      "step": 34310
    },
    {
      "epoch": 1.3728,
      "grad_norm": 3.129979133605957,
      "learning_rate": 2.7302013422818796e-05,
      "loss": 0.6015,
      "step": 34320
    },
    {
      "epoch": 1.3732,
      "grad_norm": 2.3357954025268555,
      "learning_rate": 2.7295302013422817e-05,
      "loss": 0.568,
      "step": 34330
    },
    {
      "epoch": 1.3736,
      "grad_norm": 2.385573387145996,
      "learning_rate": 2.728859060402685e-05,
      "loss": 0.5945,
      "step": 34340
    },
    {
      "epoch": 1.374,
      "grad_norm": 2.463261842727661,
      "learning_rate": 2.728187919463087e-05,
      "loss": 0.6033,
      "step": 34350
    },
    {
      "epoch": 1.3744,
      "grad_norm": 3.051234245300293,
      "learning_rate": 2.72751677852349e-05,
      "loss": 0.5336,
      "step": 34360
    },
    {
      "epoch": 1.3748,
      "grad_norm": 2.184723138809204,
      "learning_rate": 2.726845637583893e-05,
      "loss": 0.5801,
      "step": 34370
    },
    {
      "epoch": 1.3752,
      "grad_norm": 2.607983112335205,
      "learning_rate": 2.7261744966442953e-05,
      "loss": 0.4776,
      "step": 34380
    },
    {
      "epoch": 1.3756,
      "grad_norm": 3.4868783950805664,
      "learning_rate": 2.7255033557046982e-05,
      "loss": 0.5784,
      "step": 34390
    },
    {
      "epoch": 1.376,
      "grad_norm": 3.1805765628814697,
      "learning_rate": 2.7248322147651007e-05,
      "loss": 0.5435,
      "step": 34400
    },
    {
      "epoch": 1.3764,
      "grad_norm": 2.672271251678467,
      "learning_rate": 2.7241610738255036e-05,
      "loss": 0.596,
      "step": 34410
    },
    {
      "epoch": 1.3768,
      "grad_norm": 2.376296043395996,
      "learning_rate": 2.723489932885906e-05,
      "loss": 0.5436,
      "step": 34420
    },
    {
      "epoch": 1.3772,
      "grad_norm": 2.7147700786590576,
      "learning_rate": 2.722818791946309e-05,
      "loss": 0.5967,
      "step": 34430
    },
    {
      "epoch": 1.3776,
      "grad_norm": 3.3799471855163574,
      "learning_rate": 2.7221476510067118e-05,
      "loss": 0.6511,
      "step": 34440
    },
    {
      "epoch": 1.3780000000000001,
      "grad_norm": 2.405508279800415,
      "learning_rate": 2.721476510067114e-05,
      "loss": 0.5883,
      "step": 34450
    },
    {
      "epoch": 1.3784,
      "grad_norm": 3.0940287113189697,
      "learning_rate": 2.720805369127517e-05,
      "loss": 0.6183,
      "step": 34460
    },
    {
      "epoch": 1.3788,
      "grad_norm": 2.708141326904297,
      "learning_rate": 2.7201342281879193e-05,
      "loss": 0.5624,
      "step": 34470
    },
    {
      "epoch": 1.3792,
      "grad_norm": 2.514981269836426,
      "learning_rate": 2.7194630872483222e-05,
      "loss": 0.6015,
      "step": 34480
    },
    {
      "epoch": 1.3796,
      "grad_norm": 2.5707387924194336,
      "learning_rate": 2.7187919463087247e-05,
      "loss": 0.6291,
      "step": 34490
    },
    {
      "epoch": 1.38,
      "grad_norm": 2.544523239135742,
      "learning_rate": 2.7181208053691276e-05,
      "loss": 0.6313,
      "step": 34500
    },
    {
      "epoch": 1.3804,
      "grad_norm": 2.4132003784179688,
      "learning_rate": 2.7174496644295304e-05,
      "loss": 0.557,
      "step": 34510
    },
    {
      "epoch": 1.3808,
      "grad_norm": 3.0474348068237305,
      "learning_rate": 2.716778523489933e-05,
      "loss": 0.5956,
      "step": 34520
    },
    {
      "epoch": 1.3812,
      "grad_norm": 2.8123512268066406,
      "learning_rate": 2.7161073825503358e-05,
      "loss": 0.494,
      "step": 34530
    },
    {
      "epoch": 1.3816,
      "grad_norm": 3.0568926334381104,
      "learning_rate": 2.7154362416107383e-05,
      "loss": 0.5776,
      "step": 34540
    },
    {
      "epoch": 1.3820000000000001,
      "grad_norm": 2.580054759979248,
      "learning_rate": 2.714765100671141e-05,
      "loss": 0.5443,
      "step": 34550
    },
    {
      "epoch": 1.3824,
      "grad_norm": 3.183929443359375,
      "learning_rate": 2.714093959731544e-05,
      "loss": 0.5753,
      "step": 34560
    },
    {
      "epoch": 1.3828,
      "grad_norm": 2.397838592529297,
      "learning_rate": 2.7134228187919465e-05,
      "loss": 0.5745,
      "step": 34570
    },
    {
      "epoch": 1.3832,
      "grad_norm": 2.274021863937378,
      "learning_rate": 2.7127516778523494e-05,
      "loss": 0.5313,
      "step": 34580
    },
    {
      "epoch": 1.3836,
      "grad_norm": 3.394627332687378,
      "learning_rate": 2.7120805369127516e-05,
      "loss": 0.5975,
      "step": 34590
    },
    {
      "epoch": 1.384,
      "grad_norm": 2.547903537750244,
      "learning_rate": 2.7114093959731544e-05,
      "loss": 0.5897,
      "step": 34600
    },
    {
      "epoch": 1.3844,
      "grad_norm": 2.912142038345337,
      "learning_rate": 2.710738255033557e-05,
      "loss": 0.5871,
      "step": 34610
    },
    {
      "epoch": 1.3848,
      "grad_norm": 1.892734408378601,
      "learning_rate": 2.7100671140939598e-05,
      "loss": 0.5279,
      "step": 34620
    },
    {
      "epoch": 1.3852,
      "grad_norm": 3.286735773086548,
      "learning_rate": 2.7093959731543626e-05,
      "loss": 0.5283,
      "step": 34630
    },
    {
      "epoch": 1.3856,
      "grad_norm": 2.6820013523101807,
      "learning_rate": 2.708724832214765e-05,
      "loss": 0.5893,
      "step": 34640
    },
    {
      "epoch": 1.3860000000000001,
      "grad_norm": 2.289576768875122,
      "learning_rate": 2.708053691275168e-05,
      "loss": 0.5663,
      "step": 34650
    },
    {
      "epoch": 1.3864,
      "grad_norm": 2.993304491043091,
      "learning_rate": 2.7073825503355705e-05,
      "loss": 0.6207,
      "step": 34660
    },
    {
      "epoch": 1.3868,
      "grad_norm": 2.471841812133789,
      "learning_rate": 2.7067114093959734e-05,
      "loss": 0.5553,
      "step": 34670
    },
    {
      "epoch": 1.3872,
      "grad_norm": 3.030229330062866,
      "learning_rate": 2.7060402684563756e-05,
      "loss": 0.4615,
      "step": 34680
    },
    {
      "epoch": 1.3876,
      "grad_norm": 2.7023661136627197,
      "learning_rate": 2.7053691275167788e-05,
      "loss": 0.5321,
      "step": 34690
    },
    {
      "epoch": 1.388,
      "grad_norm": 2.6854608058929443,
      "learning_rate": 2.7046979865771816e-05,
      "loss": 0.5388,
      "step": 34700
    },
    {
      "epoch": 1.3884,
      "grad_norm": 2.7205939292907715,
      "learning_rate": 2.7040268456375838e-05,
      "loss": 0.5547,
      "step": 34710
    },
    {
      "epoch": 1.3888,
      "grad_norm": 2.9188811779022217,
      "learning_rate": 2.703355704697987e-05,
      "loss": 0.526,
      "step": 34720
    },
    {
      "epoch": 1.3892,
      "grad_norm": 2.5733542442321777,
      "learning_rate": 2.702684563758389e-05,
      "loss": 0.5615,
      "step": 34730
    },
    {
      "epoch": 1.3896,
      "grad_norm": 3.1374449729919434,
      "learning_rate": 2.702013422818792e-05,
      "loss": 0.607,
      "step": 34740
    },
    {
      "epoch": 1.3900000000000001,
      "grad_norm": 3.350754499435425,
      "learning_rate": 2.701342281879195e-05,
      "loss": 0.5817,
      "step": 34750
    },
    {
      "epoch": 1.3904,
      "grad_norm": 3.1171483993530273,
      "learning_rate": 2.7006711409395974e-05,
      "loss": 0.5541,
      "step": 34760
    },
    {
      "epoch": 1.3908,
      "grad_norm": 2.7059946060180664,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 0.5712,
      "step": 34770
    },
    {
      "epoch": 1.3912,
      "grad_norm": 2.9518892765045166,
      "learning_rate": 2.6993288590604028e-05,
      "loss": 0.5946,
      "step": 34780
    },
    {
      "epoch": 1.3916,
      "grad_norm": 2.2125155925750732,
      "learning_rate": 2.6986577181208056e-05,
      "loss": 0.5206,
      "step": 34790
    },
    {
      "epoch": 1.392,
      "grad_norm": 2.4197964668273926,
      "learning_rate": 2.6979865771812078e-05,
      "loss": 0.5906,
      "step": 34800
    },
    {
      "epoch": 1.3924,
      "grad_norm": 2.5772464275360107,
      "learning_rate": 2.697315436241611e-05,
      "loss": 0.5151,
      "step": 34810
    },
    {
      "epoch": 1.3928,
      "grad_norm": 2.657111883163452,
      "learning_rate": 2.696644295302014e-05,
      "loss": 0.5851,
      "step": 34820
    },
    {
      "epoch": 1.3932,
      "grad_norm": 3.560655117034912,
      "learning_rate": 2.695973154362416e-05,
      "loss": 0.6048,
      "step": 34830
    },
    {
      "epoch": 1.3936,
      "grad_norm": 2.511737823486328,
      "learning_rate": 2.6953020134228192e-05,
      "loss": 0.6503,
      "step": 34840
    },
    {
      "epoch": 1.3940000000000001,
      "grad_norm": 2.4636502265930176,
      "learning_rate": 2.6946308724832214e-05,
      "loss": 0.5237,
      "step": 34850
    },
    {
      "epoch": 1.3944,
      "grad_norm": 2.855022430419922,
      "learning_rate": 2.6939597315436242e-05,
      "loss": 0.4993,
      "step": 34860
    },
    {
      "epoch": 1.3948,
      "grad_norm": 2.1077463626861572,
      "learning_rate": 2.6932885906040268e-05,
      "loss": 0.5567,
      "step": 34870
    },
    {
      "epoch": 1.3952,
      "grad_norm": 3.0757253170013428,
      "learning_rate": 2.6926174496644296e-05,
      "loss": 0.668,
      "step": 34880
    },
    {
      "epoch": 1.3956,
      "grad_norm": 2.684079885482788,
      "learning_rate": 2.6919463087248325e-05,
      "loss": 0.5812,
      "step": 34890
    },
    {
      "epoch": 1.396,
      "grad_norm": 1.9395198822021484,
      "learning_rate": 2.691275167785235e-05,
      "loss": 0.5261,
      "step": 34900
    },
    {
      "epoch": 1.3963999999999999,
      "grad_norm": 3.2072110176086426,
      "learning_rate": 2.690604026845638e-05,
      "loss": 0.5336,
      "step": 34910
    },
    {
      "epoch": 1.3968,
      "grad_norm": 2.8048946857452393,
      "learning_rate": 2.6899328859060403e-05,
      "loss": 0.5785,
      "step": 34920
    },
    {
      "epoch": 1.3972,
      "grad_norm": 3.045239210128784,
      "learning_rate": 2.6892617449664432e-05,
      "loss": 0.6655,
      "step": 34930
    },
    {
      "epoch": 1.3976,
      "grad_norm": 2.949960708618164,
      "learning_rate": 2.688590604026846e-05,
      "loss": 0.6099,
      "step": 34940
    },
    {
      "epoch": 1.3980000000000001,
      "grad_norm": 2.431633472442627,
      "learning_rate": 2.6879194630872482e-05,
      "loss": 0.5209,
      "step": 34950
    },
    {
      "epoch": 1.3984,
      "grad_norm": 2.8223233222961426,
      "learning_rate": 2.6872483221476514e-05,
      "loss": 0.6247,
      "step": 34960
    },
    {
      "epoch": 1.3988,
      "grad_norm": 3.056190252304077,
      "learning_rate": 2.6865771812080536e-05,
      "loss": 0.5898,
      "step": 34970
    },
    {
      "epoch": 1.3992,
      "grad_norm": 2.482973337173462,
      "learning_rate": 2.6859060402684565e-05,
      "loss": 0.5257,
      "step": 34980
    },
    {
      "epoch": 1.3996,
      "grad_norm": 2.9174702167510986,
      "learning_rate": 2.685234899328859e-05,
      "loss": 0.6202,
      "step": 34990
    },
    {
      "epoch": 1.4,
      "grad_norm": 2.2551753520965576,
      "learning_rate": 2.6845637583892618e-05,
      "loss": 0.5379,
      "step": 35000
    },
    {
      "epoch": 1.4003999999999999,
      "grad_norm": 2.578572988510132,
      "learning_rate": 2.6838926174496647e-05,
      "loss": 0.5519,
      "step": 35010
    },
    {
      "epoch": 1.4008,
      "grad_norm": 2.2947447299957275,
      "learning_rate": 2.6832214765100672e-05,
      "loss": 0.5376,
      "step": 35020
    },
    {
      "epoch": 1.4012,
      "grad_norm": 2.712967872619629,
      "learning_rate": 2.68255033557047e-05,
      "loss": 0.6003,
      "step": 35030
    },
    {
      "epoch": 1.4016,
      "grad_norm": 2.330397367477417,
      "learning_rate": 2.6818791946308726e-05,
      "loss": 0.548,
      "step": 35040
    },
    {
      "epoch": 1.4020000000000001,
      "grad_norm": 2.4232370853424072,
      "learning_rate": 2.6812080536912754e-05,
      "loss": 0.5031,
      "step": 35050
    },
    {
      "epoch": 1.4024,
      "grad_norm": 3.1827592849731445,
      "learning_rate": 2.6805369127516776e-05,
      "loss": 0.6147,
      "step": 35060
    },
    {
      "epoch": 1.4028,
      "grad_norm": 2.4148671627044678,
      "learning_rate": 2.6798657718120808e-05,
      "loss": 0.6366,
      "step": 35070
    },
    {
      "epoch": 1.4032,
      "grad_norm": 2.6676409244537354,
      "learning_rate": 2.6791946308724837e-05,
      "loss": 0.5724,
      "step": 35080
    },
    {
      "epoch": 1.4036,
      "grad_norm": 3.216931104660034,
      "learning_rate": 2.6785234899328858e-05,
      "loss": 0.5924,
      "step": 35090
    },
    {
      "epoch": 1.404,
      "grad_norm": 3.0618889331817627,
      "learning_rate": 2.6778523489932887e-05,
      "loss": 0.6751,
      "step": 35100
    },
    {
      "epoch": 1.4043999999999999,
      "grad_norm": 2.205953598022461,
      "learning_rate": 2.6771812080536912e-05,
      "loss": 0.5268,
      "step": 35110
    },
    {
      "epoch": 1.4048,
      "grad_norm": 2.1502044200897217,
      "learning_rate": 2.676510067114094e-05,
      "loss": 0.5271,
      "step": 35120
    },
    {
      "epoch": 1.4052,
      "grad_norm": 2.194913864135742,
      "learning_rate": 2.6758389261744966e-05,
      "loss": 0.6353,
      "step": 35130
    },
    {
      "epoch": 1.4056,
      "grad_norm": 2.6009280681610107,
      "learning_rate": 2.6751677852348994e-05,
      "loss": 0.5822,
      "step": 35140
    },
    {
      "epoch": 1.4060000000000001,
      "grad_norm": 3.2859768867492676,
      "learning_rate": 2.6744966442953023e-05,
      "loss": 0.6497,
      "step": 35150
    },
    {
      "epoch": 1.4064,
      "grad_norm": 3.1903254985809326,
      "learning_rate": 2.6738255033557048e-05,
      "loss": 0.4967,
      "step": 35160
    },
    {
      "epoch": 1.4068,
      "grad_norm": 2.839989185333252,
      "learning_rate": 2.6731543624161076e-05,
      "loss": 0.5521,
      "step": 35170
    },
    {
      "epoch": 1.4072,
      "grad_norm": 3.037018060684204,
      "learning_rate": 2.6724832214765098e-05,
      "loss": 0.615,
      "step": 35180
    },
    {
      "epoch": 1.4076,
      "grad_norm": 3.481653928756714,
      "learning_rate": 2.671812080536913e-05,
      "loss": 0.5582,
      "step": 35190
    },
    {
      "epoch": 1.408,
      "grad_norm": 2.7122983932495117,
      "learning_rate": 2.671140939597316e-05,
      "loss": 0.5764,
      "step": 35200
    },
    {
      "epoch": 1.4083999999999999,
      "grad_norm": 2.717803955078125,
      "learning_rate": 2.670469798657718e-05,
      "loss": 0.5853,
      "step": 35210
    },
    {
      "epoch": 1.4088,
      "grad_norm": 2.558359384536743,
      "learning_rate": 2.6697986577181212e-05,
      "loss": 0.6218,
      "step": 35220
    },
    {
      "epoch": 1.4092,
      "grad_norm": 2.769988775253296,
      "learning_rate": 2.6691275167785234e-05,
      "loss": 0.5738,
      "step": 35230
    },
    {
      "epoch": 1.4096,
      "grad_norm": 2.5454256534576416,
      "learning_rate": 2.6684563758389263e-05,
      "loss": 0.5393,
      "step": 35240
    },
    {
      "epoch": 1.41,
      "grad_norm": 1.926540493965149,
      "learning_rate": 2.6677852348993288e-05,
      "loss": 0.5645,
      "step": 35250
    },
    {
      "epoch": 1.4104,
      "grad_norm": 3.161022186279297,
      "learning_rate": 2.6671140939597316e-05,
      "loss": 0.545,
      "step": 35260
    },
    {
      "epoch": 1.4108,
      "grad_norm": 2.2041289806365967,
      "learning_rate": 2.6664429530201345e-05,
      "loss": 0.5542,
      "step": 35270
    },
    {
      "epoch": 1.4112,
      "grad_norm": 3.1085591316223145,
      "learning_rate": 2.665771812080537e-05,
      "loss": 0.5521,
      "step": 35280
    },
    {
      "epoch": 1.4116,
      "grad_norm": 2.3958992958068848,
      "learning_rate": 2.66510067114094e-05,
      "loss": 0.5693,
      "step": 35290
    },
    {
      "epoch": 1.412,
      "grad_norm": 2.9360568523406982,
      "learning_rate": 2.6644295302013424e-05,
      "loss": 0.6067,
      "step": 35300
    },
    {
      "epoch": 1.4123999999999999,
      "grad_norm": 2.7631936073303223,
      "learning_rate": 2.6637583892617452e-05,
      "loss": 0.5725,
      "step": 35310
    },
    {
      "epoch": 1.4128,
      "grad_norm": 2.720324993133545,
      "learning_rate": 2.6630872483221474e-05,
      "loss": 0.576,
      "step": 35320
    },
    {
      "epoch": 1.4132,
      "grad_norm": 3.3037660121917725,
      "learning_rate": 2.6624161073825503e-05,
      "loss": 0.604,
      "step": 35330
    },
    {
      "epoch": 1.4136,
      "grad_norm": 2.920347213745117,
      "learning_rate": 2.6617449664429535e-05,
      "loss": 0.5441,
      "step": 35340
    },
    {
      "epoch": 1.414,
      "grad_norm": 2.1680963039398193,
      "learning_rate": 2.6610738255033556e-05,
      "loss": 0.5864,
      "step": 35350
    },
    {
      "epoch": 1.4144,
      "grad_norm": 2.870677947998047,
      "learning_rate": 2.6604026845637585e-05,
      "loss": 0.4997,
      "step": 35360
    },
    {
      "epoch": 1.4148,
      "grad_norm": 2.992478847503662,
      "learning_rate": 2.659731543624161e-05,
      "loss": 0.567,
      "step": 35370
    },
    {
      "epoch": 1.4152,
      "grad_norm": 2.557982921600342,
      "learning_rate": 2.659060402684564e-05,
      "loss": 0.6045,
      "step": 35380
    },
    {
      "epoch": 1.4156,
      "grad_norm": 3.07393217086792,
      "learning_rate": 2.6583892617449667e-05,
      "loss": 0.63,
      "step": 35390
    },
    {
      "epoch": 1.416,
      "grad_norm": 2.523709297180176,
      "learning_rate": 2.6577181208053692e-05,
      "loss": 0.5582,
      "step": 35400
    },
    {
      "epoch": 1.4163999999999999,
      "grad_norm": 2.733161449432373,
      "learning_rate": 2.657046979865772e-05,
      "loss": 0.5993,
      "step": 35410
    },
    {
      "epoch": 1.4168,
      "grad_norm": 2.741288661956787,
      "learning_rate": 2.6563758389261746e-05,
      "loss": 0.5162,
      "step": 35420
    },
    {
      "epoch": 1.4172,
      "grad_norm": 2.1032774448394775,
      "learning_rate": 2.6557046979865775e-05,
      "loss": 0.5438,
      "step": 35430
    },
    {
      "epoch": 1.4176,
      "grad_norm": 2.246156692504883,
      "learning_rate": 2.6550335570469796e-05,
      "loss": 0.5427,
      "step": 35440
    },
    {
      "epoch": 1.418,
      "grad_norm": 3.143313407897949,
      "learning_rate": 2.654362416107383e-05,
      "loss": 0.6758,
      "step": 35450
    },
    {
      "epoch": 1.4184,
      "grad_norm": 3.4986560344696045,
      "learning_rate": 2.6536912751677857e-05,
      "loss": 0.6449,
      "step": 35460
    },
    {
      "epoch": 1.4188,
      "grad_norm": 2.3271987438201904,
      "learning_rate": 2.653020134228188e-05,
      "loss": 0.6765,
      "step": 35470
    },
    {
      "epoch": 1.4192,
      "grad_norm": 2.400935173034668,
      "learning_rate": 2.6523489932885907e-05,
      "loss": 0.5796,
      "step": 35480
    },
    {
      "epoch": 1.4196,
      "grad_norm": 2.342604160308838,
      "learning_rate": 2.6516778523489932e-05,
      "loss": 0.5894,
      "step": 35490
    },
    {
      "epoch": 1.42,
      "grad_norm": 2.6063196659088135,
      "learning_rate": 2.651006711409396e-05,
      "loss": 0.5614,
      "step": 35500
    },
    {
      "epoch": 1.4203999999999999,
      "grad_norm": 2.6495816707611084,
      "learning_rate": 2.6503355704697986e-05,
      "loss": 0.6009,
      "step": 35510
    },
    {
      "epoch": 1.4208,
      "grad_norm": 2.9372427463531494,
      "learning_rate": 2.6496644295302015e-05,
      "loss": 0.5747,
      "step": 35520
    },
    {
      "epoch": 1.4212,
      "grad_norm": 2.433185338973999,
      "learning_rate": 2.6489932885906043e-05,
      "loss": 0.5066,
      "step": 35530
    },
    {
      "epoch": 1.4216,
      "grad_norm": 2.6853983402252197,
      "learning_rate": 2.648322147651007e-05,
      "loss": 0.5467,
      "step": 35540
    },
    {
      "epoch": 1.422,
      "grad_norm": 2.644815683364868,
      "learning_rate": 2.6476510067114097e-05,
      "loss": 0.6264,
      "step": 35550
    },
    {
      "epoch": 1.4224,
      "grad_norm": 3.3513693809509277,
      "learning_rate": 2.646979865771812e-05,
      "loss": 0.5707,
      "step": 35560
    },
    {
      "epoch": 1.4228,
      "grad_norm": 2.10626220703125,
      "learning_rate": 2.646308724832215e-05,
      "loss": 0.4775,
      "step": 35570
    },
    {
      "epoch": 1.4232,
      "grad_norm": 3.7738540172576904,
      "learning_rate": 2.645637583892618e-05,
      "loss": 0.5817,
      "step": 35580
    },
    {
      "epoch": 1.4236,
      "grad_norm": 2.5716171264648438,
      "learning_rate": 2.64496644295302e-05,
      "loss": 0.4906,
      "step": 35590
    },
    {
      "epoch": 1.424,
      "grad_norm": 1.8958836793899536,
      "learning_rate": 2.6442953020134233e-05,
      "loss": 0.5085,
      "step": 35600
    },
    {
      "epoch": 1.4243999999999999,
      "grad_norm": 3.2555761337280273,
      "learning_rate": 2.6436241610738255e-05,
      "loss": 0.6269,
      "step": 35610
    },
    {
      "epoch": 1.4248,
      "grad_norm": 2.7517237663269043,
      "learning_rate": 2.6429530201342283e-05,
      "loss": 0.5462,
      "step": 35620
    },
    {
      "epoch": 1.4252,
      "grad_norm": 2.858560085296631,
      "learning_rate": 2.642281879194631e-05,
      "loss": 0.5132,
      "step": 35630
    },
    {
      "epoch": 1.4256,
      "grad_norm": 3.0833067893981934,
      "learning_rate": 2.6416107382550337e-05,
      "loss": 0.5268,
      "step": 35640
    },
    {
      "epoch": 1.426,
      "grad_norm": 2.4705440998077393,
      "learning_rate": 2.6409395973154365e-05,
      "loss": 0.6085,
      "step": 35650
    },
    {
      "epoch": 1.4264000000000001,
      "grad_norm": 2.6396148204803467,
      "learning_rate": 2.640268456375839e-05,
      "loss": 0.5364,
      "step": 35660
    },
    {
      "epoch": 1.4268,
      "grad_norm": 2.728290319442749,
      "learning_rate": 2.639597315436242e-05,
      "loss": 0.5884,
      "step": 35670
    },
    {
      "epoch": 1.4272,
      "grad_norm": 2.3013405799865723,
      "learning_rate": 2.638926174496644e-05,
      "loss": 0.4953,
      "step": 35680
    },
    {
      "epoch": 1.4276,
      "grad_norm": 2.539273262023926,
      "learning_rate": 2.6382550335570473e-05,
      "loss": 0.5759,
      "step": 35690
    },
    {
      "epoch": 1.428,
      "grad_norm": 2.5708799362182617,
      "learning_rate": 2.6375838926174495e-05,
      "loss": 0.5286,
      "step": 35700
    },
    {
      "epoch": 1.4284,
      "grad_norm": 2.5235188007354736,
      "learning_rate": 2.6369127516778523e-05,
      "loss": 0.587,
      "step": 35710
    },
    {
      "epoch": 1.4288,
      "grad_norm": 3.188103675842285,
      "learning_rate": 2.6362416107382555e-05,
      "loss": 0.5849,
      "step": 35720
    },
    {
      "epoch": 1.4292,
      "grad_norm": 2.595041275024414,
      "learning_rate": 2.6355704697986577e-05,
      "loss": 0.5182,
      "step": 35730
    },
    {
      "epoch": 1.4296,
      "grad_norm": 2.678737163543701,
      "learning_rate": 2.6348993288590605e-05,
      "loss": 0.5687,
      "step": 35740
    },
    {
      "epoch": 1.43,
      "grad_norm": 3.1612484455108643,
      "learning_rate": 2.634228187919463e-05,
      "loss": 0.6003,
      "step": 35750
    },
    {
      "epoch": 1.4304000000000001,
      "grad_norm": 2.677351474761963,
      "learning_rate": 2.633557046979866e-05,
      "loss": 0.492,
      "step": 35760
    },
    {
      "epoch": 1.4308,
      "grad_norm": 2.7499468326568604,
      "learning_rate": 2.6328859060402688e-05,
      "loss": 0.5695,
      "step": 35770
    },
    {
      "epoch": 1.4312,
      "grad_norm": 2.420618772506714,
      "learning_rate": 2.6322147651006713e-05,
      "loss": 0.5964,
      "step": 35780
    },
    {
      "epoch": 1.4316,
      "grad_norm": 3.9956369400024414,
      "learning_rate": 2.631543624161074e-05,
      "loss": 0.5785,
      "step": 35790
    },
    {
      "epoch": 1.432,
      "grad_norm": 3.254347324371338,
      "learning_rate": 2.6308724832214767e-05,
      "loss": 0.5521,
      "step": 35800
    },
    {
      "epoch": 1.4324,
      "grad_norm": 2.6574721336364746,
      "learning_rate": 2.6302013422818795e-05,
      "loss": 0.6805,
      "step": 35810
    },
    {
      "epoch": 1.4328,
      "grad_norm": 2.2407143115997314,
      "learning_rate": 2.6295302013422817e-05,
      "loss": 0.5091,
      "step": 35820
    },
    {
      "epoch": 1.4332,
      "grad_norm": 2.7222955226898193,
      "learning_rate": 2.6288590604026845e-05,
      "loss": 0.5294,
      "step": 35830
    },
    {
      "epoch": 1.4336,
      "grad_norm": 2.133875608444214,
      "learning_rate": 2.6281879194630877e-05,
      "loss": 0.5848,
      "step": 35840
    },
    {
      "epoch": 1.434,
      "grad_norm": 2.591592311859131,
      "learning_rate": 2.62751677852349e-05,
      "loss": 0.5562,
      "step": 35850
    },
    {
      "epoch": 1.4344000000000001,
      "grad_norm": 2.4334893226623535,
      "learning_rate": 2.6268456375838928e-05,
      "loss": 0.5524,
      "step": 35860
    },
    {
      "epoch": 1.4348,
      "grad_norm": 2.0812346935272217,
      "learning_rate": 2.6261744966442953e-05,
      "loss": 0.5884,
      "step": 35870
    },
    {
      "epoch": 1.4352,
      "grad_norm": 2.9426136016845703,
      "learning_rate": 2.625503355704698e-05,
      "loss": 0.5306,
      "step": 35880
    },
    {
      "epoch": 1.4356,
      "grad_norm": 2.2070045471191406,
      "learning_rate": 2.6248322147651007e-05,
      "loss": 0.4666,
      "step": 35890
    },
    {
      "epoch": 1.436,
      "grad_norm": 2.1998047828674316,
      "learning_rate": 2.6241610738255035e-05,
      "loss": 0.506,
      "step": 35900
    },
    {
      "epoch": 1.4364,
      "grad_norm": 2.5840680599212646,
      "learning_rate": 2.6234899328859064e-05,
      "loss": 0.4836,
      "step": 35910
    },
    {
      "epoch": 1.4368,
      "grad_norm": 3.0636472702026367,
      "learning_rate": 2.622818791946309e-05,
      "loss": 0.5621,
      "step": 35920
    },
    {
      "epoch": 1.4372,
      "grad_norm": 2.594674825668335,
      "learning_rate": 2.6221476510067117e-05,
      "loss": 0.6182,
      "step": 35930
    },
    {
      "epoch": 1.4376,
      "grad_norm": 2.803065538406372,
      "learning_rate": 2.621476510067114e-05,
      "loss": 0.5915,
      "step": 35940
    },
    {
      "epoch": 1.438,
      "grad_norm": 3.1538615226745605,
      "learning_rate": 2.620805369127517e-05,
      "loss": 0.5734,
      "step": 35950
    },
    {
      "epoch": 1.4384000000000001,
      "grad_norm": 2.948133945465088,
      "learning_rate": 2.6201342281879193e-05,
      "loss": 0.6012,
      "step": 35960
    },
    {
      "epoch": 1.4388,
      "grad_norm": 2.3725640773773193,
      "learning_rate": 2.619463087248322e-05,
      "loss": 0.5891,
      "step": 35970
    },
    {
      "epoch": 1.4392,
      "grad_norm": 2.9423563480377197,
      "learning_rate": 2.618791946308725e-05,
      "loss": 0.6448,
      "step": 35980
    },
    {
      "epoch": 1.4396,
      "grad_norm": 2.8245599269866943,
      "learning_rate": 2.6181208053691275e-05,
      "loss": 0.5889,
      "step": 35990
    },
    {
      "epoch": 1.44,
      "grad_norm": 3.227966785430908,
      "learning_rate": 2.6174496644295304e-05,
      "loss": 0.5911,
      "step": 36000
    },
    {
      "epoch": 1.4404,
      "grad_norm": 2.034250020980835,
      "learning_rate": 2.616778523489933e-05,
      "loss": 0.536,
      "step": 36010
    },
    {
      "epoch": 1.4408,
      "grad_norm": 2.5329737663269043,
      "learning_rate": 2.6161073825503357e-05,
      "loss": 0.4736,
      "step": 36020
    },
    {
      "epoch": 1.4412,
      "grad_norm": 2.7773396968841553,
      "learning_rate": 2.6154362416107386e-05,
      "loss": 0.5568,
      "step": 36030
    },
    {
      "epoch": 1.4416,
      "grad_norm": 2.646273374557495,
      "learning_rate": 2.614765100671141e-05,
      "loss": 0.531,
      "step": 36040
    },
    {
      "epoch": 1.442,
      "grad_norm": 2.1383845806121826,
      "learning_rate": 2.614093959731544e-05,
      "loss": 0.5824,
      "step": 36050
    },
    {
      "epoch": 1.4424000000000001,
      "grad_norm": 3.105398178100586,
      "learning_rate": 2.613422818791946e-05,
      "loss": 0.5065,
      "step": 36060
    },
    {
      "epoch": 1.4428,
      "grad_norm": 2.620455026626587,
      "learning_rate": 2.6127516778523493e-05,
      "loss": 0.567,
      "step": 36070
    },
    {
      "epoch": 1.4432,
      "grad_norm": 2.4134531021118164,
      "learning_rate": 2.6120805369127515e-05,
      "loss": 0.4719,
      "step": 36080
    },
    {
      "epoch": 1.4436,
      "grad_norm": 2.5797033309936523,
      "learning_rate": 2.6114093959731544e-05,
      "loss": 0.5665,
      "step": 36090
    },
    {
      "epoch": 1.444,
      "grad_norm": 2.933306932449341,
      "learning_rate": 2.6107382550335576e-05,
      "loss": 0.5775,
      "step": 36100
    },
    {
      "epoch": 1.4444,
      "grad_norm": 2.9407424926757812,
      "learning_rate": 2.6100671140939597e-05,
      "loss": 0.5963,
      "step": 36110
    },
    {
      "epoch": 1.4447999999999999,
      "grad_norm": 2.2337241172790527,
      "learning_rate": 2.6093959731543626e-05,
      "loss": 0.527,
      "step": 36120
    },
    {
      "epoch": 1.4452,
      "grad_norm": 2.7636163234710693,
      "learning_rate": 2.608724832214765e-05,
      "loss": 0.6421,
      "step": 36130
    },
    {
      "epoch": 1.4456,
      "grad_norm": 2.4245805740356445,
      "learning_rate": 2.608053691275168e-05,
      "loss": 0.4844,
      "step": 36140
    },
    {
      "epoch": 1.446,
      "grad_norm": 2.7548274993896484,
      "learning_rate": 2.6073825503355705e-05,
      "loss": 0.6176,
      "step": 36150
    },
    {
      "epoch": 1.4464000000000001,
      "grad_norm": 2.7972936630249023,
      "learning_rate": 2.6067114093959733e-05,
      "loss": 0.5507,
      "step": 36160
    },
    {
      "epoch": 1.4468,
      "grad_norm": 2.9106035232543945,
      "learning_rate": 2.6060402684563762e-05,
      "loss": 0.4461,
      "step": 36170
    },
    {
      "epoch": 1.4472,
      "grad_norm": 2.218921184539795,
      "learning_rate": 2.6053691275167787e-05,
      "loss": 0.5428,
      "step": 36180
    },
    {
      "epoch": 1.4476,
      "grad_norm": 2.699341058731079,
      "learning_rate": 2.6046979865771816e-05,
      "loss": 0.5215,
      "step": 36190
    },
    {
      "epoch": 1.448,
      "grad_norm": 3.4278993606567383,
      "learning_rate": 2.6040268456375837e-05,
      "loss": 0.6273,
      "step": 36200
    },
    {
      "epoch": 1.4484,
      "grad_norm": 2.69577956199646,
      "learning_rate": 2.6033557046979866e-05,
      "loss": 0.4886,
      "step": 36210
    },
    {
      "epoch": 1.4487999999999999,
      "grad_norm": 2.592609405517578,
      "learning_rate": 2.6026845637583898e-05,
      "loss": 0.5156,
      "step": 36220
    },
    {
      "epoch": 1.4492,
      "grad_norm": 3.0227279663085938,
      "learning_rate": 2.602013422818792e-05,
      "loss": 0.5376,
      "step": 36230
    },
    {
      "epoch": 1.4496,
      "grad_norm": 2.752723455429077,
      "learning_rate": 2.6013422818791948e-05,
      "loss": 0.6904,
      "step": 36240
    },
    {
      "epoch": 1.45,
      "grad_norm": 3.262756824493408,
      "learning_rate": 2.6006711409395973e-05,
      "loss": 0.6545,
      "step": 36250
    },
    {
      "epoch": 1.4504000000000001,
      "grad_norm": 2.9271557331085205,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 0.5858,
      "step": 36260
    },
    {
      "epoch": 1.4508,
      "grad_norm": 3.090057373046875,
      "learning_rate": 2.5993288590604027e-05,
      "loss": 0.5532,
      "step": 36270
    },
    {
      "epoch": 1.4512,
      "grad_norm": 3.227517604827881,
      "learning_rate": 2.5986577181208055e-05,
      "loss": 0.5521,
      "step": 36280
    },
    {
      "epoch": 1.4516,
      "grad_norm": 3.1592719554901123,
      "learning_rate": 2.5979865771812084e-05,
      "loss": 0.5532,
      "step": 36290
    },
    {
      "epoch": 1.452,
      "grad_norm": 3.2237093448638916,
      "learning_rate": 2.597315436241611e-05,
      "loss": 0.5498,
      "step": 36300
    },
    {
      "epoch": 1.4524,
      "grad_norm": 3.3650102615356445,
      "learning_rate": 2.5966442953020138e-05,
      "loss": 0.5194,
      "step": 36310
    },
    {
      "epoch": 1.4527999999999999,
      "grad_norm": 2.3378124237060547,
      "learning_rate": 2.595973154362416e-05,
      "loss": 0.5774,
      "step": 36320
    },
    {
      "epoch": 1.4532,
      "grad_norm": 3.2040863037109375,
      "learning_rate": 2.595302013422819e-05,
      "loss": 0.5936,
      "step": 36330
    },
    {
      "epoch": 1.4536,
      "grad_norm": 2.0499002933502197,
      "learning_rate": 2.5946308724832213e-05,
      "loss": 0.5243,
      "step": 36340
    },
    {
      "epoch": 1.454,
      "grad_norm": 3.213324785232544,
      "learning_rate": 2.5939597315436242e-05,
      "loss": 0.5261,
      "step": 36350
    },
    {
      "epoch": 1.4544000000000001,
      "grad_norm": 2.877056837081909,
      "learning_rate": 2.593288590604027e-05,
      "loss": 0.5634,
      "step": 36360
    },
    {
      "epoch": 1.4548,
      "grad_norm": 2.8727149963378906,
      "learning_rate": 2.5926174496644295e-05,
      "loss": 0.4844,
      "step": 36370
    },
    {
      "epoch": 1.4552,
      "grad_norm": 2.7308053970336914,
      "learning_rate": 2.5919463087248324e-05,
      "loss": 0.4785,
      "step": 36380
    },
    {
      "epoch": 1.4556,
      "grad_norm": 3.195993661880493,
      "learning_rate": 2.591275167785235e-05,
      "loss": 0.5336,
      "step": 36390
    },
    {
      "epoch": 1.456,
      "grad_norm": 3.2930331230163574,
      "learning_rate": 2.5906040268456378e-05,
      "loss": 0.5529,
      "step": 36400
    },
    {
      "epoch": 1.4564,
      "grad_norm": 3.693255662918091,
      "learning_rate": 2.5899328859060406e-05,
      "loss": 0.598,
      "step": 36410
    },
    {
      "epoch": 1.4567999999999999,
      "grad_norm": 1.9049161672592163,
      "learning_rate": 2.589261744966443e-05,
      "loss": 0.5147,
      "step": 36420
    },
    {
      "epoch": 1.4572,
      "grad_norm": 2.4841713905334473,
      "learning_rate": 2.588590604026846e-05,
      "loss": 0.5096,
      "step": 36430
    },
    {
      "epoch": 1.4576,
      "grad_norm": 3.2409942150115967,
      "learning_rate": 2.5879194630872482e-05,
      "loss": 0.5321,
      "step": 36440
    },
    {
      "epoch": 1.458,
      "grad_norm": 2.976515054702759,
      "learning_rate": 2.5872483221476514e-05,
      "loss": 0.57,
      "step": 36450
    },
    {
      "epoch": 1.4584,
      "grad_norm": 3.572202444076538,
      "learning_rate": 2.5865771812080535e-05,
      "loss": 0.5412,
      "step": 36460
    },
    {
      "epoch": 1.4588,
      "grad_norm": 3.4166407585144043,
      "learning_rate": 2.5859060402684564e-05,
      "loss": 0.5346,
      "step": 36470
    },
    {
      "epoch": 1.4592,
      "grad_norm": 2.780485153198242,
      "learning_rate": 2.5852348993288596e-05,
      "loss": 0.5418,
      "step": 36480
    },
    {
      "epoch": 1.4596,
      "grad_norm": 3.299661874771118,
      "learning_rate": 2.5845637583892618e-05,
      "loss": 0.6278,
      "step": 36490
    },
    {
      "epoch": 1.46,
      "grad_norm": 3.349745273590088,
      "learning_rate": 2.5838926174496646e-05,
      "loss": 0.5995,
      "step": 36500
    },
    {
      "epoch": 1.4604,
      "grad_norm": 2.3562352657318115,
      "learning_rate": 2.583221476510067e-05,
      "loss": 0.5848,
      "step": 36510
    },
    {
      "epoch": 1.4607999999999999,
      "grad_norm": 3.137176513671875,
      "learning_rate": 2.58255033557047e-05,
      "loss": 0.5751,
      "step": 36520
    },
    {
      "epoch": 1.4612,
      "grad_norm": 2.4878876209259033,
      "learning_rate": 2.5818791946308725e-05,
      "loss": 0.6423,
      "step": 36530
    },
    {
      "epoch": 1.4616,
      "grad_norm": 2.0488388538360596,
      "learning_rate": 2.5812080536912754e-05,
      "loss": 0.566,
      "step": 36540
    },
    {
      "epoch": 1.462,
      "grad_norm": 2.4067890644073486,
      "learning_rate": 2.5805369127516782e-05,
      "loss": 0.6061,
      "step": 36550
    },
    {
      "epoch": 1.4624,
      "grad_norm": 3.091836452484131,
      "learning_rate": 2.5798657718120804e-05,
      "loss": 0.5863,
      "step": 36560
    },
    {
      "epoch": 1.4628,
      "grad_norm": 2.8622424602508545,
      "learning_rate": 2.5791946308724836e-05,
      "loss": 0.5423,
      "step": 36570
    },
    {
      "epoch": 1.4632,
      "grad_norm": 3.078680992126465,
      "learning_rate": 2.5785234899328858e-05,
      "loss": 0.5989,
      "step": 36580
    },
    {
      "epoch": 1.4636,
      "grad_norm": 2.4410274028778076,
      "learning_rate": 2.5778523489932886e-05,
      "loss": 0.5018,
      "step": 36590
    },
    {
      "epoch": 1.464,
      "grad_norm": 2.661849021911621,
      "learning_rate": 2.5771812080536918e-05,
      "loss": 0.6034,
      "step": 36600
    },
    {
      "epoch": 1.4644,
      "grad_norm": 2.7388792037963867,
      "learning_rate": 2.576510067114094e-05,
      "loss": 0.558,
      "step": 36610
    },
    {
      "epoch": 1.4647999999999999,
      "grad_norm": 2.4948861598968506,
      "learning_rate": 2.575838926174497e-05,
      "loss": 0.4902,
      "step": 36620
    },
    {
      "epoch": 1.4652,
      "grad_norm": 2.956343412399292,
      "learning_rate": 2.5751677852348994e-05,
      "loss": 0.6488,
      "step": 36630
    },
    {
      "epoch": 1.4656,
      "grad_norm": 2.9997074604034424,
      "learning_rate": 2.5744966442953022e-05,
      "loss": 0.6306,
      "step": 36640
    },
    {
      "epoch": 1.466,
      "grad_norm": 2.5732390880584717,
      "learning_rate": 2.5738255033557047e-05,
      "loss": 0.5438,
      "step": 36650
    },
    {
      "epoch": 1.4664,
      "grad_norm": 2.442680597305298,
      "learning_rate": 2.5731543624161076e-05,
      "loss": 0.4957,
      "step": 36660
    },
    {
      "epoch": 1.4668,
      "grad_norm": 3.236173152923584,
      "learning_rate": 2.5724832214765104e-05,
      "loss": 0.6346,
      "step": 36670
    },
    {
      "epoch": 1.4672,
      "grad_norm": 3.13169002532959,
      "learning_rate": 2.571812080536913e-05,
      "loss": 0.5138,
      "step": 36680
    },
    {
      "epoch": 1.4676,
      "grad_norm": 2.7528235912323,
      "learning_rate": 2.5711409395973158e-05,
      "loss": 0.4894,
      "step": 36690
    },
    {
      "epoch": 1.468,
      "grad_norm": 3.238039970397949,
      "learning_rate": 2.570469798657718e-05,
      "loss": 0.5101,
      "step": 36700
    },
    {
      "epoch": 1.4684,
      "grad_norm": 2.3669583797454834,
      "learning_rate": 2.569798657718121e-05,
      "loss": 0.605,
      "step": 36710
    },
    {
      "epoch": 1.4687999999999999,
      "grad_norm": 3.0188755989074707,
      "learning_rate": 2.5691275167785234e-05,
      "loss": 0.5871,
      "step": 36720
    },
    {
      "epoch": 1.4692,
      "grad_norm": 2.58939790725708,
      "learning_rate": 2.5684563758389262e-05,
      "loss": 0.5605,
      "step": 36730
    },
    {
      "epoch": 1.4696,
      "grad_norm": 2.6737136840820312,
      "learning_rate": 2.567785234899329e-05,
      "loss": 0.5965,
      "step": 36740
    },
    {
      "epoch": 1.47,
      "grad_norm": 2.7875354290008545,
      "learning_rate": 2.5671140939597316e-05,
      "loss": 0.5173,
      "step": 36750
    },
    {
      "epoch": 1.4704,
      "grad_norm": 2.8723134994506836,
      "learning_rate": 2.5664429530201344e-05,
      "loss": 0.5553,
      "step": 36760
    },
    {
      "epoch": 1.4708,
      "grad_norm": 2.583167552947998,
      "learning_rate": 2.565771812080537e-05,
      "loss": 0.6018,
      "step": 36770
    },
    {
      "epoch": 1.4712,
      "grad_norm": 2.5236947536468506,
      "learning_rate": 2.5651006711409398e-05,
      "loss": 0.5106,
      "step": 36780
    },
    {
      "epoch": 1.4716,
      "grad_norm": 3.1424005031585693,
      "learning_rate": 2.564429530201342e-05,
      "loss": 0.6462,
      "step": 36790
    },
    {
      "epoch": 1.472,
      "grad_norm": 2.834322929382324,
      "learning_rate": 2.5637583892617452e-05,
      "loss": 0.5786,
      "step": 36800
    },
    {
      "epoch": 1.4724,
      "grad_norm": 2.6453795433044434,
      "learning_rate": 2.563087248322148e-05,
      "loss": 0.56,
      "step": 36810
    },
    {
      "epoch": 1.4727999999999999,
      "grad_norm": 2.7428572177886963,
      "learning_rate": 2.5624161073825502e-05,
      "loss": 0.5455,
      "step": 36820
    },
    {
      "epoch": 1.4732,
      "grad_norm": 2.583994150161743,
      "learning_rate": 2.5617449664429534e-05,
      "loss": 0.594,
      "step": 36830
    },
    {
      "epoch": 1.4736,
      "grad_norm": 1.8002547025680542,
      "learning_rate": 2.5610738255033556e-05,
      "loss": 0.5447,
      "step": 36840
    },
    {
      "epoch": 1.474,
      "grad_norm": 2.895000696182251,
      "learning_rate": 2.5604026845637584e-05,
      "loss": 0.5713,
      "step": 36850
    },
    {
      "epoch": 1.4744,
      "grad_norm": 2.8514351844787598,
      "learning_rate": 2.5597315436241613e-05,
      "loss": 0.5769,
      "step": 36860
    },
    {
      "epoch": 1.4748,
      "grad_norm": 2.048725128173828,
      "learning_rate": 2.5590604026845638e-05,
      "loss": 0.5307,
      "step": 36870
    },
    {
      "epoch": 1.4752,
      "grad_norm": 2.769104242324829,
      "learning_rate": 2.5583892617449667e-05,
      "loss": 0.5347,
      "step": 36880
    },
    {
      "epoch": 1.4756,
      "grad_norm": 3.116734743118286,
      "learning_rate": 2.5577181208053692e-05,
      "loss": 0.6149,
      "step": 36890
    },
    {
      "epoch": 1.476,
      "grad_norm": 2.386364221572876,
      "learning_rate": 2.557046979865772e-05,
      "loss": 0.5381,
      "step": 36900
    },
    {
      "epoch": 1.4764,
      "grad_norm": 2.3295726776123047,
      "learning_rate": 2.5563758389261742e-05,
      "loss": 0.5807,
      "step": 36910
    },
    {
      "epoch": 1.4768,
      "grad_norm": 2.3729398250579834,
      "learning_rate": 2.5557046979865774e-05,
      "loss": 0.5837,
      "step": 36920
    },
    {
      "epoch": 1.4772,
      "grad_norm": 2.8296146392822266,
      "learning_rate": 2.5550335570469803e-05,
      "loss": 0.5915,
      "step": 36930
    },
    {
      "epoch": 1.4776,
      "grad_norm": 2.471320867538452,
      "learning_rate": 2.5543624161073824e-05,
      "loss": 0.5433,
      "step": 36940
    },
    {
      "epoch": 1.478,
      "grad_norm": 2.5838751792907715,
      "learning_rate": 2.5536912751677856e-05,
      "loss": 0.6298,
      "step": 36950
    },
    {
      "epoch": 1.4784,
      "grad_norm": 3.439619779586792,
      "learning_rate": 2.5530201342281878e-05,
      "loss": 0.5703,
      "step": 36960
    },
    {
      "epoch": 1.4788000000000001,
      "grad_norm": 2.553189754486084,
      "learning_rate": 2.5523489932885907e-05,
      "loss": 0.5646,
      "step": 36970
    },
    {
      "epoch": 1.4792,
      "grad_norm": 1.8370928764343262,
      "learning_rate": 2.5516778523489932e-05,
      "loss": 0.5145,
      "step": 36980
    },
    {
      "epoch": 1.4796,
      "grad_norm": 2.916912317276001,
      "learning_rate": 2.551006711409396e-05,
      "loss": 0.5943,
      "step": 36990
    },
    {
      "epoch": 1.48,
      "grad_norm": 2.8876752853393555,
      "learning_rate": 2.550335570469799e-05,
      "loss": 0.5873,
      "step": 37000
    },
    {
      "epoch": 1.4804,
      "grad_norm": 3.537280797958374,
      "learning_rate": 2.5496644295302014e-05,
      "loss": 0.6485,
      "step": 37010
    },
    {
      "epoch": 1.4808,
      "grad_norm": 2.8223392963409424,
      "learning_rate": 2.5489932885906043e-05,
      "loss": 0.5893,
      "step": 37020
    },
    {
      "epoch": 1.4812,
      "grad_norm": 2.9722044467926025,
      "learning_rate": 2.5483221476510068e-05,
      "loss": 0.6303,
      "step": 37030
    },
    {
      "epoch": 1.4816,
      "grad_norm": 2.4250986576080322,
      "learning_rate": 2.5476510067114096e-05,
      "loss": 0.5627,
      "step": 37040
    },
    {
      "epoch": 1.482,
      "grad_norm": 1.8323719501495361,
      "learning_rate": 2.5469798657718125e-05,
      "loss": 0.507,
      "step": 37050
    },
    {
      "epoch": 1.4824,
      "grad_norm": 2.1440467834472656,
      "learning_rate": 2.5463087248322147e-05,
      "loss": 0.5722,
      "step": 37060
    },
    {
      "epoch": 1.4828000000000001,
      "grad_norm": 2.5678493976593018,
      "learning_rate": 2.545637583892618e-05,
      "loss": 0.5748,
      "step": 37070
    },
    {
      "epoch": 1.4832,
      "grad_norm": 2.4381139278411865,
      "learning_rate": 2.54496644295302e-05,
      "loss": 0.5155,
      "step": 37080
    },
    {
      "epoch": 1.4836,
      "grad_norm": 2.7247366905212402,
      "learning_rate": 2.544295302013423e-05,
      "loss": 0.4807,
      "step": 37090
    },
    {
      "epoch": 1.484,
      "grad_norm": 2.703946590423584,
      "learning_rate": 2.5436241610738254e-05,
      "loss": 0.5501,
      "step": 37100
    },
    {
      "epoch": 1.4844,
      "grad_norm": 1.905004620552063,
      "learning_rate": 2.5429530201342283e-05,
      "loss": 0.7145,
      "step": 37110
    },
    {
      "epoch": 1.4848,
      "grad_norm": 2.106916666030884,
      "learning_rate": 2.542281879194631e-05,
      "loss": 0.5755,
      "step": 37120
    },
    {
      "epoch": 1.4852,
      "grad_norm": 2.6328351497650146,
      "learning_rate": 2.5416107382550336e-05,
      "loss": 0.5456,
      "step": 37130
    },
    {
      "epoch": 1.4856,
      "grad_norm": 2.9123666286468506,
      "learning_rate": 2.5409395973154365e-05,
      "loss": 0.6752,
      "step": 37140
    },
    {
      "epoch": 1.486,
      "grad_norm": 2.9308881759643555,
      "learning_rate": 2.540268456375839e-05,
      "loss": 0.5785,
      "step": 37150
    },
    {
      "epoch": 1.4864,
      "grad_norm": 2.869265079498291,
      "learning_rate": 2.539597315436242e-05,
      "loss": 0.55,
      "step": 37160
    },
    {
      "epoch": 1.4868000000000001,
      "grad_norm": 2.7924351692199707,
      "learning_rate": 2.538926174496644e-05,
      "loss": 0.4922,
      "step": 37170
    },
    {
      "epoch": 1.4872,
      "grad_norm": 2.862611770629883,
      "learning_rate": 2.5382550335570472e-05,
      "loss": 0.4923,
      "step": 37180
    },
    {
      "epoch": 1.4876,
      "grad_norm": 2.813065767288208,
      "learning_rate": 2.53758389261745e-05,
      "loss": 0.5554,
      "step": 37190
    },
    {
      "epoch": 1.488,
      "grad_norm": 3.0344815254211426,
      "learning_rate": 2.5369127516778523e-05,
      "loss": 0.6021,
      "step": 37200
    },
    {
      "epoch": 1.4884,
      "grad_norm": 2.8841593265533447,
      "learning_rate": 2.536241610738255e-05,
      "loss": 0.6213,
      "step": 37210
    },
    {
      "epoch": 1.4888,
      "grad_norm": 3.0777714252471924,
      "learning_rate": 2.5355704697986576e-05,
      "loss": 0.6378,
      "step": 37220
    },
    {
      "epoch": 1.4892,
      "grad_norm": 3.1595048904418945,
      "learning_rate": 2.5348993288590605e-05,
      "loss": 0.5644,
      "step": 37230
    },
    {
      "epoch": 1.4896,
      "grad_norm": 2.91270112991333,
      "learning_rate": 2.5342281879194633e-05,
      "loss": 0.5994,
      "step": 37240
    },
    {
      "epoch": 1.49,
      "grad_norm": 3.093432664871216,
      "learning_rate": 2.533557046979866e-05,
      "loss": 0.5788,
      "step": 37250
    },
    {
      "epoch": 1.4904,
      "grad_norm": 2.490532159805298,
      "learning_rate": 2.5328859060402687e-05,
      "loss": 0.5622,
      "step": 37260
    },
    {
      "epoch": 1.4908000000000001,
      "grad_norm": 2.525907516479492,
      "learning_rate": 2.5322147651006712e-05,
      "loss": 0.6242,
      "step": 37270
    },
    {
      "epoch": 1.4912,
      "grad_norm": 2.7414638996124268,
      "learning_rate": 2.531543624161074e-05,
      "loss": 0.5368,
      "step": 37280
    },
    {
      "epoch": 1.4916,
      "grad_norm": 3.7276558876037598,
      "learning_rate": 2.5308724832214763e-05,
      "loss": 0.5866,
      "step": 37290
    },
    {
      "epoch": 1.492,
      "grad_norm": 2.660886764526367,
      "learning_rate": 2.5302013422818795e-05,
      "loss": 0.6174,
      "step": 37300
    },
    {
      "epoch": 1.4924,
      "grad_norm": 2.668645143508911,
      "learning_rate": 2.5295302013422823e-05,
      "loss": 0.5871,
      "step": 37310
    },
    {
      "epoch": 1.4928,
      "grad_norm": 3.040041446685791,
      "learning_rate": 2.5288590604026845e-05,
      "loss": 0.561,
      "step": 37320
    },
    {
      "epoch": 1.4932,
      "grad_norm": 2.220400810241699,
      "learning_rate": 2.5281879194630877e-05,
      "loss": 0.6087,
      "step": 37330
    },
    {
      "epoch": 1.4936,
      "grad_norm": 3.0435667037963867,
      "learning_rate": 2.52751677852349e-05,
      "loss": 0.5089,
      "step": 37340
    },
    {
      "epoch": 1.494,
      "grad_norm": 2.396512508392334,
      "learning_rate": 2.5268456375838927e-05,
      "loss": 0.5365,
      "step": 37350
    },
    {
      "epoch": 1.4944,
      "grad_norm": 3.173161268234253,
      "learning_rate": 2.5261744966442952e-05,
      "loss": 0.625,
      "step": 37360
    },
    {
      "epoch": 1.4948000000000001,
      "grad_norm": 3.1050655841827393,
      "learning_rate": 2.525503355704698e-05,
      "loss": 0.4707,
      "step": 37370
    },
    {
      "epoch": 1.4952,
      "grad_norm": 2.3381154537200928,
      "learning_rate": 2.524832214765101e-05,
      "loss": 0.5029,
      "step": 37380
    },
    {
      "epoch": 1.4956,
      "grad_norm": 2.611171007156372,
      "learning_rate": 2.5241610738255034e-05,
      "loss": 0.5215,
      "step": 37390
    },
    {
      "epoch": 1.496,
      "grad_norm": 2.6950151920318604,
      "learning_rate": 2.5234899328859063e-05,
      "loss": 0.562,
      "step": 37400
    },
    {
      "epoch": 1.4964,
      "grad_norm": 2.537557601928711,
      "learning_rate": 2.5228187919463088e-05,
      "loss": 0.5068,
      "step": 37410
    },
    {
      "epoch": 1.4968,
      "grad_norm": 2.14274001121521,
      "learning_rate": 2.5221476510067117e-05,
      "loss": 0.6777,
      "step": 37420
    },
    {
      "epoch": 1.4971999999999999,
      "grad_norm": 3.0807437896728516,
      "learning_rate": 2.521476510067114e-05,
      "loss": 0.5865,
      "step": 37430
    },
    {
      "epoch": 1.4976,
      "grad_norm": 2.6821744441986084,
      "learning_rate": 2.5208053691275167e-05,
      "loss": 0.5193,
      "step": 37440
    },
    {
      "epoch": 1.498,
      "grad_norm": 2.764157772064209,
      "learning_rate": 2.52013422818792e-05,
      "loss": 0.5535,
      "step": 37450
    },
    {
      "epoch": 1.4984,
      "grad_norm": 2.981377363204956,
      "learning_rate": 2.519463087248322e-05,
      "loss": 0.5848,
      "step": 37460
    },
    {
      "epoch": 1.4988000000000001,
      "grad_norm": 2.6571078300476074,
      "learning_rate": 2.518791946308725e-05,
      "loss": 0.5821,
      "step": 37470
    },
    {
      "epoch": 1.4992,
      "grad_norm": 4.067335605621338,
      "learning_rate": 2.5181208053691274e-05,
      "loss": 0.5934,
      "step": 37480
    },
    {
      "epoch": 1.4996,
      "grad_norm": 3.2474193572998047,
      "learning_rate": 2.5174496644295303e-05,
      "loss": 0.4983,
      "step": 37490
    },
    {
      "epoch": 1.5,
      "grad_norm": 2.790346145629883,
      "learning_rate": 2.516778523489933e-05,
      "loss": 0.5275,
      "step": 37500
    },
    {
      "epoch": 1.5004,
      "grad_norm": 2.5030651092529297,
      "learning_rate": 2.5161073825503357e-05,
      "loss": 0.5649,
      "step": 37510
    },
    {
      "epoch": 1.5008,
      "grad_norm": 3.5515148639678955,
      "learning_rate": 2.5154362416107385e-05,
      "loss": 0.6338,
      "step": 37520
    },
    {
      "epoch": 1.5011999999999999,
      "grad_norm": 3.1775074005126953,
      "learning_rate": 2.514765100671141e-05,
      "loss": 0.6847,
      "step": 37530
    },
    {
      "epoch": 1.5016,
      "grad_norm": 2.6866824626922607,
      "learning_rate": 2.514093959731544e-05,
      "loss": 0.5593,
      "step": 37540
    },
    {
      "epoch": 1.502,
      "grad_norm": 2.5395984649658203,
      "learning_rate": 2.513422818791946e-05,
      "loss": 0.6041,
      "step": 37550
    },
    {
      "epoch": 1.5024,
      "grad_norm": 2.367567539215088,
      "learning_rate": 2.5127516778523493e-05,
      "loss": 0.4982,
      "step": 37560
    },
    {
      "epoch": 1.5028000000000001,
      "grad_norm": 2.703603982925415,
      "learning_rate": 2.512080536912752e-05,
      "loss": 0.5949,
      "step": 37570
    },
    {
      "epoch": 1.5032,
      "grad_norm": 2.4476115703582764,
      "learning_rate": 2.5114093959731543e-05,
      "loss": 0.4954,
      "step": 37580
    },
    {
      "epoch": 1.5036,
      "grad_norm": 1.9845610857009888,
      "learning_rate": 2.510738255033557e-05,
      "loss": 0.5688,
      "step": 37590
    },
    {
      "epoch": 1.504,
      "grad_norm": 2.480929136276245,
      "learning_rate": 2.5100671140939597e-05,
      "loss": 0.5354,
      "step": 37600
    },
    {
      "epoch": 1.5044,
      "grad_norm": 2.701751470565796,
      "learning_rate": 2.5093959731543625e-05,
      "loss": 0.5755,
      "step": 37610
    },
    {
      "epoch": 1.5048,
      "grad_norm": 2.631992816925049,
      "learning_rate": 2.508724832214765e-05,
      "loss": 0.5355,
      "step": 37620
    },
    {
      "epoch": 1.5051999999999999,
      "grad_norm": 2.4979166984558105,
      "learning_rate": 2.508053691275168e-05,
      "loss": 0.5206,
      "step": 37630
    },
    {
      "epoch": 1.5056,
      "grad_norm": 2.9956870079040527,
      "learning_rate": 2.5073825503355708e-05,
      "loss": 0.4442,
      "step": 37640
    },
    {
      "epoch": 1.506,
      "grad_norm": 2.4454774856567383,
      "learning_rate": 2.5067114093959733e-05,
      "loss": 0.542,
      "step": 37650
    },
    {
      "epoch": 1.5064,
      "grad_norm": 2.9033501148223877,
      "learning_rate": 2.506040268456376e-05,
      "loss": 0.644,
      "step": 37660
    },
    {
      "epoch": 1.5068000000000001,
      "grad_norm": 2.4071731567382812,
      "learning_rate": 2.5053691275167783e-05,
      "loss": 0.478,
      "step": 37670
    },
    {
      "epoch": 1.5072,
      "grad_norm": 2.3279929161071777,
      "learning_rate": 2.5046979865771815e-05,
      "loss": 0.5083,
      "step": 37680
    },
    {
      "epoch": 1.5076,
      "grad_norm": 2.7637414932250977,
      "learning_rate": 2.5040268456375843e-05,
      "loss": 0.604,
      "step": 37690
    },
    {
      "epoch": 1.508,
      "grad_norm": 2.981034994125366,
      "learning_rate": 2.5033557046979865e-05,
      "loss": 0.5406,
      "step": 37700
    },
    {
      "epoch": 1.5084,
      "grad_norm": 2.6616857051849365,
      "learning_rate": 2.5026845637583897e-05,
      "loss": 0.5422,
      "step": 37710
    },
    {
      "epoch": 1.5088,
      "grad_norm": 3.3797428607940674,
      "learning_rate": 2.502013422818792e-05,
      "loss": 0.5007,
      "step": 37720
    },
    {
      "epoch": 1.5091999999999999,
      "grad_norm": 2.471907377243042,
      "learning_rate": 2.5013422818791947e-05,
      "loss": 0.5479,
      "step": 37730
    },
    {
      "epoch": 1.5096,
      "grad_norm": 1.9472872018814087,
      "learning_rate": 2.5006711409395973e-05,
      "loss": 0.539,
      "step": 37740
    },
    {
      "epoch": 1.51,
      "grad_norm": 3.3022775650024414,
      "learning_rate": 2.5e-05,
      "loss": 0.5727,
      "step": 37750
    },
    {
      "epoch": 1.5104,
      "grad_norm": 1.8349192142486572,
      "learning_rate": 2.4993288590604026e-05,
      "loss": 0.5677,
      "step": 37760
    },
    {
      "epoch": 1.5108000000000001,
      "grad_norm": 3.2127304077148438,
      "learning_rate": 2.4986577181208055e-05,
      "loss": 0.5823,
      "step": 37770
    },
    {
      "epoch": 1.5112,
      "grad_norm": 2.7353358268737793,
      "learning_rate": 2.497986577181208e-05,
      "loss": 0.526,
      "step": 37780
    },
    {
      "epoch": 1.5116,
      "grad_norm": 2.3015971183776855,
      "learning_rate": 2.497315436241611e-05,
      "loss": 0.5654,
      "step": 37790
    },
    {
      "epoch": 1.512,
      "grad_norm": 2.972079277038574,
      "learning_rate": 2.4966442953020137e-05,
      "loss": 0.5219,
      "step": 37800
    },
    {
      "epoch": 1.5124,
      "grad_norm": 2.2328972816467285,
      "learning_rate": 2.4959731543624162e-05,
      "loss": 0.5247,
      "step": 37810
    },
    {
      "epoch": 1.5128,
      "grad_norm": 3.086045265197754,
      "learning_rate": 2.4953020134228187e-05,
      "loss": 0.5529,
      "step": 37820
    },
    {
      "epoch": 1.5131999999999999,
      "grad_norm": 2.1044209003448486,
      "learning_rate": 2.4946308724832216e-05,
      "loss": 0.552,
      "step": 37830
    },
    {
      "epoch": 1.5135999999999998,
      "grad_norm": 2.811302423477173,
      "learning_rate": 2.493959731543624e-05,
      "loss": 0.533,
      "step": 37840
    },
    {
      "epoch": 1.514,
      "grad_norm": 2.962022066116333,
      "learning_rate": 2.493288590604027e-05,
      "loss": 0.5273,
      "step": 37850
    },
    {
      "epoch": 1.5144,
      "grad_norm": 2.260281801223755,
      "learning_rate": 2.4926174496644298e-05,
      "loss": 0.4987,
      "step": 37860
    },
    {
      "epoch": 1.5148000000000001,
      "grad_norm": 3.425356864929199,
      "learning_rate": 2.4919463087248323e-05,
      "loss": 0.4756,
      "step": 37870
    },
    {
      "epoch": 1.5152,
      "grad_norm": 2.9555795192718506,
      "learning_rate": 2.491275167785235e-05,
      "loss": 0.4913,
      "step": 37880
    },
    {
      "epoch": 1.5156,
      "grad_norm": 2.766569137573242,
      "learning_rate": 2.4906040268456377e-05,
      "loss": 0.5765,
      "step": 37890
    },
    {
      "epoch": 1.516,
      "grad_norm": 2.198807954788208,
      "learning_rate": 2.4899328859060402e-05,
      "loss": 0.5418,
      "step": 37900
    },
    {
      "epoch": 1.5164,
      "grad_norm": 2.602555990219116,
      "learning_rate": 2.489261744966443e-05,
      "loss": 0.6011,
      "step": 37910
    },
    {
      "epoch": 1.5168,
      "grad_norm": 3.234424114227295,
      "learning_rate": 2.488590604026846e-05,
      "loss": 0.5939,
      "step": 37920
    },
    {
      "epoch": 1.5171999999999999,
      "grad_norm": 3.371812582015991,
      "learning_rate": 2.4879194630872485e-05,
      "loss": 0.5965,
      "step": 37930
    },
    {
      "epoch": 1.5175999999999998,
      "grad_norm": 2.488518714904785,
      "learning_rate": 2.487248322147651e-05,
      "loss": 0.5802,
      "step": 37940
    },
    {
      "epoch": 1.518,
      "grad_norm": 2.9139492511749268,
      "learning_rate": 2.4865771812080538e-05,
      "loss": 0.5953,
      "step": 37950
    },
    {
      "epoch": 1.5184,
      "grad_norm": 2.833035469055176,
      "learning_rate": 2.4859060402684563e-05,
      "loss": 0.5408,
      "step": 37960
    },
    {
      "epoch": 1.5188000000000001,
      "grad_norm": 2.6838700771331787,
      "learning_rate": 2.4852348993288592e-05,
      "loss": 0.563,
      "step": 37970
    },
    {
      "epoch": 1.5192,
      "grad_norm": 2.587299346923828,
      "learning_rate": 2.484563758389262e-05,
      "loss": 0.5051,
      "step": 37980
    },
    {
      "epoch": 1.5196,
      "grad_norm": 2.4749791622161865,
      "learning_rate": 2.4838926174496646e-05,
      "loss": 0.5777,
      "step": 37990
    },
    {
      "epoch": 1.52,
      "grad_norm": 3.097701072692871,
      "learning_rate": 2.4832214765100674e-05,
      "loss": 0.6046,
      "step": 38000
    },
    {
      "epoch": 1.5204,
      "grad_norm": 2.7627370357513428,
      "learning_rate": 2.48255033557047e-05,
      "loss": 0.5697,
      "step": 38010
    },
    {
      "epoch": 1.5208,
      "grad_norm": 2.8210041522979736,
      "learning_rate": 2.4818791946308725e-05,
      "loss": 0.4752,
      "step": 38020
    },
    {
      "epoch": 1.5211999999999999,
      "grad_norm": 2.5211734771728516,
      "learning_rate": 2.4812080536912753e-05,
      "loss": 0.6183,
      "step": 38030
    },
    {
      "epoch": 1.5215999999999998,
      "grad_norm": 2.6232988834381104,
      "learning_rate": 2.480536912751678e-05,
      "loss": 0.5104,
      "step": 38040
    },
    {
      "epoch": 1.522,
      "grad_norm": 2.7750205993652344,
      "learning_rate": 2.4798657718120807e-05,
      "loss": 0.5847,
      "step": 38050
    },
    {
      "epoch": 1.5224,
      "grad_norm": 2.4456279277801514,
      "learning_rate": 2.4791946308724835e-05,
      "loss": 0.5873,
      "step": 38060
    },
    {
      "epoch": 1.5228000000000002,
      "grad_norm": 2.927738666534424,
      "learning_rate": 2.478523489932886e-05,
      "loss": 0.5707,
      "step": 38070
    },
    {
      "epoch": 1.5232,
      "grad_norm": 3.1558659076690674,
      "learning_rate": 2.4778523489932886e-05,
      "loss": 0.5727,
      "step": 38080
    },
    {
      "epoch": 1.5236,
      "grad_norm": 3.591953992843628,
      "learning_rate": 2.4771812080536914e-05,
      "loss": 0.6798,
      "step": 38090
    },
    {
      "epoch": 1.524,
      "grad_norm": 2.958219528198242,
      "learning_rate": 2.476510067114094e-05,
      "loss": 0.5866,
      "step": 38100
    },
    {
      "epoch": 1.5244,
      "grad_norm": 2.527710437774658,
      "learning_rate": 2.4758389261744968e-05,
      "loss": 0.6235,
      "step": 38110
    },
    {
      "epoch": 1.5248,
      "grad_norm": 2.5534000396728516,
      "learning_rate": 2.4751677852348996e-05,
      "loss": 0.5181,
      "step": 38120
    },
    {
      "epoch": 1.5252,
      "grad_norm": 2.333871603012085,
      "learning_rate": 2.474496644295302e-05,
      "loss": 0.5654,
      "step": 38130
    },
    {
      "epoch": 1.5255999999999998,
      "grad_norm": 2.5426878929138184,
      "learning_rate": 2.4738255033557047e-05,
      "loss": 0.4829,
      "step": 38140
    },
    {
      "epoch": 1.526,
      "grad_norm": 2.7160027027130127,
      "learning_rate": 2.4731543624161075e-05,
      "loss": 0.6076,
      "step": 38150
    },
    {
      "epoch": 1.5264,
      "grad_norm": 3.4048290252685547,
      "learning_rate": 2.47248322147651e-05,
      "loss": 0.5444,
      "step": 38160
    },
    {
      "epoch": 1.5268000000000002,
      "grad_norm": 3.4330742359161377,
      "learning_rate": 2.471812080536913e-05,
      "loss": 0.5342,
      "step": 38170
    },
    {
      "epoch": 1.5272000000000001,
      "grad_norm": 2.9822797775268555,
      "learning_rate": 2.4711409395973158e-05,
      "loss": 0.579,
      "step": 38180
    },
    {
      "epoch": 1.5276,
      "grad_norm": 2.8747920989990234,
      "learning_rate": 2.4704697986577183e-05,
      "loss": 0.543,
      "step": 38190
    },
    {
      "epoch": 1.528,
      "grad_norm": 2.475930690765381,
      "learning_rate": 2.4697986577181208e-05,
      "loss": 0.5977,
      "step": 38200
    },
    {
      "epoch": 1.5284,
      "grad_norm": 3.84147572517395,
      "learning_rate": 2.4691275167785236e-05,
      "loss": 0.6471,
      "step": 38210
    },
    {
      "epoch": 1.5288,
      "grad_norm": 3.1640427112579346,
      "learning_rate": 2.468456375838926e-05,
      "loss": 0.5458,
      "step": 38220
    },
    {
      "epoch": 1.5292,
      "grad_norm": 2.5705721378326416,
      "learning_rate": 2.467785234899329e-05,
      "loss": 0.6183,
      "step": 38230
    },
    {
      "epoch": 1.5295999999999998,
      "grad_norm": 2.2813899517059326,
      "learning_rate": 2.467114093959732e-05,
      "loss": 0.5018,
      "step": 38240
    },
    {
      "epoch": 1.53,
      "grad_norm": 2.55203914642334,
      "learning_rate": 2.4664429530201344e-05,
      "loss": 0.5214,
      "step": 38250
    },
    {
      "epoch": 1.5304,
      "grad_norm": 2.7907400131225586,
      "learning_rate": 2.465771812080537e-05,
      "loss": 0.6195,
      "step": 38260
    },
    {
      "epoch": 1.5308000000000002,
      "grad_norm": 3.598001003265381,
      "learning_rate": 2.4651006711409398e-05,
      "loss": 0.5952,
      "step": 38270
    },
    {
      "epoch": 1.5312000000000001,
      "grad_norm": 2.607992172241211,
      "learning_rate": 2.4644295302013423e-05,
      "loss": 0.6576,
      "step": 38280
    },
    {
      "epoch": 1.5316,
      "grad_norm": 3.0003976821899414,
      "learning_rate": 2.463758389261745e-05,
      "loss": 0.6017,
      "step": 38290
    },
    {
      "epoch": 1.532,
      "grad_norm": 2.450432062149048,
      "learning_rate": 2.463087248322148e-05,
      "loss": 0.6075,
      "step": 38300
    },
    {
      "epoch": 1.5324,
      "grad_norm": 2.369159460067749,
      "learning_rate": 2.4624161073825505e-05,
      "loss": 0.6322,
      "step": 38310
    },
    {
      "epoch": 1.5328,
      "grad_norm": 3.122053861618042,
      "learning_rate": 2.461744966442953e-05,
      "loss": 0.5639,
      "step": 38320
    },
    {
      "epoch": 1.5332,
      "grad_norm": 2.858243227005005,
      "learning_rate": 2.461073825503356e-05,
      "loss": 0.6205,
      "step": 38330
    },
    {
      "epoch": 1.5335999999999999,
      "grad_norm": 3.472639322280884,
      "learning_rate": 2.4604026845637584e-05,
      "loss": 0.586,
      "step": 38340
    },
    {
      "epoch": 1.534,
      "grad_norm": 2.799459218978882,
      "learning_rate": 2.4597315436241612e-05,
      "loss": 0.5741,
      "step": 38350
    },
    {
      "epoch": 1.5344,
      "grad_norm": 2.422492504119873,
      "learning_rate": 2.459060402684564e-05,
      "loss": 0.4978,
      "step": 38360
    },
    {
      "epoch": 1.5348000000000002,
      "grad_norm": 2.717952013015747,
      "learning_rate": 2.4583892617449666e-05,
      "loss": 0.557,
      "step": 38370
    },
    {
      "epoch": 1.5352000000000001,
      "grad_norm": 2.5551486015319824,
      "learning_rate": 2.457718120805369e-05,
      "loss": 0.6287,
      "step": 38380
    },
    {
      "epoch": 1.5356,
      "grad_norm": 2.7004735469818115,
      "learning_rate": 2.457046979865772e-05,
      "loss": 0.5281,
      "step": 38390
    },
    {
      "epoch": 1.536,
      "grad_norm": 3.201381206512451,
      "learning_rate": 2.4563758389261745e-05,
      "loss": 0.65,
      "step": 38400
    },
    {
      "epoch": 1.5364,
      "grad_norm": 2.6205639839172363,
      "learning_rate": 2.4557046979865773e-05,
      "loss": 0.5159,
      "step": 38410
    },
    {
      "epoch": 1.5368,
      "grad_norm": 2.4986045360565186,
      "learning_rate": 2.45503355704698e-05,
      "loss": 0.5568,
      "step": 38420
    },
    {
      "epoch": 1.5372,
      "grad_norm": 2.412958860397339,
      "learning_rate": 2.4543624161073827e-05,
      "loss": 0.5906,
      "step": 38430
    },
    {
      "epoch": 1.5375999999999999,
      "grad_norm": 2.7758543491363525,
      "learning_rate": 2.4536912751677856e-05,
      "loss": 0.5778,
      "step": 38440
    },
    {
      "epoch": 1.538,
      "grad_norm": 3.151141881942749,
      "learning_rate": 2.453020134228188e-05,
      "loss": 0.5537,
      "step": 38450
    },
    {
      "epoch": 1.5384,
      "grad_norm": 3.025355577468872,
      "learning_rate": 2.4523489932885906e-05,
      "loss": 0.4896,
      "step": 38460
    },
    {
      "epoch": 1.5388,
      "grad_norm": 3.559069871902466,
      "learning_rate": 2.4516778523489935e-05,
      "loss": 0.6289,
      "step": 38470
    },
    {
      "epoch": 1.5392000000000001,
      "grad_norm": 2.3747429847717285,
      "learning_rate": 2.451006711409396e-05,
      "loss": 0.5375,
      "step": 38480
    },
    {
      "epoch": 1.5396,
      "grad_norm": 2.8461318016052246,
      "learning_rate": 2.450335570469799e-05,
      "loss": 0.4996,
      "step": 38490
    },
    {
      "epoch": 1.54,
      "grad_norm": 2.582630157470703,
      "learning_rate": 2.4496644295302017e-05,
      "loss": 0.5424,
      "step": 38500
    },
    {
      "epoch": 1.5404,
      "grad_norm": 2.8065102100372314,
      "learning_rate": 2.4489932885906042e-05,
      "loss": 0.6108,
      "step": 38510
    },
    {
      "epoch": 1.5408,
      "grad_norm": 2.615813970565796,
      "learning_rate": 2.4483221476510067e-05,
      "loss": 0.5718,
      "step": 38520
    },
    {
      "epoch": 1.5412,
      "grad_norm": 2.9710605144500732,
      "learning_rate": 2.4476510067114096e-05,
      "loss": 0.5776,
      "step": 38530
    },
    {
      "epoch": 1.5415999999999999,
      "grad_norm": 1.9539529085159302,
      "learning_rate": 2.446979865771812e-05,
      "loss": 0.6195,
      "step": 38540
    },
    {
      "epoch": 1.542,
      "grad_norm": 2.9900355339050293,
      "learning_rate": 2.446308724832215e-05,
      "loss": 0.5828,
      "step": 38550
    },
    {
      "epoch": 1.5424,
      "grad_norm": 2.78928542137146,
      "learning_rate": 2.4456375838926178e-05,
      "loss": 0.5506,
      "step": 38560
    },
    {
      "epoch": 1.5428,
      "grad_norm": 2.262523889541626,
      "learning_rate": 2.4449664429530203e-05,
      "loss": 0.5252,
      "step": 38570
    },
    {
      "epoch": 1.5432000000000001,
      "grad_norm": 2.531486749649048,
      "learning_rate": 2.444295302013423e-05,
      "loss": 0.5252,
      "step": 38580
    },
    {
      "epoch": 1.5436,
      "grad_norm": 2.392003297805786,
      "learning_rate": 2.4436241610738257e-05,
      "loss": 0.4882,
      "step": 38590
    },
    {
      "epoch": 1.544,
      "grad_norm": 3.258025884628296,
      "learning_rate": 2.4429530201342282e-05,
      "loss": 0.5817,
      "step": 38600
    },
    {
      "epoch": 1.5444,
      "grad_norm": 2.7856619358062744,
      "learning_rate": 2.4422818791946307e-05,
      "loss": 0.5641,
      "step": 38610
    },
    {
      "epoch": 1.5448,
      "grad_norm": 2.9184582233428955,
      "learning_rate": 2.441610738255034e-05,
      "loss": 0.5662,
      "step": 38620
    },
    {
      "epoch": 1.5452,
      "grad_norm": 2.5946996212005615,
      "learning_rate": 2.4409395973154364e-05,
      "loss": 0.5813,
      "step": 38630
    },
    {
      "epoch": 1.5455999999999999,
      "grad_norm": 2.8156721591949463,
      "learning_rate": 2.440268456375839e-05,
      "loss": 0.6157,
      "step": 38640
    },
    {
      "epoch": 1.546,
      "grad_norm": 3.3642420768737793,
      "learning_rate": 2.4395973154362418e-05,
      "loss": 0.6547,
      "step": 38650
    },
    {
      "epoch": 1.5464,
      "grad_norm": 2.7471799850463867,
      "learning_rate": 2.4389261744966443e-05,
      "loss": 0.5358,
      "step": 38660
    },
    {
      "epoch": 1.5468,
      "grad_norm": 2.8332278728485107,
      "learning_rate": 2.4382550335570468e-05,
      "loss": 0.6555,
      "step": 38670
    },
    {
      "epoch": 1.5472000000000001,
      "grad_norm": 2.6305904388427734,
      "learning_rate": 2.43758389261745e-05,
      "loss": 0.5684,
      "step": 38680
    },
    {
      "epoch": 1.5476,
      "grad_norm": 2.346090793609619,
      "learning_rate": 2.4369127516778525e-05,
      "loss": 0.5426,
      "step": 38690
    },
    {
      "epoch": 1.548,
      "grad_norm": 3.0279839038848877,
      "learning_rate": 2.436241610738255e-05,
      "loss": 0.5592,
      "step": 38700
    },
    {
      "epoch": 1.5484,
      "grad_norm": 2.087559223175049,
      "learning_rate": 2.435570469798658e-05,
      "loss": 0.5798,
      "step": 38710
    },
    {
      "epoch": 1.5488,
      "grad_norm": 2.6168782711029053,
      "learning_rate": 2.4348993288590604e-05,
      "loss": 0.545,
      "step": 38720
    },
    {
      "epoch": 1.5492,
      "grad_norm": 3.3856091499328613,
      "learning_rate": 2.434228187919463e-05,
      "loss": 0.5026,
      "step": 38730
    },
    {
      "epoch": 1.5495999999999999,
      "grad_norm": 3.7397046089172363,
      "learning_rate": 2.4335570469798658e-05,
      "loss": 0.5845,
      "step": 38740
    },
    {
      "epoch": 1.55,
      "grad_norm": 2.601886034011841,
      "learning_rate": 2.4328859060402687e-05,
      "loss": 0.605,
      "step": 38750
    },
    {
      "epoch": 1.5504,
      "grad_norm": 3.3463873863220215,
      "learning_rate": 2.432214765100671e-05,
      "loss": 0.558,
      "step": 38760
    },
    {
      "epoch": 1.5508,
      "grad_norm": 2.560420274734497,
      "learning_rate": 2.431543624161074e-05,
      "loss": 0.5432,
      "step": 38770
    },
    {
      "epoch": 1.5512000000000001,
      "grad_norm": 2.4091272354125977,
      "learning_rate": 2.4308724832214765e-05,
      "loss": 0.6594,
      "step": 38780
    },
    {
      "epoch": 1.5516,
      "grad_norm": 2.380720615386963,
      "learning_rate": 2.4302013422818794e-05,
      "loss": 0.5892,
      "step": 38790
    },
    {
      "epoch": 1.552,
      "grad_norm": 3.1049718856811523,
      "learning_rate": 2.429530201342282e-05,
      "loss": 0.5675,
      "step": 38800
    },
    {
      "epoch": 1.5524,
      "grad_norm": 3.008061170578003,
      "learning_rate": 2.4288590604026848e-05,
      "loss": 0.5395,
      "step": 38810
    },
    {
      "epoch": 1.5528,
      "grad_norm": 2.704399824142456,
      "learning_rate": 2.4281879194630873e-05,
      "loss": 0.5554,
      "step": 38820
    },
    {
      "epoch": 1.5532,
      "grad_norm": 2.9821701049804688,
      "learning_rate": 2.42751677852349e-05,
      "loss": 0.6414,
      "step": 38830
    },
    {
      "epoch": 1.5535999999999999,
      "grad_norm": 3.2163498401641846,
      "learning_rate": 2.4268456375838926e-05,
      "loss": 0.6618,
      "step": 38840
    },
    {
      "epoch": 1.554,
      "grad_norm": 3.323793649673462,
      "learning_rate": 2.4261744966442955e-05,
      "loss": 0.5997,
      "step": 38850
    },
    {
      "epoch": 1.5544,
      "grad_norm": 3.0748517513275146,
      "learning_rate": 2.425503355704698e-05,
      "loss": 0.6988,
      "step": 38860
    },
    {
      "epoch": 1.5548,
      "grad_norm": 2.219923257827759,
      "learning_rate": 2.424832214765101e-05,
      "loss": 0.5655,
      "step": 38870
    },
    {
      "epoch": 1.5552000000000001,
      "grad_norm": 3.3293442726135254,
      "learning_rate": 2.4241610738255034e-05,
      "loss": 0.5422,
      "step": 38880
    },
    {
      "epoch": 1.5556,
      "grad_norm": 2.893049478530884,
      "learning_rate": 2.4234899328859062e-05,
      "loss": 0.5684,
      "step": 38890
    },
    {
      "epoch": 1.556,
      "grad_norm": 2.314378261566162,
      "learning_rate": 2.4228187919463088e-05,
      "loss": 0.5855,
      "step": 38900
    },
    {
      "epoch": 1.5564,
      "grad_norm": 2.7244791984558105,
      "learning_rate": 2.4221476510067116e-05,
      "loss": 0.548,
      "step": 38910
    },
    {
      "epoch": 1.5568,
      "grad_norm": 2.678618907928467,
      "learning_rate": 2.421476510067114e-05,
      "loss": 0.6202,
      "step": 38920
    },
    {
      "epoch": 1.5572,
      "grad_norm": 2.64030385017395,
      "learning_rate": 2.4208053691275166e-05,
      "loss": 0.53,
      "step": 38930
    },
    {
      "epoch": 1.5575999999999999,
      "grad_norm": 2.0420548915863037,
      "learning_rate": 2.42013422818792e-05,
      "loss": 0.5725,
      "step": 38940
    },
    {
      "epoch": 1.558,
      "grad_norm": 2.1494617462158203,
      "learning_rate": 2.4194630872483224e-05,
      "loss": 0.555,
      "step": 38950
    },
    {
      "epoch": 1.5584,
      "grad_norm": 2.906529664993286,
      "learning_rate": 2.418791946308725e-05,
      "loss": 0.6224,
      "step": 38960
    },
    {
      "epoch": 1.5588,
      "grad_norm": 3.1070237159729004,
      "learning_rate": 2.4181208053691277e-05,
      "loss": 0.5526,
      "step": 38970
    },
    {
      "epoch": 1.5592000000000001,
      "grad_norm": 2.249626636505127,
      "learning_rate": 2.4174496644295302e-05,
      "loss": 0.4965,
      "step": 38980
    },
    {
      "epoch": 1.5596,
      "grad_norm": 3.403726816177368,
      "learning_rate": 2.4167785234899328e-05,
      "loss": 0.6023,
      "step": 38990
    },
    {
      "epoch": 1.56,
      "grad_norm": 3.2876839637756348,
      "learning_rate": 2.416107382550336e-05,
      "loss": 0.596,
      "step": 39000
    },
    {
      "epoch": 1.5604,
      "grad_norm": 2.639159917831421,
      "learning_rate": 2.4154362416107385e-05,
      "loss": 0.5332,
      "step": 39010
    },
    {
      "epoch": 1.5608,
      "grad_norm": 3.10111927986145,
      "learning_rate": 2.414765100671141e-05,
      "loss": 0.5868,
      "step": 39020
    },
    {
      "epoch": 1.5612,
      "grad_norm": 1.6275585889816284,
      "learning_rate": 2.414093959731544e-05,
      "loss": 0.5278,
      "step": 39030
    },
    {
      "epoch": 1.5615999999999999,
      "grad_norm": 2.9833784103393555,
      "learning_rate": 2.4134228187919464e-05,
      "loss": 0.6265,
      "step": 39040
    },
    {
      "epoch": 1.562,
      "grad_norm": 2.155838966369629,
      "learning_rate": 2.412751677852349e-05,
      "loss": 0.4727,
      "step": 39050
    },
    {
      "epoch": 1.5624,
      "grad_norm": 2.3808159828186035,
      "learning_rate": 2.412080536912752e-05,
      "loss": 0.5279,
      "step": 39060
    },
    {
      "epoch": 1.5628,
      "grad_norm": 2.7206380367279053,
      "learning_rate": 2.4114093959731546e-05,
      "loss": 0.699,
      "step": 39070
    },
    {
      "epoch": 1.5632000000000001,
      "grad_norm": 2.9776437282562256,
      "learning_rate": 2.410738255033557e-05,
      "loss": 0.5711,
      "step": 39080
    },
    {
      "epoch": 1.5636,
      "grad_norm": 3.521542549133301,
      "learning_rate": 2.41006711409396e-05,
      "loss": 0.6157,
      "step": 39090
    },
    {
      "epoch": 1.564,
      "grad_norm": 2.2380306720733643,
      "learning_rate": 2.4093959731543625e-05,
      "loss": 0.6629,
      "step": 39100
    },
    {
      "epoch": 1.5644,
      "grad_norm": 2.7366464138031006,
      "learning_rate": 2.408724832214765e-05,
      "loss": 0.5694,
      "step": 39110
    },
    {
      "epoch": 1.5648,
      "grad_norm": 2.6956582069396973,
      "learning_rate": 2.408053691275168e-05,
      "loss": 0.5958,
      "step": 39120
    },
    {
      "epoch": 1.5652,
      "grad_norm": 3.669156789779663,
      "learning_rate": 2.4073825503355707e-05,
      "loss": 0.6278,
      "step": 39130
    },
    {
      "epoch": 1.5655999999999999,
      "grad_norm": 2.909284830093384,
      "learning_rate": 2.4067114093959732e-05,
      "loss": 0.5766,
      "step": 39140
    },
    {
      "epoch": 1.5659999999999998,
      "grad_norm": 3.565990924835205,
      "learning_rate": 2.406040268456376e-05,
      "loss": 0.6496,
      "step": 39150
    },
    {
      "epoch": 1.5664,
      "grad_norm": 2.4043936729431152,
      "learning_rate": 2.4053691275167786e-05,
      "loss": 0.4887,
      "step": 39160
    },
    {
      "epoch": 1.5668,
      "grad_norm": 2.623319387435913,
      "learning_rate": 2.404697986577181e-05,
      "loss": 0.5188,
      "step": 39170
    },
    {
      "epoch": 1.5672000000000001,
      "grad_norm": 2.2583115100860596,
      "learning_rate": 2.404026845637584e-05,
      "loss": 0.5354,
      "step": 39180
    },
    {
      "epoch": 1.5676,
      "grad_norm": 2.4943485260009766,
      "learning_rate": 2.4033557046979868e-05,
      "loss": 0.5399,
      "step": 39190
    },
    {
      "epoch": 1.568,
      "grad_norm": 2.8423242568969727,
      "learning_rate": 2.4026845637583893e-05,
      "loss": 0.639,
      "step": 39200
    },
    {
      "epoch": 1.5684,
      "grad_norm": 2.6675193309783936,
      "learning_rate": 2.4020134228187922e-05,
      "loss": 0.6003,
      "step": 39210
    },
    {
      "epoch": 1.5688,
      "grad_norm": 2.798827886581421,
      "learning_rate": 2.4013422818791947e-05,
      "loss": 0.5752,
      "step": 39220
    },
    {
      "epoch": 1.5692,
      "grad_norm": 2.225911855697632,
      "learning_rate": 2.4006711409395975e-05,
      "loss": 0.5691,
      "step": 39230
    },
    {
      "epoch": 1.5695999999999999,
      "grad_norm": 2.6626594066619873,
      "learning_rate": 2.4e-05,
      "loss": 0.5797,
      "step": 39240
    },
    {
      "epoch": 1.5699999999999998,
      "grad_norm": 2.32879638671875,
      "learning_rate": 2.3993288590604026e-05,
      "loss": 0.5469,
      "step": 39250
    },
    {
      "epoch": 1.5704,
      "grad_norm": 2.9320757389068604,
      "learning_rate": 2.3986577181208054e-05,
      "loss": 0.5742,
      "step": 39260
    },
    {
      "epoch": 1.5708,
      "grad_norm": 2.451772451400757,
      "learning_rate": 2.3979865771812083e-05,
      "loss": 0.5449,
      "step": 39270
    },
    {
      "epoch": 1.5712000000000002,
      "grad_norm": 4.130728721618652,
      "learning_rate": 2.3973154362416108e-05,
      "loss": 0.5597,
      "step": 39280
    },
    {
      "epoch": 1.5716,
      "grad_norm": 2.496481418609619,
      "learning_rate": 2.3966442953020137e-05,
      "loss": 0.5986,
      "step": 39290
    },
    {
      "epoch": 1.572,
      "grad_norm": 2.975541591644287,
      "learning_rate": 2.3959731543624162e-05,
      "loss": 0.4994,
      "step": 39300
    },
    {
      "epoch": 1.5724,
      "grad_norm": 2.6312248706817627,
      "learning_rate": 2.3953020134228187e-05,
      "loss": 0.5354,
      "step": 39310
    },
    {
      "epoch": 1.5728,
      "grad_norm": 2.8163726329803467,
      "learning_rate": 2.3946308724832215e-05,
      "loss": 0.568,
      "step": 39320
    },
    {
      "epoch": 1.5732,
      "grad_norm": 2.673659563064575,
      "learning_rate": 2.3939597315436244e-05,
      "loss": 0.5827,
      "step": 39330
    },
    {
      "epoch": 1.5735999999999999,
      "grad_norm": 2.9873228073120117,
      "learning_rate": 2.393288590604027e-05,
      "loss": 0.5881,
      "step": 39340
    },
    {
      "epoch": 1.5739999999999998,
      "grad_norm": 2.721719264984131,
      "learning_rate": 2.3926174496644298e-05,
      "loss": 0.492,
      "step": 39350
    },
    {
      "epoch": 1.5744,
      "grad_norm": 2.8127200603485107,
      "learning_rate": 2.3919463087248323e-05,
      "loss": 0.6074,
      "step": 39360
    },
    {
      "epoch": 1.5748,
      "grad_norm": 3.3365674018859863,
      "learning_rate": 2.3912751677852348e-05,
      "loss": 0.6061,
      "step": 39370
    },
    {
      "epoch": 1.5752000000000002,
      "grad_norm": 3.5409247875213623,
      "learning_rate": 2.390604026845638e-05,
      "loss": 0.6797,
      "step": 39380
    },
    {
      "epoch": 1.5756000000000001,
      "grad_norm": 2.978337526321411,
      "learning_rate": 2.3899328859060405e-05,
      "loss": 0.5997,
      "step": 39390
    },
    {
      "epoch": 1.576,
      "grad_norm": 2.558394193649292,
      "learning_rate": 2.389261744966443e-05,
      "loss": 0.551,
      "step": 39400
    },
    {
      "epoch": 1.5764,
      "grad_norm": 3.0551764965057373,
      "learning_rate": 2.388590604026846e-05,
      "loss": 0.5258,
      "step": 39410
    },
    {
      "epoch": 1.5768,
      "grad_norm": 2.890597343444824,
      "learning_rate": 2.3879194630872484e-05,
      "loss": 0.5908,
      "step": 39420
    },
    {
      "epoch": 1.5772,
      "grad_norm": 2.617252826690674,
      "learning_rate": 2.387248322147651e-05,
      "loss": 0.497,
      "step": 39430
    },
    {
      "epoch": 1.5776,
      "grad_norm": 2.788947105407715,
      "learning_rate": 2.3865771812080538e-05,
      "loss": 0.5623,
      "step": 39440
    },
    {
      "epoch": 1.5779999999999998,
      "grad_norm": 2.5144128799438477,
      "learning_rate": 2.3859060402684566e-05,
      "loss": 0.5086,
      "step": 39450
    },
    {
      "epoch": 1.5784,
      "grad_norm": 3.412367582321167,
      "learning_rate": 2.385234899328859e-05,
      "loss": 0.5919,
      "step": 39460
    },
    {
      "epoch": 1.5788,
      "grad_norm": 3.088503837585449,
      "learning_rate": 2.384563758389262e-05,
      "loss": 0.5604,
      "step": 39470
    },
    {
      "epoch": 1.5792000000000002,
      "grad_norm": 3.1225476264953613,
      "learning_rate": 2.3838926174496645e-05,
      "loss": 0.6292,
      "step": 39480
    },
    {
      "epoch": 1.5796000000000001,
      "grad_norm": 2.9676308631896973,
      "learning_rate": 2.383221476510067e-05,
      "loss": 0.5363,
      "step": 39490
    },
    {
      "epoch": 1.58,
      "grad_norm": 2.8965532779693604,
      "learning_rate": 2.38255033557047e-05,
      "loss": 0.6579,
      "step": 39500
    },
    {
      "epoch": 1.5804,
      "grad_norm": 2.347464084625244,
      "learning_rate": 2.3818791946308727e-05,
      "loss": 0.6056,
      "step": 39510
    },
    {
      "epoch": 1.5808,
      "grad_norm": 1.9736964702606201,
      "learning_rate": 2.3812080536912752e-05,
      "loss": 0.5693,
      "step": 39520
    },
    {
      "epoch": 1.5812,
      "grad_norm": 2.3054263591766357,
      "learning_rate": 2.380536912751678e-05,
      "loss": 0.5801,
      "step": 39530
    },
    {
      "epoch": 1.5816,
      "grad_norm": 2.583007574081421,
      "learning_rate": 2.3798657718120806e-05,
      "loss": 0.6551,
      "step": 39540
    },
    {
      "epoch": 1.5819999999999999,
      "grad_norm": 3.3764028549194336,
      "learning_rate": 2.379194630872483e-05,
      "loss": 0.6489,
      "step": 39550
    },
    {
      "epoch": 1.5824,
      "grad_norm": 3.4809532165527344,
      "learning_rate": 2.378523489932886e-05,
      "loss": 0.603,
      "step": 39560
    },
    {
      "epoch": 1.5828,
      "grad_norm": 2.9945366382598877,
      "learning_rate": 2.3778523489932885e-05,
      "loss": 0.5414,
      "step": 39570
    },
    {
      "epoch": 1.5832000000000002,
      "grad_norm": 2.9636502265930176,
      "learning_rate": 2.3771812080536914e-05,
      "loss": 0.617,
      "step": 39580
    },
    {
      "epoch": 1.5836000000000001,
      "grad_norm": 2.362541913986206,
      "learning_rate": 2.3765100671140942e-05,
      "loss": 0.5139,
      "step": 39590
    },
    {
      "epoch": 1.584,
      "grad_norm": 2.227790594100952,
      "learning_rate": 2.3758389261744967e-05,
      "loss": 0.5761,
      "step": 39600
    },
    {
      "epoch": 1.5844,
      "grad_norm": 2.2312285900115967,
      "learning_rate": 2.3751677852348992e-05,
      "loss": 0.6024,
      "step": 39610
    },
    {
      "epoch": 1.5848,
      "grad_norm": 3.2056775093078613,
      "learning_rate": 2.374496644295302e-05,
      "loss": 0.6248,
      "step": 39620
    },
    {
      "epoch": 1.5852,
      "grad_norm": 2.8145811557769775,
      "learning_rate": 2.3738255033557046e-05,
      "loss": 0.5782,
      "step": 39630
    },
    {
      "epoch": 1.5856,
      "grad_norm": 2.2272355556488037,
      "learning_rate": 2.3731543624161075e-05,
      "loss": 0.562,
      "step": 39640
    },
    {
      "epoch": 1.5859999999999999,
      "grad_norm": 2.9752986431121826,
      "learning_rate": 2.3724832214765103e-05,
      "loss": 0.5111,
      "step": 39650
    },
    {
      "epoch": 1.5864,
      "grad_norm": 2.5405755043029785,
      "learning_rate": 2.371812080536913e-05,
      "loss": 0.592,
      "step": 39660
    },
    {
      "epoch": 1.5868,
      "grad_norm": 2.8493311405181885,
      "learning_rate": 2.3711409395973157e-05,
      "loss": 0.5465,
      "step": 39670
    },
    {
      "epoch": 1.5872000000000002,
      "grad_norm": 3.1601226329803467,
      "learning_rate": 2.3704697986577182e-05,
      "loss": 0.6239,
      "step": 39680
    },
    {
      "epoch": 1.5876000000000001,
      "grad_norm": 3.5067269802093506,
      "learning_rate": 2.3697986577181207e-05,
      "loss": 0.5817,
      "step": 39690
    },
    {
      "epoch": 1.588,
      "grad_norm": 3.188338041305542,
      "learning_rate": 2.3691275167785236e-05,
      "loss": 0.7006,
      "step": 39700
    },
    {
      "epoch": 1.5884,
      "grad_norm": 2.5999207496643066,
      "learning_rate": 2.3684563758389264e-05,
      "loss": 0.5335,
      "step": 39710
    },
    {
      "epoch": 1.5888,
      "grad_norm": 2.565582752227783,
      "learning_rate": 2.367785234899329e-05,
      "loss": 0.5367,
      "step": 39720
    },
    {
      "epoch": 1.5892,
      "grad_norm": 2.078073501586914,
      "learning_rate": 2.3671140939597318e-05,
      "loss": 0.5311,
      "step": 39730
    },
    {
      "epoch": 1.5896,
      "grad_norm": 2.797581672668457,
      "learning_rate": 2.3664429530201343e-05,
      "loss": 0.6283,
      "step": 39740
    },
    {
      "epoch": 1.5899999999999999,
      "grad_norm": 2.9748528003692627,
      "learning_rate": 2.365771812080537e-05,
      "loss": 0.5784,
      "step": 39750
    },
    {
      "epoch": 1.5904,
      "grad_norm": 2.914327621459961,
      "learning_rate": 2.3651006711409397e-05,
      "loss": 0.5457,
      "step": 39760
    },
    {
      "epoch": 1.5908,
      "grad_norm": 2.3295557498931885,
      "learning_rate": 2.3644295302013426e-05,
      "loss": 0.4865,
      "step": 39770
    },
    {
      "epoch": 1.5912,
      "grad_norm": 4.217319488525391,
      "learning_rate": 2.363758389261745e-05,
      "loss": 0.4598,
      "step": 39780
    },
    {
      "epoch": 1.5916000000000001,
      "grad_norm": 2.7373392581939697,
      "learning_rate": 2.363087248322148e-05,
      "loss": 0.6008,
      "step": 39790
    },
    {
      "epoch": 1.592,
      "grad_norm": 2.4451756477355957,
      "learning_rate": 2.3624161073825504e-05,
      "loss": 0.5326,
      "step": 39800
    },
    {
      "epoch": 1.5924,
      "grad_norm": 2.2478108406066895,
      "learning_rate": 2.361744966442953e-05,
      "loss": 0.4599,
      "step": 39810
    },
    {
      "epoch": 1.5928,
      "grad_norm": 2.6663901805877686,
      "learning_rate": 2.3610738255033558e-05,
      "loss": 0.5856,
      "step": 39820
    },
    {
      "epoch": 1.5932,
      "grad_norm": 3.0279688835144043,
      "learning_rate": 2.3604026845637587e-05,
      "loss": 0.5776,
      "step": 39830
    },
    {
      "epoch": 1.5936,
      "grad_norm": 2.6508185863494873,
      "learning_rate": 2.3597315436241612e-05,
      "loss": 0.6152,
      "step": 39840
    },
    {
      "epoch": 1.5939999999999999,
      "grad_norm": 3.008669853210449,
      "learning_rate": 2.359060402684564e-05,
      "loss": 0.5144,
      "step": 39850
    },
    {
      "epoch": 1.5944,
      "grad_norm": 2.604945182800293,
      "learning_rate": 2.3583892617449665e-05,
      "loss": 0.5554,
      "step": 39860
    },
    {
      "epoch": 1.5948,
      "grad_norm": 2.718045711517334,
      "learning_rate": 2.357718120805369e-05,
      "loss": 0.532,
      "step": 39870
    },
    {
      "epoch": 1.5952,
      "grad_norm": 2.8109023571014404,
      "learning_rate": 2.357046979865772e-05,
      "loss": 0.5687,
      "step": 39880
    },
    {
      "epoch": 1.5956000000000001,
      "grad_norm": 2.6001052856445312,
      "learning_rate": 2.3563758389261744e-05,
      "loss": 0.5223,
      "step": 39890
    },
    {
      "epoch": 1.596,
      "grad_norm": 2.4422216415405273,
      "learning_rate": 2.3557046979865773e-05,
      "loss": 0.493,
      "step": 39900
    },
    {
      "epoch": 1.5964,
      "grad_norm": 2.6245596408843994,
      "learning_rate": 2.35503355704698e-05,
      "loss": 0.4422,
      "step": 39910
    },
    {
      "epoch": 1.5968,
      "grad_norm": 2.472245931625366,
      "learning_rate": 2.3543624161073827e-05,
      "loss": 0.5373,
      "step": 39920
    },
    {
      "epoch": 1.5972,
      "grad_norm": 3.0880789756774902,
      "learning_rate": 2.3536912751677852e-05,
      "loss": 0.6566,
      "step": 39930
    },
    {
      "epoch": 1.5976,
      "grad_norm": 2.431217908859253,
      "learning_rate": 2.353020134228188e-05,
      "loss": 0.6071,
      "step": 39940
    },
    {
      "epoch": 1.5979999999999999,
      "grad_norm": 3.1408445835113525,
      "learning_rate": 2.3523489932885905e-05,
      "loss": 0.5521,
      "step": 39950
    },
    {
      "epoch": 1.5984,
      "grad_norm": 1.9940038919448853,
      "learning_rate": 2.3516778523489934e-05,
      "loss": 0.5414,
      "step": 39960
    },
    {
      "epoch": 1.5988,
      "grad_norm": 3.047882080078125,
      "learning_rate": 2.3510067114093963e-05,
      "loss": 0.562,
      "step": 39970
    },
    {
      "epoch": 1.5992,
      "grad_norm": 2.389514684677124,
      "learning_rate": 2.3503355704697988e-05,
      "loss": 0.521,
      "step": 39980
    },
    {
      "epoch": 1.5996000000000001,
      "grad_norm": 3.0260794162750244,
      "learning_rate": 2.3496644295302013e-05,
      "loss": 0.5741,
      "step": 39990
    },
    {
      "epoch": 1.6,
      "grad_norm": 2.9047956466674805,
      "learning_rate": 2.348993288590604e-05,
      "loss": 0.5113,
      "step": 40000
    },
    {
      "epoch": 1.6004,
      "grad_norm": 2.9902255535125732,
      "learning_rate": 2.3483221476510067e-05,
      "loss": 0.5915,
      "step": 40010
    },
    {
      "epoch": 1.6008,
      "grad_norm": 3.0891218185424805,
      "learning_rate": 2.3476510067114095e-05,
      "loss": 0.5469,
      "step": 40020
    },
    {
      "epoch": 1.6012,
      "grad_norm": 2.7342398166656494,
      "learning_rate": 2.3469798657718124e-05,
      "loss": 0.595,
      "step": 40030
    },
    {
      "epoch": 1.6016,
      "grad_norm": 2.481541872024536,
      "learning_rate": 2.346308724832215e-05,
      "loss": 0.5544,
      "step": 40040
    },
    {
      "epoch": 1.6019999999999999,
      "grad_norm": 3.3177144527435303,
      "learning_rate": 2.3456375838926174e-05,
      "loss": 0.6075,
      "step": 40050
    },
    {
      "epoch": 1.6024,
      "grad_norm": 2.977851152420044,
      "learning_rate": 2.3449664429530203e-05,
      "loss": 0.6107,
      "step": 40060
    },
    {
      "epoch": 1.6028,
      "grad_norm": 3.1144051551818848,
      "learning_rate": 2.3442953020134228e-05,
      "loss": 0.5058,
      "step": 40070
    },
    {
      "epoch": 1.6032,
      "grad_norm": 2.9981913566589355,
      "learning_rate": 2.3436241610738256e-05,
      "loss": 0.5271,
      "step": 40080
    },
    {
      "epoch": 1.6036000000000001,
      "grad_norm": 2.0805277824401855,
      "learning_rate": 2.3429530201342285e-05,
      "loss": 0.5235,
      "step": 40090
    },
    {
      "epoch": 1.604,
      "grad_norm": 2.5688953399658203,
      "learning_rate": 2.342281879194631e-05,
      "loss": 0.5684,
      "step": 40100
    },
    {
      "epoch": 1.6044,
      "grad_norm": 2.428868532180786,
      "learning_rate": 2.341610738255034e-05,
      "loss": 0.6193,
      "step": 40110
    },
    {
      "epoch": 1.6048,
      "grad_norm": 3.229102373123169,
      "learning_rate": 2.3409395973154364e-05,
      "loss": 0.638,
      "step": 40120
    },
    {
      "epoch": 1.6052,
      "grad_norm": 2.644406318664551,
      "learning_rate": 2.340268456375839e-05,
      "loss": 0.5095,
      "step": 40130
    },
    {
      "epoch": 1.6056,
      "grad_norm": 2.3041532039642334,
      "learning_rate": 2.3395973154362417e-05,
      "loss": 0.5628,
      "step": 40140
    },
    {
      "epoch": 1.6059999999999999,
      "grad_norm": 3.3286972045898438,
      "learning_rate": 2.3389261744966446e-05,
      "loss": 0.4706,
      "step": 40150
    },
    {
      "epoch": 1.6064,
      "grad_norm": 2.9027297496795654,
      "learning_rate": 2.338255033557047e-05,
      "loss": 0.5913,
      "step": 40160
    },
    {
      "epoch": 1.6068,
      "grad_norm": 2.8139214515686035,
      "learning_rate": 2.33758389261745e-05,
      "loss": 0.5666,
      "step": 40170
    },
    {
      "epoch": 1.6072,
      "grad_norm": 2.657221555709839,
      "learning_rate": 2.3369127516778525e-05,
      "loss": 0.5067,
      "step": 40180
    },
    {
      "epoch": 1.6076000000000001,
      "grad_norm": 2.3615806102752686,
      "learning_rate": 2.336241610738255e-05,
      "loss": 0.5618,
      "step": 40190
    },
    {
      "epoch": 1.608,
      "grad_norm": 2.7037293910980225,
      "learning_rate": 2.335570469798658e-05,
      "loss": 0.5442,
      "step": 40200
    },
    {
      "epoch": 1.6084,
      "grad_norm": 2.8994970321655273,
      "learning_rate": 2.3348993288590607e-05,
      "loss": 0.5781,
      "step": 40210
    },
    {
      "epoch": 1.6088,
      "grad_norm": 2.6351611614227295,
      "learning_rate": 2.3342281879194632e-05,
      "loss": 0.6237,
      "step": 40220
    },
    {
      "epoch": 1.6092,
      "grad_norm": 2.428544759750366,
      "learning_rate": 2.333557046979866e-05,
      "loss": 0.5797,
      "step": 40230
    },
    {
      "epoch": 1.6096,
      "grad_norm": 2.9238016605377197,
      "learning_rate": 2.3328859060402686e-05,
      "loss": 0.5596,
      "step": 40240
    },
    {
      "epoch": 1.6099999999999999,
      "grad_norm": 3.2287158966064453,
      "learning_rate": 2.332214765100671e-05,
      "loss": 0.5738,
      "step": 40250
    },
    {
      "epoch": 1.6104,
      "grad_norm": 2.6694109439849854,
      "learning_rate": 2.331543624161074e-05,
      "loss": 0.5874,
      "step": 40260
    },
    {
      "epoch": 1.6108,
      "grad_norm": 2.9880118370056152,
      "learning_rate": 2.3308724832214765e-05,
      "loss": 0.6171,
      "step": 40270
    },
    {
      "epoch": 1.6112,
      "grad_norm": 2.747755289077759,
      "learning_rate": 2.3302013422818793e-05,
      "loss": 0.522,
      "step": 40280
    },
    {
      "epoch": 1.6116000000000001,
      "grad_norm": 3.42559552192688,
      "learning_rate": 2.3295302013422822e-05,
      "loss": 0.5733,
      "step": 40290
    },
    {
      "epoch": 1.612,
      "grad_norm": 3.06028151512146,
      "learning_rate": 2.3288590604026847e-05,
      "loss": 0.5781,
      "step": 40300
    },
    {
      "epoch": 1.6124,
      "grad_norm": 2.029587507247925,
      "learning_rate": 2.3281879194630872e-05,
      "loss": 0.6072,
      "step": 40310
    },
    {
      "epoch": 1.6128,
      "grad_norm": 3.0008978843688965,
      "learning_rate": 2.32751677852349e-05,
      "loss": 0.5637,
      "step": 40320
    },
    {
      "epoch": 1.6132,
      "grad_norm": 2.520941972732544,
      "learning_rate": 2.3268456375838926e-05,
      "loss": 0.5255,
      "step": 40330
    },
    {
      "epoch": 1.6136,
      "grad_norm": 2.587939500808716,
      "learning_rate": 2.3261744966442954e-05,
      "loss": 0.5241,
      "step": 40340
    },
    {
      "epoch": 1.6139999999999999,
      "grad_norm": 2.099940776824951,
      "learning_rate": 2.3255033557046983e-05,
      "loss": 0.5394,
      "step": 40350
    },
    {
      "epoch": 1.6143999999999998,
      "grad_norm": 2.5464978218078613,
      "learning_rate": 2.3248322147651008e-05,
      "loss": 0.5185,
      "step": 40360
    },
    {
      "epoch": 1.6148,
      "grad_norm": 2.5731348991394043,
      "learning_rate": 2.3241610738255033e-05,
      "loss": 0.5278,
      "step": 40370
    },
    {
      "epoch": 1.6152,
      "grad_norm": 2.4941565990448,
      "learning_rate": 2.3234899328859062e-05,
      "loss": 0.4895,
      "step": 40380
    },
    {
      "epoch": 1.6156000000000001,
      "grad_norm": 2.3436481952667236,
      "learning_rate": 2.3228187919463087e-05,
      "loss": 0.5737,
      "step": 40390
    },
    {
      "epoch": 1.616,
      "grad_norm": 2.1855461597442627,
      "learning_rate": 2.3221476510067116e-05,
      "loss": 0.5404,
      "step": 40400
    },
    {
      "epoch": 1.6164,
      "grad_norm": 2.7654123306274414,
      "learning_rate": 2.3214765100671144e-05,
      "loss": 0.5679,
      "step": 40410
    },
    {
      "epoch": 1.6168,
      "grad_norm": 3.3058652877807617,
      "learning_rate": 2.320805369127517e-05,
      "loss": 0.5484,
      "step": 40420
    },
    {
      "epoch": 1.6172,
      "grad_norm": 2.438812017440796,
      "learning_rate": 2.3201342281879194e-05,
      "loss": 0.5081,
      "step": 40430
    },
    {
      "epoch": 1.6176,
      "grad_norm": 2.9213225841522217,
      "learning_rate": 2.3194630872483223e-05,
      "loss": 0.5928,
      "step": 40440
    },
    {
      "epoch": 1.6179999999999999,
      "grad_norm": 2.731635570526123,
      "learning_rate": 2.3187919463087248e-05,
      "loss": 0.5731,
      "step": 40450
    },
    {
      "epoch": 1.6183999999999998,
      "grad_norm": 2.482039213180542,
      "learning_rate": 2.3181208053691277e-05,
      "loss": 0.5446,
      "step": 40460
    },
    {
      "epoch": 1.6188,
      "grad_norm": 3.113003730773926,
      "learning_rate": 2.3174496644295305e-05,
      "loss": 0.6036,
      "step": 40470
    },
    {
      "epoch": 1.6192,
      "grad_norm": 2.8899197578430176,
      "learning_rate": 2.316778523489933e-05,
      "loss": 0.5663,
      "step": 40480
    },
    {
      "epoch": 1.6196000000000002,
      "grad_norm": 2.944335699081421,
      "learning_rate": 2.3161073825503356e-05,
      "loss": 0.533,
      "step": 40490
    },
    {
      "epoch": 1.62,
      "grad_norm": 3.2229766845703125,
      "learning_rate": 2.3154362416107384e-05,
      "loss": 0.4817,
      "step": 40500
    },
    {
      "epoch": 1.6204,
      "grad_norm": 2.6286163330078125,
      "learning_rate": 2.314765100671141e-05,
      "loss": 0.5053,
      "step": 40510
    },
    {
      "epoch": 1.6208,
      "grad_norm": 2.3158414363861084,
      "learning_rate": 2.3140939597315438e-05,
      "loss": 0.5506,
      "step": 40520
    },
    {
      "epoch": 1.6212,
      "grad_norm": 2.7920522689819336,
      "learning_rate": 2.3134228187919466e-05,
      "loss": 0.5731,
      "step": 40530
    },
    {
      "epoch": 1.6216,
      "grad_norm": 2.2524116039276123,
      "learning_rate": 2.312751677852349e-05,
      "loss": 0.5774,
      "step": 40540
    },
    {
      "epoch": 1.6219999999999999,
      "grad_norm": 2.6846067905426025,
      "learning_rate": 2.312080536912752e-05,
      "loss": 0.5105,
      "step": 40550
    },
    {
      "epoch": 1.6223999999999998,
      "grad_norm": 2.8586819171905518,
      "learning_rate": 2.3114093959731545e-05,
      "loss": 0.6231,
      "step": 40560
    },
    {
      "epoch": 1.6228,
      "grad_norm": 2.4322407245635986,
      "learning_rate": 2.310738255033557e-05,
      "loss": 0.6219,
      "step": 40570
    },
    {
      "epoch": 1.6232,
      "grad_norm": 2.5281524658203125,
      "learning_rate": 2.31006711409396e-05,
      "loss": 0.5999,
      "step": 40580
    },
    {
      "epoch": 1.6236000000000002,
      "grad_norm": 2.583584785461426,
      "learning_rate": 2.3093959731543624e-05,
      "loss": 0.581,
      "step": 40590
    },
    {
      "epoch": 1.624,
      "grad_norm": 2.1058437824249268,
      "learning_rate": 2.3087248322147653e-05,
      "loss": 0.5013,
      "step": 40600
    },
    {
      "epoch": 1.6244,
      "grad_norm": 2.955148220062256,
      "learning_rate": 2.308053691275168e-05,
      "loss": 0.5561,
      "step": 40610
    },
    {
      "epoch": 1.6248,
      "grad_norm": 3.39927339553833,
      "learning_rate": 2.3073825503355706e-05,
      "loss": 0.6112,
      "step": 40620
    },
    {
      "epoch": 1.6252,
      "grad_norm": 2.660098075866699,
      "learning_rate": 2.306711409395973e-05,
      "loss": 0.5389,
      "step": 40630
    },
    {
      "epoch": 1.6256,
      "grad_norm": 3.4275057315826416,
      "learning_rate": 2.306040268456376e-05,
      "loss": 0.6405,
      "step": 40640
    },
    {
      "epoch": 1.626,
      "grad_norm": 3.8517472743988037,
      "learning_rate": 2.3053691275167785e-05,
      "loss": 0.5926,
      "step": 40650
    },
    {
      "epoch": 1.6263999999999998,
      "grad_norm": 2.185912609100342,
      "learning_rate": 2.3046979865771814e-05,
      "loss": 0.5269,
      "step": 40660
    },
    {
      "epoch": 1.6268,
      "grad_norm": 2.764988422393799,
      "learning_rate": 2.3040268456375842e-05,
      "loss": 0.5753,
      "step": 40670
    },
    {
      "epoch": 1.6272,
      "grad_norm": 2.279428482055664,
      "learning_rate": 2.3033557046979867e-05,
      "loss": 0.5528,
      "step": 40680
    },
    {
      "epoch": 1.6276000000000002,
      "grad_norm": 2.0976850986480713,
      "learning_rate": 2.3026845637583893e-05,
      "loss": 0.5006,
      "step": 40690
    },
    {
      "epoch": 1.6280000000000001,
      "grad_norm": 2.236659288406372,
      "learning_rate": 2.302013422818792e-05,
      "loss": 0.534,
      "step": 40700
    },
    {
      "epoch": 1.6284,
      "grad_norm": 3.361544370651245,
      "learning_rate": 2.3013422818791946e-05,
      "loss": 0.6173,
      "step": 40710
    },
    {
      "epoch": 1.6288,
      "grad_norm": 2.4012763500213623,
      "learning_rate": 2.300671140939597e-05,
      "loss": 0.5802,
      "step": 40720
    },
    {
      "epoch": 1.6292,
      "grad_norm": 2.8712992668151855,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 0.5764,
      "step": 40730
    },
    {
      "epoch": 1.6296,
      "grad_norm": 2.618640184402466,
      "learning_rate": 2.299328859060403e-05,
      "loss": 0.5422,
      "step": 40740
    },
    {
      "epoch": 1.63,
      "grad_norm": 2.426363468170166,
      "learning_rate": 2.2986577181208054e-05,
      "loss": 0.6,
      "step": 40750
    },
    {
      "epoch": 1.6303999999999998,
      "grad_norm": 2.5060079097747803,
      "learning_rate": 2.2979865771812082e-05,
      "loss": 0.4833,
      "step": 40760
    },
    {
      "epoch": 1.6308,
      "grad_norm": 3.709470748901367,
      "learning_rate": 2.2973154362416107e-05,
      "loss": 0.5888,
      "step": 40770
    },
    {
      "epoch": 1.6312,
      "grad_norm": 2.0643651485443115,
      "learning_rate": 2.2966442953020133e-05,
      "loss": 0.5614,
      "step": 40780
    },
    {
      "epoch": 1.6316000000000002,
      "grad_norm": 3.503960132598877,
      "learning_rate": 2.2959731543624165e-05,
      "loss": 0.709,
      "step": 40790
    },
    {
      "epoch": 1.6320000000000001,
      "grad_norm": 3.299751043319702,
      "learning_rate": 2.295302013422819e-05,
      "loss": 0.6594,
      "step": 40800
    },
    {
      "epoch": 1.6324,
      "grad_norm": 3.0867347717285156,
      "learning_rate": 2.2946308724832215e-05,
      "loss": 0.6195,
      "step": 40810
    },
    {
      "epoch": 1.6328,
      "grad_norm": 2.3868370056152344,
      "learning_rate": 2.2939597315436243e-05,
      "loss": 0.5526,
      "step": 40820
    },
    {
      "epoch": 1.6332,
      "grad_norm": 2.2591748237609863,
      "learning_rate": 2.293288590604027e-05,
      "loss": 0.4734,
      "step": 40830
    },
    {
      "epoch": 1.6336,
      "grad_norm": 1.995174765586853,
      "learning_rate": 2.2926174496644294e-05,
      "loss": 0.4774,
      "step": 40840
    },
    {
      "epoch": 1.634,
      "grad_norm": 2.9171290397644043,
      "learning_rate": 2.2919463087248326e-05,
      "loss": 0.6123,
      "step": 40850
    },
    {
      "epoch": 1.6343999999999999,
      "grad_norm": 2.889796495437622,
      "learning_rate": 2.291275167785235e-05,
      "loss": 0.5631,
      "step": 40860
    },
    {
      "epoch": 1.6348,
      "grad_norm": 2.9574973583221436,
      "learning_rate": 2.2906040268456376e-05,
      "loss": 0.5969,
      "step": 40870
    },
    {
      "epoch": 1.6352,
      "grad_norm": 2.7151901721954346,
      "learning_rate": 2.2899328859060405e-05,
      "loss": 0.5458,
      "step": 40880
    },
    {
      "epoch": 1.6356000000000002,
      "grad_norm": 2.7323968410491943,
      "learning_rate": 2.289261744966443e-05,
      "loss": 0.6057,
      "step": 40890
    },
    {
      "epoch": 1.6360000000000001,
      "grad_norm": 2.553180694580078,
      "learning_rate": 2.2885906040268458e-05,
      "loss": 0.5986,
      "step": 40900
    },
    {
      "epoch": 1.6364,
      "grad_norm": 3.0760507583618164,
      "learning_rate": 2.2879194630872483e-05,
      "loss": 0.5488,
      "step": 40910
    },
    {
      "epoch": 1.6368,
      "grad_norm": 2.829561471939087,
      "learning_rate": 2.2872483221476512e-05,
      "loss": 0.5401,
      "step": 40920
    },
    {
      "epoch": 1.6372,
      "grad_norm": 2.673797607421875,
      "learning_rate": 2.2865771812080537e-05,
      "loss": 0.5864,
      "step": 40930
    },
    {
      "epoch": 1.6376,
      "grad_norm": 2.7525928020477295,
      "learning_rate": 2.2859060402684566e-05,
      "loss": 0.6139,
      "step": 40940
    },
    {
      "epoch": 1.638,
      "grad_norm": 2.081073045730591,
      "learning_rate": 2.285234899328859e-05,
      "loss": 0.5254,
      "step": 40950
    },
    {
      "epoch": 1.6383999999999999,
      "grad_norm": 2.2444097995758057,
      "learning_rate": 2.284563758389262e-05,
      "loss": 0.6065,
      "step": 40960
    },
    {
      "epoch": 1.6388,
      "grad_norm": 2.6405136585235596,
      "learning_rate": 2.2838926174496644e-05,
      "loss": 0.5153,
      "step": 40970
    },
    {
      "epoch": 1.6392,
      "grad_norm": 2.518946647644043,
      "learning_rate": 2.2832214765100673e-05,
      "loss": 0.4851,
      "step": 40980
    },
    {
      "epoch": 1.6396,
      "grad_norm": 2.695909261703491,
      "learning_rate": 2.2825503355704698e-05,
      "loss": 0.5578,
      "step": 40990
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 2.194812297821045,
      "learning_rate": 2.2818791946308727e-05,
      "loss": 0.6245,
      "step": 41000
    },
    {
      "epoch": 1.6404,
      "grad_norm": 2.6317691802978516,
      "learning_rate": 2.2812080536912752e-05,
      "loss": 0.5611,
      "step": 41010
    },
    {
      "epoch": 1.6408,
      "grad_norm": 2.34647798538208,
      "learning_rate": 2.280536912751678e-05,
      "loss": 0.6203,
      "step": 41020
    },
    {
      "epoch": 1.6412,
      "grad_norm": 2.953145742416382,
      "learning_rate": 2.2798657718120806e-05,
      "loss": 0.6082,
      "step": 41030
    },
    {
      "epoch": 1.6416,
      "grad_norm": 1.7836753129959106,
      "learning_rate": 2.279194630872483e-05,
      "loss": 0.5063,
      "step": 41040
    },
    {
      "epoch": 1.642,
      "grad_norm": 2.0317254066467285,
      "learning_rate": 2.2785234899328863e-05,
      "loss": 0.6072,
      "step": 41050
    },
    {
      "epoch": 1.6423999999999999,
      "grad_norm": 2.7765092849731445,
      "learning_rate": 2.2778523489932888e-05,
      "loss": 0.4834,
      "step": 41060
    },
    {
      "epoch": 1.6428,
      "grad_norm": 2.1738123893737793,
      "learning_rate": 2.2771812080536913e-05,
      "loss": 0.5853,
      "step": 41070
    },
    {
      "epoch": 1.6432,
      "grad_norm": 2.0833094120025635,
      "learning_rate": 2.276510067114094e-05,
      "loss": 0.5227,
      "step": 41080
    },
    {
      "epoch": 1.6436,
      "grad_norm": 3.0406410694122314,
      "learning_rate": 2.2758389261744967e-05,
      "loss": 0.6486,
      "step": 41090
    },
    {
      "epoch": 1.6440000000000001,
      "grad_norm": 2.824096202850342,
      "learning_rate": 2.2751677852348992e-05,
      "loss": 0.6023,
      "step": 41100
    },
    {
      "epoch": 1.6444,
      "grad_norm": 3.3307108879089355,
      "learning_rate": 2.2744966442953024e-05,
      "loss": 0.615,
      "step": 41110
    },
    {
      "epoch": 1.6448,
      "grad_norm": 2.3058202266693115,
      "learning_rate": 2.273825503355705e-05,
      "loss": 0.5364,
      "step": 41120
    },
    {
      "epoch": 1.6452,
      "grad_norm": 2.60249924659729,
      "learning_rate": 2.2731543624161074e-05,
      "loss": 0.6047,
      "step": 41130
    },
    {
      "epoch": 1.6456,
      "grad_norm": 2.9833602905273438,
      "learning_rate": 2.2724832214765103e-05,
      "loss": 0.5534,
      "step": 41140
    },
    {
      "epoch": 1.646,
      "grad_norm": 1.9196295738220215,
      "learning_rate": 2.2718120805369128e-05,
      "loss": 0.555,
      "step": 41150
    },
    {
      "epoch": 1.6463999999999999,
      "grad_norm": 2.7687692642211914,
      "learning_rate": 2.2711409395973153e-05,
      "loss": 0.588,
      "step": 41160
    },
    {
      "epoch": 1.6468,
      "grad_norm": 2.5325794219970703,
      "learning_rate": 2.2704697986577185e-05,
      "loss": 0.6159,
      "step": 41170
    },
    {
      "epoch": 1.6472,
      "grad_norm": 2.120760679244995,
      "learning_rate": 2.269798657718121e-05,
      "loss": 0.491,
      "step": 41180
    },
    {
      "epoch": 1.6476,
      "grad_norm": 6.162961959838867,
      "learning_rate": 2.2691275167785235e-05,
      "loss": 0.5851,
      "step": 41190
    },
    {
      "epoch": 1.6480000000000001,
      "grad_norm": 2.441197156906128,
      "learning_rate": 2.2684563758389264e-05,
      "loss": 0.6236,
      "step": 41200
    },
    {
      "epoch": 1.6484,
      "grad_norm": 2.449831962585449,
      "learning_rate": 2.267785234899329e-05,
      "loss": 0.5217,
      "step": 41210
    },
    {
      "epoch": 1.6488,
      "grad_norm": 2.8652703762054443,
      "learning_rate": 2.2671140939597314e-05,
      "loss": 0.5929,
      "step": 41220
    },
    {
      "epoch": 1.6492,
      "grad_norm": 2.1350202560424805,
      "learning_rate": 2.2664429530201343e-05,
      "loss": 0.4912,
      "step": 41230
    },
    {
      "epoch": 1.6496,
      "grad_norm": 1.5685673952102661,
      "learning_rate": 2.265771812080537e-05,
      "loss": 0.5077,
      "step": 41240
    },
    {
      "epoch": 1.65,
      "grad_norm": 2.8218634128570557,
      "learning_rate": 2.2651006711409396e-05,
      "loss": 0.5614,
      "step": 41250
    },
    {
      "epoch": 1.6503999999999999,
      "grad_norm": 3.556000232696533,
      "learning_rate": 2.2644295302013425e-05,
      "loss": 0.5799,
      "step": 41260
    },
    {
      "epoch": 1.6508,
      "grad_norm": 2.550039529800415,
      "learning_rate": 2.263758389261745e-05,
      "loss": 0.5235,
      "step": 41270
    },
    {
      "epoch": 1.6512,
      "grad_norm": 2.5539162158966064,
      "learning_rate": 2.2630872483221475e-05,
      "loss": 0.5428,
      "step": 41280
    },
    {
      "epoch": 1.6516,
      "grad_norm": 2.6600403785705566,
      "learning_rate": 2.2624161073825504e-05,
      "loss": 0.6029,
      "step": 41290
    },
    {
      "epoch": 1.6520000000000001,
      "grad_norm": 3.2716033458709717,
      "learning_rate": 2.2617449664429532e-05,
      "loss": 0.6427,
      "step": 41300
    },
    {
      "epoch": 1.6524,
      "grad_norm": 3.5837020874023438,
      "learning_rate": 2.2610738255033557e-05,
      "loss": 0.5717,
      "step": 41310
    },
    {
      "epoch": 1.6528,
      "grad_norm": 2.8155131340026855,
      "learning_rate": 2.2604026845637586e-05,
      "loss": 0.5032,
      "step": 41320
    },
    {
      "epoch": 1.6532,
      "grad_norm": 2.7299013137817383,
      "learning_rate": 2.259731543624161e-05,
      "loss": 0.5541,
      "step": 41330
    },
    {
      "epoch": 1.6536,
      "grad_norm": 2.738527774810791,
      "learning_rate": 2.259060402684564e-05,
      "loss": 0.6746,
      "step": 41340
    },
    {
      "epoch": 1.654,
      "grad_norm": 2.2250218391418457,
      "learning_rate": 2.2583892617449665e-05,
      "loss": 0.53,
      "step": 41350
    },
    {
      "epoch": 1.6543999999999999,
      "grad_norm": 2.9538283348083496,
      "learning_rate": 2.2577181208053693e-05,
      "loss": 0.5336,
      "step": 41360
    },
    {
      "epoch": 1.6548,
      "grad_norm": 2.158147096633911,
      "learning_rate": 2.257046979865772e-05,
      "loss": 0.5321,
      "step": 41370
    },
    {
      "epoch": 1.6552,
      "grad_norm": 2.780202627182007,
      "learning_rate": 2.2563758389261747e-05,
      "loss": 0.6425,
      "step": 41380
    },
    {
      "epoch": 1.6556,
      "grad_norm": 2.4552433490753174,
      "learning_rate": 2.2557046979865772e-05,
      "loss": 0.516,
      "step": 41390
    },
    {
      "epoch": 1.6560000000000001,
      "grad_norm": 2.8548038005828857,
      "learning_rate": 2.25503355704698e-05,
      "loss": 0.4423,
      "step": 41400
    },
    {
      "epoch": 1.6564,
      "grad_norm": 3.111207962036133,
      "learning_rate": 2.2543624161073826e-05,
      "loss": 0.5454,
      "step": 41410
    },
    {
      "epoch": 1.6568,
      "grad_norm": 3.1258459091186523,
      "learning_rate": 2.253691275167785e-05,
      "loss": 0.571,
      "step": 41420
    },
    {
      "epoch": 1.6572,
      "grad_norm": 2.5639452934265137,
      "learning_rate": 2.253020134228188e-05,
      "loss": 0.5947,
      "step": 41430
    },
    {
      "epoch": 1.6576,
      "grad_norm": 2.4081249237060547,
      "learning_rate": 2.2523489932885908e-05,
      "loss": 0.5457,
      "step": 41440
    },
    {
      "epoch": 1.658,
      "grad_norm": 1.911224365234375,
      "learning_rate": 2.2516778523489933e-05,
      "loss": 0.5973,
      "step": 41450
    },
    {
      "epoch": 1.6583999999999999,
      "grad_norm": 3.404054880142212,
      "learning_rate": 2.2510067114093962e-05,
      "loss": 0.5695,
      "step": 41460
    },
    {
      "epoch": 1.6588,
      "grad_norm": 2.8599038124084473,
      "learning_rate": 2.2503355704697987e-05,
      "loss": 0.6484,
      "step": 41470
    },
    {
      "epoch": 1.6592,
      "grad_norm": 2.7314183712005615,
      "learning_rate": 2.2496644295302012e-05,
      "loss": 0.6608,
      "step": 41480
    },
    {
      "epoch": 1.6596,
      "grad_norm": 2.6428933143615723,
      "learning_rate": 2.2489932885906044e-05,
      "loss": 0.6168,
      "step": 41490
    },
    {
      "epoch": 1.6600000000000001,
      "grad_norm": 2.7752041816711426,
      "learning_rate": 2.248322147651007e-05,
      "loss": 0.5878,
      "step": 41500
    },
    {
      "epoch": 1.6604,
      "grad_norm": 2.18359637260437,
      "learning_rate": 2.2476510067114095e-05,
      "loss": 0.5426,
      "step": 41510
    },
    {
      "epoch": 1.6608,
      "grad_norm": 2.8375794887542725,
      "learning_rate": 2.2469798657718123e-05,
      "loss": 0.498,
      "step": 41520
    },
    {
      "epoch": 1.6612,
      "grad_norm": 2.6363282203674316,
      "learning_rate": 2.2463087248322148e-05,
      "loss": 0.5645,
      "step": 41530
    },
    {
      "epoch": 1.6616,
      "grad_norm": 2.2581512928009033,
      "learning_rate": 2.2456375838926173e-05,
      "loss": 0.5691,
      "step": 41540
    },
    {
      "epoch": 1.662,
      "grad_norm": 2.8715410232543945,
      "learning_rate": 2.2449664429530202e-05,
      "loss": 0.5485,
      "step": 41550
    },
    {
      "epoch": 1.6623999999999999,
      "grad_norm": 2.6961708068847656,
      "learning_rate": 2.244295302013423e-05,
      "loss": 0.4702,
      "step": 41560
    },
    {
      "epoch": 1.6627999999999998,
      "grad_norm": 2.7103753089904785,
      "learning_rate": 2.2436241610738256e-05,
      "loss": 0.6773,
      "step": 41570
    },
    {
      "epoch": 1.6632,
      "grad_norm": 2.360970973968506,
      "learning_rate": 2.2429530201342284e-05,
      "loss": 0.6379,
      "step": 41580
    },
    {
      "epoch": 1.6636,
      "grad_norm": 2.3763973712921143,
      "learning_rate": 2.242281879194631e-05,
      "loss": 0.4936,
      "step": 41590
    },
    {
      "epoch": 1.6640000000000001,
      "grad_norm": 2.7562201023101807,
      "learning_rate": 2.2416107382550335e-05,
      "loss": 0.5037,
      "step": 41600
    },
    {
      "epoch": 1.6644,
      "grad_norm": 1.5403333902359009,
      "learning_rate": 2.2409395973154363e-05,
      "loss": 0.5226,
      "step": 41610
    },
    {
      "epoch": 1.6648,
      "grad_norm": 3.108356237411499,
      "learning_rate": 2.240268456375839e-05,
      "loss": 0.4886,
      "step": 41620
    },
    {
      "epoch": 1.6652,
      "grad_norm": 1.6892437934875488,
      "learning_rate": 2.2395973154362417e-05,
      "loss": 0.5498,
      "step": 41630
    },
    {
      "epoch": 1.6656,
      "grad_norm": 3.138202428817749,
      "learning_rate": 2.2389261744966445e-05,
      "loss": 0.5898,
      "step": 41640
    },
    {
      "epoch": 1.666,
      "grad_norm": 2.318293571472168,
      "learning_rate": 2.238255033557047e-05,
      "loss": 0.5541,
      "step": 41650
    },
    {
      "epoch": 1.6663999999999999,
      "grad_norm": 2.808279514312744,
      "learning_rate": 2.2375838926174496e-05,
      "loss": 0.5709,
      "step": 41660
    },
    {
      "epoch": 1.6667999999999998,
      "grad_norm": 2.6673130989074707,
      "learning_rate": 2.2369127516778524e-05,
      "loss": 0.5767,
      "step": 41670
    },
    {
      "epoch": 1.6672,
      "grad_norm": 2.574824571609497,
      "learning_rate": 2.2362416107382553e-05,
      "loss": 0.5149,
      "step": 41680
    },
    {
      "epoch": 1.6676,
      "grad_norm": 3.0504887104034424,
      "learning_rate": 2.2355704697986578e-05,
      "loss": 0.6477,
      "step": 41690
    },
    {
      "epoch": 1.6680000000000001,
      "grad_norm": 2.407762050628662,
      "learning_rate": 2.2348993288590606e-05,
      "loss": 0.5912,
      "step": 41700
    },
    {
      "epoch": 1.6684,
      "grad_norm": 3.287343740463257,
      "learning_rate": 2.234228187919463e-05,
      "loss": 0.5069,
      "step": 41710
    },
    {
      "epoch": 1.6688,
      "grad_norm": 2.1713223457336426,
      "learning_rate": 2.2335570469798657e-05,
      "loss": 0.6426,
      "step": 41720
    },
    {
      "epoch": 1.6692,
      "grad_norm": 3.3969905376434326,
      "learning_rate": 2.2328859060402685e-05,
      "loss": 0.5613,
      "step": 41730
    },
    {
      "epoch": 1.6696,
      "grad_norm": 3.1264536380767822,
      "learning_rate": 2.232214765100671e-05,
      "loss": 0.6213,
      "step": 41740
    },
    {
      "epoch": 1.67,
      "grad_norm": 2.5483317375183105,
      "learning_rate": 2.231543624161074e-05,
      "loss": 0.5856,
      "step": 41750
    },
    {
      "epoch": 1.6703999999999999,
      "grad_norm": 2.219916343688965,
      "learning_rate": 2.2308724832214768e-05,
      "loss": 0.5515,
      "step": 41760
    },
    {
      "epoch": 1.6707999999999998,
      "grad_norm": 2.752638339996338,
      "learning_rate": 2.2302013422818793e-05,
      "loss": 0.7052,
      "step": 41770
    },
    {
      "epoch": 1.6712,
      "grad_norm": 2.80684494972229,
      "learning_rate": 2.229530201342282e-05,
      "loss": 0.6476,
      "step": 41780
    },
    {
      "epoch": 1.6716,
      "grad_norm": 2.9092586040496826,
      "learning_rate": 2.2288590604026846e-05,
      "loss": 0.5975,
      "step": 41790
    },
    {
      "epoch": 1.6720000000000002,
      "grad_norm": 2.765392780303955,
      "learning_rate": 2.228187919463087e-05,
      "loss": 0.693,
      "step": 41800
    },
    {
      "epoch": 1.6724,
      "grad_norm": 2.367232084274292,
      "learning_rate": 2.22751677852349e-05,
      "loss": 0.4846,
      "step": 41810
    },
    {
      "epoch": 1.6728,
      "grad_norm": 1.9216490983963013,
      "learning_rate": 2.226845637583893e-05,
      "loss": 0.5294,
      "step": 41820
    },
    {
      "epoch": 1.6732,
      "grad_norm": 2.371189832687378,
      "learning_rate": 2.2261744966442954e-05,
      "loss": 0.4996,
      "step": 41830
    },
    {
      "epoch": 1.6736,
      "grad_norm": 2.902101993560791,
      "learning_rate": 2.2255033557046982e-05,
      "loss": 0.4939,
      "step": 41840
    },
    {
      "epoch": 1.674,
      "grad_norm": 3.1977932453155518,
      "learning_rate": 2.2248322147651008e-05,
      "loss": 0.6085,
      "step": 41850
    },
    {
      "epoch": 1.6743999999999999,
      "grad_norm": 2.0162322521209717,
      "learning_rate": 2.2241610738255033e-05,
      "loss": 0.5499,
      "step": 41860
    },
    {
      "epoch": 1.6747999999999998,
      "grad_norm": 2.3321025371551514,
      "learning_rate": 2.223489932885906e-05,
      "loss": 0.5548,
      "step": 41870
    },
    {
      "epoch": 1.6752,
      "grad_norm": 2.649139165878296,
      "learning_rate": 2.222818791946309e-05,
      "loss": 0.5999,
      "step": 41880
    },
    {
      "epoch": 1.6756,
      "grad_norm": 2.4953372478485107,
      "learning_rate": 2.2221476510067115e-05,
      "loss": 0.5718,
      "step": 41890
    },
    {
      "epoch": 1.6760000000000002,
      "grad_norm": 2.952967405319214,
      "learning_rate": 2.2214765100671144e-05,
      "loss": 0.6266,
      "step": 41900
    },
    {
      "epoch": 1.6764000000000001,
      "grad_norm": 2.2852354049682617,
      "learning_rate": 2.220805369127517e-05,
      "loss": 0.5889,
      "step": 41910
    },
    {
      "epoch": 1.6768,
      "grad_norm": 1.9248989820480347,
      "learning_rate": 2.2201342281879194e-05,
      "loss": 0.5181,
      "step": 41920
    },
    {
      "epoch": 1.6772,
      "grad_norm": 3.372443199157715,
      "learning_rate": 2.2194630872483222e-05,
      "loss": 0.4968,
      "step": 41930
    },
    {
      "epoch": 1.6776,
      "grad_norm": 3.2848119735717773,
      "learning_rate": 2.218791946308725e-05,
      "loss": 0.5786,
      "step": 41940
    },
    {
      "epoch": 1.678,
      "grad_norm": 2.7585370540618896,
      "learning_rate": 2.2181208053691276e-05,
      "loss": 0.5847,
      "step": 41950
    },
    {
      "epoch": 1.6784,
      "grad_norm": 2.023923397064209,
      "learning_rate": 2.2174496644295305e-05,
      "loss": 0.5465,
      "step": 41960
    },
    {
      "epoch": 1.6787999999999998,
      "grad_norm": 2.8386988639831543,
      "learning_rate": 2.216778523489933e-05,
      "loss": 0.5361,
      "step": 41970
    },
    {
      "epoch": 1.6792,
      "grad_norm": 3.579127073287964,
      "learning_rate": 2.2161073825503355e-05,
      "loss": 0.7073,
      "step": 41980
    },
    {
      "epoch": 1.6796,
      "grad_norm": 2.7937827110290527,
      "learning_rate": 2.2154362416107384e-05,
      "loss": 0.635,
      "step": 41990
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 2.991161346435547,
      "learning_rate": 2.2147651006711412e-05,
      "loss": 0.5882,
      "step": 42000
    },
    {
      "epoch": 1.6804000000000001,
      "grad_norm": 2.3518643379211426,
      "learning_rate": 2.2140939597315437e-05,
      "loss": 0.5834,
      "step": 42010
    },
    {
      "epoch": 1.6808,
      "grad_norm": 2.9346213340759277,
      "learning_rate": 2.2134228187919466e-05,
      "loss": 0.5312,
      "step": 42020
    },
    {
      "epoch": 1.6812,
      "grad_norm": 2.1525990962982178,
      "learning_rate": 2.212751677852349e-05,
      "loss": 0.5847,
      "step": 42030
    },
    {
      "epoch": 1.6816,
      "grad_norm": 2.5439202785491943,
      "learning_rate": 2.2120805369127516e-05,
      "loss": 0.5746,
      "step": 42040
    },
    {
      "epoch": 1.682,
      "grad_norm": 1.9058846235275269,
      "learning_rate": 2.2114093959731545e-05,
      "loss": 0.511,
      "step": 42050
    },
    {
      "epoch": 1.6824,
      "grad_norm": 2.765648126602173,
      "learning_rate": 2.210738255033557e-05,
      "loss": 0.5269,
      "step": 42060
    },
    {
      "epoch": 1.6827999999999999,
      "grad_norm": 2.9651358127593994,
      "learning_rate": 2.21006711409396e-05,
      "loss": 0.5834,
      "step": 42070
    },
    {
      "epoch": 1.6832,
      "grad_norm": 3.537318468093872,
      "learning_rate": 2.2093959731543627e-05,
      "loss": 0.649,
      "step": 42080
    },
    {
      "epoch": 1.6836,
      "grad_norm": 3.6095027923583984,
      "learning_rate": 2.2087248322147652e-05,
      "loss": 0.6361,
      "step": 42090
    },
    {
      "epoch": 1.6840000000000002,
      "grad_norm": 2.7294516563415527,
      "learning_rate": 2.2080536912751677e-05,
      "loss": 0.6375,
      "step": 42100
    },
    {
      "epoch": 1.6844000000000001,
      "grad_norm": 2.8084936141967773,
      "learning_rate": 2.2073825503355706e-05,
      "loss": 0.5562,
      "step": 42110
    },
    {
      "epoch": 1.6848,
      "grad_norm": 2.8149898052215576,
      "learning_rate": 2.206711409395973e-05,
      "loss": 0.4393,
      "step": 42120
    },
    {
      "epoch": 1.6852,
      "grad_norm": 2.872434139251709,
      "learning_rate": 2.206040268456376e-05,
      "loss": 0.5486,
      "step": 42130
    },
    {
      "epoch": 1.6856,
      "grad_norm": 2.157977819442749,
      "learning_rate": 2.2053691275167788e-05,
      "loss": 0.5933,
      "step": 42140
    },
    {
      "epoch": 1.686,
      "grad_norm": 3.216418504714966,
      "learning_rate": 2.2046979865771813e-05,
      "loss": 0.4974,
      "step": 42150
    },
    {
      "epoch": 1.6864,
      "grad_norm": 2.672184705734253,
      "learning_rate": 2.204026845637584e-05,
      "loss": 0.5268,
      "step": 42160
    },
    {
      "epoch": 1.6867999999999999,
      "grad_norm": 2.5879437923431396,
      "learning_rate": 2.2033557046979867e-05,
      "loss": 0.5062,
      "step": 42170
    },
    {
      "epoch": 1.6872,
      "grad_norm": 2.231755256652832,
      "learning_rate": 2.2026845637583892e-05,
      "loss": 0.6428,
      "step": 42180
    },
    {
      "epoch": 1.6876,
      "grad_norm": 2.7130544185638428,
      "learning_rate": 2.202013422818792e-05,
      "loss": 0.5743,
      "step": 42190
    },
    {
      "epoch": 1.688,
      "grad_norm": 3.282332181930542,
      "learning_rate": 2.201342281879195e-05,
      "loss": 0.5865,
      "step": 42200
    },
    {
      "epoch": 1.6884000000000001,
      "grad_norm": 2.890429735183716,
      "learning_rate": 2.2006711409395974e-05,
      "loss": 0.5494,
      "step": 42210
    },
    {
      "epoch": 1.6888,
      "grad_norm": 3.4651365280151367,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.5405,
      "step": 42220
    },
    {
      "epoch": 1.6892,
      "grad_norm": 2.6828644275665283,
      "learning_rate": 2.1993288590604028e-05,
      "loss": 0.6009,
      "step": 42230
    },
    {
      "epoch": 1.6896,
      "grad_norm": 3.1066081523895264,
      "learning_rate": 2.1986577181208053e-05,
      "loss": 0.633,
      "step": 42240
    },
    {
      "epoch": 1.69,
      "grad_norm": 3.1131341457366943,
      "learning_rate": 2.197986577181208e-05,
      "loss": 0.5799,
      "step": 42250
    },
    {
      "epoch": 1.6904,
      "grad_norm": 2.9601221084594727,
      "learning_rate": 2.197315436241611e-05,
      "loss": 0.5416,
      "step": 42260
    },
    {
      "epoch": 1.6907999999999999,
      "grad_norm": 2.4263293743133545,
      "learning_rate": 2.1966442953020135e-05,
      "loss": 0.6265,
      "step": 42270
    },
    {
      "epoch": 1.6912,
      "grad_norm": 2.6701560020446777,
      "learning_rate": 2.1959731543624164e-05,
      "loss": 0.5612,
      "step": 42280
    },
    {
      "epoch": 1.6916,
      "grad_norm": 3.156578540802002,
      "learning_rate": 2.195302013422819e-05,
      "loss": 0.6219,
      "step": 42290
    },
    {
      "epoch": 1.692,
      "grad_norm": 3.5248069763183594,
      "learning_rate": 2.1946308724832214e-05,
      "loss": 0.6622,
      "step": 42300
    },
    {
      "epoch": 1.6924000000000001,
      "grad_norm": 2.6486282348632812,
      "learning_rate": 2.1939597315436243e-05,
      "loss": 0.531,
      "step": 42310
    },
    {
      "epoch": 1.6928,
      "grad_norm": 2.912567377090454,
      "learning_rate": 2.193288590604027e-05,
      "loss": 0.6344,
      "step": 42320
    },
    {
      "epoch": 1.6932,
      "grad_norm": 2.931783437728882,
      "learning_rate": 2.1926174496644297e-05,
      "loss": 0.5672,
      "step": 42330
    },
    {
      "epoch": 1.6936,
      "grad_norm": 3.007028341293335,
      "learning_rate": 2.1919463087248325e-05,
      "loss": 0.5953,
      "step": 42340
    },
    {
      "epoch": 1.694,
      "grad_norm": 2.872852325439453,
      "learning_rate": 2.191275167785235e-05,
      "loss": 0.5295,
      "step": 42350
    },
    {
      "epoch": 1.6944,
      "grad_norm": 2.3530094623565674,
      "learning_rate": 2.1906040268456375e-05,
      "loss": 0.553,
      "step": 42360
    },
    {
      "epoch": 1.6947999999999999,
      "grad_norm": 2.2945656776428223,
      "learning_rate": 2.1899328859060404e-05,
      "loss": 0.5147,
      "step": 42370
    },
    {
      "epoch": 1.6952,
      "grad_norm": 2.4951767921447754,
      "learning_rate": 2.189261744966443e-05,
      "loss": 0.581,
      "step": 42380
    },
    {
      "epoch": 1.6956,
      "grad_norm": 2.7763116359710693,
      "learning_rate": 2.1885906040268458e-05,
      "loss": 0.5072,
      "step": 42390
    },
    {
      "epoch": 1.696,
      "grad_norm": 2.1566600799560547,
      "learning_rate": 2.1879194630872486e-05,
      "loss": 0.4734,
      "step": 42400
    },
    {
      "epoch": 1.6964000000000001,
      "grad_norm": 2.619520664215088,
      "learning_rate": 2.187248322147651e-05,
      "loss": 0.5383,
      "step": 42410
    },
    {
      "epoch": 1.6968,
      "grad_norm": 2.335458278656006,
      "learning_rate": 2.1865771812080536e-05,
      "loss": 0.5771,
      "step": 42420
    },
    {
      "epoch": 1.6972,
      "grad_norm": 2.5445377826690674,
      "learning_rate": 2.1859060402684565e-05,
      "loss": 0.5733,
      "step": 42430
    },
    {
      "epoch": 1.6976,
      "grad_norm": 3.1825952529907227,
      "learning_rate": 2.185234899328859e-05,
      "loss": 0.5676,
      "step": 42440
    },
    {
      "epoch": 1.698,
      "grad_norm": 1.9838721752166748,
      "learning_rate": 2.184563758389262e-05,
      "loss": 0.5127,
      "step": 42450
    },
    {
      "epoch": 1.6984,
      "grad_norm": 3.077357292175293,
      "learning_rate": 2.1838926174496647e-05,
      "loss": 0.5826,
      "step": 42460
    },
    {
      "epoch": 1.6987999999999999,
      "grad_norm": 3.4359982013702393,
      "learning_rate": 2.1832214765100672e-05,
      "loss": 0.6811,
      "step": 42470
    },
    {
      "epoch": 1.6992,
      "grad_norm": 2.479416608810425,
      "learning_rate": 2.1825503355704698e-05,
      "loss": 0.5452,
      "step": 42480
    },
    {
      "epoch": 1.6996,
      "grad_norm": 1.9939063787460327,
      "learning_rate": 2.1818791946308726e-05,
      "loss": 0.6029,
      "step": 42490
    },
    {
      "epoch": 1.7,
      "grad_norm": 2.0592129230499268,
      "learning_rate": 2.181208053691275e-05,
      "loss": 0.51,
      "step": 42500
    },
    {
      "epoch": 1.7004000000000001,
      "grad_norm": 2.9620144367218018,
      "learning_rate": 2.180536912751678e-05,
      "loss": 0.5838,
      "step": 42510
    },
    {
      "epoch": 1.7008,
      "grad_norm": 3.069793939590454,
      "learning_rate": 2.179865771812081e-05,
      "loss": 0.548,
      "step": 42520
    },
    {
      "epoch": 1.7012,
      "grad_norm": 3.5036935806274414,
      "learning_rate": 2.1791946308724834e-05,
      "loss": 0.6253,
      "step": 42530
    },
    {
      "epoch": 1.7016,
      "grad_norm": 2.286916732788086,
      "learning_rate": 2.178523489932886e-05,
      "loss": 0.6124,
      "step": 42540
    },
    {
      "epoch": 1.702,
      "grad_norm": 2.2948951721191406,
      "learning_rate": 2.1778523489932887e-05,
      "loss": 0.5645,
      "step": 42550
    },
    {
      "epoch": 1.7024,
      "grad_norm": 2.8160712718963623,
      "learning_rate": 2.1771812080536912e-05,
      "loss": 0.5406,
      "step": 42560
    },
    {
      "epoch": 1.7027999999999999,
      "grad_norm": 2.7318320274353027,
      "learning_rate": 2.176510067114094e-05,
      "loss": 0.5814,
      "step": 42570
    },
    {
      "epoch": 1.7032,
      "grad_norm": 2.958997964859009,
      "learning_rate": 2.175838926174497e-05,
      "loss": 0.5873,
      "step": 42580
    },
    {
      "epoch": 1.7036,
      "grad_norm": 2.475358724594116,
      "learning_rate": 2.1751677852348995e-05,
      "loss": 0.5616,
      "step": 42590
    },
    {
      "epoch": 1.704,
      "grad_norm": 3.2370011806488037,
      "learning_rate": 2.174496644295302e-05,
      "loss": 0.6614,
      "step": 42600
    },
    {
      "epoch": 1.7044000000000001,
      "grad_norm": 2.910834789276123,
      "learning_rate": 2.173825503355705e-05,
      "loss": 0.5555,
      "step": 42610
    },
    {
      "epoch": 1.7048,
      "grad_norm": 2.731477737426758,
      "learning_rate": 2.1731543624161074e-05,
      "loss": 0.5727,
      "step": 42620
    },
    {
      "epoch": 1.7052,
      "grad_norm": 2.5505032539367676,
      "learning_rate": 2.1724832214765102e-05,
      "loss": 0.5307,
      "step": 42630
    },
    {
      "epoch": 1.7056,
      "grad_norm": 2.4970791339874268,
      "learning_rate": 2.171812080536913e-05,
      "loss": 0.5773,
      "step": 42640
    },
    {
      "epoch": 1.706,
      "grad_norm": 2.620026111602783,
      "learning_rate": 2.1711409395973156e-05,
      "loss": 0.5277,
      "step": 42650
    },
    {
      "epoch": 1.7064,
      "grad_norm": 2.971322536468506,
      "learning_rate": 2.1704697986577184e-05,
      "loss": 0.5228,
      "step": 42660
    },
    {
      "epoch": 1.7067999999999999,
      "grad_norm": 2.5635018348693848,
      "learning_rate": 2.169798657718121e-05,
      "loss": 0.5652,
      "step": 42670
    },
    {
      "epoch": 1.7072,
      "grad_norm": 2.8156423568725586,
      "learning_rate": 2.1691275167785235e-05,
      "loss": 0.5508,
      "step": 42680
    },
    {
      "epoch": 1.7076,
      "grad_norm": 3.009244441986084,
      "learning_rate": 2.1684563758389263e-05,
      "loss": 0.5491,
      "step": 42690
    },
    {
      "epoch": 1.708,
      "grad_norm": 2.210890054702759,
      "learning_rate": 2.167785234899329e-05,
      "loss": 0.564,
      "step": 42700
    },
    {
      "epoch": 1.7084000000000001,
      "grad_norm": 2.263371467590332,
      "learning_rate": 2.1671140939597317e-05,
      "loss": 0.5573,
      "step": 42710
    },
    {
      "epoch": 1.7088,
      "grad_norm": 2.5694048404693604,
      "learning_rate": 2.1664429530201345e-05,
      "loss": 0.5625,
      "step": 42720
    },
    {
      "epoch": 1.7092,
      "grad_norm": 2.5479061603546143,
      "learning_rate": 2.165771812080537e-05,
      "loss": 0.5138,
      "step": 42730
    },
    {
      "epoch": 1.7096,
      "grad_norm": 2.9020118713378906,
      "learning_rate": 2.1651006711409396e-05,
      "loss": 0.5214,
      "step": 42740
    },
    {
      "epoch": 1.71,
      "grad_norm": 2.797091007232666,
      "learning_rate": 2.1644295302013424e-05,
      "loss": 0.6014,
      "step": 42750
    },
    {
      "epoch": 1.7104,
      "grad_norm": 3.773780345916748,
      "learning_rate": 2.163758389261745e-05,
      "loss": 0.6373,
      "step": 42760
    },
    {
      "epoch": 1.7107999999999999,
      "grad_norm": 2.385791063308716,
      "learning_rate": 2.1630872483221478e-05,
      "loss": 0.5963,
      "step": 42770
    },
    {
      "epoch": 1.7112,
      "grad_norm": 2.445831060409546,
      "learning_rate": 2.1624161073825507e-05,
      "loss": 0.5173,
      "step": 42780
    },
    {
      "epoch": 1.7116,
      "grad_norm": 2.9938364028930664,
      "learning_rate": 2.1617449664429532e-05,
      "loss": 0.6202,
      "step": 42790
    },
    {
      "epoch": 1.712,
      "grad_norm": 2.8222100734710693,
      "learning_rate": 2.1610738255033557e-05,
      "loss": 0.5974,
      "step": 42800
    },
    {
      "epoch": 1.7124000000000001,
      "grad_norm": 2.8946073055267334,
      "learning_rate": 2.1604026845637585e-05,
      "loss": 0.5575,
      "step": 42810
    },
    {
      "epoch": 1.7128,
      "grad_norm": 3.1883676052093506,
      "learning_rate": 2.159731543624161e-05,
      "loss": 0.6272,
      "step": 42820
    },
    {
      "epoch": 1.7132,
      "grad_norm": 2.7481439113616943,
      "learning_rate": 2.159060402684564e-05,
      "loss": 0.5558,
      "step": 42830
    },
    {
      "epoch": 1.7136,
      "grad_norm": 2.299994707107544,
      "learning_rate": 2.1583892617449668e-05,
      "loss": 0.5477,
      "step": 42840
    },
    {
      "epoch": 1.714,
      "grad_norm": 3.0269851684570312,
      "learning_rate": 2.1577181208053693e-05,
      "loss": 0.5623,
      "step": 42850
    },
    {
      "epoch": 1.7144,
      "grad_norm": 1.7150037288665771,
      "learning_rate": 2.1570469798657718e-05,
      "loss": 0.5321,
      "step": 42860
    },
    {
      "epoch": 1.7147999999999999,
      "grad_norm": 1.6270626783370972,
      "learning_rate": 2.1563758389261747e-05,
      "loss": 0.6225,
      "step": 42870
    },
    {
      "epoch": 1.7151999999999998,
      "grad_norm": 3.068838596343994,
      "learning_rate": 2.1557046979865772e-05,
      "loss": 0.5788,
      "step": 42880
    },
    {
      "epoch": 1.7156,
      "grad_norm": 3.2069592475891113,
      "learning_rate": 2.1550335570469797e-05,
      "loss": 0.5736,
      "step": 42890
    },
    {
      "epoch": 1.716,
      "grad_norm": 3.087278366088867,
      "learning_rate": 2.154362416107383e-05,
      "loss": 0.5529,
      "step": 42900
    },
    {
      "epoch": 1.7164000000000001,
      "grad_norm": 2.8991830348968506,
      "learning_rate": 2.1536912751677854e-05,
      "loss": 0.5529,
      "step": 42910
    },
    {
      "epoch": 1.7168,
      "grad_norm": 2.5356545448303223,
      "learning_rate": 2.153020134228188e-05,
      "loss": 0.5711,
      "step": 42920
    },
    {
      "epoch": 1.7172,
      "grad_norm": 3.2970528602600098,
      "learning_rate": 2.1523489932885908e-05,
      "loss": 0.4868,
      "step": 42930
    },
    {
      "epoch": 1.7176,
      "grad_norm": 2.4389772415161133,
      "learning_rate": 2.1516778523489933e-05,
      "loss": 0.5159,
      "step": 42940
    },
    {
      "epoch": 1.718,
      "grad_norm": 2.3057680130004883,
      "learning_rate": 2.1510067114093958e-05,
      "loss": 0.5302,
      "step": 42950
    },
    {
      "epoch": 1.7184,
      "grad_norm": 2.6245734691619873,
      "learning_rate": 2.150335570469799e-05,
      "loss": 0.5524,
      "step": 42960
    },
    {
      "epoch": 1.7187999999999999,
      "grad_norm": 2.4229705333709717,
      "learning_rate": 2.1496644295302015e-05,
      "loss": 0.6291,
      "step": 42970
    },
    {
      "epoch": 1.7191999999999998,
      "grad_norm": 3.5946109294891357,
      "learning_rate": 2.148993288590604e-05,
      "loss": 0.6335,
      "step": 42980
    },
    {
      "epoch": 1.7196,
      "grad_norm": 2.80438494682312,
      "learning_rate": 2.148322147651007e-05,
      "loss": 0.6009,
      "step": 42990
    },
    {
      "epoch": 1.72,
      "grad_norm": 2.3422317504882812,
      "learning_rate": 2.1476510067114094e-05,
      "loss": 0.5192,
      "step": 43000
    },
    {
      "epoch": 1.7204000000000002,
      "grad_norm": 3.0217998027801514,
      "learning_rate": 2.1469798657718123e-05,
      "loss": 0.6051,
      "step": 43010
    },
    {
      "epoch": 1.7208,
      "grad_norm": 2.9639554023742676,
      "learning_rate": 2.1463087248322148e-05,
      "loss": 0.5719,
      "step": 43020
    },
    {
      "epoch": 1.7212,
      "grad_norm": 2.826765775680542,
      "learning_rate": 2.1456375838926176e-05,
      "loss": 0.6366,
      "step": 43030
    },
    {
      "epoch": 1.7216,
      "grad_norm": 3.007667303085327,
      "learning_rate": 2.14496644295302e-05,
      "loss": 0.5176,
      "step": 43040
    },
    {
      "epoch": 1.722,
      "grad_norm": 2.9052395820617676,
      "learning_rate": 2.144295302013423e-05,
      "loss": 0.5215,
      "step": 43050
    },
    {
      "epoch": 1.7224,
      "grad_norm": 2.4227519035339355,
      "learning_rate": 2.1436241610738255e-05,
      "loss": 0.6279,
      "step": 43060
    },
    {
      "epoch": 1.7227999999999999,
      "grad_norm": 2.466273784637451,
      "learning_rate": 2.1429530201342284e-05,
      "loss": 0.6034,
      "step": 43070
    },
    {
      "epoch": 1.7231999999999998,
      "grad_norm": 3.0456793308258057,
      "learning_rate": 2.142281879194631e-05,
      "loss": 0.5717,
      "step": 43080
    },
    {
      "epoch": 1.7236,
      "grad_norm": 3.0270941257476807,
      "learning_rate": 2.1416107382550337e-05,
      "loss": 0.4982,
      "step": 43090
    },
    {
      "epoch": 1.724,
      "grad_norm": 2.4976308345794678,
      "learning_rate": 2.1409395973154362e-05,
      "loss": 0.6315,
      "step": 43100
    },
    {
      "epoch": 1.7244000000000002,
      "grad_norm": 2.854556083679199,
      "learning_rate": 2.140268456375839e-05,
      "loss": 0.5499,
      "step": 43110
    },
    {
      "epoch": 1.7248,
      "grad_norm": 2.3985462188720703,
      "learning_rate": 2.1395973154362416e-05,
      "loss": 0.4796,
      "step": 43120
    },
    {
      "epoch": 1.7252,
      "grad_norm": 2.4999873638153076,
      "learning_rate": 2.1389261744966445e-05,
      "loss": 0.672,
      "step": 43130
    },
    {
      "epoch": 1.7256,
      "grad_norm": 3.1551172733306885,
      "learning_rate": 2.138255033557047e-05,
      "loss": 0.6132,
      "step": 43140
    },
    {
      "epoch": 1.726,
      "grad_norm": 2.9953806400299072,
      "learning_rate": 2.13758389261745e-05,
      "loss": 0.5951,
      "step": 43150
    },
    {
      "epoch": 1.7264,
      "grad_norm": 2.608067512512207,
      "learning_rate": 2.1369127516778527e-05,
      "loss": 0.4738,
      "step": 43160
    },
    {
      "epoch": 1.7268,
      "grad_norm": 2.870009422302246,
      "learning_rate": 2.1362416107382552e-05,
      "loss": 0.5372,
      "step": 43170
    },
    {
      "epoch": 1.7271999999999998,
      "grad_norm": 3.01194429397583,
      "learning_rate": 2.1355704697986577e-05,
      "loss": 0.5703,
      "step": 43180
    },
    {
      "epoch": 1.7276,
      "grad_norm": 3.0469167232513428,
      "learning_rate": 2.1348993288590606e-05,
      "loss": 0.5198,
      "step": 43190
    },
    {
      "epoch": 1.728,
      "grad_norm": 2.4611876010894775,
      "learning_rate": 2.134228187919463e-05,
      "loss": 0.5778,
      "step": 43200
    },
    {
      "epoch": 1.7284000000000002,
      "grad_norm": 2.262010335922241,
      "learning_rate": 2.1335570469798656e-05,
      "loss": 0.5014,
      "step": 43210
    },
    {
      "epoch": 1.7288000000000001,
      "grad_norm": 2.7172951698303223,
      "learning_rate": 2.1328859060402688e-05,
      "loss": 0.5089,
      "step": 43220
    },
    {
      "epoch": 1.7292,
      "grad_norm": 2.379817485809326,
      "learning_rate": 2.1322147651006713e-05,
      "loss": 0.5879,
      "step": 43230
    },
    {
      "epoch": 1.7296,
      "grad_norm": 2.6906774044036865,
      "learning_rate": 2.131543624161074e-05,
      "loss": 0.5408,
      "step": 43240
    },
    {
      "epoch": 1.73,
      "grad_norm": 2.83186674118042,
      "learning_rate": 2.1308724832214767e-05,
      "loss": 0.5795,
      "step": 43250
    },
    {
      "epoch": 1.7304,
      "grad_norm": 2.2752764225006104,
      "learning_rate": 2.1302013422818792e-05,
      "loss": 0.488,
      "step": 43260
    },
    {
      "epoch": 1.7308,
      "grad_norm": 2.5248849391937256,
      "learning_rate": 2.1295302013422817e-05,
      "loss": 0.626,
      "step": 43270
    },
    {
      "epoch": 1.7311999999999999,
      "grad_norm": 2.473529100418091,
      "learning_rate": 2.128859060402685e-05,
      "loss": 0.5241,
      "step": 43280
    },
    {
      "epoch": 1.7316,
      "grad_norm": 2.9019267559051514,
      "learning_rate": 2.1281879194630874e-05,
      "loss": 0.558,
      "step": 43290
    },
    {
      "epoch": 1.732,
      "grad_norm": 2.122687816619873,
      "learning_rate": 2.12751677852349e-05,
      "loss": 0.4526,
      "step": 43300
    },
    {
      "epoch": 1.7324000000000002,
      "grad_norm": 2.649799108505249,
      "learning_rate": 2.1268456375838928e-05,
      "loss": 0.5031,
      "step": 43310
    },
    {
      "epoch": 1.7328000000000001,
      "grad_norm": 3.181832790374756,
      "learning_rate": 2.1261744966442953e-05,
      "loss": 0.5801,
      "step": 43320
    },
    {
      "epoch": 1.7332,
      "grad_norm": 3.4775967597961426,
      "learning_rate": 2.125503355704698e-05,
      "loss": 0.6001,
      "step": 43330
    },
    {
      "epoch": 1.7336,
      "grad_norm": 2.9573044776916504,
      "learning_rate": 2.1248322147651007e-05,
      "loss": 0.5447,
      "step": 43340
    },
    {
      "epoch": 1.734,
      "grad_norm": 2.751427173614502,
      "learning_rate": 2.1241610738255036e-05,
      "loss": 0.5945,
      "step": 43350
    },
    {
      "epoch": 1.7344,
      "grad_norm": 2.7304892539978027,
      "learning_rate": 2.123489932885906e-05,
      "loss": 0.6156,
      "step": 43360
    },
    {
      "epoch": 1.7348,
      "grad_norm": 2.176257371902466,
      "learning_rate": 2.122818791946309e-05,
      "loss": 0.5026,
      "step": 43370
    },
    {
      "epoch": 1.7351999999999999,
      "grad_norm": 3.197422742843628,
      "learning_rate": 2.1221476510067114e-05,
      "loss": 0.5473,
      "step": 43380
    },
    {
      "epoch": 1.7356,
      "grad_norm": 2.2109475135803223,
      "learning_rate": 2.121476510067114e-05,
      "loss": 0.5791,
      "step": 43390
    },
    {
      "epoch": 1.736,
      "grad_norm": 2.3194291591644287,
      "learning_rate": 2.1208053691275168e-05,
      "loss": 0.4802,
      "step": 43400
    },
    {
      "epoch": 1.7364000000000002,
      "grad_norm": 2.484179973602295,
      "learning_rate": 2.1201342281879197e-05,
      "loss": 0.5587,
      "step": 43410
    },
    {
      "epoch": 1.7368000000000001,
      "grad_norm": 2.898825168609619,
      "learning_rate": 2.1194630872483222e-05,
      "loss": 0.604,
      "step": 43420
    },
    {
      "epoch": 1.7372,
      "grad_norm": 2.4362876415252686,
      "learning_rate": 2.118791946308725e-05,
      "loss": 0.5964,
      "step": 43430
    },
    {
      "epoch": 1.7376,
      "grad_norm": 3.2114856243133545,
      "learning_rate": 2.1181208053691275e-05,
      "loss": 0.5884,
      "step": 43440
    },
    {
      "epoch": 1.738,
      "grad_norm": 2.851862907409668,
      "learning_rate": 2.1174496644295304e-05,
      "loss": 0.5813,
      "step": 43450
    },
    {
      "epoch": 1.7384,
      "grad_norm": 2.6917948722839355,
      "learning_rate": 2.116778523489933e-05,
      "loss": 0.5222,
      "step": 43460
    },
    {
      "epoch": 1.7388,
      "grad_norm": 1.8830993175506592,
      "learning_rate": 2.1161073825503358e-05,
      "loss": 0.4998,
      "step": 43470
    },
    {
      "epoch": 1.7391999999999999,
      "grad_norm": 2.929845094680786,
      "learning_rate": 2.1154362416107383e-05,
      "loss": 0.4804,
      "step": 43480
    },
    {
      "epoch": 1.7396,
      "grad_norm": 1.982411503791809,
      "learning_rate": 2.114765100671141e-05,
      "loss": 0.572,
      "step": 43490
    },
    {
      "epoch": 1.74,
      "grad_norm": 1.8943312168121338,
      "learning_rate": 2.1140939597315437e-05,
      "loss": 0.5614,
      "step": 43500
    },
    {
      "epoch": 1.7404,
      "grad_norm": 3.1857521533966064,
      "learning_rate": 2.1134228187919465e-05,
      "loss": 0.6401,
      "step": 43510
    },
    {
      "epoch": 1.7408000000000001,
      "grad_norm": 2.756923198699951,
      "learning_rate": 2.112751677852349e-05,
      "loss": 0.5832,
      "step": 43520
    },
    {
      "epoch": 1.7412,
      "grad_norm": 2.6043806076049805,
      "learning_rate": 2.1120805369127515e-05,
      "loss": 0.5431,
      "step": 43530
    },
    {
      "epoch": 1.7416,
      "grad_norm": 6.407642841339111,
      "learning_rate": 2.1114093959731544e-05,
      "loss": 0.5911,
      "step": 43540
    },
    {
      "epoch": 1.742,
      "grad_norm": 2.7227768898010254,
      "learning_rate": 2.1107382550335573e-05,
      "loss": 0.5766,
      "step": 43550
    },
    {
      "epoch": 1.7424,
      "grad_norm": 3.018038511276245,
      "learning_rate": 2.1100671140939598e-05,
      "loss": 0.5711,
      "step": 43560
    },
    {
      "epoch": 1.7428,
      "grad_norm": 2.5918262004852295,
      "learning_rate": 2.1093959731543626e-05,
      "loss": 0.601,
      "step": 43570
    },
    {
      "epoch": 1.7431999999999999,
      "grad_norm": 1.8935637474060059,
      "learning_rate": 2.108724832214765e-05,
      "loss": 0.5397,
      "step": 43580
    },
    {
      "epoch": 1.7436,
      "grad_norm": 2.4352798461914062,
      "learning_rate": 2.1080536912751677e-05,
      "loss": 0.5339,
      "step": 43590
    },
    {
      "epoch": 1.744,
      "grad_norm": 2.87884259223938,
      "learning_rate": 2.107382550335571e-05,
      "loss": 0.5411,
      "step": 43600
    },
    {
      "epoch": 1.7444,
      "grad_norm": 3.406332492828369,
      "learning_rate": 2.1067114093959734e-05,
      "loss": 0.6004,
      "step": 43610
    },
    {
      "epoch": 1.7448000000000001,
      "grad_norm": 2.545825958251953,
      "learning_rate": 2.106040268456376e-05,
      "loss": 0.5509,
      "step": 43620
    },
    {
      "epoch": 1.7452,
      "grad_norm": 2.5498955249786377,
      "learning_rate": 2.1053691275167787e-05,
      "loss": 0.5976,
      "step": 43630
    },
    {
      "epoch": 1.7456,
      "grad_norm": 1.8582340478897095,
      "learning_rate": 2.1046979865771813e-05,
      "loss": 0.5063,
      "step": 43640
    },
    {
      "epoch": 1.746,
      "grad_norm": 2.3414883613586426,
      "learning_rate": 2.1040268456375838e-05,
      "loss": 0.5145,
      "step": 43650
    },
    {
      "epoch": 1.7464,
      "grad_norm": 2.8827219009399414,
      "learning_rate": 2.1033557046979866e-05,
      "loss": 0.5148,
      "step": 43660
    },
    {
      "epoch": 1.7468,
      "grad_norm": 2.641139507293701,
      "learning_rate": 2.1026845637583895e-05,
      "loss": 0.5096,
      "step": 43670
    },
    {
      "epoch": 1.7471999999999999,
      "grad_norm": 2.953707218170166,
      "learning_rate": 2.102013422818792e-05,
      "loss": 0.548,
      "step": 43680
    },
    {
      "epoch": 1.7476,
      "grad_norm": 2.0565149784088135,
      "learning_rate": 2.101342281879195e-05,
      "loss": 0.5808,
      "step": 43690
    },
    {
      "epoch": 1.748,
      "grad_norm": 3.148242235183716,
      "learning_rate": 2.1006711409395974e-05,
      "loss": 0.5519,
      "step": 43700
    },
    {
      "epoch": 1.7484,
      "grad_norm": 3.0818042755126953,
      "learning_rate": 2.1e-05,
      "loss": 0.6251,
      "step": 43710
    },
    {
      "epoch": 1.7488000000000001,
      "grad_norm": 2.6541600227355957,
      "learning_rate": 2.0993288590604027e-05,
      "loss": 0.5392,
      "step": 43720
    },
    {
      "epoch": 1.7492,
      "grad_norm": 2.2776939868927,
      "learning_rate": 2.0986577181208056e-05,
      "loss": 0.4666,
      "step": 43730
    },
    {
      "epoch": 1.7496,
      "grad_norm": 2.653275728225708,
      "learning_rate": 2.097986577181208e-05,
      "loss": 0.5465,
      "step": 43740
    },
    {
      "epoch": 1.75,
      "grad_norm": 2.759794235229492,
      "learning_rate": 2.097315436241611e-05,
      "loss": 0.6289,
      "step": 43750
    },
    {
      "epoch": 1.7504,
      "grad_norm": 2.513136386871338,
      "learning_rate": 2.0966442953020135e-05,
      "loss": 0.5511,
      "step": 43760
    },
    {
      "epoch": 1.7508,
      "grad_norm": 3.3415465354919434,
      "learning_rate": 2.095973154362416e-05,
      "loss": 0.6176,
      "step": 43770
    },
    {
      "epoch": 1.7511999999999999,
      "grad_norm": 3.193068742752075,
      "learning_rate": 2.095302013422819e-05,
      "loss": 0.6336,
      "step": 43780
    },
    {
      "epoch": 1.7516,
      "grad_norm": 3.4575254917144775,
      "learning_rate": 2.0946308724832217e-05,
      "loss": 0.5591,
      "step": 43790
    },
    {
      "epoch": 1.752,
      "grad_norm": 2.4776012897491455,
      "learning_rate": 2.0939597315436242e-05,
      "loss": 0.4971,
      "step": 43800
    },
    {
      "epoch": 1.7524,
      "grad_norm": 2.8159799575805664,
      "learning_rate": 2.093288590604027e-05,
      "loss": 0.5351,
      "step": 43810
    },
    {
      "epoch": 1.7528000000000001,
      "grad_norm": 2.591301441192627,
      "learning_rate": 2.0926174496644296e-05,
      "loss": 0.5665,
      "step": 43820
    },
    {
      "epoch": 1.7532,
      "grad_norm": 3.046217679977417,
      "learning_rate": 2.091946308724832e-05,
      "loss": 0.6089,
      "step": 43830
    },
    {
      "epoch": 1.7536,
      "grad_norm": 2.7196507453918457,
      "learning_rate": 2.091275167785235e-05,
      "loss": 0.5373,
      "step": 43840
    },
    {
      "epoch": 1.754,
      "grad_norm": 2.7581417560577393,
      "learning_rate": 2.0906040268456375e-05,
      "loss": 0.5825,
      "step": 43850
    },
    {
      "epoch": 1.7544,
      "grad_norm": 2.7003672122955322,
      "learning_rate": 2.0899328859060403e-05,
      "loss": 0.6013,
      "step": 43860
    },
    {
      "epoch": 1.7548,
      "grad_norm": 3.066924571990967,
      "learning_rate": 2.0892617449664432e-05,
      "loss": 0.5273,
      "step": 43870
    },
    {
      "epoch": 1.7551999999999999,
      "grad_norm": 2.824544906616211,
      "learning_rate": 2.0885906040268457e-05,
      "loss": 0.6609,
      "step": 43880
    },
    {
      "epoch": 1.7556,
      "grad_norm": 2.4509057998657227,
      "learning_rate": 2.0879194630872486e-05,
      "loss": 0.4908,
      "step": 43890
    },
    {
      "epoch": 1.756,
      "grad_norm": 2.8725032806396484,
      "learning_rate": 2.087248322147651e-05,
      "loss": 0.5581,
      "step": 43900
    },
    {
      "epoch": 1.7564,
      "grad_norm": 2.2531158924102783,
      "learning_rate": 2.0865771812080536e-05,
      "loss": 0.609,
      "step": 43910
    },
    {
      "epoch": 1.7568000000000001,
      "grad_norm": 2.5084376335144043,
      "learning_rate": 2.0859060402684564e-05,
      "loss": 0.5843,
      "step": 43920
    },
    {
      "epoch": 1.7572,
      "grad_norm": 2.8487024307250977,
      "learning_rate": 2.0852348993288593e-05,
      "loss": 0.5553,
      "step": 43930
    },
    {
      "epoch": 1.7576,
      "grad_norm": 2.514514446258545,
      "learning_rate": 2.0845637583892618e-05,
      "loss": 0.5229,
      "step": 43940
    },
    {
      "epoch": 1.758,
      "grad_norm": 2.8039937019348145,
      "learning_rate": 2.0838926174496647e-05,
      "loss": 0.455,
      "step": 43950
    },
    {
      "epoch": 1.7584,
      "grad_norm": 2.5955889225006104,
      "learning_rate": 2.0832214765100672e-05,
      "loss": 0.5408,
      "step": 43960
    },
    {
      "epoch": 1.7588,
      "grad_norm": 2.357050895690918,
      "learning_rate": 2.0825503355704697e-05,
      "loss": 0.5158,
      "step": 43970
    },
    {
      "epoch": 1.7591999999999999,
      "grad_norm": 2.4198954105377197,
      "learning_rate": 2.0818791946308726e-05,
      "loss": 0.5653,
      "step": 43980
    },
    {
      "epoch": 1.7596,
      "grad_norm": 3.126457929611206,
      "learning_rate": 2.0812080536912754e-05,
      "loss": 0.5839,
      "step": 43990
    },
    {
      "epoch": 1.76,
      "grad_norm": 2.4497878551483154,
      "learning_rate": 2.080536912751678e-05,
      "loss": 0.5574,
      "step": 44000
    },
    {
      "epoch": 1.7604,
      "grad_norm": 3.0589582920074463,
      "learning_rate": 2.0798657718120808e-05,
      "loss": 0.5521,
      "step": 44010
    },
    {
      "epoch": 1.7608000000000001,
      "grad_norm": 2.5426650047302246,
      "learning_rate": 2.0791946308724833e-05,
      "loss": 0.6173,
      "step": 44020
    },
    {
      "epoch": 1.7612,
      "grad_norm": 2.442702054977417,
      "learning_rate": 2.0785234899328858e-05,
      "loss": 0.5711,
      "step": 44030
    },
    {
      "epoch": 1.7616,
      "grad_norm": 3.143533706665039,
      "learning_rate": 2.0778523489932887e-05,
      "loss": 0.5688,
      "step": 44040
    },
    {
      "epoch": 1.762,
      "grad_norm": 2.474442481994629,
      "learning_rate": 2.0771812080536915e-05,
      "loss": 0.5002,
      "step": 44050
    },
    {
      "epoch": 1.7624,
      "grad_norm": 3.7763633728027344,
      "learning_rate": 2.076510067114094e-05,
      "loss": 0.5565,
      "step": 44060
    },
    {
      "epoch": 1.7628,
      "grad_norm": 2.345245122909546,
      "learning_rate": 2.075838926174497e-05,
      "loss": 0.5669,
      "step": 44070
    },
    {
      "epoch": 1.7631999999999999,
      "grad_norm": 2.249422311782837,
      "learning_rate": 2.0751677852348994e-05,
      "loss": 0.5466,
      "step": 44080
    },
    {
      "epoch": 1.7635999999999998,
      "grad_norm": 2.092942953109741,
      "learning_rate": 2.074496644295302e-05,
      "loss": 0.468,
      "step": 44090
    },
    {
      "epoch": 1.764,
      "grad_norm": 2.9592814445495605,
      "learning_rate": 2.0738255033557048e-05,
      "loss": 0.5833,
      "step": 44100
    },
    {
      "epoch": 1.7644,
      "grad_norm": 3.0605368614196777,
      "learning_rate": 2.0731543624161076e-05,
      "loss": 0.5663,
      "step": 44110
    },
    {
      "epoch": 1.7648000000000001,
      "grad_norm": 2.9572083950042725,
      "learning_rate": 2.07248322147651e-05,
      "loss": 0.59,
      "step": 44120
    },
    {
      "epoch": 1.7652,
      "grad_norm": 3.2084152698516846,
      "learning_rate": 2.071812080536913e-05,
      "loss": 0.6228,
      "step": 44130
    },
    {
      "epoch": 1.7656,
      "grad_norm": 2.1999990940093994,
      "learning_rate": 2.0711409395973155e-05,
      "loss": 0.5469,
      "step": 44140
    },
    {
      "epoch": 1.766,
      "grad_norm": 2.8896400928497314,
      "learning_rate": 2.070469798657718e-05,
      "loss": 0.5811,
      "step": 44150
    },
    {
      "epoch": 1.7664,
      "grad_norm": 3.343930244445801,
      "learning_rate": 2.069798657718121e-05,
      "loss": 0.6149,
      "step": 44160
    },
    {
      "epoch": 1.7668,
      "grad_norm": 2.6069655418395996,
      "learning_rate": 2.0691275167785234e-05,
      "loss": 0.5341,
      "step": 44170
    },
    {
      "epoch": 1.7671999999999999,
      "grad_norm": 3.250910758972168,
      "learning_rate": 2.0684563758389263e-05,
      "loss": 0.5916,
      "step": 44180
    },
    {
      "epoch": 1.7675999999999998,
      "grad_norm": 2.6194000244140625,
      "learning_rate": 2.067785234899329e-05,
      "loss": 0.5162,
      "step": 44190
    },
    {
      "epoch": 1.768,
      "grad_norm": 2.6954548358917236,
      "learning_rate": 2.0671140939597316e-05,
      "loss": 0.5888,
      "step": 44200
    },
    {
      "epoch": 1.7684,
      "grad_norm": 3.077068567276001,
      "learning_rate": 2.066442953020134e-05,
      "loss": 0.6029,
      "step": 44210
    },
    {
      "epoch": 1.7688000000000001,
      "grad_norm": 3.0194547176361084,
      "learning_rate": 2.065771812080537e-05,
      "loss": 0.5312,
      "step": 44220
    },
    {
      "epoch": 1.7692,
      "grad_norm": 2.613271474838257,
      "learning_rate": 2.0651006711409395e-05,
      "loss": 0.5182,
      "step": 44230
    },
    {
      "epoch": 1.7696,
      "grad_norm": 2.9866366386413574,
      "learning_rate": 2.0644295302013424e-05,
      "loss": 0.6068,
      "step": 44240
    },
    {
      "epoch": 1.77,
      "grad_norm": 2.73701810836792,
      "learning_rate": 2.0637583892617452e-05,
      "loss": 0.5175,
      "step": 44250
    },
    {
      "epoch": 1.7704,
      "grad_norm": 2.80625581741333,
      "learning_rate": 2.0630872483221477e-05,
      "loss": 0.5164,
      "step": 44260
    },
    {
      "epoch": 1.7708,
      "grad_norm": 2.5876212120056152,
      "learning_rate": 2.0624161073825503e-05,
      "loss": 0.5683,
      "step": 44270
    },
    {
      "epoch": 1.7711999999999999,
      "grad_norm": 1.9274851083755493,
      "learning_rate": 2.061744966442953e-05,
      "loss": 0.4907,
      "step": 44280
    },
    {
      "epoch": 1.7715999999999998,
      "grad_norm": 3.264381170272827,
      "learning_rate": 2.0610738255033556e-05,
      "loss": 0.6134,
      "step": 44290
    },
    {
      "epoch": 1.772,
      "grad_norm": 2.114858388900757,
      "learning_rate": 2.0604026845637585e-05,
      "loss": 0.5542,
      "step": 44300
    },
    {
      "epoch": 1.7724,
      "grad_norm": 2.743152618408203,
      "learning_rate": 2.0597315436241613e-05,
      "loss": 0.5415,
      "step": 44310
    },
    {
      "epoch": 1.7728000000000002,
      "grad_norm": 2.674067735671997,
      "learning_rate": 2.059060402684564e-05,
      "loss": 0.518,
      "step": 44320
    },
    {
      "epoch": 1.7732,
      "grad_norm": 3.4932498931884766,
      "learning_rate": 2.0583892617449667e-05,
      "loss": 0.6162,
      "step": 44330
    },
    {
      "epoch": 1.7736,
      "grad_norm": 3.984935998916626,
      "learning_rate": 2.0577181208053692e-05,
      "loss": 0.6075,
      "step": 44340
    },
    {
      "epoch": 1.774,
      "grad_norm": 2.639585256576538,
      "learning_rate": 2.0570469798657717e-05,
      "loss": 0.5486,
      "step": 44350
    },
    {
      "epoch": 1.7744,
      "grad_norm": 2.627779960632324,
      "learning_rate": 2.0563758389261746e-05,
      "loss": 0.5266,
      "step": 44360
    },
    {
      "epoch": 1.7748,
      "grad_norm": 2.6300220489501953,
      "learning_rate": 2.0557046979865775e-05,
      "loss": 0.5943,
      "step": 44370
    },
    {
      "epoch": 1.7752,
      "grad_norm": 2.474684000015259,
      "learning_rate": 2.05503355704698e-05,
      "loss": 0.49,
      "step": 44380
    },
    {
      "epoch": 1.7755999999999998,
      "grad_norm": 2.6540637016296387,
      "learning_rate": 2.0543624161073828e-05,
      "loss": 0.5612,
      "step": 44390
    },
    {
      "epoch": 1.776,
      "grad_norm": 2.7320122718811035,
      "learning_rate": 2.0536912751677853e-05,
      "loss": 0.6193,
      "step": 44400
    },
    {
      "epoch": 1.7764,
      "grad_norm": 3.9432730674743652,
      "learning_rate": 2.053020134228188e-05,
      "loss": 0.6296,
      "step": 44410
    },
    {
      "epoch": 1.7768000000000002,
      "grad_norm": 2.801994800567627,
      "learning_rate": 2.0523489932885907e-05,
      "loss": 0.5018,
      "step": 44420
    },
    {
      "epoch": 1.7772000000000001,
      "grad_norm": 2.118889093399048,
      "learning_rate": 2.0516778523489936e-05,
      "loss": 0.5183,
      "step": 44430
    },
    {
      "epoch": 1.7776,
      "grad_norm": 2.384512186050415,
      "learning_rate": 2.051006711409396e-05,
      "loss": 0.5722,
      "step": 44440
    },
    {
      "epoch": 1.778,
      "grad_norm": 2.2750742435455322,
      "learning_rate": 2.050335570469799e-05,
      "loss": 0.5261,
      "step": 44450
    },
    {
      "epoch": 1.7784,
      "grad_norm": 2.6475305557250977,
      "learning_rate": 2.0496644295302015e-05,
      "loss": 0.5811,
      "step": 44460
    },
    {
      "epoch": 1.7788,
      "grad_norm": 3.4048264026641846,
      "learning_rate": 2.048993288590604e-05,
      "loss": 0.5348,
      "step": 44470
    },
    {
      "epoch": 1.7792,
      "grad_norm": 2.697361946105957,
      "learning_rate": 2.0483221476510068e-05,
      "loss": 0.6548,
      "step": 44480
    },
    {
      "epoch": 1.7795999999999998,
      "grad_norm": 3.2328226566314697,
      "learning_rate": 2.0476510067114093e-05,
      "loss": 0.6283,
      "step": 44490
    },
    {
      "epoch": 1.78,
      "grad_norm": 2.9390809535980225,
      "learning_rate": 2.0469798657718122e-05,
      "loss": 0.549,
      "step": 44500
    },
    {
      "epoch": 1.7804,
      "grad_norm": 2.490873098373413,
      "learning_rate": 2.046308724832215e-05,
      "loss": 0.57,
      "step": 44510
    },
    {
      "epoch": 1.7808000000000002,
      "grad_norm": 3.1394529342651367,
      "learning_rate": 2.0456375838926176e-05,
      "loss": 0.5793,
      "step": 44520
    },
    {
      "epoch": 1.7812000000000001,
      "grad_norm": 2.9860167503356934,
      "learning_rate": 2.04496644295302e-05,
      "loss": 0.6559,
      "step": 44530
    },
    {
      "epoch": 1.7816,
      "grad_norm": 2.693199872970581,
      "learning_rate": 2.044295302013423e-05,
      "loss": 0.5373,
      "step": 44540
    },
    {
      "epoch": 1.782,
      "grad_norm": 2.8457958698272705,
      "learning_rate": 2.0436241610738254e-05,
      "loss": 0.534,
      "step": 44550
    },
    {
      "epoch": 1.7824,
      "grad_norm": 3.0634446144104004,
      "learning_rate": 2.0429530201342283e-05,
      "loss": 0.5378,
      "step": 44560
    },
    {
      "epoch": 1.7828,
      "grad_norm": 2.3076419830322266,
      "learning_rate": 2.042281879194631e-05,
      "loss": 0.512,
      "step": 44570
    },
    {
      "epoch": 1.7832,
      "grad_norm": 2.3806424140930176,
      "learning_rate": 2.0416107382550337e-05,
      "loss": 0.5195,
      "step": 44580
    },
    {
      "epoch": 1.7835999999999999,
      "grad_norm": 2.443814277648926,
      "learning_rate": 2.0409395973154362e-05,
      "loss": 0.5288,
      "step": 44590
    },
    {
      "epoch": 1.784,
      "grad_norm": 3.135143995285034,
      "learning_rate": 2.040268456375839e-05,
      "loss": 0.5384,
      "step": 44600
    },
    {
      "epoch": 1.7844,
      "grad_norm": 4.093446731567383,
      "learning_rate": 2.0395973154362416e-05,
      "loss": 0.6029,
      "step": 44610
    },
    {
      "epoch": 1.7848000000000002,
      "grad_norm": 2.55056095123291,
      "learning_rate": 2.0389261744966444e-05,
      "loss": 0.5829,
      "step": 44620
    },
    {
      "epoch": 1.7852000000000001,
      "grad_norm": 2.493912696838379,
      "learning_rate": 2.0382550335570473e-05,
      "loss": 0.6243,
      "step": 44630
    },
    {
      "epoch": 1.7856,
      "grad_norm": 2.5017521381378174,
      "learning_rate": 2.0375838926174498e-05,
      "loss": 0.4693,
      "step": 44640
    },
    {
      "epoch": 1.786,
      "grad_norm": 2.3150365352630615,
      "learning_rate": 2.0369127516778523e-05,
      "loss": 0.5176,
      "step": 44650
    },
    {
      "epoch": 1.7864,
      "grad_norm": 2.8638007640838623,
      "learning_rate": 2.036241610738255e-05,
      "loss": 0.5919,
      "step": 44660
    },
    {
      "epoch": 1.7868,
      "grad_norm": 2.4762356281280518,
      "learning_rate": 2.0355704697986577e-05,
      "loss": 0.5297,
      "step": 44670
    },
    {
      "epoch": 1.7872,
      "grad_norm": 2.505126476287842,
      "learning_rate": 2.0348993288590605e-05,
      "loss": 0.5248,
      "step": 44680
    },
    {
      "epoch": 1.7875999999999999,
      "grad_norm": 3.15728759765625,
      "learning_rate": 2.0342281879194634e-05,
      "loss": 0.5025,
      "step": 44690
    },
    {
      "epoch": 1.788,
      "grad_norm": 2.3818359375,
      "learning_rate": 2.033557046979866e-05,
      "loss": 0.5644,
      "step": 44700
    },
    {
      "epoch": 1.7884,
      "grad_norm": 2.5612435340881348,
      "learning_rate": 2.0328859060402684e-05,
      "loss": 0.5377,
      "step": 44710
    },
    {
      "epoch": 1.7888,
      "grad_norm": 2.614701509475708,
      "learning_rate": 2.0322147651006713e-05,
      "loss": 0.5614,
      "step": 44720
    },
    {
      "epoch": 1.7892000000000001,
      "grad_norm": 2.845768451690674,
      "learning_rate": 2.0315436241610738e-05,
      "loss": 0.4891,
      "step": 44730
    },
    {
      "epoch": 1.7896,
      "grad_norm": 2.976762056350708,
      "learning_rate": 2.0308724832214766e-05,
      "loss": 0.591,
      "step": 44740
    },
    {
      "epoch": 1.79,
      "grad_norm": 3.496711492538452,
      "learning_rate": 2.0302013422818795e-05,
      "loss": 0.6017,
      "step": 44750
    },
    {
      "epoch": 1.7904,
      "grad_norm": 3.2325918674468994,
      "learning_rate": 2.029530201342282e-05,
      "loss": 0.6469,
      "step": 44760
    },
    {
      "epoch": 1.7908,
      "grad_norm": 2.6433286666870117,
      "learning_rate": 2.028859060402685e-05,
      "loss": 0.5106,
      "step": 44770
    },
    {
      "epoch": 1.7912,
      "grad_norm": 2.754142999649048,
      "learning_rate": 2.0281879194630874e-05,
      "loss": 0.5478,
      "step": 44780
    },
    {
      "epoch": 1.7915999999999999,
      "grad_norm": 3.788942575454712,
      "learning_rate": 2.02751677852349e-05,
      "loss": 0.6751,
      "step": 44790
    },
    {
      "epoch": 1.792,
      "grad_norm": 2.7632670402526855,
      "learning_rate": 2.0268456375838928e-05,
      "loss": 0.5441,
      "step": 44800
    },
    {
      "epoch": 1.7924,
      "grad_norm": 2.638458251953125,
      "learning_rate": 2.0261744966442953e-05,
      "loss": 0.5719,
      "step": 44810
    },
    {
      "epoch": 1.7928,
      "grad_norm": 2.639875888824463,
      "learning_rate": 2.025503355704698e-05,
      "loss": 0.634,
      "step": 44820
    },
    {
      "epoch": 1.7932000000000001,
      "grad_norm": 2.5845868587493896,
      "learning_rate": 2.024832214765101e-05,
      "loss": 0.5935,
      "step": 44830
    },
    {
      "epoch": 1.7936,
      "grad_norm": 2.9889962673187256,
      "learning_rate": 2.0241610738255035e-05,
      "loss": 0.5234,
      "step": 44840
    },
    {
      "epoch": 1.794,
      "grad_norm": 2.596529722213745,
      "learning_rate": 2.023489932885906e-05,
      "loss": 0.5324,
      "step": 44850
    },
    {
      "epoch": 1.7944,
      "grad_norm": 2.9153571128845215,
      "learning_rate": 2.022818791946309e-05,
      "loss": 0.6266,
      "step": 44860
    },
    {
      "epoch": 1.7948,
      "grad_norm": 2.881923198699951,
      "learning_rate": 2.0221476510067114e-05,
      "loss": 0.5175,
      "step": 44870
    },
    {
      "epoch": 1.7952,
      "grad_norm": 2.5635316371917725,
      "learning_rate": 2.0214765100671142e-05,
      "loss": 0.6237,
      "step": 44880
    },
    {
      "epoch": 1.7955999999999999,
      "grad_norm": 2.6598734855651855,
      "learning_rate": 2.020805369127517e-05,
      "loss": 0.5469,
      "step": 44890
    },
    {
      "epoch": 1.796,
      "grad_norm": 2.7269527912139893,
      "learning_rate": 2.0201342281879196e-05,
      "loss": 0.6369,
      "step": 44900
    },
    {
      "epoch": 1.7964,
      "grad_norm": 2.3533682823181152,
      "learning_rate": 2.019463087248322e-05,
      "loss": 0.5275,
      "step": 44910
    },
    {
      "epoch": 1.7968,
      "grad_norm": 2.0415172576904297,
      "learning_rate": 2.018791946308725e-05,
      "loss": 0.5358,
      "step": 44920
    },
    {
      "epoch": 1.7972000000000001,
      "grad_norm": 2.5982048511505127,
      "learning_rate": 2.0181208053691275e-05,
      "loss": 0.6293,
      "step": 44930
    },
    {
      "epoch": 1.7976,
      "grad_norm": 3.3608641624450684,
      "learning_rate": 2.0174496644295303e-05,
      "loss": 0.6794,
      "step": 44940
    },
    {
      "epoch": 1.798,
      "grad_norm": 2.4124717712402344,
      "learning_rate": 2.0167785234899332e-05,
      "loss": 0.5644,
      "step": 44950
    },
    {
      "epoch": 1.7984,
      "grad_norm": 2.8620307445526123,
      "learning_rate": 2.0161073825503357e-05,
      "loss": 0.5538,
      "step": 44960
    },
    {
      "epoch": 1.7988,
      "grad_norm": 2.7589025497436523,
      "learning_rate": 2.0154362416107382e-05,
      "loss": 0.5566,
      "step": 44970
    },
    {
      "epoch": 1.7992,
      "grad_norm": 2.456069231033325,
      "learning_rate": 2.014765100671141e-05,
      "loss": 0.5538,
      "step": 44980
    },
    {
      "epoch": 1.7995999999999999,
      "grad_norm": 2.491528034210205,
      "learning_rate": 2.0140939597315436e-05,
      "loss": 0.5575,
      "step": 44990
    },
    {
      "epoch": 1.8,
      "grad_norm": 2.857100009918213,
      "learning_rate": 2.013422818791946e-05,
      "loss": 0.5977,
      "step": 45000
    },
    {
      "epoch": 1.8004,
      "grad_norm": 2.399636745452881,
      "learning_rate": 2.0127516778523493e-05,
      "loss": 0.5764,
      "step": 45010
    },
    {
      "epoch": 1.8008,
      "grad_norm": 2.3574626445770264,
      "learning_rate": 2.0120805369127518e-05,
      "loss": 0.5559,
      "step": 45020
    },
    {
      "epoch": 1.8012000000000001,
      "grad_norm": 2.4168150424957275,
      "learning_rate": 2.0114093959731543e-05,
      "loss": 0.5958,
      "step": 45030
    },
    {
      "epoch": 1.8016,
      "grad_norm": 3.0599281787872314,
      "learning_rate": 2.0107382550335572e-05,
      "loss": 0.5665,
      "step": 45040
    },
    {
      "epoch": 1.802,
      "grad_norm": 3.216000556945801,
      "learning_rate": 2.0100671140939597e-05,
      "loss": 0.546,
      "step": 45050
    },
    {
      "epoch": 1.8024,
      "grad_norm": 2.8425774574279785,
      "learning_rate": 2.0093959731543622e-05,
      "loss": 0.5697,
      "step": 45060
    },
    {
      "epoch": 1.8028,
      "grad_norm": 1.693489909172058,
      "learning_rate": 2.0087248322147654e-05,
      "loss": 0.4739,
      "step": 45070
    },
    {
      "epoch": 1.8032,
      "grad_norm": 2.4794232845306396,
      "learning_rate": 2.008053691275168e-05,
      "loss": 0.5679,
      "step": 45080
    },
    {
      "epoch": 1.8035999999999999,
      "grad_norm": 2.829404354095459,
      "learning_rate": 2.0073825503355705e-05,
      "loss": 0.5689,
      "step": 45090
    },
    {
      "epoch": 1.804,
      "grad_norm": 3.1546452045440674,
      "learning_rate": 2.0067114093959733e-05,
      "loss": 0.6286,
      "step": 45100
    },
    {
      "epoch": 1.8044,
      "grad_norm": 2.1310312747955322,
      "learning_rate": 2.0060402684563758e-05,
      "loss": 0.4831,
      "step": 45110
    },
    {
      "epoch": 1.8048,
      "grad_norm": 2.746267795562744,
      "learning_rate": 2.0053691275167787e-05,
      "loss": 0.5711,
      "step": 45120
    },
    {
      "epoch": 1.8052000000000001,
      "grad_norm": 2.717923879623413,
      "learning_rate": 2.0046979865771815e-05,
      "loss": 0.5481,
      "step": 45130
    },
    {
      "epoch": 1.8056,
      "grad_norm": 3.0322859287261963,
      "learning_rate": 2.004026845637584e-05,
      "loss": 0.607,
      "step": 45140
    },
    {
      "epoch": 1.806,
      "grad_norm": 2.9242897033691406,
      "learning_rate": 2.0033557046979866e-05,
      "loss": 0.6235,
      "step": 45150
    },
    {
      "epoch": 1.8064,
      "grad_norm": 2.8765716552734375,
      "learning_rate": 2.0026845637583894e-05,
      "loss": 0.5121,
      "step": 45160
    },
    {
      "epoch": 1.8068,
      "grad_norm": 2.712404251098633,
      "learning_rate": 2.002013422818792e-05,
      "loss": 0.5587,
      "step": 45170
    },
    {
      "epoch": 1.8072,
      "grad_norm": 2.4962925910949707,
      "learning_rate": 2.0013422818791948e-05,
      "loss": 0.5542,
      "step": 45180
    },
    {
      "epoch": 1.8075999999999999,
      "grad_norm": 2.956489324569702,
      "learning_rate": 2.0006711409395973e-05,
      "loss": 0.5314,
      "step": 45190
    },
    {
      "epoch": 1.808,
      "grad_norm": 2.819361925125122,
      "learning_rate": 2e-05,
      "loss": 0.5244,
      "step": 45200
    },
    {
      "epoch": 1.8084,
      "grad_norm": 2.7106339931488037,
      "learning_rate": 1.9993288590604027e-05,
      "loss": 0.6241,
      "step": 45210
    },
    {
      "epoch": 1.8088,
      "grad_norm": 2.4149229526519775,
      "learning_rate": 1.9986577181208055e-05,
      "loss": 0.5291,
      "step": 45220
    },
    {
      "epoch": 1.8092000000000001,
      "grad_norm": 2.5481081008911133,
      "learning_rate": 1.997986577181208e-05,
      "loss": 0.5288,
      "step": 45230
    },
    {
      "epoch": 1.8096,
      "grad_norm": 2.539592981338501,
      "learning_rate": 1.997315436241611e-05,
      "loss": 0.5998,
      "step": 45240
    },
    {
      "epoch": 1.81,
      "grad_norm": 2.834974527359009,
      "learning_rate": 1.9966442953020134e-05,
      "loss": 0.5878,
      "step": 45250
    },
    {
      "epoch": 1.8104,
      "grad_norm": 2.9994723796844482,
      "learning_rate": 1.9959731543624163e-05,
      "loss": 0.582,
      "step": 45260
    },
    {
      "epoch": 1.8108,
      "grad_norm": 2.469621419906616,
      "learning_rate": 1.995302013422819e-05,
      "loss": 0.5328,
      "step": 45270
    },
    {
      "epoch": 1.8112,
      "grad_norm": 2.7333667278289795,
      "learning_rate": 1.9946308724832216e-05,
      "loss": 0.5902,
      "step": 45280
    },
    {
      "epoch": 1.8115999999999999,
      "grad_norm": 2.897722005844116,
      "learning_rate": 1.993959731543624e-05,
      "loss": 0.5465,
      "step": 45290
    },
    {
      "epoch": 1.812,
      "grad_norm": 2.744204521179199,
      "learning_rate": 1.993288590604027e-05,
      "loss": 0.531,
      "step": 45300
    },
    {
      "epoch": 1.8124,
      "grad_norm": 3.445727586746216,
      "learning_rate": 1.9926174496644295e-05,
      "loss": 0.6205,
      "step": 45310
    },
    {
      "epoch": 1.8128,
      "grad_norm": 2.6552462577819824,
      "learning_rate": 1.991946308724832e-05,
      "loss": 0.506,
      "step": 45320
    },
    {
      "epoch": 1.8132000000000001,
      "grad_norm": 1.8796004056930542,
      "learning_rate": 1.9912751677852352e-05,
      "loss": 0.5009,
      "step": 45330
    },
    {
      "epoch": 1.8136,
      "grad_norm": 2.9990856647491455,
      "learning_rate": 1.9906040268456378e-05,
      "loss": 0.5436,
      "step": 45340
    },
    {
      "epoch": 1.814,
      "grad_norm": 2.525287628173828,
      "learning_rate": 1.9899328859060403e-05,
      "loss": 0.5833,
      "step": 45350
    },
    {
      "epoch": 1.8144,
      "grad_norm": 2.401324987411499,
      "learning_rate": 1.989261744966443e-05,
      "loss": 0.5167,
      "step": 45360
    },
    {
      "epoch": 1.8148,
      "grad_norm": 3.231318712234497,
      "learning_rate": 1.9885906040268456e-05,
      "loss": 0.5546,
      "step": 45370
    },
    {
      "epoch": 1.8152,
      "grad_norm": 2.8845090866088867,
      "learning_rate": 1.987919463087248e-05,
      "loss": 0.6313,
      "step": 45380
    },
    {
      "epoch": 1.8155999999999999,
      "grad_norm": 2.4739010334014893,
      "learning_rate": 1.9872483221476514e-05,
      "loss": 0.5297,
      "step": 45390
    },
    {
      "epoch": 1.8159999999999998,
      "grad_norm": 2.68060040473938,
      "learning_rate": 1.986577181208054e-05,
      "loss": 0.6416,
      "step": 45400
    },
    {
      "epoch": 1.8164,
      "grad_norm": 2.2392189502716064,
      "learning_rate": 1.9859060402684564e-05,
      "loss": 0.6037,
      "step": 45410
    },
    {
      "epoch": 1.8168,
      "grad_norm": 2.1331639289855957,
      "learning_rate": 1.9852348993288592e-05,
      "loss": 0.5921,
      "step": 45420
    },
    {
      "epoch": 1.8172000000000001,
      "grad_norm": 2.748310089111328,
      "learning_rate": 1.9845637583892618e-05,
      "loss": 0.6303,
      "step": 45430
    },
    {
      "epoch": 1.8176,
      "grad_norm": 2.981414318084717,
      "learning_rate": 1.9838926174496643e-05,
      "loss": 0.5949,
      "step": 45440
    },
    {
      "epoch": 1.818,
      "grad_norm": 3.010514259338379,
      "learning_rate": 1.9832214765100675e-05,
      "loss": 0.5329,
      "step": 45450
    },
    {
      "epoch": 1.8184,
      "grad_norm": 2.7613415718078613,
      "learning_rate": 1.98255033557047e-05,
      "loss": 0.558,
      "step": 45460
    },
    {
      "epoch": 1.8188,
      "grad_norm": 2.0055785179138184,
      "learning_rate": 1.9818791946308725e-05,
      "loss": 0.5481,
      "step": 45470
    },
    {
      "epoch": 1.8192,
      "grad_norm": 2.5625338554382324,
      "learning_rate": 1.9812080536912754e-05,
      "loss": 0.5669,
      "step": 45480
    },
    {
      "epoch": 1.8195999999999999,
      "grad_norm": 3.7483432292938232,
      "learning_rate": 1.980536912751678e-05,
      "loss": 0.6515,
      "step": 45490
    },
    {
      "epoch": 1.8199999999999998,
      "grad_norm": 2.3349175453186035,
      "learning_rate": 1.9798657718120804e-05,
      "loss": 0.4492,
      "step": 45500
    },
    {
      "epoch": 1.8204,
      "grad_norm": 2.7178726196289062,
      "learning_rate": 1.9791946308724832e-05,
      "loss": 0.6119,
      "step": 45510
    },
    {
      "epoch": 1.8208,
      "grad_norm": 2.3447675704956055,
      "learning_rate": 1.978523489932886e-05,
      "loss": 0.568,
      "step": 45520
    },
    {
      "epoch": 1.8212000000000002,
      "grad_norm": 2.5962817668914795,
      "learning_rate": 1.9778523489932886e-05,
      "loss": 0.5528,
      "step": 45530
    },
    {
      "epoch": 1.8216,
      "grad_norm": 2.657012939453125,
      "learning_rate": 1.9771812080536915e-05,
      "loss": 0.4842,
      "step": 45540
    },
    {
      "epoch": 1.822,
      "grad_norm": 3.314387321472168,
      "learning_rate": 1.976510067114094e-05,
      "loss": 0.5407,
      "step": 45550
    },
    {
      "epoch": 1.8224,
      "grad_norm": 2.7407853603363037,
      "learning_rate": 1.975838926174497e-05,
      "loss": 0.5241,
      "step": 45560
    },
    {
      "epoch": 1.8228,
      "grad_norm": 3.14687180519104,
      "learning_rate": 1.9751677852348994e-05,
      "loss": 0.5929,
      "step": 45570
    },
    {
      "epoch": 1.8232,
      "grad_norm": 2.5546131134033203,
      "learning_rate": 1.9744966442953022e-05,
      "loss": 0.5442,
      "step": 45580
    },
    {
      "epoch": 1.8235999999999999,
      "grad_norm": 2.5197432041168213,
      "learning_rate": 1.9738255033557047e-05,
      "loss": 0.4704,
      "step": 45590
    },
    {
      "epoch": 1.8239999999999998,
      "grad_norm": 2.2497165203094482,
      "learning_rate": 1.9731543624161076e-05,
      "loss": 0.5177,
      "step": 45600
    },
    {
      "epoch": 1.8244,
      "grad_norm": 3.2030487060546875,
      "learning_rate": 1.97248322147651e-05,
      "loss": 0.5157,
      "step": 45610
    },
    {
      "epoch": 1.8248,
      "grad_norm": 2.6416492462158203,
      "learning_rate": 1.971812080536913e-05,
      "loss": 0.5337,
      "step": 45620
    },
    {
      "epoch": 1.8252000000000002,
      "grad_norm": 1.7561246156692505,
      "learning_rate": 1.9711409395973155e-05,
      "loss": 0.4785,
      "step": 45630
    },
    {
      "epoch": 1.8256000000000001,
      "grad_norm": 3.298650026321411,
      "learning_rate": 1.970469798657718e-05,
      "loss": 0.539,
      "step": 45640
    },
    {
      "epoch": 1.826,
      "grad_norm": 2.3526227474212646,
      "learning_rate": 1.969798657718121e-05,
      "loss": 0.5296,
      "step": 45650
    },
    {
      "epoch": 1.8264,
      "grad_norm": 2.55330753326416,
      "learning_rate": 1.9691275167785237e-05,
      "loss": 0.5523,
      "step": 45660
    },
    {
      "epoch": 1.8268,
      "grad_norm": 2.9003965854644775,
      "learning_rate": 1.9684563758389262e-05,
      "loss": 0.5727,
      "step": 45670
    },
    {
      "epoch": 1.8272,
      "grad_norm": 2.8873376846313477,
      "learning_rate": 1.967785234899329e-05,
      "loss": 0.4949,
      "step": 45680
    },
    {
      "epoch": 1.8276,
      "grad_norm": 2.250661611557007,
      "learning_rate": 1.9671140939597316e-05,
      "loss": 0.5984,
      "step": 45690
    },
    {
      "epoch": 1.8279999999999998,
      "grad_norm": 2.9870104789733887,
      "learning_rate": 1.966442953020134e-05,
      "loss": 0.532,
      "step": 45700
    },
    {
      "epoch": 1.8284,
      "grad_norm": 2.955169439315796,
      "learning_rate": 1.9657718120805373e-05,
      "loss": 0.5697,
      "step": 45710
    },
    {
      "epoch": 1.8288,
      "grad_norm": 2.537931203842163,
      "learning_rate": 1.9651006711409398e-05,
      "loss": 0.527,
      "step": 45720
    },
    {
      "epoch": 1.8292000000000002,
      "grad_norm": 2.286607503890991,
      "learning_rate": 1.9644295302013423e-05,
      "loss": 0.5839,
      "step": 45730
    },
    {
      "epoch": 1.8296000000000001,
      "grad_norm": 3.1042537689208984,
      "learning_rate": 1.9637583892617452e-05,
      "loss": 0.5343,
      "step": 45740
    },
    {
      "epoch": 1.83,
      "grad_norm": 2.7619922161102295,
      "learning_rate": 1.9630872483221477e-05,
      "loss": 0.5857,
      "step": 45750
    },
    {
      "epoch": 1.8304,
      "grad_norm": 2.428206205368042,
      "learning_rate": 1.9624161073825502e-05,
      "loss": 0.5421,
      "step": 45760
    },
    {
      "epoch": 1.8308,
      "grad_norm": 2.524854898452759,
      "learning_rate": 1.9617449664429534e-05,
      "loss": 0.5594,
      "step": 45770
    },
    {
      "epoch": 1.8312,
      "grad_norm": 3.2609028816223145,
      "learning_rate": 1.961073825503356e-05,
      "loss": 0.5703,
      "step": 45780
    },
    {
      "epoch": 1.8316,
      "grad_norm": 2.6635162830352783,
      "learning_rate": 1.9604026845637584e-05,
      "loss": 0.5724,
      "step": 45790
    },
    {
      "epoch": 1.8319999999999999,
      "grad_norm": 3.290205240249634,
      "learning_rate": 1.9597315436241613e-05,
      "loss": 0.5324,
      "step": 45800
    },
    {
      "epoch": 1.8324,
      "grad_norm": 2.7028005123138428,
      "learning_rate": 1.9590604026845638e-05,
      "loss": 0.5456,
      "step": 45810
    },
    {
      "epoch": 1.8328,
      "grad_norm": 2.5510785579681396,
      "learning_rate": 1.9583892617449663e-05,
      "loss": 0.533,
      "step": 45820
    },
    {
      "epoch": 1.8332000000000002,
      "grad_norm": 3.055436134338379,
      "learning_rate": 1.957718120805369e-05,
      "loss": 0.546,
      "step": 45830
    },
    {
      "epoch": 1.8336000000000001,
      "grad_norm": 2.2112555503845215,
      "learning_rate": 1.957046979865772e-05,
      "loss": 0.4632,
      "step": 45840
    },
    {
      "epoch": 1.834,
      "grad_norm": 3.173121690750122,
      "learning_rate": 1.9563758389261745e-05,
      "loss": 0.4792,
      "step": 45850
    },
    {
      "epoch": 1.8344,
      "grad_norm": 1.8172982931137085,
      "learning_rate": 1.9557046979865774e-05,
      "loss": 0.5619,
      "step": 45860
    },
    {
      "epoch": 1.8348,
      "grad_norm": 3.166027545928955,
      "learning_rate": 1.95503355704698e-05,
      "loss": 0.5246,
      "step": 45870
    },
    {
      "epoch": 1.8352,
      "grad_norm": 2.437314033508301,
      "learning_rate": 1.9543624161073824e-05,
      "loss": 0.5341,
      "step": 45880
    },
    {
      "epoch": 1.8356,
      "grad_norm": 2.940258026123047,
      "learning_rate": 1.9536912751677853e-05,
      "loss": 0.62,
      "step": 45890
    },
    {
      "epoch": 1.8359999999999999,
      "grad_norm": 2.7173728942871094,
      "learning_rate": 1.953020134228188e-05,
      "loss": 0.5203,
      "step": 45900
    },
    {
      "epoch": 1.8364,
      "grad_norm": 3.163822889328003,
      "learning_rate": 1.9523489932885907e-05,
      "loss": 0.5762,
      "step": 45910
    },
    {
      "epoch": 1.8368,
      "grad_norm": 2.2332847118377686,
      "learning_rate": 1.9516778523489935e-05,
      "loss": 0.5789,
      "step": 45920
    },
    {
      "epoch": 1.8372000000000002,
      "grad_norm": 3.3191630840301514,
      "learning_rate": 1.951006711409396e-05,
      "loss": 0.5988,
      "step": 45930
    },
    {
      "epoch": 1.8376000000000001,
      "grad_norm": 2.752890110015869,
      "learning_rate": 1.9503355704697985e-05,
      "loss": 0.555,
      "step": 45940
    },
    {
      "epoch": 1.838,
      "grad_norm": 3.201855182647705,
      "learning_rate": 1.9496644295302014e-05,
      "loss": 0.4839,
      "step": 45950
    },
    {
      "epoch": 1.8384,
      "grad_norm": 2.37625789642334,
      "learning_rate": 1.948993288590604e-05,
      "loss": 0.5585,
      "step": 45960
    },
    {
      "epoch": 1.8388,
      "grad_norm": 2.934252977371216,
      "learning_rate": 1.9483221476510068e-05,
      "loss": 0.5322,
      "step": 45970
    },
    {
      "epoch": 1.8392,
      "grad_norm": 2.534839630126953,
      "learning_rate": 1.9476510067114096e-05,
      "loss": 0.5427,
      "step": 45980
    },
    {
      "epoch": 1.8396,
      "grad_norm": 2.944895029067993,
      "learning_rate": 1.946979865771812e-05,
      "loss": 0.4987,
      "step": 45990
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 2.5164949893951416,
      "learning_rate": 1.946308724832215e-05,
      "loss": 0.5959,
      "step": 46000
    },
    {
      "epoch": 1.8404,
      "grad_norm": 2.883680820465088,
      "learning_rate": 1.9456375838926175e-05,
      "loss": 0.5076,
      "step": 46010
    },
    {
      "epoch": 1.8408,
      "grad_norm": 2.101635694503784,
      "learning_rate": 1.94496644295302e-05,
      "loss": 0.5208,
      "step": 46020
    },
    {
      "epoch": 1.8412,
      "grad_norm": 2.858182191848755,
      "learning_rate": 1.944295302013423e-05,
      "loss": 0.5473,
      "step": 46030
    },
    {
      "epoch": 1.8416000000000001,
      "grad_norm": 2.377148151397705,
      "learning_rate": 1.9436241610738257e-05,
      "loss": 0.5487,
      "step": 46040
    },
    {
      "epoch": 1.842,
      "grad_norm": 2.5041322708129883,
      "learning_rate": 1.9429530201342282e-05,
      "loss": 0.5827,
      "step": 46050
    },
    {
      "epoch": 1.8424,
      "grad_norm": 3.30849289894104,
      "learning_rate": 1.942281879194631e-05,
      "loss": 0.5869,
      "step": 46060
    },
    {
      "epoch": 1.8428,
      "grad_norm": 2.8352203369140625,
      "learning_rate": 1.9416107382550336e-05,
      "loss": 0.6032,
      "step": 46070
    },
    {
      "epoch": 1.8432,
      "grad_norm": 3.170482635498047,
      "learning_rate": 1.940939597315436e-05,
      "loss": 0.4872,
      "step": 46080
    },
    {
      "epoch": 1.8436,
      "grad_norm": 2.1066770553588867,
      "learning_rate": 1.940268456375839e-05,
      "loss": 0.5169,
      "step": 46090
    },
    {
      "epoch": 1.8439999999999999,
      "grad_norm": 3.252344846725464,
      "learning_rate": 1.939597315436242e-05,
      "loss": 0.5659,
      "step": 46100
    },
    {
      "epoch": 1.8444,
      "grad_norm": 2.6131746768951416,
      "learning_rate": 1.9389261744966444e-05,
      "loss": 0.5072,
      "step": 46110
    },
    {
      "epoch": 1.8448,
      "grad_norm": 2.8568968772888184,
      "learning_rate": 1.9382550335570472e-05,
      "loss": 0.5336,
      "step": 46120
    },
    {
      "epoch": 1.8452,
      "grad_norm": 3.2057859897613525,
      "learning_rate": 1.9375838926174497e-05,
      "loss": 0.5684,
      "step": 46130
    },
    {
      "epoch": 1.8456000000000001,
      "grad_norm": 2.6266067028045654,
      "learning_rate": 1.9369127516778522e-05,
      "loss": 0.5234,
      "step": 46140
    },
    {
      "epoch": 1.846,
      "grad_norm": 3.1201255321502686,
      "learning_rate": 1.936241610738255e-05,
      "loss": 0.6878,
      "step": 46150
    },
    {
      "epoch": 1.8464,
      "grad_norm": 3.3110740184783936,
      "learning_rate": 1.935570469798658e-05,
      "loss": 0.5746,
      "step": 46160
    },
    {
      "epoch": 1.8468,
      "grad_norm": 2.9003310203552246,
      "learning_rate": 1.9348993288590605e-05,
      "loss": 0.6223,
      "step": 46170
    },
    {
      "epoch": 1.8472,
      "grad_norm": 2.784040927886963,
      "learning_rate": 1.9342281879194633e-05,
      "loss": 0.5266,
      "step": 46180
    },
    {
      "epoch": 1.8476,
      "grad_norm": 2.800323486328125,
      "learning_rate": 1.933557046979866e-05,
      "loss": 0.6268,
      "step": 46190
    },
    {
      "epoch": 1.8479999999999999,
      "grad_norm": 2.6656148433685303,
      "learning_rate": 1.9328859060402684e-05,
      "loss": 0.5378,
      "step": 46200
    },
    {
      "epoch": 1.8484,
      "grad_norm": 2.5194764137268066,
      "learning_rate": 1.9322147651006712e-05,
      "loss": 0.5839,
      "step": 46210
    },
    {
      "epoch": 1.8488,
      "grad_norm": 2.6443681716918945,
      "learning_rate": 1.931543624161074e-05,
      "loss": 0.4756,
      "step": 46220
    },
    {
      "epoch": 1.8492,
      "grad_norm": 2.9743716716766357,
      "learning_rate": 1.9308724832214766e-05,
      "loss": 0.6261,
      "step": 46230
    },
    {
      "epoch": 1.8496000000000001,
      "grad_norm": 2.6233842372894287,
      "learning_rate": 1.9302013422818794e-05,
      "loss": 0.5965,
      "step": 46240
    },
    {
      "epoch": 1.85,
      "grad_norm": 3.055572748184204,
      "learning_rate": 1.929530201342282e-05,
      "loss": 0.5803,
      "step": 46250
    },
    {
      "epoch": 1.8504,
      "grad_norm": 2.366560459136963,
      "learning_rate": 1.9288590604026845e-05,
      "loss": 0.4848,
      "step": 46260
    },
    {
      "epoch": 1.8508,
      "grad_norm": 2.4355945587158203,
      "learning_rate": 1.9281879194630873e-05,
      "loss": 0.5884,
      "step": 46270
    },
    {
      "epoch": 1.8512,
      "grad_norm": 2.407496213912964,
      "learning_rate": 1.9275167785234902e-05,
      "loss": 0.5225,
      "step": 46280
    },
    {
      "epoch": 1.8516,
      "grad_norm": 2.208869218826294,
      "learning_rate": 1.9268456375838927e-05,
      "loss": 0.5649,
      "step": 46290
    },
    {
      "epoch": 1.8519999999999999,
      "grad_norm": 2.8129630088806152,
      "learning_rate": 1.9261744966442955e-05,
      "loss": 0.6497,
      "step": 46300
    },
    {
      "epoch": 1.8524,
      "grad_norm": 2.6725895404815674,
      "learning_rate": 1.925503355704698e-05,
      "loss": 0.5237,
      "step": 46310
    },
    {
      "epoch": 1.8528,
      "grad_norm": 3.2157223224639893,
      "learning_rate": 1.9248322147651006e-05,
      "loss": 0.6395,
      "step": 46320
    },
    {
      "epoch": 1.8532,
      "grad_norm": 2.797149419784546,
      "learning_rate": 1.9241610738255034e-05,
      "loss": 0.5815,
      "step": 46330
    },
    {
      "epoch": 1.8536000000000001,
      "grad_norm": 2.3236427307128906,
      "learning_rate": 1.923489932885906e-05,
      "loss": 0.4977,
      "step": 46340
    },
    {
      "epoch": 1.854,
      "grad_norm": 2.2514753341674805,
      "learning_rate": 1.9228187919463088e-05,
      "loss": 0.5364,
      "step": 46350
    },
    {
      "epoch": 1.8544,
      "grad_norm": 2.9955265522003174,
      "learning_rate": 1.9221476510067117e-05,
      "loss": 0.5903,
      "step": 46360
    },
    {
      "epoch": 1.8548,
      "grad_norm": 2.395695447921753,
      "learning_rate": 1.9214765100671142e-05,
      "loss": 0.55,
      "step": 46370
    },
    {
      "epoch": 1.8552,
      "grad_norm": 2.621123790740967,
      "learning_rate": 1.9208053691275167e-05,
      "loss": 0.4683,
      "step": 46380
    },
    {
      "epoch": 1.8556,
      "grad_norm": 2.950779914855957,
      "learning_rate": 1.9201342281879195e-05,
      "loss": 0.6202,
      "step": 46390
    },
    {
      "epoch": 1.8559999999999999,
      "grad_norm": 1.8002104759216309,
      "learning_rate": 1.919463087248322e-05,
      "loss": 0.5302,
      "step": 46400
    },
    {
      "epoch": 1.8564,
      "grad_norm": 3.9413559436798096,
      "learning_rate": 1.918791946308725e-05,
      "loss": 0.4892,
      "step": 46410
    },
    {
      "epoch": 1.8568,
      "grad_norm": 2.9158670902252197,
      "learning_rate": 1.9181208053691278e-05,
      "loss": 0.4923,
      "step": 46420
    },
    {
      "epoch": 1.8572,
      "grad_norm": 2.894861936569214,
      "learning_rate": 1.9174496644295303e-05,
      "loss": 0.5222,
      "step": 46430
    },
    {
      "epoch": 1.8576000000000001,
      "grad_norm": 3.0614051818847656,
      "learning_rate": 1.916778523489933e-05,
      "loss": 0.5224,
      "step": 46440
    },
    {
      "epoch": 1.858,
      "grad_norm": 2.681245803833008,
      "learning_rate": 1.9161073825503357e-05,
      "loss": 0.5564,
      "step": 46450
    },
    {
      "epoch": 1.8584,
      "grad_norm": 2.5873701572418213,
      "learning_rate": 1.9154362416107382e-05,
      "loss": 0.5926,
      "step": 46460
    },
    {
      "epoch": 1.8588,
      "grad_norm": 2.601886034011841,
      "learning_rate": 1.914765100671141e-05,
      "loss": 0.569,
      "step": 46470
    },
    {
      "epoch": 1.8592,
      "grad_norm": 4.798733234405518,
      "learning_rate": 1.914093959731544e-05,
      "loss": 0.5865,
      "step": 46480
    },
    {
      "epoch": 1.8596,
      "grad_norm": 2.9804513454437256,
      "learning_rate": 1.9134228187919464e-05,
      "loss": 0.6074,
      "step": 46490
    },
    {
      "epoch": 1.8599999999999999,
      "grad_norm": 2.8374862670898438,
      "learning_rate": 1.9127516778523493e-05,
      "loss": 0.5733,
      "step": 46500
    },
    {
      "epoch": 1.8604,
      "grad_norm": 3.2054316997528076,
      "learning_rate": 1.9120805369127518e-05,
      "loss": 0.5811,
      "step": 46510
    },
    {
      "epoch": 1.8608,
      "grad_norm": 2.870732545852661,
      "learning_rate": 1.9114093959731543e-05,
      "loss": 0.5747,
      "step": 46520
    },
    {
      "epoch": 1.8612,
      "grad_norm": 2.658790111541748,
      "learning_rate": 1.910738255033557e-05,
      "loss": 0.6106,
      "step": 46530
    },
    {
      "epoch": 1.8616000000000001,
      "grad_norm": 2.471437454223633,
      "learning_rate": 1.91006711409396e-05,
      "loss": 0.5415,
      "step": 46540
    },
    {
      "epoch": 1.862,
      "grad_norm": 3.1224617958068848,
      "learning_rate": 1.9093959731543625e-05,
      "loss": 0.615,
      "step": 46550
    },
    {
      "epoch": 1.8624,
      "grad_norm": 2.022502899169922,
      "learning_rate": 1.9087248322147654e-05,
      "loss": 0.5423,
      "step": 46560
    },
    {
      "epoch": 1.8628,
      "grad_norm": 2.863093852996826,
      "learning_rate": 1.908053691275168e-05,
      "loss": 0.6078,
      "step": 46570
    },
    {
      "epoch": 1.8632,
      "grad_norm": 2.476836681365967,
      "learning_rate": 1.9073825503355704e-05,
      "loss": 0.5453,
      "step": 46580
    },
    {
      "epoch": 1.8636,
      "grad_norm": 2.0557219982147217,
      "learning_rate": 1.9067114093959733e-05,
      "loss": 0.577,
      "step": 46590
    },
    {
      "epoch": 1.8639999999999999,
      "grad_norm": 2.831465482711792,
      "learning_rate": 1.906040268456376e-05,
      "loss": 0.4888,
      "step": 46600
    },
    {
      "epoch": 1.8643999999999998,
      "grad_norm": 3.57875657081604,
      "learning_rate": 1.9053691275167786e-05,
      "loss": 0.5999,
      "step": 46610
    },
    {
      "epoch": 1.8648,
      "grad_norm": 3.561058759689331,
      "learning_rate": 1.9046979865771815e-05,
      "loss": 0.6345,
      "step": 46620
    },
    {
      "epoch": 1.8652,
      "grad_norm": 3.138010025024414,
      "learning_rate": 1.904026845637584e-05,
      "loss": 0.6163,
      "step": 46630
    },
    {
      "epoch": 1.8656000000000001,
      "grad_norm": 2.418771266937256,
      "learning_rate": 1.9033557046979865e-05,
      "loss": 0.4915,
      "step": 46640
    },
    {
      "epoch": 1.866,
      "grad_norm": 2.250067949295044,
      "learning_rate": 1.9026845637583894e-05,
      "loss": 0.5203,
      "step": 46650
    },
    {
      "epoch": 1.8664,
      "grad_norm": 2.12617826461792,
      "learning_rate": 1.902013422818792e-05,
      "loss": 0.5687,
      "step": 46660
    },
    {
      "epoch": 1.8668,
      "grad_norm": 2.0935826301574707,
      "learning_rate": 1.9013422818791947e-05,
      "loss": 0.5124,
      "step": 46670
    },
    {
      "epoch": 1.8672,
      "grad_norm": 2.6412456035614014,
      "learning_rate": 1.9006711409395976e-05,
      "loss": 0.5033,
      "step": 46680
    },
    {
      "epoch": 1.8676,
      "grad_norm": 2.689540147781372,
      "learning_rate": 1.9e-05,
      "loss": 0.5606,
      "step": 46690
    },
    {
      "epoch": 1.8679999999999999,
      "grad_norm": 2.935272216796875,
      "learning_rate": 1.8993288590604026e-05,
      "loss": 0.6413,
      "step": 46700
    },
    {
      "epoch": 1.8683999999999998,
      "grad_norm": 3.1059322357177734,
      "learning_rate": 1.8986577181208055e-05,
      "loss": 0.5533,
      "step": 46710
    },
    {
      "epoch": 1.8688,
      "grad_norm": 2.915238857269287,
      "learning_rate": 1.897986577181208e-05,
      "loss": 0.5117,
      "step": 46720
    },
    {
      "epoch": 1.8692,
      "grad_norm": 2.1414153575897217,
      "learning_rate": 1.897315436241611e-05,
      "loss": 0.5667,
      "step": 46730
    },
    {
      "epoch": 1.8696000000000002,
      "grad_norm": 3.0036346912384033,
      "learning_rate": 1.8966442953020137e-05,
      "loss": 0.5553,
      "step": 46740
    },
    {
      "epoch": 1.87,
      "grad_norm": 3.2537410259246826,
      "learning_rate": 1.8959731543624162e-05,
      "loss": 0.5699,
      "step": 46750
    },
    {
      "epoch": 1.8704,
      "grad_norm": 1.7969082593917847,
      "learning_rate": 1.8953020134228187e-05,
      "loss": 0.5306,
      "step": 46760
    },
    {
      "epoch": 1.8708,
      "grad_norm": 2.688213348388672,
      "learning_rate": 1.8946308724832216e-05,
      "loss": 0.6319,
      "step": 46770
    },
    {
      "epoch": 1.8712,
      "grad_norm": 2.6577236652374268,
      "learning_rate": 1.893959731543624e-05,
      "loss": 0.549,
      "step": 46780
    },
    {
      "epoch": 1.8716,
      "grad_norm": 2.3440167903900146,
      "learning_rate": 1.893288590604027e-05,
      "loss": 0.51,
      "step": 46790
    },
    {
      "epoch": 1.8719999999999999,
      "grad_norm": 2.708425760269165,
      "learning_rate": 1.8926174496644298e-05,
      "loss": 0.4758,
      "step": 46800
    },
    {
      "epoch": 1.8723999999999998,
      "grad_norm": 3.0092029571533203,
      "learning_rate": 1.8919463087248323e-05,
      "loss": 0.5678,
      "step": 46810
    },
    {
      "epoch": 1.8728,
      "grad_norm": 2.9686200618743896,
      "learning_rate": 1.891275167785235e-05,
      "loss": 0.4572,
      "step": 46820
    },
    {
      "epoch": 1.8732,
      "grad_norm": 2.7090256214141846,
      "learning_rate": 1.8906040268456377e-05,
      "loss": 0.6257,
      "step": 46830
    },
    {
      "epoch": 1.8736000000000002,
      "grad_norm": 1.9047478437423706,
      "learning_rate": 1.8899328859060402e-05,
      "loss": 0.5597,
      "step": 46840
    },
    {
      "epoch": 1.874,
      "grad_norm": 2.568690299987793,
      "learning_rate": 1.889261744966443e-05,
      "loss": 0.4992,
      "step": 46850
    },
    {
      "epoch": 1.8744,
      "grad_norm": 3.5936172008514404,
      "learning_rate": 1.888590604026846e-05,
      "loss": 0.578,
      "step": 46860
    },
    {
      "epoch": 1.8748,
      "grad_norm": 2.906872272491455,
      "learning_rate": 1.8879194630872484e-05,
      "loss": 0.5328,
      "step": 46870
    },
    {
      "epoch": 1.8752,
      "grad_norm": 3.1510889530181885,
      "learning_rate": 1.8872483221476513e-05,
      "loss": 0.5678,
      "step": 46880
    },
    {
      "epoch": 1.8756,
      "grad_norm": 2.1227903366088867,
      "learning_rate": 1.8865771812080538e-05,
      "loss": 0.5539,
      "step": 46890
    },
    {
      "epoch": 1.876,
      "grad_norm": 2.996133804321289,
      "learning_rate": 1.8859060402684563e-05,
      "loss": 0.6147,
      "step": 46900
    },
    {
      "epoch": 1.8763999999999998,
      "grad_norm": 2.7935240268707275,
      "learning_rate": 1.8852348993288592e-05,
      "loss": 0.6055,
      "step": 46910
    },
    {
      "epoch": 1.8768,
      "grad_norm": 2.0159082412719727,
      "learning_rate": 1.884563758389262e-05,
      "loss": 0.5018,
      "step": 46920
    },
    {
      "epoch": 1.8772,
      "grad_norm": 2.1854519844055176,
      "learning_rate": 1.8838926174496646e-05,
      "loss": 0.5226,
      "step": 46930
    },
    {
      "epoch": 1.8776000000000002,
      "grad_norm": 2.130398750305176,
      "learning_rate": 1.8832214765100674e-05,
      "loss": 0.5186,
      "step": 46940
    },
    {
      "epoch": 1.8780000000000001,
      "grad_norm": 3.4687294960021973,
      "learning_rate": 1.88255033557047e-05,
      "loss": 0.5476,
      "step": 46950
    },
    {
      "epoch": 1.8784,
      "grad_norm": 2.8062148094177246,
      "learning_rate": 1.8818791946308724e-05,
      "loss": 0.6254,
      "step": 46960
    },
    {
      "epoch": 1.8788,
      "grad_norm": 2.672673225402832,
      "learning_rate": 1.8812080536912753e-05,
      "loss": 0.5648,
      "step": 46970
    },
    {
      "epoch": 1.8792,
      "grad_norm": 2.293753147125244,
      "learning_rate": 1.8805369127516778e-05,
      "loss": 0.5492,
      "step": 46980
    },
    {
      "epoch": 1.8796,
      "grad_norm": 2.8960275650024414,
      "learning_rate": 1.8798657718120807e-05,
      "loss": 0.5553,
      "step": 46990
    },
    {
      "epoch": 1.88,
      "grad_norm": 2.631429433822632,
      "learning_rate": 1.8791946308724835e-05,
      "loss": 0.525,
      "step": 47000
    },
    {
      "epoch": 1.8803999999999998,
      "grad_norm": 1.8421752452850342,
      "learning_rate": 1.878523489932886e-05,
      "loss": 0.5017,
      "step": 47010
    },
    {
      "epoch": 1.8808,
      "grad_norm": 3.135529041290283,
      "learning_rate": 1.8778523489932886e-05,
      "loss": 0.6338,
      "step": 47020
    },
    {
      "epoch": 1.8812,
      "grad_norm": 2.8416969776153564,
      "learning_rate": 1.8771812080536914e-05,
      "loss": 0.5594,
      "step": 47030
    },
    {
      "epoch": 1.8816000000000002,
      "grad_norm": 2.671135425567627,
      "learning_rate": 1.876510067114094e-05,
      "loss": 0.4954,
      "step": 47040
    },
    {
      "epoch": 1.8820000000000001,
      "grad_norm": 2.264803647994995,
      "learning_rate": 1.8758389261744968e-05,
      "loss": 0.5033,
      "step": 47050
    },
    {
      "epoch": 1.8824,
      "grad_norm": 2.0038931369781494,
      "learning_rate": 1.8751677852348996e-05,
      "loss": 0.5801,
      "step": 47060
    },
    {
      "epoch": 1.8828,
      "grad_norm": 3.0520846843719482,
      "learning_rate": 1.874496644295302e-05,
      "loss": 0.5592,
      "step": 47070
    },
    {
      "epoch": 1.8832,
      "grad_norm": 2.1845970153808594,
      "learning_rate": 1.8738255033557047e-05,
      "loss": 0.5614,
      "step": 47080
    },
    {
      "epoch": 1.8836,
      "grad_norm": 2.328728437423706,
      "learning_rate": 1.8731543624161075e-05,
      "loss": 0.5707,
      "step": 47090
    },
    {
      "epoch": 1.884,
      "grad_norm": 2.5566940307617188,
      "learning_rate": 1.87248322147651e-05,
      "loss": 0.5207,
      "step": 47100
    },
    {
      "epoch": 1.8843999999999999,
      "grad_norm": 3.353437662124634,
      "learning_rate": 1.8718120805369125e-05,
      "loss": 0.653,
      "step": 47110
    },
    {
      "epoch": 1.8848,
      "grad_norm": 2.051793098449707,
      "learning_rate": 1.8711409395973157e-05,
      "loss": 0.6006,
      "step": 47120
    },
    {
      "epoch": 1.8852,
      "grad_norm": 3.004732847213745,
      "learning_rate": 1.8704697986577183e-05,
      "loss": 0.5341,
      "step": 47130
    },
    {
      "epoch": 1.8856000000000002,
      "grad_norm": 2.4091103076934814,
      "learning_rate": 1.8697986577181208e-05,
      "loss": 0.6207,
      "step": 47140
    },
    {
      "epoch": 1.8860000000000001,
      "grad_norm": 2.4991767406463623,
      "learning_rate": 1.8691275167785236e-05,
      "loss": 0.5791,
      "step": 47150
    },
    {
      "epoch": 1.8864,
      "grad_norm": 2.6184287071228027,
      "learning_rate": 1.868456375838926e-05,
      "loss": 0.5871,
      "step": 47160
    },
    {
      "epoch": 1.8868,
      "grad_norm": 2.9051499366760254,
      "learning_rate": 1.8677852348993287e-05,
      "loss": 0.5474,
      "step": 47170
    },
    {
      "epoch": 1.8872,
      "grad_norm": 2.0889792442321777,
      "learning_rate": 1.867114093959732e-05,
      "loss": 0.5925,
      "step": 47180
    },
    {
      "epoch": 1.8876,
      "grad_norm": 2.930727958679199,
      "learning_rate": 1.8664429530201344e-05,
      "loss": 0.5359,
      "step": 47190
    },
    {
      "epoch": 1.888,
      "grad_norm": 2.6390843391418457,
      "learning_rate": 1.865771812080537e-05,
      "loss": 0.5723,
      "step": 47200
    },
    {
      "epoch": 1.8883999999999999,
      "grad_norm": 2.5643808841705322,
      "learning_rate": 1.8651006711409397e-05,
      "loss": 0.5858,
      "step": 47210
    },
    {
      "epoch": 1.8888,
      "grad_norm": 2.4492099285125732,
      "learning_rate": 1.8644295302013423e-05,
      "loss": 0.6184,
      "step": 47220
    },
    {
      "epoch": 1.8892,
      "grad_norm": 2.905831813812256,
      "learning_rate": 1.863758389261745e-05,
      "loss": 0.515,
      "step": 47230
    },
    {
      "epoch": 1.8896,
      "grad_norm": 2.8277664184570312,
      "learning_rate": 1.863087248322148e-05,
      "loss": 0.5626,
      "step": 47240
    },
    {
      "epoch": 1.8900000000000001,
      "grad_norm": 2.7951512336730957,
      "learning_rate": 1.8624161073825505e-05,
      "loss": 0.6561,
      "step": 47250
    },
    {
      "epoch": 1.8904,
      "grad_norm": 2.782400608062744,
      "learning_rate": 1.861744966442953e-05,
      "loss": 0.5551,
      "step": 47260
    },
    {
      "epoch": 1.8908,
      "grad_norm": 2.8384103775024414,
      "learning_rate": 1.861073825503356e-05,
      "loss": 0.6121,
      "step": 47270
    },
    {
      "epoch": 1.8912,
      "grad_norm": 3.10579514503479,
      "learning_rate": 1.8604026845637584e-05,
      "loss": 0.5373,
      "step": 47280
    },
    {
      "epoch": 1.8916,
      "grad_norm": 2.3091676235198975,
      "learning_rate": 1.8597315436241612e-05,
      "loss": 0.5451,
      "step": 47290
    },
    {
      "epoch": 1.892,
      "grad_norm": 2.196298360824585,
      "learning_rate": 1.8590604026845637e-05,
      "loss": 0.5619,
      "step": 47300
    },
    {
      "epoch": 1.8923999999999999,
      "grad_norm": 2.930729866027832,
      "learning_rate": 1.8583892617449666e-05,
      "loss": 0.546,
      "step": 47310
    },
    {
      "epoch": 1.8928,
      "grad_norm": 2.4922831058502197,
      "learning_rate": 1.857718120805369e-05,
      "loss": 0.6071,
      "step": 47320
    },
    {
      "epoch": 1.8932,
      "grad_norm": 3.199812889099121,
      "learning_rate": 1.857046979865772e-05,
      "loss": 0.6509,
      "step": 47330
    },
    {
      "epoch": 1.8936,
      "grad_norm": 2.736464023590088,
      "learning_rate": 1.8563758389261745e-05,
      "loss": 0.5105,
      "step": 47340
    },
    {
      "epoch": 1.8940000000000001,
      "grad_norm": 2.751481533050537,
      "learning_rate": 1.8557046979865773e-05,
      "loss": 0.5674,
      "step": 47350
    },
    {
      "epoch": 1.8944,
      "grad_norm": 2.785036087036133,
      "learning_rate": 1.85503355704698e-05,
      "loss": 0.6341,
      "step": 47360
    },
    {
      "epoch": 1.8948,
      "grad_norm": 2.6742985248565674,
      "learning_rate": 1.8543624161073827e-05,
      "loss": 0.5292,
      "step": 47370
    },
    {
      "epoch": 1.8952,
      "grad_norm": 2.352764844894409,
      "learning_rate": 1.8536912751677856e-05,
      "loss": 0.5963,
      "step": 47380
    },
    {
      "epoch": 1.8956,
      "grad_norm": 2.5020368099212646,
      "learning_rate": 1.853020134228188e-05,
      "loss": 0.6006,
      "step": 47390
    },
    {
      "epoch": 1.896,
      "grad_norm": 3.582256317138672,
      "learning_rate": 1.8523489932885906e-05,
      "loss": 0.5627,
      "step": 47400
    },
    {
      "epoch": 1.8963999999999999,
      "grad_norm": 2.6607587337493896,
      "learning_rate": 1.8516778523489934e-05,
      "loss": 0.5435,
      "step": 47410
    },
    {
      "epoch": 1.8968,
      "grad_norm": 3.075200080871582,
      "learning_rate": 1.851006711409396e-05,
      "loss": 0.5496,
      "step": 47420
    },
    {
      "epoch": 1.8972,
      "grad_norm": 1.9930789470672607,
      "learning_rate": 1.8503355704697988e-05,
      "loss": 0.5639,
      "step": 47430
    },
    {
      "epoch": 1.8976,
      "grad_norm": 3.3405003547668457,
      "learning_rate": 1.8496644295302017e-05,
      "loss": 0.5797,
      "step": 47440
    },
    {
      "epoch": 1.8980000000000001,
      "grad_norm": 2.3664095401763916,
      "learning_rate": 1.8489932885906042e-05,
      "loss": 0.5675,
      "step": 47450
    },
    {
      "epoch": 1.8984,
      "grad_norm": 3.1402344703674316,
      "learning_rate": 1.8483221476510067e-05,
      "loss": 0.5958,
      "step": 47460
    },
    {
      "epoch": 1.8988,
      "grad_norm": 3.306022882461548,
      "learning_rate": 1.8476510067114096e-05,
      "loss": 0.5378,
      "step": 47470
    },
    {
      "epoch": 1.8992,
      "grad_norm": 2.4875755310058594,
      "learning_rate": 1.846979865771812e-05,
      "loss": 0.5087,
      "step": 47480
    },
    {
      "epoch": 1.8996,
      "grad_norm": 3.0411455631256104,
      "learning_rate": 1.8463087248322146e-05,
      "loss": 0.6186,
      "step": 47490
    },
    {
      "epoch": 1.9,
      "grad_norm": 3.1355063915252686,
      "learning_rate": 1.8456375838926178e-05,
      "loss": 0.6268,
      "step": 47500
    },
    {
      "epoch": 1.9003999999999999,
      "grad_norm": 2.7624998092651367,
      "learning_rate": 1.8449664429530203e-05,
      "loss": 0.6184,
      "step": 47510
    },
    {
      "epoch": 1.9008,
      "grad_norm": 2.495765447616577,
      "learning_rate": 1.8442953020134228e-05,
      "loss": 0.5743,
      "step": 47520
    },
    {
      "epoch": 1.9012,
      "grad_norm": 2.272857666015625,
      "learning_rate": 1.8436241610738257e-05,
      "loss": 0.5413,
      "step": 47530
    },
    {
      "epoch": 1.9016,
      "grad_norm": 2.795588254928589,
      "learning_rate": 1.8429530201342282e-05,
      "loss": 0.4987,
      "step": 47540
    },
    {
      "epoch": 1.9020000000000001,
      "grad_norm": 3.4246160984039307,
      "learning_rate": 1.8422818791946307e-05,
      "loss": 0.5907,
      "step": 47550
    },
    {
      "epoch": 1.9024,
      "grad_norm": 2.6655499935150146,
      "learning_rate": 1.841610738255034e-05,
      "loss": 0.5813,
      "step": 47560
    },
    {
      "epoch": 1.9028,
      "grad_norm": 1.9339135885238647,
      "learning_rate": 1.8409395973154364e-05,
      "loss": 0.5575,
      "step": 47570
    },
    {
      "epoch": 1.9032,
      "grad_norm": 2.8175203800201416,
      "learning_rate": 1.840268456375839e-05,
      "loss": 0.5519,
      "step": 47580
    },
    {
      "epoch": 1.9036,
      "grad_norm": 3.471862316131592,
      "learning_rate": 1.8395973154362418e-05,
      "loss": 0.6267,
      "step": 47590
    },
    {
      "epoch": 1.904,
      "grad_norm": 1.7891499996185303,
      "learning_rate": 1.8389261744966443e-05,
      "loss": 0.5987,
      "step": 47600
    },
    {
      "epoch": 1.9043999999999999,
      "grad_norm": 2.538877010345459,
      "learning_rate": 1.8382550335570468e-05,
      "loss": 0.5724,
      "step": 47610
    },
    {
      "epoch": 1.9048,
      "grad_norm": 2.771268367767334,
      "learning_rate": 1.8375838926174497e-05,
      "loss": 0.521,
      "step": 47620
    },
    {
      "epoch": 1.9052,
      "grad_norm": 2.1692676544189453,
      "learning_rate": 1.8369127516778525e-05,
      "loss": 0.5221,
      "step": 47630
    },
    {
      "epoch": 1.9056,
      "grad_norm": 3.0863661766052246,
      "learning_rate": 1.836241610738255e-05,
      "loss": 0.5795,
      "step": 47640
    },
    {
      "epoch": 1.9060000000000001,
      "grad_norm": 2.8027477264404297,
      "learning_rate": 1.835570469798658e-05,
      "loss": 0.5923,
      "step": 47650
    },
    {
      "epoch": 1.9064,
      "grad_norm": 2.162693500518799,
      "learning_rate": 1.8348993288590604e-05,
      "loss": 0.5469,
      "step": 47660
    },
    {
      "epoch": 1.9068,
      "grad_norm": 3.0385913848876953,
      "learning_rate": 1.8342281879194633e-05,
      "loss": 0.5266,
      "step": 47670
    },
    {
      "epoch": 1.9072,
      "grad_norm": 2.233628273010254,
      "learning_rate": 1.8335570469798658e-05,
      "loss": 0.5872,
      "step": 47680
    },
    {
      "epoch": 1.9076,
      "grad_norm": 2.632932424545288,
      "learning_rate": 1.8328859060402686e-05,
      "loss": 0.5676,
      "step": 47690
    },
    {
      "epoch": 1.908,
      "grad_norm": 2.222343683242798,
      "learning_rate": 1.832214765100671e-05,
      "loss": 0.5291,
      "step": 47700
    },
    {
      "epoch": 1.9083999999999999,
      "grad_norm": 2.6189372539520264,
      "learning_rate": 1.831543624161074e-05,
      "loss": 0.574,
      "step": 47710
    },
    {
      "epoch": 1.9088,
      "grad_norm": 2.984131336212158,
      "learning_rate": 1.8308724832214765e-05,
      "loss": 0.5449,
      "step": 47720
    },
    {
      "epoch": 1.9092,
      "grad_norm": 2.5808496475219727,
      "learning_rate": 1.8302013422818794e-05,
      "loss": 0.5945,
      "step": 47730
    },
    {
      "epoch": 1.9096,
      "grad_norm": 2.35155987739563,
      "learning_rate": 1.829530201342282e-05,
      "loss": 0.5756,
      "step": 47740
    },
    {
      "epoch": 1.9100000000000001,
      "grad_norm": 2.9367902278900146,
      "learning_rate": 1.8288590604026847e-05,
      "loss": 0.5717,
      "step": 47750
    },
    {
      "epoch": 1.9104,
      "grad_norm": 3.156269073486328,
      "learning_rate": 1.8281879194630873e-05,
      "loss": 0.606,
      "step": 47760
    },
    {
      "epoch": 1.9108,
      "grad_norm": 2.515012741088867,
      "learning_rate": 1.82751677852349e-05,
      "loss": 0.5551,
      "step": 47770
    },
    {
      "epoch": 1.9112,
      "grad_norm": 2.147998809814453,
      "learning_rate": 1.8268456375838926e-05,
      "loss": 0.4837,
      "step": 47780
    },
    {
      "epoch": 1.9116,
      "grad_norm": 2.325801134109497,
      "learning_rate": 1.8261744966442955e-05,
      "loss": 0.6063,
      "step": 47790
    },
    {
      "epoch": 1.912,
      "grad_norm": 2.54748797416687,
      "learning_rate": 1.825503355704698e-05,
      "loss": 0.5518,
      "step": 47800
    },
    {
      "epoch": 1.9123999999999999,
      "grad_norm": 1.7585984468460083,
      "learning_rate": 1.8248322147651005e-05,
      "loss": 0.6628,
      "step": 47810
    },
    {
      "epoch": 1.9127999999999998,
      "grad_norm": 2.3769822120666504,
      "learning_rate": 1.8241610738255037e-05,
      "loss": 0.5552,
      "step": 47820
    },
    {
      "epoch": 1.9132,
      "grad_norm": 2.414741277694702,
      "learning_rate": 1.8234899328859062e-05,
      "loss": 0.5437,
      "step": 47830
    },
    {
      "epoch": 1.9136,
      "grad_norm": 3.253844738006592,
      "learning_rate": 1.8228187919463087e-05,
      "loss": 0.5673,
      "step": 47840
    },
    {
      "epoch": 1.9140000000000001,
      "grad_norm": 2.855478286743164,
      "learning_rate": 1.8221476510067116e-05,
      "loss": 0.5314,
      "step": 47850
    },
    {
      "epoch": 1.9144,
      "grad_norm": 2.598285436630249,
      "learning_rate": 1.821476510067114e-05,
      "loss": 0.6467,
      "step": 47860
    },
    {
      "epoch": 1.9148,
      "grad_norm": 2.7697086334228516,
      "learning_rate": 1.8208053691275166e-05,
      "loss": 0.4461,
      "step": 47870
    },
    {
      "epoch": 1.9152,
      "grad_norm": 2.586052894592285,
      "learning_rate": 1.8201342281879198e-05,
      "loss": 0.6091,
      "step": 47880
    },
    {
      "epoch": 1.9156,
      "grad_norm": 2.331690788269043,
      "learning_rate": 1.8194630872483223e-05,
      "loss": 0.6122,
      "step": 47890
    },
    {
      "epoch": 1.916,
      "grad_norm": 1.9588375091552734,
      "learning_rate": 1.818791946308725e-05,
      "loss": 0.5611,
      "step": 47900
    },
    {
      "epoch": 1.9163999999999999,
      "grad_norm": 2.3848230838775635,
      "learning_rate": 1.8181208053691277e-05,
      "loss": 0.5529,
      "step": 47910
    },
    {
      "epoch": 1.9167999999999998,
      "grad_norm": 2.153486490249634,
      "learning_rate": 1.8174496644295302e-05,
      "loss": 0.5374,
      "step": 47920
    },
    {
      "epoch": 1.9172,
      "grad_norm": 2.5991060733795166,
      "learning_rate": 1.8167785234899327e-05,
      "loss": 0.5669,
      "step": 47930
    },
    {
      "epoch": 1.9176,
      "grad_norm": 2.744983196258545,
      "learning_rate": 1.8161073825503356e-05,
      "loss": 0.503,
      "step": 47940
    },
    {
      "epoch": 1.9180000000000001,
      "grad_norm": 2.8101933002471924,
      "learning_rate": 1.8154362416107385e-05,
      "loss": 0.5902,
      "step": 47950
    },
    {
      "epoch": 1.9184,
      "grad_norm": 3.019900321960449,
      "learning_rate": 1.814765100671141e-05,
      "loss": 0.5748,
      "step": 47960
    },
    {
      "epoch": 1.9188,
      "grad_norm": 3.0208489894866943,
      "learning_rate": 1.8140939597315438e-05,
      "loss": 0.6157,
      "step": 47970
    },
    {
      "epoch": 1.9192,
      "grad_norm": 2.79093337059021,
      "learning_rate": 1.8134228187919463e-05,
      "loss": 0.6276,
      "step": 47980
    },
    {
      "epoch": 1.9196,
      "grad_norm": 2.397700548171997,
      "learning_rate": 1.812751677852349e-05,
      "loss": 0.6325,
      "step": 47990
    },
    {
      "epoch": 1.92,
      "grad_norm": 2.865743637084961,
      "learning_rate": 1.8120805369127517e-05,
      "loss": 0.5737,
      "step": 48000
    },
    {
      "epoch": 1.9203999999999999,
      "grad_norm": 2.7227470874786377,
      "learning_rate": 1.8114093959731546e-05,
      "loss": 0.4927,
      "step": 48010
    },
    {
      "epoch": 1.9207999999999998,
      "grad_norm": 3.0761117935180664,
      "learning_rate": 1.810738255033557e-05,
      "loss": 0.5964,
      "step": 48020
    },
    {
      "epoch": 1.9212,
      "grad_norm": 2.352463960647583,
      "learning_rate": 1.81006711409396e-05,
      "loss": 0.5878,
      "step": 48030
    },
    {
      "epoch": 1.9216,
      "grad_norm": 2.8156754970550537,
      "learning_rate": 1.8093959731543625e-05,
      "loss": 0.6216,
      "step": 48040
    },
    {
      "epoch": 1.9220000000000002,
      "grad_norm": 3.0475783348083496,
      "learning_rate": 1.808724832214765e-05,
      "loss": 0.5966,
      "step": 48050
    },
    {
      "epoch": 1.9224,
      "grad_norm": 3.329530715942383,
      "learning_rate": 1.8080536912751678e-05,
      "loss": 0.6455,
      "step": 48060
    },
    {
      "epoch": 1.9228,
      "grad_norm": 2.3654959201812744,
      "learning_rate": 1.8073825503355707e-05,
      "loss": 0.4533,
      "step": 48070
    },
    {
      "epoch": 1.9232,
      "grad_norm": 2.1669626235961914,
      "learning_rate": 1.8067114093959732e-05,
      "loss": 0.5425,
      "step": 48080
    },
    {
      "epoch": 1.9236,
      "grad_norm": 2.654548168182373,
      "learning_rate": 1.806040268456376e-05,
      "loss": 0.5636,
      "step": 48090
    },
    {
      "epoch": 1.924,
      "grad_norm": 2.2194371223449707,
      "learning_rate": 1.8053691275167786e-05,
      "loss": 0.4949,
      "step": 48100
    },
    {
      "epoch": 1.9243999999999999,
      "grad_norm": 2.859931707382202,
      "learning_rate": 1.8046979865771814e-05,
      "loss": 0.5414,
      "step": 48110
    },
    {
      "epoch": 1.9247999999999998,
      "grad_norm": 2.4497549533843994,
      "learning_rate": 1.804026845637584e-05,
      "loss": 0.5832,
      "step": 48120
    },
    {
      "epoch": 1.9252,
      "grad_norm": 2.2120871543884277,
      "learning_rate": 1.8033557046979864e-05,
      "loss": 0.5844,
      "step": 48130
    },
    {
      "epoch": 1.9256,
      "grad_norm": 1.9989416599273682,
      "learning_rate": 1.8026845637583893e-05,
      "loss": 0.4848,
      "step": 48140
    },
    {
      "epoch": 1.9260000000000002,
      "grad_norm": 2.4177091121673584,
      "learning_rate": 1.802013422818792e-05,
      "loss": 0.5313,
      "step": 48150
    },
    {
      "epoch": 1.9264000000000001,
      "grad_norm": 2.3666276931762695,
      "learning_rate": 1.8013422818791947e-05,
      "loss": 0.5227,
      "step": 48160
    },
    {
      "epoch": 1.9268,
      "grad_norm": 2.4623868465423584,
      "learning_rate": 1.8006711409395975e-05,
      "loss": 0.516,
      "step": 48170
    },
    {
      "epoch": 1.9272,
      "grad_norm": 3.254544734954834,
      "learning_rate": 1.8e-05,
      "loss": 0.5575,
      "step": 48180
    },
    {
      "epoch": 1.9276,
      "grad_norm": 2.072925567626953,
      "learning_rate": 1.7993288590604026e-05,
      "loss": 0.4342,
      "step": 48190
    },
    {
      "epoch": 1.928,
      "grad_norm": 2.404703140258789,
      "learning_rate": 1.7986577181208054e-05,
      "loss": 0.5454,
      "step": 48200
    },
    {
      "epoch": 1.9284,
      "grad_norm": 3.171802282333374,
      "learning_rate": 1.7979865771812083e-05,
      "loss": 0.6185,
      "step": 48210
    },
    {
      "epoch": 1.9287999999999998,
      "grad_norm": 2.667466163635254,
      "learning_rate": 1.7973154362416108e-05,
      "loss": 0.4908,
      "step": 48220
    },
    {
      "epoch": 1.9292,
      "grad_norm": 3.0162546634674072,
      "learning_rate": 1.7966442953020136e-05,
      "loss": 0.5187,
      "step": 48230
    },
    {
      "epoch": 1.9296,
      "grad_norm": 2.8869271278381348,
      "learning_rate": 1.795973154362416e-05,
      "loss": 0.5355,
      "step": 48240
    },
    {
      "epoch": 1.9300000000000002,
      "grad_norm": 2.310997724533081,
      "learning_rate": 1.7953020134228187e-05,
      "loss": 0.495,
      "step": 48250
    },
    {
      "epoch": 1.9304000000000001,
      "grad_norm": 2.6085093021392822,
      "learning_rate": 1.7946308724832215e-05,
      "loss": 0.6809,
      "step": 48260
    },
    {
      "epoch": 1.9308,
      "grad_norm": 3.0625498294830322,
      "learning_rate": 1.7939597315436244e-05,
      "loss": 0.5822,
      "step": 48270
    },
    {
      "epoch": 1.9312,
      "grad_norm": 3.170116901397705,
      "learning_rate": 1.793288590604027e-05,
      "loss": 0.5651,
      "step": 48280
    },
    {
      "epoch": 1.9316,
      "grad_norm": 2.313271999359131,
      "learning_rate": 1.7926174496644298e-05,
      "loss": 0.4619,
      "step": 48290
    },
    {
      "epoch": 1.932,
      "grad_norm": 2.221338987350464,
      "learning_rate": 1.7919463087248323e-05,
      "loss": 0.6299,
      "step": 48300
    },
    {
      "epoch": 1.9324,
      "grad_norm": 3.0424652099609375,
      "learning_rate": 1.7912751677852348e-05,
      "loss": 0.5629,
      "step": 48310
    },
    {
      "epoch": 1.9327999999999999,
      "grad_norm": 2.0268940925598145,
      "learning_rate": 1.7906040268456376e-05,
      "loss": 0.6011,
      "step": 48320
    },
    {
      "epoch": 1.9332,
      "grad_norm": 3.0049378871917725,
      "learning_rate": 1.7899328859060405e-05,
      "loss": 0.5932,
      "step": 48330
    },
    {
      "epoch": 1.9336,
      "grad_norm": 1.9524673223495483,
      "learning_rate": 1.789261744966443e-05,
      "loss": 0.5105,
      "step": 48340
    },
    {
      "epoch": 1.9340000000000002,
      "grad_norm": 2.823448896408081,
      "learning_rate": 1.788590604026846e-05,
      "loss": 0.5554,
      "step": 48350
    },
    {
      "epoch": 1.9344000000000001,
      "grad_norm": 2.9151108264923096,
      "learning_rate": 1.7879194630872484e-05,
      "loss": 0.5405,
      "step": 48360
    },
    {
      "epoch": 1.9348,
      "grad_norm": 2.6741459369659424,
      "learning_rate": 1.787248322147651e-05,
      "loss": 0.4842,
      "step": 48370
    },
    {
      "epoch": 1.9352,
      "grad_norm": 3.093078136444092,
      "learning_rate": 1.7865771812080538e-05,
      "loss": 0.5497,
      "step": 48380
    },
    {
      "epoch": 1.9356,
      "grad_norm": 3.5346155166625977,
      "learning_rate": 1.7859060402684566e-05,
      "loss": 0.5666,
      "step": 48390
    },
    {
      "epoch": 1.936,
      "grad_norm": 2.9525375366210938,
      "learning_rate": 1.785234899328859e-05,
      "loss": 0.5779,
      "step": 48400
    },
    {
      "epoch": 1.9364,
      "grad_norm": 2.763758897781372,
      "learning_rate": 1.784563758389262e-05,
      "loss": 0.5561,
      "step": 48410
    },
    {
      "epoch": 1.9367999999999999,
      "grad_norm": 2.9050192832946777,
      "learning_rate": 1.7838926174496645e-05,
      "loss": 0.612,
      "step": 48420
    },
    {
      "epoch": 1.9372,
      "grad_norm": 2.6008105278015137,
      "learning_rate": 1.783221476510067e-05,
      "loss": 0.5959,
      "step": 48430
    },
    {
      "epoch": 1.9376,
      "grad_norm": 3.367928981781006,
      "learning_rate": 1.78255033557047e-05,
      "loss": 0.5925,
      "step": 48440
    },
    {
      "epoch": 1.938,
      "grad_norm": 2.8473527431488037,
      "learning_rate": 1.7818791946308724e-05,
      "loss": 0.5457,
      "step": 48450
    },
    {
      "epoch": 1.9384000000000001,
      "grad_norm": 2.5781850814819336,
      "learning_rate": 1.7812080536912752e-05,
      "loss": 0.6358,
      "step": 48460
    },
    {
      "epoch": 1.9388,
      "grad_norm": 2.20802640914917,
      "learning_rate": 1.780536912751678e-05,
      "loss": 0.5299,
      "step": 48470
    },
    {
      "epoch": 1.9392,
      "grad_norm": 2.7572484016418457,
      "learning_rate": 1.7798657718120806e-05,
      "loss": 0.5424,
      "step": 48480
    },
    {
      "epoch": 1.9396,
      "grad_norm": 3.639378547668457,
      "learning_rate": 1.779194630872483e-05,
      "loss": 0.5363,
      "step": 48490
    },
    {
      "epoch": 1.94,
      "grad_norm": 2.101300001144409,
      "learning_rate": 1.778523489932886e-05,
      "loss": 0.5543,
      "step": 48500
    },
    {
      "epoch": 1.9404,
      "grad_norm": 2.796790599822998,
      "learning_rate": 1.7778523489932885e-05,
      "loss": 0.6452,
      "step": 48510
    },
    {
      "epoch": 1.9407999999999999,
      "grad_norm": 2.796074628829956,
      "learning_rate": 1.7771812080536913e-05,
      "loss": 0.5757,
      "step": 48520
    },
    {
      "epoch": 1.9412,
      "grad_norm": 2.663114070892334,
      "learning_rate": 1.7765100671140942e-05,
      "loss": 0.4852,
      "step": 48530
    },
    {
      "epoch": 1.9416,
      "grad_norm": 2.6935970783233643,
      "learning_rate": 1.7758389261744967e-05,
      "loss": 0.4873,
      "step": 48540
    },
    {
      "epoch": 1.942,
      "grad_norm": 2.8393993377685547,
      "learning_rate": 1.7751677852348996e-05,
      "loss": 0.5566,
      "step": 48550
    },
    {
      "epoch": 1.9424000000000001,
      "grad_norm": 1.918228268623352,
      "learning_rate": 1.774496644295302e-05,
      "loss": 0.5001,
      "step": 48560
    },
    {
      "epoch": 1.9428,
      "grad_norm": 2.0414392948150635,
      "learning_rate": 1.7738255033557046e-05,
      "loss": 0.4636,
      "step": 48570
    },
    {
      "epoch": 1.9432,
      "grad_norm": 2.643134355545044,
      "learning_rate": 1.7731543624161075e-05,
      "loss": 0.5638,
      "step": 48580
    },
    {
      "epoch": 1.9436,
      "grad_norm": 2.567080497741699,
      "learning_rate": 1.7724832214765103e-05,
      "loss": 0.5967,
      "step": 48590
    },
    {
      "epoch": 1.944,
      "grad_norm": 2.7043752670288086,
      "learning_rate": 1.7718120805369128e-05,
      "loss": 0.6295,
      "step": 48600
    },
    {
      "epoch": 1.9444,
      "grad_norm": 2.8271563053131104,
      "learning_rate": 1.7711409395973157e-05,
      "loss": 0.5349,
      "step": 48610
    },
    {
      "epoch": 1.9447999999999999,
      "grad_norm": 3.0513553619384766,
      "learning_rate": 1.7704697986577182e-05,
      "loss": 0.5639,
      "step": 48620
    },
    {
      "epoch": 1.9452,
      "grad_norm": 2.535466432571411,
      "learning_rate": 1.7697986577181207e-05,
      "loss": 0.6202,
      "step": 48630
    },
    {
      "epoch": 1.9456,
      "grad_norm": 2.5949060916900635,
      "learning_rate": 1.7691275167785236e-05,
      "loss": 0.4957,
      "step": 48640
    },
    {
      "epoch": 1.946,
      "grad_norm": 3.587620735168457,
      "learning_rate": 1.7684563758389264e-05,
      "loss": 0.5456,
      "step": 48650
    },
    {
      "epoch": 1.9464000000000001,
      "grad_norm": 3.062089443206787,
      "learning_rate": 1.767785234899329e-05,
      "loss": 0.5906,
      "step": 48660
    },
    {
      "epoch": 1.9468,
      "grad_norm": 2.778913736343384,
      "learning_rate": 1.7671140939597318e-05,
      "loss": 0.5983,
      "step": 48670
    },
    {
      "epoch": 1.9472,
      "grad_norm": 2.2524685859680176,
      "learning_rate": 1.7664429530201343e-05,
      "loss": 0.5371,
      "step": 48680
    },
    {
      "epoch": 1.9476,
      "grad_norm": 2.9618918895721436,
      "learning_rate": 1.7657718120805368e-05,
      "loss": 0.6089,
      "step": 48690
    },
    {
      "epoch": 1.948,
      "grad_norm": 2.822382926940918,
      "learning_rate": 1.7651006711409397e-05,
      "loss": 0.5699,
      "step": 48700
    },
    {
      "epoch": 1.9484,
      "grad_norm": 2.4517133235931396,
      "learning_rate": 1.7644295302013425e-05,
      "loss": 0.6536,
      "step": 48710
    },
    {
      "epoch": 1.9487999999999999,
      "grad_norm": 1.8074016571044922,
      "learning_rate": 1.763758389261745e-05,
      "loss": 0.5743,
      "step": 48720
    },
    {
      "epoch": 1.9492,
      "grad_norm": 2.838932752609253,
      "learning_rate": 1.763087248322148e-05,
      "loss": 0.5796,
      "step": 48730
    },
    {
      "epoch": 1.9496,
      "grad_norm": 2.74912166595459,
      "learning_rate": 1.7624161073825504e-05,
      "loss": 0.5866,
      "step": 48740
    },
    {
      "epoch": 1.95,
      "grad_norm": 2.657989025115967,
      "learning_rate": 1.761744966442953e-05,
      "loss": 0.6211,
      "step": 48750
    },
    {
      "epoch": 1.9504000000000001,
      "grad_norm": 2.73846697807312,
      "learning_rate": 1.7610738255033558e-05,
      "loss": 0.5389,
      "step": 48760
    },
    {
      "epoch": 1.9508,
      "grad_norm": 3.2489242553710938,
      "learning_rate": 1.7604026845637583e-05,
      "loss": 0.5752,
      "step": 48770
    },
    {
      "epoch": 1.9512,
      "grad_norm": 3.1571414470672607,
      "learning_rate": 1.759731543624161e-05,
      "loss": 0.5999,
      "step": 48780
    },
    {
      "epoch": 1.9516,
      "grad_norm": 2.5737481117248535,
      "learning_rate": 1.759060402684564e-05,
      "loss": 0.516,
      "step": 48790
    },
    {
      "epoch": 1.952,
      "grad_norm": 2.359553813934326,
      "learning_rate": 1.7583892617449665e-05,
      "loss": 0.5237,
      "step": 48800
    },
    {
      "epoch": 1.9524,
      "grad_norm": 2.706066370010376,
      "learning_rate": 1.757718120805369e-05,
      "loss": 0.5863,
      "step": 48810
    },
    {
      "epoch": 1.9527999999999999,
      "grad_norm": 1.5450165271759033,
      "learning_rate": 1.757046979865772e-05,
      "loss": 0.512,
      "step": 48820
    },
    {
      "epoch": 1.9532,
      "grad_norm": 2.4771411418914795,
      "learning_rate": 1.7563758389261744e-05,
      "loss": 0.599,
      "step": 48830
    },
    {
      "epoch": 1.9536,
      "grad_norm": 2.821777105331421,
      "learning_rate": 1.7557046979865773e-05,
      "loss": 0.5175,
      "step": 48840
    },
    {
      "epoch": 1.954,
      "grad_norm": 2.7238738536834717,
      "learning_rate": 1.75503355704698e-05,
      "loss": 0.5799,
      "step": 48850
    },
    {
      "epoch": 1.9544000000000001,
      "grad_norm": 3.5174477100372314,
      "learning_rate": 1.7543624161073826e-05,
      "loss": 0.5393,
      "step": 48860
    },
    {
      "epoch": 1.9548,
      "grad_norm": 2.6721882820129395,
      "learning_rate": 1.753691275167785e-05,
      "loss": 0.5474,
      "step": 48870
    },
    {
      "epoch": 1.9552,
      "grad_norm": 3.1511428356170654,
      "learning_rate": 1.753020134228188e-05,
      "loss": 0.5835,
      "step": 48880
    },
    {
      "epoch": 1.9556,
      "grad_norm": 2.656254768371582,
      "learning_rate": 1.7523489932885905e-05,
      "loss": 0.4644,
      "step": 48890
    },
    {
      "epoch": 1.956,
      "grad_norm": 3.3485565185546875,
      "learning_rate": 1.7516778523489934e-05,
      "loss": 0.5876,
      "step": 48900
    },
    {
      "epoch": 1.9564,
      "grad_norm": 2.634805917739868,
      "learning_rate": 1.7510067114093962e-05,
      "loss": 0.5789,
      "step": 48910
    },
    {
      "epoch": 1.9567999999999999,
      "grad_norm": 2.9749555587768555,
      "learning_rate": 1.7503355704697988e-05,
      "loss": 0.6221,
      "step": 48920
    },
    {
      "epoch": 1.9572,
      "grad_norm": 3.1985106468200684,
      "learning_rate": 1.7496644295302013e-05,
      "loss": 0.5133,
      "step": 48930
    },
    {
      "epoch": 1.9576,
      "grad_norm": 2.2850334644317627,
      "learning_rate": 1.748993288590604e-05,
      "loss": 0.5588,
      "step": 48940
    },
    {
      "epoch": 1.958,
      "grad_norm": 3.0495731830596924,
      "learning_rate": 1.7483221476510066e-05,
      "loss": 0.6235,
      "step": 48950
    },
    {
      "epoch": 1.9584000000000001,
      "grad_norm": 2.341740608215332,
      "learning_rate": 1.7476510067114095e-05,
      "loss": 0.528,
      "step": 48960
    },
    {
      "epoch": 1.9588,
      "grad_norm": 5.5614542961120605,
      "learning_rate": 1.7469798657718124e-05,
      "loss": 0.6281,
      "step": 48970
    },
    {
      "epoch": 1.9592,
      "grad_norm": 2.782745838165283,
      "learning_rate": 1.746308724832215e-05,
      "loss": 0.5986,
      "step": 48980
    },
    {
      "epoch": 1.9596,
      "grad_norm": 2.402944803237915,
      "learning_rate": 1.7456375838926177e-05,
      "loss": 0.5145,
      "step": 48990
    },
    {
      "epoch": 1.96,
      "grad_norm": 2.7618050575256348,
      "learning_rate": 1.7449664429530202e-05,
      "loss": 0.5183,
      "step": 49000
    },
    {
      "epoch": 1.9604,
      "grad_norm": 2.3903441429138184,
      "learning_rate": 1.7442953020134228e-05,
      "loss": 0.6614,
      "step": 49010
    },
    {
      "epoch": 1.9607999999999999,
      "grad_norm": 2.1972625255584717,
      "learning_rate": 1.7436241610738256e-05,
      "loss": 0.5569,
      "step": 49020
    },
    {
      "epoch": 1.9612,
      "grad_norm": 2.736607551574707,
      "learning_rate": 1.7429530201342285e-05,
      "loss": 0.561,
      "step": 49030
    },
    {
      "epoch": 1.9616,
      "grad_norm": 2.313805341720581,
      "learning_rate": 1.742281879194631e-05,
      "loss": 0.5536,
      "step": 49040
    },
    {
      "epoch": 1.962,
      "grad_norm": 3.0059759616851807,
      "learning_rate": 1.741610738255034e-05,
      "loss": 0.5371,
      "step": 49050
    },
    {
      "epoch": 1.9624000000000001,
      "grad_norm": 3.5908877849578857,
      "learning_rate": 1.7409395973154364e-05,
      "loss": 0.6694,
      "step": 49060
    },
    {
      "epoch": 1.9628,
      "grad_norm": 2.557128667831421,
      "learning_rate": 1.740268456375839e-05,
      "loss": 0.5797,
      "step": 49070
    },
    {
      "epoch": 1.9632,
      "grad_norm": 2.210195541381836,
      "learning_rate": 1.7395973154362417e-05,
      "loss": 0.5109,
      "step": 49080
    },
    {
      "epoch": 1.9636,
      "grad_norm": 2.6787164211273193,
      "learning_rate": 1.7389261744966442e-05,
      "loss": 0.4719,
      "step": 49090
    },
    {
      "epoch": 1.964,
      "grad_norm": 2.802560567855835,
      "learning_rate": 1.738255033557047e-05,
      "loss": 0.5945,
      "step": 49100
    },
    {
      "epoch": 1.9644,
      "grad_norm": 2.800013542175293,
      "learning_rate": 1.73758389261745e-05,
      "loss": 0.5241,
      "step": 49110
    },
    {
      "epoch": 1.9647999999999999,
      "grad_norm": 2.662039041519165,
      "learning_rate": 1.7369127516778525e-05,
      "loss": 0.483,
      "step": 49120
    },
    {
      "epoch": 1.9651999999999998,
      "grad_norm": 2.6479811668395996,
      "learning_rate": 1.736241610738255e-05,
      "loss": 0.6176,
      "step": 49130
    },
    {
      "epoch": 1.9656,
      "grad_norm": 2.415553092956543,
      "learning_rate": 1.735570469798658e-05,
      "loss": 0.4719,
      "step": 49140
    },
    {
      "epoch": 1.966,
      "grad_norm": 3.0980522632598877,
      "learning_rate": 1.7348993288590604e-05,
      "loss": 0.5326,
      "step": 49150
    },
    {
      "epoch": 1.9664000000000001,
      "grad_norm": 2.5748183727264404,
      "learning_rate": 1.7342281879194632e-05,
      "loss": 0.4772,
      "step": 49160
    },
    {
      "epoch": 1.9668,
      "grad_norm": 3.324958324432373,
      "learning_rate": 1.733557046979866e-05,
      "loss": 0.5219,
      "step": 49170
    },
    {
      "epoch": 1.9672,
      "grad_norm": 1.8479667901992798,
      "learning_rate": 1.7328859060402686e-05,
      "loss": 0.503,
      "step": 49180
    },
    {
      "epoch": 1.9676,
      "grad_norm": 2.708097219467163,
      "learning_rate": 1.732214765100671e-05,
      "loss": 0.6026,
      "step": 49190
    },
    {
      "epoch": 1.968,
      "grad_norm": 2.4517054557800293,
      "learning_rate": 1.731543624161074e-05,
      "loss": 0.5079,
      "step": 49200
    },
    {
      "epoch": 1.9684,
      "grad_norm": 3.1741559505462646,
      "learning_rate": 1.7308724832214765e-05,
      "loss": 0.5577,
      "step": 49210
    },
    {
      "epoch": 1.9687999999999999,
      "grad_norm": 2.7373948097229004,
      "learning_rate": 1.7302013422818793e-05,
      "loss": 0.6408,
      "step": 49220
    },
    {
      "epoch": 1.9691999999999998,
      "grad_norm": 2.4928696155548096,
      "learning_rate": 1.7295302013422822e-05,
      "loss": 0.4623,
      "step": 49230
    },
    {
      "epoch": 1.9696,
      "grad_norm": 2.76426362991333,
      "learning_rate": 1.7288590604026847e-05,
      "loss": 0.6095,
      "step": 49240
    },
    {
      "epoch": 1.97,
      "grad_norm": 2.330019950866699,
      "learning_rate": 1.7281879194630872e-05,
      "loss": 0.5954,
      "step": 49250
    },
    {
      "epoch": 1.9704000000000002,
      "grad_norm": 2.693499803543091,
      "learning_rate": 1.72751677852349e-05,
      "loss": 0.5379,
      "step": 49260
    },
    {
      "epoch": 1.9708,
      "grad_norm": 2.7307615280151367,
      "learning_rate": 1.7268456375838926e-05,
      "loss": 0.4963,
      "step": 49270
    },
    {
      "epoch": 1.9712,
      "grad_norm": 2.577521324157715,
      "learning_rate": 1.726174496644295e-05,
      "loss": 0.5781,
      "step": 49280
    },
    {
      "epoch": 1.9716,
      "grad_norm": 3.022287130355835,
      "learning_rate": 1.7255033557046983e-05,
      "loss": 0.5664,
      "step": 49290
    },
    {
      "epoch": 1.972,
      "grad_norm": 2.3888463973999023,
      "learning_rate": 1.7248322147651008e-05,
      "loss": 0.5829,
      "step": 49300
    },
    {
      "epoch": 1.9724,
      "grad_norm": 2.493835687637329,
      "learning_rate": 1.7241610738255033e-05,
      "loss": 0.5454,
      "step": 49310
    },
    {
      "epoch": 1.9727999999999999,
      "grad_norm": 3.010373830795288,
      "learning_rate": 1.7234899328859062e-05,
      "loss": 0.5919,
      "step": 49320
    },
    {
      "epoch": 1.9731999999999998,
      "grad_norm": 2.8996028900146484,
      "learning_rate": 1.7228187919463087e-05,
      "loss": 0.5005,
      "step": 49330
    },
    {
      "epoch": 1.9736,
      "grad_norm": 3.1480579376220703,
      "learning_rate": 1.7221476510067115e-05,
      "loss": 0.6541,
      "step": 49340
    },
    {
      "epoch": 1.974,
      "grad_norm": 2.689126968383789,
      "learning_rate": 1.7214765100671144e-05,
      "loss": 0.5961,
      "step": 49350
    },
    {
      "epoch": 1.9744000000000002,
      "grad_norm": 2.707451343536377,
      "learning_rate": 1.720805369127517e-05,
      "loss": 0.5438,
      "step": 49360
    },
    {
      "epoch": 1.9748,
      "grad_norm": 2.8734328746795654,
      "learning_rate": 1.7201342281879194e-05,
      "loss": 0.6165,
      "step": 49370
    },
    {
      "epoch": 1.9752,
      "grad_norm": 2.8739876747131348,
      "learning_rate": 1.7194630872483223e-05,
      "loss": 0.478,
      "step": 49380
    },
    {
      "epoch": 1.9756,
      "grad_norm": 3.162625312805176,
      "learning_rate": 1.7187919463087248e-05,
      "loss": 0.5996,
      "step": 49390
    },
    {
      "epoch": 1.976,
      "grad_norm": 3.3783671855926514,
      "learning_rate": 1.7181208053691277e-05,
      "loss": 0.5527,
      "step": 49400
    },
    {
      "epoch": 1.9764,
      "grad_norm": 2.739091157913208,
      "learning_rate": 1.71744966442953e-05,
      "loss": 0.542,
      "step": 49410
    },
    {
      "epoch": 1.9768,
      "grad_norm": 2.52177095413208,
      "learning_rate": 1.716778523489933e-05,
      "loss": 0.5118,
      "step": 49420
    },
    {
      "epoch": 1.9771999999999998,
      "grad_norm": 3.53361177444458,
      "learning_rate": 1.7161073825503355e-05,
      "loss": 0.5454,
      "step": 49430
    },
    {
      "epoch": 1.9776,
      "grad_norm": 2.5213820934295654,
      "learning_rate": 1.7154362416107384e-05,
      "loss": 0.5213,
      "step": 49440
    },
    {
      "epoch": 1.978,
      "grad_norm": 2.324359893798828,
      "learning_rate": 1.714765100671141e-05,
      "loss": 0.5578,
      "step": 49450
    },
    {
      "epoch": 1.9784000000000002,
      "grad_norm": 2.4586751461029053,
      "learning_rate": 1.7140939597315438e-05,
      "loss": 0.4893,
      "step": 49460
    },
    {
      "epoch": 1.9788000000000001,
      "grad_norm": 3.154107093811035,
      "learning_rate": 1.7134228187919463e-05,
      "loss": 0.5591,
      "step": 49470
    },
    {
      "epoch": 1.9792,
      "grad_norm": 2.5093493461608887,
      "learning_rate": 1.712751677852349e-05,
      "loss": 0.5706,
      "step": 49480
    },
    {
      "epoch": 1.9796,
      "grad_norm": 3.0883405208587646,
      "learning_rate": 1.712080536912752e-05,
      "loss": 0.5338,
      "step": 49490
    },
    {
      "epoch": 1.98,
      "grad_norm": 2.8468143939971924,
      "learning_rate": 1.7114093959731545e-05,
      "loss": 0.5827,
      "step": 49500
    },
    {
      "epoch": 1.9804,
      "grad_norm": 2.4720706939697266,
      "learning_rate": 1.710738255033557e-05,
      "loss": 0.4985,
      "step": 49510
    },
    {
      "epoch": 1.9808,
      "grad_norm": 2.0337283611297607,
      "learning_rate": 1.71006711409396e-05,
      "loss": 0.5452,
      "step": 49520
    },
    {
      "epoch": 1.9811999999999999,
      "grad_norm": 2.24648380279541,
      "learning_rate": 1.7093959731543624e-05,
      "loss": 0.5325,
      "step": 49530
    },
    {
      "epoch": 1.9816,
      "grad_norm": 2.3907933235168457,
      "learning_rate": 1.7087248322147652e-05,
      "loss": 0.4364,
      "step": 49540
    },
    {
      "epoch": 1.982,
      "grad_norm": 3.2938590049743652,
      "learning_rate": 1.708053691275168e-05,
      "loss": 0.6341,
      "step": 49550
    },
    {
      "epoch": 1.9824000000000002,
      "grad_norm": 3.0686867237091064,
      "learning_rate": 1.7073825503355706e-05,
      "loss": 0.5573,
      "step": 49560
    },
    {
      "epoch": 1.9828000000000001,
      "grad_norm": 2.6629042625427246,
      "learning_rate": 1.706711409395973e-05,
      "loss": 0.5046,
      "step": 49570
    },
    {
      "epoch": 1.9832,
      "grad_norm": 2.096064567565918,
      "learning_rate": 1.706040268456376e-05,
      "loss": 0.5749,
      "step": 49580
    },
    {
      "epoch": 1.9836,
      "grad_norm": 2.791926622390747,
      "learning_rate": 1.7053691275167785e-05,
      "loss": 0.6571,
      "step": 49590
    },
    {
      "epoch": 1.984,
      "grad_norm": 2.58264422416687,
      "learning_rate": 1.704697986577181e-05,
      "loss": 0.5291,
      "step": 49600
    },
    {
      "epoch": 1.9844,
      "grad_norm": 2.569427967071533,
      "learning_rate": 1.7040268456375842e-05,
      "loss": 0.5906,
      "step": 49610
    },
    {
      "epoch": 1.9848,
      "grad_norm": 2.8652851581573486,
      "learning_rate": 1.7033557046979867e-05,
      "loss": 0.5707,
      "step": 49620
    },
    {
      "epoch": 1.9851999999999999,
      "grad_norm": 2.8602547645568848,
      "learning_rate": 1.7026845637583892e-05,
      "loss": 0.4966,
      "step": 49630
    },
    {
      "epoch": 1.9856,
      "grad_norm": 2.3437232971191406,
      "learning_rate": 1.702013422818792e-05,
      "loss": 0.5173,
      "step": 49640
    },
    {
      "epoch": 1.986,
      "grad_norm": 3.1369616985321045,
      "learning_rate": 1.7013422818791946e-05,
      "loss": 0.5974,
      "step": 49650
    },
    {
      "epoch": 1.9864000000000002,
      "grad_norm": 2.7590553760528564,
      "learning_rate": 1.700671140939597e-05,
      "loss": 0.694,
      "step": 49660
    },
    {
      "epoch": 1.9868000000000001,
      "grad_norm": 2.906348466873169,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 0.6119,
      "step": 49670
    },
    {
      "epoch": 1.9872,
      "grad_norm": 1.8880430459976196,
      "learning_rate": 1.699328859060403e-05,
      "loss": 0.5319,
      "step": 49680
    },
    {
      "epoch": 1.9876,
      "grad_norm": 2.5041308403015137,
      "learning_rate": 1.6986577181208054e-05,
      "loss": 0.6193,
      "step": 49690
    },
    {
      "epoch": 1.988,
      "grad_norm": 2.2540087699890137,
      "learning_rate": 1.6979865771812082e-05,
      "loss": 0.5918,
      "step": 49700
    },
    {
      "epoch": 1.9884,
      "grad_norm": 2.5504720211029053,
      "learning_rate": 1.6973154362416107e-05,
      "loss": 0.554,
      "step": 49710
    },
    {
      "epoch": 1.9888,
      "grad_norm": 2.2081801891326904,
      "learning_rate": 1.6966442953020132e-05,
      "loss": 0.4917,
      "step": 49720
    },
    {
      "epoch": 1.9891999999999999,
      "grad_norm": 3.2951698303222656,
      "learning_rate": 1.695973154362416e-05,
      "loss": 0.6626,
      "step": 49730
    },
    {
      "epoch": 1.9896,
      "grad_norm": 2.273059844970703,
      "learning_rate": 1.695302013422819e-05,
      "loss": 0.5194,
      "step": 49740
    },
    {
      "epoch": 1.99,
      "grad_norm": 3.0244879722595215,
      "learning_rate": 1.6946308724832215e-05,
      "loss": 0.5377,
      "step": 49750
    },
    {
      "epoch": 1.9904,
      "grad_norm": 2.3675625324249268,
      "learning_rate": 1.6939597315436243e-05,
      "loss": 0.518,
      "step": 49760
    },
    {
      "epoch": 1.9908000000000001,
      "grad_norm": 2.603679656982422,
      "learning_rate": 1.693288590604027e-05,
      "loss": 0.5515,
      "step": 49770
    },
    {
      "epoch": 1.9912,
      "grad_norm": 2.639381170272827,
      "learning_rate": 1.6926174496644297e-05,
      "loss": 0.5002,
      "step": 49780
    },
    {
      "epoch": 1.9916,
      "grad_norm": 3.526676893234253,
      "learning_rate": 1.6919463087248322e-05,
      "loss": 0.5821,
      "step": 49790
    },
    {
      "epoch": 1.992,
      "grad_norm": 2.672102451324463,
      "learning_rate": 1.691275167785235e-05,
      "loss": 0.5869,
      "step": 49800
    },
    {
      "epoch": 1.9924,
      "grad_norm": 3.4709393978118896,
      "learning_rate": 1.6906040268456376e-05,
      "loss": 0.6349,
      "step": 49810
    },
    {
      "epoch": 1.9928,
      "grad_norm": 3.4582087993621826,
      "learning_rate": 1.6899328859060404e-05,
      "loss": 0.5899,
      "step": 49820
    },
    {
      "epoch": 1.9931999999999999,
      "grad_norm": 2.7269632816314697,
      "learning_rate": 1.689261744966443e-05,
      "loss": 0.5896,
      "step": 49830
    },
    {
      "epoch": 1.9936,
      "grad_norm": 2.728214740753174,
      "learning_rate": 1.6885906040268458e-05,
      "loss": 0.549,
      "step": 49840
    },
    {
      "epoch": 1.994,
      "grad_norm": 2.2594223022460938,
      "learning_rate": 1.6879194630872483e-05,
      "loss": 0.5281,
      "step": 49850
    },
    {
      "epoch": 1.9944,
      "grad_norm": 2.6022050380706787,
      "learning_rate": 1.6872483221476512e-05,
      "loss": 0.5364,
      "step": 49860
    },
    {
      "epoch": 1.9948000000000001,
      "grad_norm": 2.6596248149871826,
      "learning_rate": 1.6865771812080537e-05,
      "loss": 0.5509,
      "step": 49870
    },
    {
      "epoch": 1.9952,
      "grad_norm": 2.7964186668395996,
      "learning_rate": 1.6859060402684565e-05,
      "loss": 0.4671,
      "step": 49880
    },
    {
      "epoch": 1.9956,
      "grad_norm": 3.455672264099121,
      "learning_rate": 1.685234899328859e-05,
      "loss": 0.6293,
      "step": 49890
    },
    {
      "epoch": 1.996,
      "grad_norm": 2.892303228378296,
      "learning_rate": 1.684563758389262e-05,
      "loss": 0.5445,
      "step": 49900
    },
    {
      "epoch": 1.9964,
      "grad_norm": 3.203838586807251,
      "learning_rate": 1.6838926174496644e-05,
      "loss": 0.515,
      "step": 49910
    },
    {
      "epoch": 1.9968,
      "grad_norm": 3.0464651584625244,
      "learning_rate": 1.683221476510067e-05,
      "loss": 0.6062,
      "step": 49920
    },
    {
      "epoch": 1.9971999999999999,
      "grad_norm": 2.8515632152557373,
      "learning_rate": 1.68255033557047e-05,
      "loss": 0.4953,
      "step": 49930
    },
    {
      "epoch": 1.9976,
      "grad_norm": 2.335726261138916,
      "learning_rate": 1.6818791946308727e-05,
      "loss": 0.4717,
      "step": 49940
    },
    {
      "epoch": 1.998,
      "grad_norm": 2.123169183731079,
      "learning_rate": 1.6812080536912752e-05,
      "loss": 0.5467,
      "step": 49950
    },
    {
      "epoch": 1.9984,
      "grad_norm": 4.033069133758545,
      "learning_rate": 1.680536912751678e-05,
      "loss": 0.5622,
      "step": 49960
    },
    {
      "epoch": 1.9988000000000001,
      "grad_norm": 2.7210311889648438,
      "learning_rate": 1.6798657718120805e-05,
      "loss": 0.532,
      "step": 49970
    },
    {
      "epoch": 1.9992,
      "grad_norm": 2.898007869720459,
      "learning_rate": 1.679194630872483e-05,
      "loss": 0.5621,
      "step": 49980
    },
    {
      "epoch": 1.9996,
      "grad_norm": 2.800842523574829,
      "learning_rate": 1.6785234899328863e-05,
      "loss": 0.5585,
      "step": 49990
    },
    {
      "epoch": 2.0,
      "grad_norm": 2.6040728092193604,
      "learning_rate": 1.6778523489932888e-05,
      "loss": 0.5341,
      "step": 50000
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.6504039764404297,
      "eval_runtime": 1.3365,
      "eval_samples_per_second": 1496.484,
      "eval_steps_per_second": 23.944,
      "step": 50000
    },
    {
      "epoch": 2.0004,
      "grad_norm": 2.5696773529052734,
      "learning_rate": 1.6771812080536913e-05,
      "loss": 0.5648,
      "step": 50010
    },
    {
      "epoch": 2.0008,
      "grad_norm": 2.3049354553222656,
      "learning_rate": 1.676510067114094e-05,
      "loss": 0.4788,
      "step": 50020
    },
    {
      "epoch": 2.0012,
      "grad_norm": 2.601274251937866,
      "learning_rate": 1.6758389261744967e-05,
      "loss": 0.5027,
      "step": 50030
    },
    {
      "epoch": 2.0016,
      "grad_norm": 2.5302202701568604,
      "learning_rate": 1.6751677852348992e-05,
      "loss": 0.5099,
      "step": 50040
    },
    {
      "epoch": 2.002,
      "grad_norm": 2.8792505264282227,
      "learning_rate": 1.6744966442953024e-05,
      "loss": 0.5416,
      "step": 50050
    },
    {
      "epoch": 2.0024,
      "grad_norm": 2.2594354152679443,
      "learning_rate": 1.673825503355705e-05,
      "loss": 0.5067,
      "step": 50060
    },
    {
      "epoch": 2.0028,
      "grad_norm": 2.4742908477783203,
      "learning_rate": 1.6731543624161074e-05,
      "loss": 0.4827,
      "step": 50070
    },
    {
      "epoch": 2.0032,
      "grad_norm": 3.15800404548645,
      "learning_rate": 1.6724832214765103e-05,
      "loss": 0.4732,
      "step": 50080
    },
    {
      "epoch": 2.0036,
      "grad_norm": 2.7723240852355957,
      "learning_rate": 1.6718120805369128e-05,
      "loss": 0.5403,
      "step": 50090
    },
    {
      "epoch": 2.004,
      "grad_norm": 2.2384161949157715,
      "learning_rate": 1.6711409395973153e-05,
      "loss": 0.4793,
      "step": 50100
    },
    {
      "epoch": 2.0044,
      "grad_norm": 2.798023223876953,
      "learning_rate": 1.670469798657718e-05,
      "loss": 0.5432,
      "step": 50110
    },
    {
      "epoch": 2.0048,
      "grad_norm": 2.9503509998321533,
      "learning_rate": 1.669798657718121e-05,
      "loss": 0.5697,
      "step": 50120
    },
    {
      "epoch": 2.0052,
      "grad_norm": 2.6887855529785156,
      "learning_rate": 1.6691275167785235e-05,
      "loss": 0.5343,
      "step": 50130
    },
    {
      "epoch": 2.0056,
      "grad_norm": 3.2860682010650635,
      "learning_rate": 1.6684563758389264e-05,
      "loss": 0.4887,
      "step": 50140
    },
    {
      "epoch": 2.006,
      "grad_norm": 3.2354373931884766,
      "learning_rate": 1.667785234899329e-05,
      "loss": 0.5132,
      "step": 50150
    },
    {
      "epoch": 2.0064,
      "grad_norm": 2.199130058288574,
      "learning_rate": 1.6671140939597314e-05,
      "loss": 0.4597,
      "step": 50160
    },
    {
      "epoch": 2.0068,
      "grad_norm": 2.844743490219116,
      "learning_rate": 1.6664429530201343e-05,
      "loss": 0.4573,
      "step": 50170
    },
    {
      "epoch": 2.0072,
      "grad_norm": 2.547537088394165,
      "learning_rate": 1.665771812080537e-05,
      "loss": 0.4474,
      "step": 50180
    },
    {
      "epoch": 2.0076,
      "grad_norm": 2.170335292816162,
      "learning_rate": 1.6651006711409396e-05,
      "loss": 0.4409,
      "step": 50190
    },
    {
      "epoch": 2.008,
      "grad_norm": 2.789763927459717,
      "learning_rate": 1.6644295302013425e-05,
      "loss": 0.5248,
      "step": 50200
    },
    {
      "epoch": 2.0084,
      "grad_norm": 2.3592748641967773,
      "learning_rate": 1.663758389261745e-05,
      "loss": 0.4461,
      "step": 50210
    },
    {
      "epoch": 2.0088,
      "grad_norm": 2.660520315170288,
      "learning_rate": 1.663087248322148e-05,
      "loss": 0.4513,
      "step": 50220
    },
    {
      "epoch": 2.0092,
      "grad_norm": 2.5375282764434814,
      "learning_rate": 1.6624161073825504e-05,
      "loss": 0.4747,
      "step": 50230
    },
    {
      "epoch": 2.0096,
      "grad_norm": 2.3265597820281982,
      "learning_rate": 1.661744966442953e-05,
      "loss": 0.4972,
      "step": 50240
    },
    {
      "epoch": 2.01,
      "grad_norm": 2.4127235412597656,
      "learning_rate": 1.6610738255033557e-05,
      "loss": 0.4701,
      "step": 50250
    },
    {
      "epoch": 2.0104,
      "grad_norm": 1.9878902435302734,
      "learning_rate": 1.6604026845637586e-05,
      "loss": 0.5112,
      "step": 50260
    },
    {
      "epoch": 2.0108,
      "grad_norm": 2.4794921875,
      "learning_rate": 1.659731543624161e-05,
      "loss": 0.4954,
      "step": 50270
    },
    {
      "epoch": 2.0112,
      "grad_norm": 2.4761996269226074,
      "learning_rate": 1.659060402684564e-05,
      "loss": 0.4164,
      "step": 50280
    },
    {
      "epoch": 2.0116,
      "grad_norm": 3.4142956733703613,
      "learning_rate": 1.6583892617449665e-05,
      "loss": 0.5482,
      "step": 50290
    },
    {
      "epoch": 2.012,
      "grad_norm": 3.0197558403015137,
      "learning_rate": 1.657718120805369e-05,
      "loss": 0.5263,
      "step": 50300
    },
    {
      "epoch": 2.0124,
      "grad_norm": 2.4411332607269287,
      "learning_rate": 1.657046979865772e-05,
      "loss": 0.4545,
      "step": 50310
    },
    {
      "epoch": 2.0128,
      "grad_norm": 3.075387477874756,
      "learning_rate": 1.6563758389261747e-05,
      "loss": 0.5694,
      "step": 50320
    },
    {
      "epoch": 2.0132,
      "grad_norm": 2.4557712078094482,
      "learning_rate": 1.6557046979865772e-05,
      "loss": 0.4997,
      "step": 50330
    },
    {
      "epoch": 2.0136,
      "grad_norm": 2.2766358852386475,
      "learning_rate": 1.65503355704698e-05,
      "loss": 0.5019,
      "step": 50340
    },
    {
      "epoch": 2.014,
      "grad_norm": 2.5968282222747803,
      "learning_rate": 1.6543624161073826e-05,
      "loss": 0.4367,
      "step": 50350
    },
    {
      "epoch": 2.0144,
      "grad_norm": 2.8799171447753906,
      "learning_rate": 1.653691275167785e-05,
      "loss": 0.4911,
      "step": 50360
    },
    {
      "epoch": 2.0148,
      "grad_norm": 2.902766704559326,
      "learning_rate": 1.6530201342281883e-05,
      "loss": 0.5399,
      "step": 50370
    },
    {
      "epoch": 2.0152,
      "grad_norm": 1.6113194227218628,
      "learning_rate": 1.6523489932885908e-05,
      "loss": 0.4449,
      "step": 50380
    },
    {
      "epoch": 2.0156,
      "grad_norm": 2.943739652633667,
      "learning_rate": 1.6516778523489933e-05,
      "loss": 0.501,
      "step": 50390
    },
    {
      "epoch": 2.016,
      "grad_norm": 2.543492555618286,
      "learning_rate": 1.6510067114093962e-05,
      "loss": 0.4717,
      "step": 50400
    },
    {
      "epoch": 2.0164,
      "grad_norm": 2.758333444595337,
      "learning_rate": 1.6503355704697987e-05,
      "loss": 0.497,
      "step": 50410
    },
    {
      "epoch": 2.0168,
      "grad_norm": 2.704422950744629,
      "learning_rate": 1.6496644295302012e-05,
      "loss": 0.4796,
      "step": 50420
    },
    {
      "epoch": 2.0172,
      "grad_norm": 2.3008015155792236,
      "learning_rate": 1.648993288590604e-05,
      "loss": 0.529,
      "step": 50430
    },
    {
      "epoch": 2.0176,
      "grad_norm": 2.771721601486206,
      "learning_rate": 1.648322147651007e-05,
      "loss": 0.5219,
      "step": 50440
    },
    {
      "epoch": 2.018,
      "grad_norm": 1.8364707231521606,
      "learning_rate": 1.6476510067114094e-05,
      "loss": 0.4903,
      "step": 50450
    },
    {
      "epoch": 2.0184,
      "grad_norm": 1.9112333059310913,
      "learning_rate": 1.6469798657718123e-05,
      "loss": 0.4679,
      "step": 50460
    },
    {
      "epoch": 2.0188,
      "grad_norm": 2.5000298023223877,
      "learning_rate": 1.6463087248322148e-05,
      "loss": 0.4688,
      "step": 50470
    },
    {
      "epoch": 2.0192,
      "grad_norm": 2.4065542221069336,
      "learning_rate": 1.6456375838926173e-05,
      "loss": 0.5199,
      "step": 50480
    },
    {
      "epoch": 2.0196,
      "grad_norm": 2.352560520172119,
      "learning_rate": 1.6449664429530202e-05,
      "loss": 0.4425,
      "step": 50490
    },
    {
      "epoch": 2.02,
      "grad_norm": 1.9527385234832764,
      "learning_rate": 1.644295302013423e-05,
      "loss": 0.4781,
      "step": 50500
    },
    {
      "epoch": 2.0204,
      "grad_norm": 2.45707368850708,
      "learning_rate": 1.6436241610738256e-05,
      "loss": 0.4639,
      "step": 50510
    },
    {
      "epoch": 2.0208,
      "grad_norm": 2.3033628463745117,
      "learning_rate": 1.6429530201342284e-05,
      "loss": 0.4798,
      "step": 50520
    },
    {
      "epoch": 2.0212,
      "grad_norm": 2.7064011096954346,
      "learning_rate": 1.642281879194631e-05,
      "loss": 0.5218,
      "step": 50530
    },
    {
      "epoch": 2.0216,
      "grad_norm": 2.9760093688964844,
      "learning_rate": 1.6416107382550334e-05,
      "loss": 0.4919,
      "step": 50540
    },
    {
      "epoch": 2.022,
      "grad_norm": 2.533578634262085,
      "learning_rate": 1.6409395973154363e-05,
      "loss": 0.4914,
      "step": 50550
    },
    {
      "epoch": 2.0224,
      "grad_norm": 2.596951484680176,
      "learning_rate": 1.6402684563758388e-05,
      "loss": 0.503,
      "step": 50560
    },
    {
      "epoch": 2.0228,
      "grad_norm": 3.048793315887451,
      "learning_rate": 1.6395973154362417e-05,
      "loss": 0.491,
      "step": 50570
    },
    {
      "epoch": 2.0232,
      "grad_norm": 2.8471858501434326,
      "learning_rate": 1.6389261744966445e-05,
      "loss": 0.4797,
      "step": 50580
    },
    {
      "epoch": 2.0236,
      "grad_norm": 2.8830575942993164,
      "learning_rate": 1.638255033557047e-05,
      "loss": 0.46,
      "step": 50590
    },
    {
      "epoch": 2.024,
      "grad_norm": 2.4234163761138916,
      "learning_rate": 1.6375838926174496e-05,
      "loss": 0.5343,
      "step": 50600
    },
    {
      "epoch": 2.0244,
      "grad_norm": 3.1300292015075684,
      "learning_rate": 1.6369127516778524e-05,
      "loss": 0.5261,
      "step": 50610
    },
    {
      "epoch": 2.0248,
      "grad_norm": 2.8997011184692383,
      "learning_rate": 1.636241610738255e-05,
      "loss": 0.4619,
      "step": 50620
    },
    {
      "epoch": 2.0252,
      "grad_norm": 3.2630128860473633,
      "learning_rate": 1.6355704697986578e-05,
      "loss": 0.4869,
      "step": 50630
    },
    {
      "epoch": 2.0256,
      "grad_norm": 2.773514986038208,
      "learning_rate": 1.6348993288590606e-05,
      "loss": 0.4953,
      "step": 50640
    },
    {
      "epoch": 2.026,
      "grad_norm": 1.9225972890853882,
      "learning_rate": 1.634228187919463e-05,
      "loss": 0.4692,
      "step": 50650
    },
    {
      "epoch": 2.0264,
      "grad_norm": 1.8346445560455322,
      "learning_rate": 1.633557046979866e-05,
      "loss": 0.409,
      "step": 50660
    },
    {
      "epoch": 2.0268,
      "grad_norm": 2.274513006210327,
      "learning_rate": 1.6328859060402685e-05,
      "loss": 0.4435,
      "step": 50670
    },
    {
      "epoch": 2.0272,
      "grad_norm": 2.435744524002075,
      "learning_rate": 1.632214765100671e-05,
      "loss": 0.5133,
      "step": 50680
    },
    {
      "epoch": 2.0276,
      "grad_norm": 3.1032333374023438,
      "learning_rate": 1.631543624161074e-05,
      "loss": 0.4779,
      "step": 50690
    },
    {
      "epoch": 2.028,
      "grad_norm": 1.6624114513397217,
      "learning_rate": 1.6308724832214767e-05,
      "loss": 0.4443,
      "step": 50700
    },
    {
      "epoch": 2.0284,
      "grad_norm": 3.049140214920044,
      "learning_rate": 1.6302013422818793e-05,
      "loss": 0.4606,
      "step": 50710
    },
    {
      "epoch": 2.0288,
      "grad_norm": 2.2952487468719482,
      "learning_rate": 1.629530201342282e-05,
      "loss": 0.5015,
      "step": 50720
    },
    {
      "epoch": 2.0292,
      "grad_norm": 3.344184398651123,
      "learning_rate": 1.6288590604026846e-05,
      "loss": 0.5069,
      "step": 50730
    },
    {
      "epoch": 2.0296,
      "grad_norm": 2.9314627647399902,
      "learning_rate": 1.628187919463087e-05,
      "loss": 0.5208,
      "step": 50740
    },
    {
      "epoch": 2.03,
      "grad_norm": 2.7509193420410156,
      "learning_rate": 1.62751677852349e-05,
      "loss": 0.4853,
      "step": 50750
    },
    {
      "epoch": 2.0304,
      "grad_norm": 1.976218581199646,
      "learning_rate": 1.626845637583893e-05,
      "loss": 0.4772,
      "step": 50760
    },
    {
      "epoch": 2.0308,
      "grad_norm": 2.389669418334961,
      "learning_rate": 1.6261744966442954e-05,
      "loss": 0.521,
      "step": 50770
    },
    {
      "epoch": 2.0312,
      "grad_norm": 2.210425615310669,
      "learning_rate": 1.6255033557046982e-05,
      "loss": 0.5016,
      "step": 50780
    },
    {
      "epoch": 2.0316,
      "grad_norm": 2.3846704959869385,
      "learning_rate": 1.6248322147651007e-05,
      "loss": 0.5161,
      "step": 50790
    },
    {
      "epoch": 2.032,
      "grad_norm": 2.39668869972229,
      "learning_rate": 1.6241610738255033e-05,
      "loss": 0.4979,
      "step": 50800
    },
    {
      "epoch": 2.0324,
      "grad_norm": 2.462252140045166,
      "learning_rate": 1.623489932885906e-05,
      "loss": 0.5508,
      "step": 50810
    },
    {
      "epoch": 2.0328,
      "grad_norm": 2.373999834060669,
      "learning_rate": 1.622818791946309e-05,
      "loss": 0.391,
      "step": 50820
    },
    {
      "epoch": 2.0332,
      "grad_norm": 2.573467254638672,
      "learning_rate": 1.6221476510067115e-05,
      "loss": 0.482,
      "step": 50830
    },
    {
      "epoch": 2.0336,
      "grad_norm": 2.307288646697998,
      "learning_rate": 1.6214765100671143e-05,
      "loss": 0.5101,
      "step": 50840
    },
    {
      "epoch": 2.034,
      "grad_norm": 3.3649914264678955,
      "learning_rate": 1.620805369127517e-05,
      "loss": 0.5812,
      "step": 50850
    },
    {
      "epoch": 2.0344,
      "grad_norm": 2.6202268600463867,
      "learning_rate": 1.6201342281879194e-05,
      "loss": 0.4986,
      "step": 50860
    },
    {
      "epoch": 2.0348,
      "grad_norm": 2.93178129196167,
      "learning_rate": 1.6194630872483222e-05,
      "loss": 0.5226,
      "step": 50870
    },
    {
      "epoch": 2.0352,
      "grad_norm": 2.4672787189483643,
      "learning_rate": 1.6187919463087247e-05,
      "loss": 0.4706,
      "step": 50880
    },
    {
      "epoch": 2.0356,
      "grad_norm": 2.906320333480835,
      "learning_rate": 1.6181208053691276e-05,
      "loss": 0.5265,
      "step": 50890
    },
    {
      "epoch": 2.036,
      "grad_norm": 2.6943185329437256,
      "learning_rate": 1.6174496644295304e-05,
      "loss": 0.5288,
      "step": 50900
    },
    {
      "epoch": 2.0364,
      "grad_norm": 2.7372756004333496,
      "learning_rate": 1.616778523489933e-05,
      "loss": 0.5214,
      "step": 50910
    },
    {
      "epoch": 2.0368,
      "grad_norm": 3.302107572555542,
      "learning_rate": 1.6161073825503355e-05,
      "loss": 0.5052,
      "step": 50920
    },
    {
      "epoch": 2.0372,
      "grad_norm": 3.277796745300293,
      "learning_rate": 1.6154362416107383e-05,
      "loss": 0.5117,
      "step": 50930
    },
    {
      "epoch": 2.0376,
      "grad_norm": 2.4920601844787598,
      "learning_rate": 1.614765100671141e-05,
      "loss": 0.6347,
      "step": 50940
    },
    {
      "epoch": 2.038,
      "grad_norm": 2.3178999423980713,
      "learning_rate": 1.6140939597315437e-05,
      "loss": 0.4467,
      "step": 50950
    },
    {
      "epoch": 2.0384,
      "grad_norm": 2.8748939037323,
      "learning_rate": 1.6134228187919466e-05,
      "loss": 0.4588,
      "step": 50960
    },
    {
      "epoch": 2.0388,
      "grad_norm": 2.825193405151367,
      "learning_rate": 1.612751677852349e-05,
      "loss": 0.5104,
      "step": 50970
    },
    {
      "epoch": 2.0392,
      "grad_norm": 2.595384359359741,
      "learning_rate": 1.6120805369127516e-05,
      "loss": 0.5422,
      "step": 50980
    },
    {
      "epoch": 2.0396,
      "grad_norm": 2.822540760040283,
      "learning_rate": 1.6114093959731544e-05,
      "loss": 0.5287,
      "step": 50990
    },
    {
      "epoch": 2.04,
      "grad_norm": 3.1582083702087402,
      "learning_rate": 1.610738255033557e-05,
      "loss": 0.5632,
      "step": 51000
    },
    {
      "epoch": 2.0404,
      "grad_norm": 2.3196632862091064,
      "learning_rate": 1.6100671140939598e-05,
      "loss": 0.4551,
      "step": 51010
    },
    {
      "epoch": 2.0408,
      "grad_norm": 2.1055619716644287,
      "learning_rate": 1.6093959731543627e-05,
      "loss": 0.4123,
      "step": 51020
    },
    {
      "epoch": 2.0412,
      "grad_norm": 2.2322614192962646,
      "learning_rate": 1.6087248322147652e-05,
      "loss": 0.4329,
      "step": 51030
    },
    {
      "epoch": 2.0416,
      "grad_norm": 2.784196138381958,
      "learning_rate": 1.6080536912751677e-05,
      "loss": 0.4918,
      "step": 51040
    },
    {
      "epoch": 2.042,
      "grad_norm": 2.7674942016601562,
      "learning_rate": 1.6073825503355706e-05,
      "loss": 0.5637,
      "step": 51050
    },
    {
      "epoch": 2.0424,
      "grad_norm": 2.23415207862854,
      "learning_rate": 1.606711409395973e-05,
      "loss": 0.4875,
      "step": 51060
    },
    {
      "epoch": 2.0428,
      "grad_norm": 3.078167676925659,
      "learning_rate": 1.606040268456376e-05,
      "loss": 0.4967,
      "step": 51070
    },
    {
      "epoch": 2.0432,
      "grad_norm": 2.892035722732544,
      "learning_rate": 1.6053691275167788e-05,
      "loss": 0.4776,
      "step": 51080
    },
    {
      "epoch": 2.0436,
      "grad_norm": 2.012993812561035,
      "learning_rate": 1.6046979865771813e-05,
      "loss": 0.4449,
      "step": 51090
    },
    {
      "epoch": 2.044,
      "grad_norm": 1.995856523513794,
      "learning_rate": 1.604026845637584e-05,
      "loss": 0.5043,
      "step": 51100
    },
    {
      "epoch": 2.0444,
      "grad_norm": 2.17486310005188,
      "learning_rate": 1.6033557046979867e-05,
      "loss": 0.4228,
      "step": 51110
    },
    {
      "epoch": 2.0448,
      "grad_norm": 2.935251235961914,
      "learning_rate": 1.6026845637583892e-05,
      "loss": 0.5102,
      "step": 51120
    },
    {
      "epoch": 2.0452,
      "grad_norm": 2.641711950302124,
      "learning_rate": 1.602013422818792e-05,
      "loss": 0.4643,
      "step": 51130
    },
    {
      "epoch": 2.0456,
      "grad_norm": 3.1567935943603516,
      "learning_rate": 1.601342281879195e-05,
      "loss": 0.5448,
      "step": 51140
    },
    {
      "epoch": 2.046,
      "grad_norm": 2.0444886684417725,
      "learning_rate": 1.6006711409395974e-05,
      "loss": 0.521,
      "step": 51150
    },
    {
      "epoch": 2.0464,
      "grad_norm": 2.3600800037384033,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.5168,
      "step": 51160
    },
    {
      "epoch": 2.0468,
      "grad_norm": 2.480868101119995,
      "learning_rate": 1.5993288590604028e-05,
      "loss": 0.5015,
      "step": 51170
    },
    {
      "epoch": 2.0472,
      "grad_norm": 2.468202829360962,
      "learning_rate": 1.5986577181208053e-05,
      "loss": 0.5015,
      "step": 51180
    },
    {
      "epoch": 2.0476,
      "grad_norm": 2.9248909950256348,
      "learning_rate": 1.597986577181208e-05,
      "loss": 0.4561,
      "step": 51190
    },
    {
      "epoch": 2.048,
      "grad_norm": 1.6427522897720337,
      "learning_rate": 1.597315436241611e-05,
      "loss": 0.4324,
      "step": 51200
    },
    {
      "epoch": 2.0484,
      "grad_norm": 1.8677490949630737,
      "learning_rate": 1.5966442953020135e-05,
      "loss": 0.4334,
      "step": 51210
    },
    {
      "epoch": 2.0488,
      "grad_norm": 2.58097505569458,
      "learning_rate": 1.5959731543624164e-05,
      "loss": 0.4805,
      "step": 51220
    },
    {
      "epoch": 2.0492,
      "grad_norm": 2.383338212966919,
      "learning_rate": 1.595302013422819e-05,
      "loss": 0.5098,
      "step": 51230
    },
    {
      "epoch": 2.0496,
      "grad_norm": 3.235504150390625,
      "learning_rate": 1.5946308724832214e-05,
      "loss": 0.5567,
      "step": 51240
    },
    {
      "epoch": 2.05,
      "grad_norm": 2.6520659923553467,
      "learning_rate": 1.5939597315436243e-05,
      "loss": 0.5181,
      "step": 51250
    },
    {
      "epoch": 2.0504,
      "grad_norm": 2.637305736541748,
      "learning_rate": 1.5932885906040268e-05,
      "loss": 0.5961,
      "step": 51260
    },
    {
      "epoch": 2.0508,
      "grad_norm": 3.0236754417419434,
      "learning_rate": 1.5926174496644296e-05,
      "loss": 0.5651,
      "step": 51270
    },
    {
      "epoch": 2.0512,
      "grad_norm": 2.7907326221466064,
      "learning_rate": 1.5919463087248325e-05,
      "loss": 0.3954,
      "step": 51280
    },
    {
      "epoch": 2.0516,
      "grad_norm": 2.3459112644195557,
      "learning_rate": 1.591275167785235e-05,
      "loss": 0.5078,
      "step": 51290
    },
    {
      "epoch": 2.052,
      "grad_norm": 2.968430757522583,
      "learning_rate": 1.5906040268456375e-05,
      "loss": 0.5258,
      "step": 51300
    },
    {
      "epoch": 2.0524,
      "grad_norm": 2.2362284660339355,
      "learning_rate": 1.5899328859060404e-05,
      "loss": 0.4686,
      "step": 51310
    },
    {
      "epoch": 2.0528,
      "grad_norm": 3.076242685317993,
      "learning_rate": 1.589261744966443e-05,
      "loss": 0.5668,
      "step": 51320
    },
    {
      "epoch": 2.0532,
      "grad_norm": 2.2649030685424805,
      "learning_rate": 1.5885906040268457e-05,
      "loss": 0.4645,
      "step": 51330
    },
    {
      "epoch": 2.0536,
      "grad_norm": 2.332653760910034,
      "learning_rate": 1.5879194630872486e-05,
      "loss": 0.4861,
      "step": 51340
    },
    {
      "epoch": 2.054,
      "grad_norm": 2.4562134742736816,
      "learning_rate": 1.587248322147651e-05,
      "loss": 0.4965,
      "step": 51350
    },
    {
      "epoch": 2.0544,
      "grad_norm": 2.580841064453125,
      "learning_rate": 1.5865771812080536e-05,
      "loss": 0.4675,
      "step": 51360
    },
    {
      "epoch": 2.0548,
      "grad_norm": 3.1055831909179688,
      "learning_rate": 1.5859060402684565e-05,
      "loss": 0.599,
      "step": 51370
    },
    {
      "epoch": 2.0552,
      "grad_norm": 2.4212894439697266,
      "learning_rate": 1.585234899328859e-05,
      "loss": 0.5551,
      "step": 51380
    },
    {
      "epoch": 2.0556,
      "grad_norm": 2.4646918773651123,
      "learning_rate": 1.5845637583892615e-05,
      "loss": 0.4446,
      "step": 51390
    },
    {
      "epoch": 2.056,
      "grad_norm": 2.8540844917297363,
      "learning_rate": 1.5838926174496647e-05,
      "loss": 0.499,
      "step": 51400
    },
    {
      "epoch": 2.0564,
      "grad_norm": 2.1030325889587402,
      "learning_rate": 1.5832214765100672e-05,
      "loss": 0.5543,
      "step": 51410
    },
    {
      "epoch": 2.0568,
      "grad_norm": 2.261803150177002,
      "learning_rate": 1.5825503355704697e-05,
      "loss": 0.4957,
      "step": 51420
    },
    {
      "epoch": 2.0572,
      "grad_norm": 2.4033126831054688,
      "learning_rate": 1.5818791946308726e-05,
      "loss": 0.5027,
      "step": 51430
    },
    {
      "epoch": 2.0576,
      "grad_norm": 2.918632745742798,
      "learning_rate": 1.581208053691275e-05,
      "loss": 0.5585,
      "step": 51440
    },
    {
      "epoch": 2.058,
      "grad_norm": 2.368988037109375,
      "learning_rate": 1.580536912751678e-05,
      "loss": 0.5135,
      "step": 51450
    },
    {
      "epoch": 2.0584,
      "grad_norm": 2.3432013988494873,
      "learning_rate": 1.5798657718120808e-05,
      "loss": 0.5221,
      "step": 51460
    },
    {
      "epoch": 2.0588,
      "grad_norm": 2.783792734146118,
      "learning_rate": 1.5791946308724833e-05,
      "loss": 0.4978,
      "step": 51470
    },
    {
      "epoch": 2.0592,
      "grad_norm": 1.9709885120391846,
      "learning_rate": 1.578523489932886e-05,
      "loss": 0.4715,
      "step": 51480
    },
    {
      "epoch": 2.0596,
      "grad_norm": 3.355459690093994,
      "learning_rate": 1.5778523489932887e-05,
      "loss": 0.6181,
      "step": 51490
    },
    {
      "epoch": 2.06,
      "grad_norm": 2.2361650466918945,
      "learning_rate": 1.5771812080536912e-05,
      "loss": 0.4907,
      "step": 51500
    },
    {
      "epoch": 2.0604,
      "grad_norm": 2.5178236961364746,
      "learning_rate": 1.576510067114094e-05,
      "loss": 0.5091,
      "step": 51510
    },
    {
      "epoch": 2.0608,
      "grad_norm": 2.4342200756073,
      "learning_rate": 1.575838926174497e-05,
      "loss": 0.4842,
      "step": 51520
    },
    {
      "epoch": 2.0612,
      "grad_norm": 2.7889697551727295,
      "learning_rate": 1.5751677852348995e-05,
      "loss": 0.5311,
      "step": 51530
    },
    {
      "epoch": 2.0616,
      "grad_norm": 2.265904188156128,
      "learning_rate": 1.574496644295302e-05,
      "loss": 0.5166,
      "step": 51540
    },
    {
      "epoch": 2.062,
      "grad_norm": 1.9674314260482788,
      "learning_rate": 1.5738255033557048e-05,
      "loss": 0.5391,
      "step": 51550
    },
    {
      "epoch": 2.0624,
      "grad_norm": 2.881713390350342,
      "learning_rate": 1.5731543624161073e-05,
      "loss": 0.4813,
      "step": 51560
    },
    {
      "epoch": 2.0628,
      "grad_norm": 1.7558954954147339,
      "learning_rate": 1.5724832214765102e-05,
      "loss": 0.4794,
      "step": 51570
    },
    {
      "epoch": 2.0632,
      "grad_norm": 3.08894944190979,
      "learning_rate": 1.5718120805369127e-05,
      "loss": 0.5072,
      "step": 51580
    },
    {
      "epoch": 2.0636,
      "grad_norm": 2.5309178829193115,
      "learning_rate": 1.5711409395973156e-05,
      "loss": 0.4989,
      "step": 51590
    },
    {
      "epoch": 2.064,
      "grad_norm": 3.0309324264526367,
      "learning_rate": 1.5704697986577184e-05,
      "loss": 0.5531,
      "step": 51600
    },
    {
      "epoch": 2.0644,
      "grad_norm": 2.6311092376708984,
      "learning_rate": 1.569798657718121e-05,
      "loss": 0.5406,
      "step": 51610
    },
    {
      "epoch": 2.0648,
      "grad_norm": 2.680772304534912,
      "learning_rate": 1.5691275167785235e-05,
      "loss": 0.5396,
      "step": 51620
    },
    {
      "epoch": 2.0652,
      "grad_norm": 2.6490700244903564,
      "learning_rate": 1.5684563758389263e-05,
      "loss": 0.5276,
      "step": 51630
    },
    {
      "epoch": 2.0656,
      "grad_norm": 2.63504695892334,
      "learning_rate": 1.5677852348993288e-05,
      "loss": 0.4532,
      "step": 51640
    },
    {
      "epoch": 2.066,
      "grad_norm": 2.6969900131225586,
      "learning_rate": 1.5671140939597317e-05,
      "loss": 0.4374,
      "step": 51650
    },
    {
      "epoch": 2.0664,
      "grad_norm": 2.885493755340576,
      "learning_rate": 1.5664429530201345e-05,
      "loss": 0.5441,
      "step": 51660
    },
    {
      "epoch": 2.0668,
      "grad_norm": 2.611407995223999,
      "learning_rate": 1.565771812080537e-05,
      "loss": 0.4409,
      "step": 51670
    },
    {
      "epoch": 2.0672,
      "grad_norm": 2.4937262535095215,
      "learning_rate": 1.5651006711409396e-05,
      "loss": 0.4694,
      "step": 51680
    },
    {
      "epoch": 2.0676,
      "grad_norm": 2.003899335861206,
      "learning_rate": 1.5644295302013424e-05,
      "loss": 0.5434,
      "step": 51690
    },
    {
      "epoch": 2.068,
      "grad_norm": 3.037545919418335,
      "learning_rate": 1.563758389261745e-05,
      "loss": 0.5106,
      "step": 51700
    },
    {
      "epoch": 2.0684,
      "grad_norm": 3.570751905441284,
      "learning_rate": 1.5630872483221474e-05,
      "loss": 0.4793,
      "step": 51710
    },
    {
      "epoch": 2.0688,
      "grad_norm": 2.516373634338379,
      "learning_rate": 1.5624161073825506e-05,
      "loss": 0.5949,
      "step": 51720
    },
    {
      "epoch": 2.0692,
      "grad_norm": 2.7215285301208496,
      "learning_rate": 1.561744966442953e-05,
      "loss": 0.5289,
      "step": 51730
    },
    {
      "epoch": 2.0696,
      "grad_norm": 2.6605610847473145,
      "learning_rate": 1.5610738255033557e-05,
      "loss": 0.5464,
      "step": 51740
    },
    {
      "epoch": 2.07,
      "grad_norm": 2.326110601425171,
      "learning_rate": 1.5604026845637585e-05,
      "loss": 0.4791,
      "step": 51750
    },
    {
      "epoch": 2.0704,
      "grad_norm": 2.598376512527466,
      "learning_rate": 1.559731543624161e-05,
      "loss": 0.4519,
      "step": 51760
    },
    {
      "epoch": 2.0708,
      "grad_norm": 2.5219483375549316,
      "learning_rate": 1.5590604026845636e-05,
      "loss": 0.495,
      "step": 51770
    },
    {
      "epoch": 2.0712,
      "grad_norm": 2.8822336196899414,
      "learning_rate": 1.5583892617449668e-05,
      "loss": 0.5184,
      "step": 51780
    },
    {
      "epoch": 2.0716,
      "grad_norm": 2.8917598724365234,
      "learning_rate": 1.5577181208053693e-05,
      "loss": 0.5282,
      "step": 51790
    },
    {
      "epoch": 2.072,
      "grad_norm": 2.8035693168640137,
      "learning_rate": 1.5570469798657718e-05,
      "loss": 0.4691,
      "step": 51800
    },
    {
      "epoch": 2.0724,
      "grad_norm": 2.3696348667144775,
      "learning_rate": 1.5563758389261746e-05,
      "loss": 0.4583,
      "step": 51810
    },
    {
      "epoch": 2.0728,
      "grad_norm": 2.8396196365356445,
      "learning_rate": 1.555704697986577e-05,
      "loss": 0.5572,
      "step": 51820
    },
    {
      "epoch": 2.0732,
      "grad_norm": 2.382686138153076,
      "learning_rate": 1.5550335570469797e-05,
      "loss": 0.4959,
      "step": 51830
    },
    {
      "epoch": 2.0736,
      "grad_norm": 2.252499580383301,
      "learning_rate": 1.554362416107383e-05,
      "loss": 0.5204,
      "step": 51840
    },
    {
      "epoch": 2.074,
      "grad_norm": 3.1239216327667236,
      "learning_rate": 1.5536912751677854e-05,
      "loss": 0.5019,
      "step": 51850
    },
    {
      "epoch": 2.0744,
      "grad_norm": 2.710336208343506,
      "learning_rate": 1.553020134228188e-05,
      "loss": 0.5323,
      "step": 51860
    },
    {
      "epoch": 2.0748,
      "grad_norm": 2.342384099960327,
      "learning_rate": 1.5523489932885908e-05,
      "loss": 0.5225,
      "step": 51870
    },
    {
      "epoch": 2.0752,
      "grad_norm": 3.449922561645508,
      "learning_rate": 1.5516778523489933e-05,
      "loss": 0.5226,
      "step": 51880
    },
    {
      "epoch": 2.0756,
      "grad_norm": 3.008091926574707,
      "learning_rate": 1.551006711409396e-05,
      "loss": 0.5306,
      "step": 51890
    },
    {
      "epoch": 2.076,
      "grad_norm": 2.4872303009033203,
      "learning_rate": 1.5503355704697986e-05,
      "loss": 0.4611,
      "step": 51900
    },
    {
      "epoch": 2.0764,
      "grad_norm": 2.533295154571533,
      "learning_rate": 1.5496644295302015e-05,
      "loss": 0.5046,
      "step": 51910
    },
    {
      "epoch": 2.0768,
      "grad_norm": 1.9897640943527222,
      "learning_rate": 1.548993288590604e-05,
      "loss": 0.5069,
      "step": 51920
    },
    {
      "epoch": 2.0772,
      "grad_norm": 2.794161558151245,
      "learning_rate": 1.548322147651007e-05,
      "loss": 0.5244,
      "step": 51930
    },
    {
      "epoch": 2.0776,
      "grad_norm": 2.342726230621338,
      "learning_rate": 1.5476510067114094e-05,
      "loss": 0.3962,
      "step": 51940
    },
    {
      "epoch": 2.078,
      "grad_norm": 1.859786868095398,
      "learning_rate": 1.5469798657718122e-05,
      "loss": 0.4561,
      "step": 51950
    },
    {
      "epoch": 2.0784,
      "grad_norm": 2.470435380935669,
      "learning_rate": 1.5463087248322148e-05,
      "loss": 0.4467,
      "step": 51960
    },
    {
      "epoch": 2.0788,
      "grad_norm": 2.1032490730285645,
      "learning_rate": 1.5456375838926176e-05,
      "loss": 0.429,
      "step": 51970
    },
    {
      "epoch": 2.0792,
      "grad_norm": 3.027787685394287,
      "learning_rate": 1.54496644295302e-05,
      "loss": 0.548,
      "step": 51980
    },
    {
      "epoch": 2.0796,
      "grad_norm": 2.3985111713409424,
      "learning_rate": 1.544295302013423e-05,
      "loss": 0.4013,
      "step": 51990
    },
    {
      "epoch": 2.08,
      "grad_norm": 2.119706392288208,
      "learning_rate": 1.5436241610738255e-05,
      "loss": 0.5124,
      "step": 52000
    },
    {
      "epoch": 2.0804,
      "grad_norm": 3.0767128467559814,
      "learning_rate": 1.5429530201342283e-05,
      "loss": 0.5312,
      "step": 52010
    },
    {
      "epoch": 2.0808,
      "grad_norm": 2.1740944385528564,
      "learning_rate": 1.542281879194631e-05,
      "loss": 0.4328,
      "step": 52020
    },
    {
      "epoch": 2.0812,
      "grad_norm": 3.510009527206421,
      "learning_rate": 1.5416107382550334e-05,
      "loss": 0.5048,
      "step": 52030
    },
    {
      "epoch": 2.0816,
      "grad_norm": 2.6007351875305176,
      "learning_rate": 1.5409395973154366e-05,
      "loss": 0.479,
      "step": 52040
    },
    {
      "epoch": 2.082,
      "grad_norm": 2.794390916824341,
      "learning_rate": 1.540268456375839e-05,
      "loss": 0.4432,
      "step": 52050
    },
    {
      "epoch": 2.0824,
      "grad_norm": 2.271263599395752,
      "learning_rate": 1.5395973154362416e-05,
      "loss": 0.5534,
      "step": 52060
    },
    {
      "epoch": 2.0828,
      "grad_norm": 2.847198009490967,
      "learning_rate": 1.5389261744966445e-05,
      "loss": 0.4901,
      "step": 52070
    },
    {
      "epoch": 2.0832,
      "grad_norm": 3.2377941608428955,
      "learning_rate": 1.538255033557047e-05,
      "loss": 0.6178,
      "step": 52080
    },
    {
      "epoch": 2.0836,
      "grad_norm": 3.19289231300354,
      "learning_rate": 1.5375838926174495e-05,
      "loss": 0.5664,
      "step": 52090
    },
    {
      "epoch": 2.084,
      "grad_norm": 3.451002597808838,
      "learning_rate": 1.5369127516778527e-05,
      "loss": 0.514,
      "step": 52100
    },
    {
      "epoch": 2.0844,
      "grad_norm": 2.9798583984375,
      "learning_rate": 1.5362416107382552e-05,
      "loss": 0.5285,
      "step": 52110
    },
    {
      "epoch": 2.0848,
      "grad_norm": 2.6347463130950928,
      "learning_rate": 1.5355704697986577e-05,
      "loss": 0.5622,
      "step": 52120
    },
    {
      "epoch": 2.0852,
      "grad_norm": 3.666569232940674,
      "learning_rate": 1.5348993288590606e-05,
      "loss": 0.5549,
      "step": 52130
    },
    {
      "epoch": 2.0856,
      "grad_norm": 3.5238237380981445,
      "learning_rate": 1.534228187919463e-05,
      "loss": 0.5293,
      "step": 52140
    },
    {
      "epoch": 2.086,
      "grad_norm": 2.3156299591064453,
      "learning_rate": 1.5335570469798656e-05,
      "loss": 0.4976,
      "step": 52150
    },
    {
      "epoch": 2.0864,
      "grad_norm": 2.611934185028076,
      "learning_rate": 1.5328859060402688e-05,
      "loss": 0.454,
      "step": 52160
    },
    {
      "epoch": 2.0868,
      "grad_norm": 2.3162949085235596,
      "learning_rate": 1.5322147651006713e-05,
      "loss": 0.5223,
      "step": 52170
    },
    {
      "epoch": 2.0872,
      "grad_norm": 2.682495355606079,
      "learning_rate": 1.5315436241610738e-05,
      "loss": 0.4834,
      "step": 52180
    },
    {
      "epoch": 2.0876,
      "grad_norm": 2.5156052112579346,
      "learning_rate": 1.5308724832214767e-05,
      "loss": 0.5161,
      "step": 52190
    },
    {
      "epoch": 2.088,
      "grad_norm": 2.961120367050171,
      "learning_rate": 1.5302013422818792e-05,
      "loss": 0.4934,
      "step": 52200
    },
    {
      "epoch": 2.0884,
      "grad_norm": 3.446213722229004,
      "learning_rate": 1.5295302013422817e-05,
      "loss": 0.5146,
      "step": 52210
    },
    {
      "epoch": 2.0888,
      "grad_norm": 2.859436273574829,
      "learning_rate": 1.5288590604026846e-05,
      "loss": 0.4508,
      "step": 52220
    },
    {
      "epoch": 2.0892,
      "grad_norm": 2.709959030151367,
      "learning_rate": 1.5281879194630874e-05,
      "loss": 0.4746,
      "step": 52230
    },
    {
      "epoch": 2.0896,
      "grad_norm": 2.759448289871216,
      "learning_rate": 1.52751677852349e-05,
      "loss": 0.4526,
      "step": 52240
    },
    {
      "epoch": 2.09,
      "grad_norm": 2.3118467330932617,
      "learning_rate": 1.5268456375838928e-05,
      "loss": 0.4683,
      "step": 52250
    },
    {
      "epoch": 2.0904,
      "grad_norm": 2.3326170444488525,
      "learning_rate": 1.5261744966442953e-05,
      "loss": 0.5054,
      "step": 52260
    },
    {
      "epoch": 2.0908,
      "grad_norm": 1.5859010219573975,
      "learning_rate": 1.525503355704698e-05,
      "loss": 0.4753,
      "step": 52270
    },
    {
      "epoch": 2.0912,
      "grad_norm": 2.7412328720092773,
      "learning_rate": 1.5248322147651007e-05,
      "loss": 0.4735,
      "step": 52280
    },
    {
      "epoch": 2.0916,
      "grad_norm": 2.675109624862671,
      "learning_rate": 1.5241610738255035e-05,
      "loss": 0.4608,
      "step": 52290
    },
    {
      "epoch": 2.092,
      "grad_norm": 2.3753821849823,
      "learning_rate": 1.5234899328859062e-05,
      "loss": 0.5283,
      "step": 52300
    },
    {
      "epoch": 2.0924,
      "grad_norm": 2.2170052528381348,
      "learning_rate": 1.5228187919463089e-05,
      "loss": 0.4767,
      "step": 52310
    },
    {
      "epoch": 2.0928,
      "grad_norm": 2.7527620792388916,
      "learning_rate": 1.5221476510067114e-05,
      "loss": 0.5219,
      "step": 52320
    },
    {
      "epoch": 2.0932,
      "grad_norm": 2.490809679031372,
      "learning_rate": 1.5214765100671141e-05,
      "loss": 0.5047,
      "step": 52330
    },
    {
      "epoch": 2.0936,
      "grad_norm": 2.115022897720337,
      "learning_rate": 1.5208053691275168e-05,
      "loss": 0.4654,
      "step": 52340
    },
    {
      "epoch": 2.094,
      "grad_norm": 3.001297950744629,
      "learning_rate": 1.5201342281879196e-05,
      "loss": 0.5215,
      "step": 52350
    },
    {
      "epoch": 2.0944,
      "grad_norm": 2.310234308242798,
      "learning_rate": 1.5194630872483223e-05,
      "loss": 0.5341,
      "step": 52360
    },
    {
      "epoch": 2.0948,
      "grad_norm": 2.4940664768218994,
      "learning_rate": 1.518791946308725e-05,
      "loss": 0.4747,
      "step": 52370
    },
    {
      "epoch": 2.0952,
      "grad_norm": 2.289921760559082,
      "learning_rate": 1.5181208053691275e-05,
      "loss": 0.5415,
      "step": 52380
    },
    {
      "epoch": 2.0956,
      "grad_norm": 2.4920074939727783,
      "learning_rate": 1.5174496644295302e-05,
      "loss": 0.4799,
      "step": 52390
    },
    {
      "epoch": 2.096,
      "grad_norm": 2.8449127674102783,
      "learning_rate": 1.5167785234899329e-05,
      "loss": 0.422,
      "step": 52400
    },
    {
      "epoch": 2.0964,
      "grad_norm": 2.5163633823394775,
      "learning_rate": 1.5161073825503356e-05,
      "loss": 0.5514,
      "step": 52410
    },
    {
      "epoch": 2.0968,
      "grad_norm": 2.7570810317993164,
      "learning_rate": 1.5154362416107384e-05,
      "loss": 0.4719,
      "step": 52420
    },
    {
      "epoch": 2.0972,
      "grad_norm": 1.798225998878479,
      "learning_rate": 1.5147651006711411e-05,
      "loss": 0.5054,
      "step": 52430
    },
    {
      "epoch": 2.0976,
      "grad_norm": 2.7489864826202393,
      "learning_rate": 1.5140939597315436e-05,
      "loss": 0.4892,
      "step": 52440
    },
    {
      "epoch": 2.098,
      "grad_norm": 2.048203706741333,
      "learning_rate": 1.5134228187919463e-05,
      "loss": 0.5,
      "step": 52450
    },
    {
      "epoch": 2.0984,
      "grad_norm": 1.93006432056427,
      "learning_rate": 1.512751677852349e-05,
      "loss": 0.55,
      "step": 52460
    },
    {
      "epoch": 2.0987999999999998,
      "grad_norm": 2.45206880569458,
      "learning_rate": 1.5120805369127517e-05,
      "loss": 0.5502,
      "step": 52470
    },
    {
      "epoch": 2.0992,
      "grad_norm": 2.6384518146514893,
      "learning_rate": 1.5114093959731546e-05,
      "loss": 0.4897,
      "step": 52480
    },
    {
      "epoch": 2.0996,
      "grad_norm": 2.988931179046631,
      "learning_rate": 1.5107382550335572e-05,
      "loss": 0.4894,
      "step": 52490
    },
    {
      "epoch": 2.1,
      "grad_norm": 2.469923973083496,
      "learning_rate": 1.51006711409396e-05,
      "loss": 0.5949,
      "step": 52500
    },
    {
      "epoch": 2.1004,
      "grad_norm": 2.4992642402648926,
      "learning_rate": 1.5093959731543624e-05,
      "loss": 0.52,
      "step": 52510
    },
    {
      "epoch": 2.1008,
      "grad_norm": 2.430310010910034,
      "learning_rate": 1.5087248322147651e-05,
      "loss": 0.4514,
      "step": 52520
    },
    {
      "epoch": 2.1012,
      "grad_norm": 2.886479139328003,
      "learning_rate": 1.5080536912751678e-05,
      "loss": 0.5327,
      "step": 52530
    },
    {
      "epoch": 2.1016,
      "grad_norm": 2.72786283493042,
      "learning_rate": 1.5073825503355703e-05,
      "loss": 0.4247,
      "step": 52540
    },
    {
      "epoch": 2.102,
      "grad_norm": 2.9692182540893555,
      "learning_rate": 1.5067114093959734e-05,
      "loss": 0.5076,
      "step": 52550
    },
    {
      "epoch": 2.1024,
      "grad_norm": 2.565469980239868,
      "learning_rate": 1.506040268456376e-05,
      "loss": 0.5102,
      "step": 52560
    },
    {
      "epoch": 2.1028000000000002,
      "grad_norm": 2.9284801483154297,
      "learning_rate": 1.5053691275167786e-05,
      "loss": 0.4612,
      "step": 52570
    },
    {
      "epoch": 2.1032,
      "grad_norm": 2.428920269012451,
      "learning_rate": 1.5046979865771812e-05,
      "loss": 0.4677,
      "step": 52580
    },
    {
      "epoch": 2.1036,
      "grad_norm": 2.3755581378936768,
      "learning_rate": 1.504026845637584e-05,
      "loss": 0.5298,
      "step": 52590
    },
    {
      "epoch": 2.104,
      "grad_norm": 2.8559536933898926,
      "learning_rate": 1.5033557046979866e-05,
      "loss": 0.4683,
      "step": 52600
    },
    {
      "epoch": 2.1044,
      "grad_norm": 2.6079249382019043,
      "learning_rate": 1.5026845637583895e-05,
      "loss": 0.5354,
      "step": 52610
    },
    {
      "epoch": 2.1048,
      "grad_norm": 2.6822619438171387,
      "learning_rate": 1.5020134228187922e-05,
      "loss": 0.5678,
      "step": 52620
    },
    {
      "epoch": 2.1052,
      "grad_norm": 2.2040152549743652,
      "learning_rate": 1.5013422818791947e-05,
      "loss": 0.5059,
      "step": 52630
    },
    {
      "epoch": 2.1056,
      "grad_norm": 3.510694980621338,
      "learning_rate": 1.5006711409395974e-05,
      "loss": 0.6018,
      "step": 52640
    },
    {
      "epoch": 2.106,
      "grad_norm": 3.1739041805267334,
      "learning_rate": 1.5e-05,
      "loss": 0.4836,
      "step": 52650
    },
    {
      "epoch": 2.1064,
      "grad_norm": 2.5839855670928955,
      "learning_rate": 1.4993288590604027e-05,
      "loss": 0.5215,
      "step": 52660
    },
    {
      "epoch": 2.1068,
      "grad_norm": 2.426983594894409,
      "learning_rate": 1.4986577181208056e-05,
      "loss": 0.4446,
      "step": 52670
    },
    {
      "epoch": 2.1072,
      "grad_norm": 2.7383668422698975,
      "learning_rate": 1.4979865771812083e-05,
      "loss": 0.5119,
      "step": 52680
    },
    {
      "epoch": 2.1076,
      "grad_norm": 2.6006646156311035,
      "learning_rate": 1.4973154362416108e-05,
      "loss": 0.5035,
      "step": 52690
    },
    {
      "epoch": 2.108,
      "grad_norm": 2.8952136039733887,
      "learning_rate": 1.4966442953020135e-05,
      "loss": 0.5408,
      "step": 52700
    },
    {
      "epoch": 2.1084,
      "grad_norm": 2.620537519454956,
      "learning_rate": 1.4959731543624161e-05,
      "loss": 0.4574,
      "step": 52710
    },
    {
      "epoch": 2.1088,
      "grad_norm": 2.624532461166382,
      "learning_rate": 1.4953020134228188e-05,
      "loss": 0.5533,
      "step": 52720
    },
    {
      "epoch": 2.1092,
      "grad_norm": 2.132781505584717,
      "learning_rate": 1.4946308724832214e-05,
      "loss": 0.4831,
      "step": 52730
    },
    {
      "epoch": 2.1096,
      "grad_norm": 2.334676504135132,
      "learning_rate": 1.4939597315436244e-05,
      "loss": 0.4452,
      "step": 52740
    },
    {
      "epoch": 2.11,
      "grad_norm": 2.168083429336548,
      "learning_rate": 1.493288590604027e-05,
      "loss": 0.5141,
      "step": 52750
    },
    {
      "epoch": 2.1104,
      "grad_norm": 2.3397045135498047,
      "learning_rate": 1.4926174496644296e-05,
      "loss": 0.5036,
      "step": 52760
    },
    {
      "epoch": 2.1108,
      "grad_norm": 2.321420431137085,
      "learning_rate": 1.4919463087248323e-05,
      "loss": 0.4591,
      "step": 52770
    },
    {
      "epoch": 2.1112,
      "grad_norm": 2.283668279647827,
      "learning_rate": 1.491275167785235e-05,
      "loss": 0.4828,
      "step": 52780
    },
    {
      "epoch": 2.1116,
      "grad_norm": 2.793105125427246,
      "learning_rate": 1.4906040268456375e-05,
      "loss": 0.4428,
      "step": 52790
    },
    {
      "epoch": 2.112,
      "grad_norm": 2.68002986907959,
      "learning_rate": 1.4899328859060405e-05,
      "loss": 0.4809,
      "step": 52800
    },
    {
      "epoch": 2.1124,
      "grad_norm": 2.706882953643799,
      "learning_rate": 1.4892617449664432e-05,
      "loss": 0.5497,
      "step": 52810
    },
    {
      "epoch": 2.1128,
      "grad_norm": 3.0361380577087402,
      "learning_rate": 1.4885906040268457e-05,
      "loss": 0.4866,
      "step": 52820
    },
    {
      "epoch": 2.1132,
      "grad_norm": 2.151733636856079,
      "learning_rate": 1.4879194630872484e-05,
      "loss": 0.4352,
      "step": 52830
    },
    {
      "epoch": 2.1136,
      "grad_norm": 2.2954936027526855,
      "learning_rate": 1.487248322147651e-05,
      "loss": 0.5932,
      "step": 52840
    },
    {
      "epoch": 2.114,
      "grad_norm": 2.8210368156433105,
      "learning_rate": 1.4865771812080537e-05,
      "loss": 0.5174,
      "step": 52850
    },
    {
      "epoch": 2.1144,
      "grad_norm": 2.8486533164978027,
      "learning_rate": 1.4859060402684563e-05,
      "loss": 0.4912,
      "step": 52860
    },
    {
      "epoch": 2.1148,
      "grad_norm": 3.0295190811157227,
      "learning_rate": 1.4852348993288593e-05,
      "loss": 0.4926,
      "step": 52870
    },
    {
      "epoch": 2.1152,
      "grad_norm": 2.3224735260009766,
      "learning_rate": 1.4845637583892618e-05,
      "loss": 0.5187,
      "step": 52880
    },
    {
      "epoch": 2.1156,
      "grad_norm": 3.32707142829895,
      "learning_rate": 1.4838926174496645e-05,
      "loss": 0.4769,
      "step": 52890
    },
    {
      "epoch": 2.116,
      "grad_norm": 2.486013650894165,
      "learning_rate": 1.4832214765100672e-05,
      "loss": 0.4322,
      "step": 52900
    },
    {
      "epoch": 2.1164,
      "grad_norm": 3.50374698638916,
      "learning_rate": 1.4825503355704699e-05,
      "loss": 0.5441,
      "step": 52910
    },
    {
      "epoch": 2.1168,
      "grad_norm": 3.328824996948242,
      "learning_rate": 1.4818791946308724e-05,
      "loss": 0.4794,
      "step": 52920
    },
    {
      "epoch": 2.1172,
      "grad_norm": 2.0388379096984863,
      "learning_rate": 1.4812080536912754e-05,
      "loss": 0.5263,
      "step": 52930
    },
    {
      "epoch": 2.1176,
      "grad_norm": 2.4503798484802246,
      "learning_rate": 1.4805369127516779e-05,
      "loss": 0.4515,
      "step": 52940
    },
    {
      "epoch": 2.118,
      "grad_norm": 2.6759467124938965,
      "learning_rate": 1.4798657718120806e-05,
      "loss": 0.5051,
      "step": 52950
    },
    {
      "epoch": 2.1184,
      "grad_norm": 3.013479709625244,
      "learning_rate": 1.4791946308724833e-05,
      "loss": 0.5638,
      "step": 52960
    },
    {
      "epoch": 2.1188,
      "grad_norm": 2.325997829437256,
      "learning_rate": 1.478523489932886e-05,
      "loss": 0.4478,
      "step": 52970
    },
    {
      "epoch": 2.1192,
      "grad_norm": 3.647019863128662,
      "learning_rate": 1.4778523489932885e-05,
      "loss": 0.5085,
      "step": 52980
    },
    {
      "epoch": 2.1196,
      "grad_norm": 2.566237449645996,
      "learning_rate": 1.4771812080536915e-05,
      "loss": 0.4449,
      "step": 52990
    },
    {
      "epoch": 2.12,
      "grad_norm": 2.3789217472076416,
      "learning_rate": 1.4765100671140942e-05,
      "loss": 0.5391,
      "step": 53000
    },
    {
      "epoch": 2.1204,
      "grad_norm": 2.5046093463897705,
      "learning_rate": 1.4758389261744967e-05,
      "loss": 0.5764,
      "step": 53010
    },
    {
      "epoch": 2.1208,
      "grad_norm": 2.407285690307617,
      "learning_rate": 1.4751677852348994e-05,
      "loss": 0.5039,
      "step": 53020
    },
    {
      "epoch": 2.1212,
      "grad_norm": 3.339745044708252,
      "learning_rate": 1.474496644295302e-05,
      "loss": 0.5339,
      "step": 53030
    },
    {
      "epoch": 2.1216,
      "grad_norm": 3.0249009132385254,
      "learning_rate": 1.4738255033557048e-05,
      "loss": 0.5074,
      "step": 53040
    },
    {
      "epoch": 2.122,
      "grad_norm": 2.158510684967041,
      "learning_rate": 1.4731543624161073e-05,
      "loss": 0.4893,
      "step": 53050
    },
    {
      "epoch": 2.1224,
      "grad_norm": 2.0478994846343994,
      "learning_rate": 1.4724832214765103e-05,
      "loss": 0.4236,
      "step": 53060
    },
    {
      "epoch": 2.1228,
      "grad_norm": 1.830396056175232,
      "learning_rate": 1.4718120805369128e-05,
      "loss": 0.4798,
      "step": 53070
    },
    {
      "epoch": 2.1232,
      "grad_norm": 2.0294036865234375,
      "learning_rate": 1.4711409395973155e-05,
      "loss": 0.4877,
      "step": 53080
    },
    {
      "epoch": 2.1236,
      "grad_norm": 2.121051549911499,
      "learning_rate": 1.4704697986577182e-05,
      "loss": 0.5031,
      "step": 53090
    },
    {
      "epoch": 2.124,
      "grad_norm": 2.5924110412597656,
      "learning_rate": 1.4697986577181209e-05,
      "loss": 0.5591,
      "step": 53100
    },
    {
      "epoch": 2.1244,
      "grad_norm": 2.6216821670532227,
      "learning_rate": 1.4691275167785234e-05,
      "loss": 0.516,
      "step": 53110
    },
    {
      "epoch": 2.1248,
      "grad_norm": 3.1761653423309326,
      "learning_rate": 1.4684563758389264e-05,
      "loss": 0.4438,
      "step": 53120
    },
    {
      "epoch": 2.1252,
      "grad_norm": 3.4383652210235596,
      "learning_rate": 1.467785234899329e-05,
      "loss": 0.5434,
      "step": 53130
    },
    {
      "epoch": 2.1256,
      "grad_norm": 2.326850414276123,
      "learning_rate": 1.4671140939597316e-05,
      "loss": 0.4704,
      "step": 53140
    },
    {
      "epoch": 2.126,
      "grad_norm": 3.020395040512085,
      "learning_rate": 1.4664429530201343e-05,
      "loss": 0.4999,
      "step": 53150
    },
    {
      "epoch": 2.1264,
      "grad_norm": 2.6893837451934814,
      "learning_rate": 1.465771812080537e-05,
      "loss": 0.587,
      "step": 53160
    },
    {
      "epoch": 2.1268,
      "grad_norm": 3.220292329788208,
      "learning_rate": 1.4651006711409395e-05,
      "loss": 0.4779,
      "step": 53170
    },
    {
      "epoch": 2.1272,
      "grad_norm": 2.951667547225952,
      "learning_rate": 1.4644295302013422e-05,
      "loss": 0.4356,
      "step": 53180
    },
    {
      "epoch": 2.1276,
      "grad_norm": 2.248478412628174,
      "learning_rate": 1.4637583892617452e-05,
      "loss": 0.4211,
      "step": 53190
    },
    {
      "epoch": 2.128,
      "grad_norm": 2.020228147506714,
      "learning_rate": 1.4630872483221477e-05,
      "loss": 0.484,
      "step": 53200
    },
    {
      "epoch": 2.1284,
      "grad_norm": 2.2032766342163086,
      "learning_rate": 1.4624161073825504e-05,
      "loss": 0.537,
      "step": 53210
    },
    {
      "epoch": 2.1288,
      "grad_norm": 2.1469666957855225,
      "learning_rate": 1.4617449664429531e-05,
      "loss": 0.5468,
      "step": 53220
    },
    {
      "epoch": 2.1292,
      "grad_norm": 2.2600324153900146,
      "learning_rate": 1.4610738255033556e-05,
      "loss": 0.4853,
      "step": 53230
    },
    {
      "epoch": 2.1296,
      "grad_norm": 2.517146587371826,
      "learning_rate": 1.4604026845637583e-05,
      "loss": 0.4778,
      "step": 53240
    },
    {
      "epoch": 2.13,
      "grad_norm": 2.1085495948791504,
      "learning_rate": 1.4597315436241613e-05,
      "loss": 0.5028,
      "step": 53250
    },
    {
      "epoch": 2.1304,
      "grad_norm": 1.7865363359451294,
      "learning_rate": 1.4590604026845638e-05,
      "loss": 0.3992,
      "step": 53260
    },
    {
      "epoch": 2.1308,
      "grad_norm": 3.3259236812591553,
      "learning_rate": 1.4583892617449665e-05,
      "loss": 0.5347,
      "step": 53270
    },
    {
      "epoch": 2.1312,
      "grad_norm": 2.941232681274414,
      "learning_rate": 1.4577181208053692e-05,
      "loss": 0.56,
      "step": 53280
    },
    {
      "epoch": 2.1316,
      "grad_norm": 2.2095561027526855,
      "learning_rate": 1.4570469798657719e-05,
      "loss": 0.5444,
      "step": 53290
    },
    {
      "epoch": 2.132,
      "grad_norm": 2.6276681423187256,
      "learning_rate": 1.4563758389261744e-05,
      "loss": 0.4752,
      "step": 53300
    },
    {
      "epoch": 2.1324,
      "grad_norm": 2.414998769760132,
      "learning_rate": 1.4557046979865774e-05,
      "loss": 0.4777,
      "step": 53310
    },
    {
      "epoch": 2.1328,
      "grad_norm": 2.1938869953155518,
      "learning_rate": 1.45503355704698e-05,
      "loss": 0.4673,
      "step": 53320
    },
    {
      "epoch": 2.1332,
      "grad_norm": 2.0907609462738037,
      "learning_rate": 1.4543624161073826e-05,
      "loss": 0.3881,
      "step": 53330
    },
    {
      "epoch": 2.1336,
      "grad_norm": 2.8330769538879395,
      "learning_rate": 1.4536912751677853e-05,
      "loss": 0.4619,
      "step": 53340
    },
    {
      "epoch": 2.134,
      "grad_norm": 2.586202383041382,
      "learning_rate": 1.453020134228188e-05,
      "loss": 0.3739,
      "step": 53350
    },
    {
      "epoch": 2.1344,
      "grad_norm": 4.205991744995117,
      "learning_rate": 1.4523489932885905e-05,
      "loss": 0.5882,
      "step": 53360
    },
    {
      "epoch": 2.1348,
      "grad_norm": 2.144885540008545,
      "learning_rate": 1.4516778523489932e-05,
      "loss": 0.5273,
      "step": 53370
    },
    {
      "epoch": 2.1352,
      "grad_norm": 1.8109323978424072,
      "learning_rate": 1.451006711409396e-05,
      "loss": 0.4224,
      "step": 53380
    },
    {
      "epoch": 2.1356,
      "grad_norm": 2.517995834350586,
      "learning_rate": 1.4503355704697988e-05,
      "loss": 0.5109,
      "step": 53390
    },
    {
      "epoch": 2.136,
      "grad_norm": 2.2875845432281494,
      "learning_rate": 1.4496644295302014e-05,
      "loss": 0.4861,
      "step": 53400
    },
    {
      "epoch": 2.1364,
      "grad_norm": 2.313338279724121,
      "learning_rate": 1.4489932885906041e-05,
      "loss": 0.4452,
      "step": 53410
    },
    {
      "epoch": 2.1368,
      "grad_norm": 3.6015608310699463,
      "learning_rate": 1.4483221476510066e-05,
      "loss": 0.4772,
      "step": 53420
    },
    {
      "epoch": 2.1372,
      "grad_norm": 2.816964626312256,
      "learning_rate": 1.4476510067114093e-05,
      "loss": 0.4727,
      "step": 53430
    },
    {
      "epoch": 2.1376,
      "grad_norm": 3.0647826194763184,
      "learning_rate": 1.4469798657718123e-05,
      "loss": 0.5068,
      "step": 53440
    },
    {
      "epoch": 2.138,
      "grad_norm": 2.747234344482422,
      "learning_rate": 1.4463087248322149e-05,
      "loss": 0.5658,
      "step": 53450
    },
    {
      "epoch": 2.1384,
      "grad_norm": 3.1795461177825928,
      "learning_rate": 1.4456375838926175e-05,
      "loss": 0.517,
      "step": 53460
    },
    {
      "epoch": 2.1388,
      "grad_norm": 2.0986526012420654,
      "learning_rate": 1.4449664429530202e-05,
      "loss": 0.4805,
      "step": 53470
    },
    {
      "epoch": 2.1391999999999998,
      "grad_norm": 2.927152395248413,
      "learning_rate": 1.444295302013423e-05,
      "loss": 0.5766,
      "step": 53480
    },
    {
      "epoch": 2.1396,
      "grad_norm": 3.0106942653656006,
      "learning_rate": 1.4436241610738254e-05,
      "loss": 0.571,
      "step": 53490
    },
    {
      "epoch": 2.14,
      "grad_norm": 2.590369939804077,
      "learning_rate": 1.4429530201342285e-05,
      "loss": 0.4626,
      "step": 53500
    },
    {
      "epoch": 2.1404,
      "grad_norm": 2.892778158187866,
      "learning_rate": 1.442281879194631e-05,
      "loss": 0.5815,
      "step": 53510
    },
    {
      "epoch": 2.1408,
      "grad_norm": 2.967132091522217,
      "learning_rate": 1.4416107382550337e-05,
      "loss": 0.4893,
      "step": 53520
    },
    {
      "epoch": 2.1412,
      "grad_norm": 2.355813980102539,
      "learning_rate": 1.4409395973154363e-05,
      "loss": 0.5389,
      "step": 53530
    },
    {
      "epoch": 2.1416,
      "grad_norm": 2.273419141769409,
      "learning_rate": 1.440268456375839e-05,
      "loss": 0.4687,
      "step": 53540
    },
    {
      "epoch": 2.142,
      "grad_norm": 2.213750123977661,
      "learning_rate": 1.4395973154362415e-05,
      "loss": 0.4273,
      "step": 53550
    },
    {
      "epoch": 2.1424,
      "grad_norm": 2.6373116970062256,
      "learning_rate": 1.4389261744966442e-05,
      "loss": 0.526,
      "step": 53560
    },
    {
      "epoch": 2.1428,
      "grad_norm": 3.008808135986328,
      "learning_rate": 1.4382550335570471e-05,
      "loss": 0.5143,
      "step": 53570
    },
    {
      "epoch": 2.1432,
      "grad_norm": 2.0488948822021484,
      "learning_rate": 1.4375838926174498e-05,
      "loss": 0.4243,
      "step": 53580
    },
    {
      "epoch": 2.1436,
      "grad_norm": 3.0530683994293213,
      "learning_rate": 1.4369127516778525e-05,
      "loss": 0.5258,
      "step": 53590
    },
    {
      "epoch": 2.144,
      "grad_norm": 2.0005006790161133,
      "learning_rate": 1.4362416107382551e-05,
      "loss": 0.4977,
      "step": 53600
    },
    {
      "epoch": 2.1444,
      "grad_norm": 2.5479018688201904,
      "learning_rate": 1.4355704697986577e-05,
      "loss": 0.4953,
      "step": 53610
    },
    {
      "epoch": 2.1448,
      "grad_norm": 2.7677383422851562,
      "learning_rate": 1.4348993288590603e-05,
      "loss": 0.4999,
      "step": 53620
    },
    {
      "epoch": 2.1452,
      "grad_norm": 2.925720691680908,
      "learning_rate": 1.4342281879194634e-05,
      "loss": 0.4681,
      "step": 53630
    },
    {
      "epoch": 2.1456,
      "grad_norm": 2.8259170055389404,
      "learning_rate": 1.4335570469798659e-05,
      "loss": 0.4739,
      "step": 53640
    },
    {
      "epoch": 2.146,
      "grad_norm": 2.346543073654175,
      "learning_rate": 1.4328859060402686e-05,
      "loss": 0.4951,
      "step": 53650
    },
    {
      "epoch": 2.1464,
      "grad_norm": 2.539863348007202,
      "learning_rate": 1.4322147651006713e-05,
      "loss": 0.4098,
      "step": 53660
    },
    {
      "epoch": 2.1468,
      "grad_norm": 2.8226733207702637,
      "learning_rate": 1.4315436241610738e-05,
      "loss": 0.5175,
      "step": 53670
    },
    {
      "epoch": 2.1471999999999998,
      "grad_norm": 2.913654088973999,
      "learning_rate": 1.4308724832214765e-05,
      "loss": 0.4373,
      "step": 53680
    },
    {
      "epoch": 2.1476,
      "grad_norm": 3.5175180435180664,
      "learning_rate": 1.4302013422818791e-05,
      "loss": 0.6226,
      "step": 53690
    },
    {
      "epoch": 2.148,
      "grad_norm": 2.7260541915893555,
      "learning_rate": 1.429530201342282e-05,
      "loss": 0.4879,
      "step": 53700
    },
    {
      "epoch": 2.1484,
      "grad_norm": 2.6967597007751465,
      "learning_rate": 1.4288590604026847e-05,
      "loss": 0.5569,
      "step": 53710
    },
    {
      "epoch": 2.1488,
      "grad_norm": 3.0783982276916504,
      "learning_rate": 1.4281879194630874e-05,
      "loss": 0.5125,
      "step": 53720
    },
    {
      "epoch": 2.1492,
      "grad_norm": 2.3573434352874756,
      "learning_rate": 1.42751677852349e-05,
      "loss": 0.4822,
      "step": 53730
    },
    {
      "epoch": 2.1496,
      "grad_norm": 2.548008918762207,
      "learning_rate": 1.4268456375838926e-05,
      "loss": 0.5475,
      "step": 53740
    },
    {
      "epoch": 2.15,
      "grad_norm": 2.4245071411132812,
      "learning_rate": 1.4261744966442953e-05,
      "loss": 0.5243,
      "step": 53750
    },
    {
      "epoch": 2.1504,
      "grad_norm": 3.2898600101470947,
      "learning_rate": 1.4255033557046981e-05,
      "loss": 0.5525,
      "step": 53760
    },
    {
      "epoch": 2.1508,
      "grad_norm": 2.845319986343384,
      "learning_rate": 1.4248322147651008e-05,
      "loss": 0.4974,
      "step": 53770
    },
    {
      "epoch": 2.1512000000000002,
      "grad_norm": 2.893064022064209,
      "learning_rate": 1.4241610738255035e-05,
      "loss": 0.4447,
      "step": 53780
    },
    {
      "epoch": 2.1516,
      "grad_norm": 3.102569103240967,
      "learning_rate": 1.4234899328859062e-05,
      "loss": 0.5458,
      "step": 53790
    },
    {
      "epoch": 2.152,
      "grad_norm": 2.220008373260498,
      "learning_rate": 1.4228187919463087e-05,
      "loss": 0.4953,
      "step": 53800
    },
    {
      "epoch": 2.1524,
      "grad_norm": 2.9268975257873535,
      "learning_rate": 1.4221476510067114e-05,
      "loss": 0.5427,
      "step": 53810
    },
    {
      "epoch": 2.1528,
      "grad_norm": 2.488227128982544,
      "learning_rate": 1.4214765100671142e-05,
      "loss": 0.4877,
      "step": 53820
    },
    {
      "epoch": 2.1532,
      "grad_norm": 2.8574678897857666,
      "learning_rate": 1.4208053691275169e-05,
      "loss": 0.5084,
      "step": 53830
    },
    {
      "epoch": 2.1536,
      "grad_norm": 2.993946075439453,
      "learning_rate": 1.4201342281879196e-05,
      "loss": 0.471,
      "step": 53840
    },
    {
      "epoch": 2.154,
      "grad_norm": 3.058166265487671,
      "learning_rate": 1.4194630872483223e-05,
      "loss": 0.4856,
      "step": 53850
    },
    {
      "epoch": 2.1544,
      "grad_norm": 2.2809231281280518,
      "learning_rate": 1.4187919463087248e-05,
      "loss": 0.5344,
      "step": 53860
    },
    {
      "epoch": 2.1548,
      "grad_norm": 2.028188705444336,
      "learning_rate": 1.4181208053691275e-05,
      "loss": 0.545,
      "step": 53870
    },
    {
      "epoch": 2.1552,
      "grad_norm": 2.2310187816619873,
      "learning_rate": 1.4174496644295302e-05,
      "loss": 0.4912,
      "step": 53880
    },
    {
      "epoch": 2.1556,
      "grad_norm": 2.6238763332366943,
      "learning_rate": 1.416778523489933e-05,
      "loss": 0.5517,
      "step": 53890
    },
    {
      "epoch": 2.156,
      "grad_norm": 3.2072560787200928,
      "learning_rate": 1.4161073825503357e-05,
      "loss": 0.5032,
      "step": 53900
    },
    {
      "epoch": 2.1564,
      "grad_norm": 3.1974668502807617,
      "learning_rate": 1.4154362416107384e-05,
      "loss": 0.4779,
      "step": 53910
    },
    {
      "epoch": 2.1568,
      "grad_norm": 2.7026493549346924,
      "learning_rate": 1.4147651006711409e-05,
      "loss": 0.5258,
      "step": 53920
    },
    {
      "epoch": 2.1572,
      "grad_norm": 2.9207065105438232,
      "learning_rate": 1.4140939597315436e-05,
      "loss": 0.4806,
      "step": 53930
    },
    {
      "epoch": 2.1576,
      "grad_norm": 3.5255727767944336,
      "learning_rate": 1.4134228187919463e-05,
      "loss": 0.5778,
      "step": 53940
    },
    {
      "epoch": 2.158,
      "grad_norm": 2.638164758682251,
      "learning_rate": 1.4127516778523491e-05,
      "loss": 0.5118,
      "step": 53950
    },
    {
      "epoch": 2.1584,
      "grad_norm": 2.908823251724243,
      "learning_rate": 1.4120805369127518e-05,
      "loss": 0.467,
      "step": 53960
    },
    {
      "epoch": 2.1588,
      "grad_norm": 2.259049654006958,
      "learning_rate": 1.4114093959731545e-05,
      "loss": 0.4384,
      "step": 53970
    },
    {
      "epoch": 2.1592000000000002,
      "grad_norm": 2.501394271850586,
      "learning_rate": 1.4107382550335572e-05,
      "loss": 0.5563,
      "step": 53980
    },
    {
      "epoch": 2.1596,
      "grad_norm": 2.8134241104125977,
      "learning_rate": 1.4100671140939597e-05,
      "loss": 0.4959,
      "step": 53990
    },
    {
      "epoch": 2.16,
      "grad_norm": 2.3331480026245117,
      "learning_rate": 1.4093959731543624e-05,
      "loss": 0.4114,
      "step": 54000
    },
    {
      "epoch": 2.1604,
      "grad_norm": 2.5197818279266357,
      "learning_rate": 1.408724832214765e-05,
      "loss": 0.4893,
      "step": 54010
    },
    {
      "epoch": 2.1608,
      "grad_norm": 2.437317371368408,
      "learning_rate": 1.408053691275168e-05,
      "loss": 0.4843,
      "step": 54020
    },
    {
      "epoch": 2.1612,
      "grad_norm": 2.6622049808502197,
      "learning_rate": 1.4073825503355706e-05,
      "loss": 0.4924,
      "step": 54030
    },
    {
      "epoch": 2.1616,
      "grad_norm": 2.0705556869506836,
      "learning_rate": 1.4067114093959733e-05,
      "loss": 0.4791,
      "step": 54040
    },
    {
      "epoch": 2.162,
      "grad_norm": 3.1700406074523926,
      "learning_rate": 1.4060402684563758e-05,
      "loss": 0.5121,
      "step": 54050
    },
    {
      "epoch": 2.1624,
      "grad_norm": 2.9308793544769287,
      "learning_rate": 1.4053691275167785e-05,
      "loss": 0.4879,
      "step": 54060
    },
    {
      "epoch": 2.1628,
      "grad_norm": 2.811877489089966,
      "learning_rate": 1.4046979865771812e-05,
      "loss": 0.4074,
      "step": 54070
    },
    {
      "epoch": 2.1632,
      "grad_norm": 2.546616792678833,
      "learning_rate": 1.404026845637584e-05,
      "loss": 0.4934,
      "step": 54080
    },
    {
      "epoch": 2.1636,
      "grad_norm": 1.9129279851913452,
      "learning_rate": 1.4033557046979867e-05,
      "loss": 0.4633,
      "step": 54090
    },
    {
      "epoch": 2.164,
      "grad_norm": 3.418126344680786,
      "learning_rate": 1.4026845637583894e-05,
      "loss": 0.5268,
      "step": 54100
    },
    {
      "epoch": 2.1644,
      "grad_norm": 2.3079745769500732,
      "learning_rate": 1.402013422818792e-05,
      "loss": 0.5109,
      "step": 54110
    },
    {
      "epoch": 2.1648,
      "grad_norm": 2.3182106018066406,
      "learning_rate": 1.4013422818791946e-05,
      "loss": 0.4747,
      "step": 54120
    },
    {
      "epoch": 2.1652,
      "grad_norm": 2.7533552646636963,
      "learning_rate": 1.4006711409395973e-05,
      "loss": 0.4636,
      "step": 54130
    },
    {
      "epoch": 2.1656,
      "grad_norm": 2.850992441177368,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 0.5614,
      "step": 54140
    },
    {
      "epoch": 2.166,
      "grad_norm": 3.42280912399292,
      "learning_rate": 1.3993288590604028e-05,
      "loss": 0.455,
      "step": 54150
    },
    {
      "epoch": 2.1664,
      "grad_norm": 3.2363555431365967,
      "learning_rate": 1.3986577181208055e-05,
      "loss": 0.4922,
      "step": 54160
    },
    {
      "epoch": 2.1668,
      "grad_norm": 2.66483473777771,
      "learning_rate": 1.3979865771812082e-05,
      "loss": 0.4608,
      "step": 54170
    },
    {
      "epoch": 2.1672,
      "grad_norm": 2.4188520908355713,
      "learning_rate": 1.3973154362416107e-05,
      "loss": 0.4891,
      "step": 54180
    },
    {
      "epoch": 2.1676,
      "grad_norm": 3.054503917694092,
      "learning_rate": 1.3966442953020134e-05,
      "loss": 0.4686,
      "step": 54190
    },
    {
      "epoch": 2.168,
      "grad_norm": 2.774956703186035,
      "learning_rate": 1.3959731543624161e-05,
      "loss": 0.511,
      "step": 54200
    },
    {
      "epoch": 2.1684,
      "grad_norm": 2.7391042709350586,
      "learning_rate": 1.395302013422819e-05,
      "loss": 0.4893,
      "step": 54210
    },
    {
      "epoch": 2.1688,
      "grad_norm": 2.213080406188965,
      "learning_rate": 1.3946308724832216e-05,
      "loss": 0.4963,
      "step": 54220
    },
    {
      "epoch": 2.1692,
      "grad_norm": 2.6104888916015625,
      "learning_rate": 1.3939597315436243e-05,
      "loss": 0.4875,
      "step": 54230
    },
    {
      "epoch": 2.1696,
      "grad_norm": 2.8964767456054688,
      "learning_rate": 1.3932885906040268e-05,
      "loss": 0.5212,
      "step": 54240
    },
    {
      "epoch": 2.17,
      "grad_norm": 2.162851572036743,
      "learning_rate": 1.3926174496644295e-05,
      "loss": 0.5209,
      "step": 54250
    },
    {
      "epoch": 2.1704,
      "grad_norm": 2.4868009090423584,
      "learning_rate": 1.3919463087248322e-05,
      "loss": 0.5669,
      "step": 54260
    },
    {
      "epoch": 2.1708,
      "grad_norm": 2.4062750339508057,
      "learning_rate": 1.391275167785235e-05,
      "loss": 0.4246,
      "step": 54270
    },
    {
      "epoch": 2.1712,
      "grad_norm": 3.600437641143799,
      "learning_rate": 1.3906040268456377e-05,
      "loss": 0.5915,
      "step": 54280
    },
    {
      "epoch": 2.1716,
      "grad_norm": 2.955204725265503,
      "learning_rate": 1.3899328859060404e-05,
      "loss": 0.5801,
      "step": 54290
    },
    {
      "epoch": 2.172,
      "grad_norm": 2.9838907718658447,
      "learning_rate": 1.389261744966443e-05,
      "loss": 0.501,
      "step": 54300
    },
    {
      "epoch": 2.1724,
      "grad_norm": 2.799088954925537,
      "learning_rate": 1.3885906040268456e-05,
      "loss": 0.5069,
      "step": 54310
    },
    {
      "epoch": 2.1728,
      "grad_norm": 2.7305538654327393,
      "learning_rate": 1.3879194630872483e-05,
      "loss": 0.5207,
      "step": 54320
    },
    {
      "epoch": 2.1732,
      "grad_norm": 3.3969857692718506,
      "learning_rate": 1.387248322147651e-05,
      "loss": 0.508,
      "step": 54330
    },
    {
      "epoch": 2.1736,
      "grad_norm": 2.5701427459716797,
      "learning_rate": 1.3865771812080539e-05,
      "loss": 0.4562,
      "step": 54340
    },
    {
      "epoch": 2.174,
      "grad_norm": 2.2073118686676025,
      "learning_rate": 1.3859060402684565e-05,
      "loss": 0.4566,
      "step": 54350
    },
    {
      "epoch": 2.1744,
      "grad_norm": 2.2780580520629883,
      "learning_rate": 1.385234899328859e-05,
      "loss": 0.4797,
      "step": 54360
    },
    {
      "epoch": 2.1748,
      "grad_norm": 2.1965866088867188,
      "learning_rate": 1.3845637583892617e-05,
      "loss": 0.4218,
      "step": 54370
    },
    {
      "epoch": 2.1752,
      "grad_norm": 2.6181187629699707,
      "learning_rate": 1.3838926174496644e-05,
      "loss": 0.4483,
      "step": 54380
    },
    {
      "epoch": 2.1756,
      "grad_norm": 3.2617127895355225,
      "learning_rate": 1.3832214765100671e-05,
      "loss": 0.5018,
      "step": 54390
    },
    {
      "epoch": 2.176,
      "grad_norm": 3.053457260131836,
      "learning_rate": 1.38255033557047e-05,
      "loss": 0.5118,
      "step": 54400
    },
    {
      "epoch": 2.1764,
      "grad_norm": 2.8280200958251953,
      "learning_rate": 1.3818791946308727e-05,
      "loss": 0.4438,
      "step": 54410
    },
    {
      "epoch": 2.1768,
      "grad_norm": 3.091041326522827,
      "learning_rate": 1.3812080536912753e-05,
      "loss": 0.3907,
      "step": 54420
    },
    {
      "epoch": 2.1772,
      "grad_norm": 2.138376235961914,
      "learning_rate": 1.3805369127516779e-05,
      "loss": 0.5128,
      "step": 54430
    },
    {
      "epoch": 2.1776,
      "grad_norm": 1.9620559215545654,
      "learning_rate": 1.3798657718120805e-05,
      "loss": 0.53,
      "step": 54440
    },
    {
      "epoch": 2.178,
      "grad_norm": 2.9785125255584717,
      "learning_rate": 1.3791946308724832e-05,
      "loss": 0.5072,
      "step": 54450
    },
    {
      "epoch": 2.1784,
      "grad_norm": 2.646482229232788,
      "learning_rate": 1.378523489932886e-05,
      "loss": 0.5153,
      "step": 54460
    },
    {
      "epoch": 2.1788,
      "grad_norm": 3.3734536170959473,
      "learning_rate": 1.3778523489932888e-05,
      "loss": 0.4911,
      "step": 54470
    },
    {
      "epoch": 2.1792,
      "grad_norm": 2.8187925815582275,
      "learning_rate": 1.3771812080536914e-05,
      "loss": 0.5661,
      "step": 54480
    },
    {
      "epoch": 2.1796,
      "grad_norm": 2.3177058696746826,
      "learning_rate": 1.376510067114094e-05,
      "loss": 0.4796,
      "step": 54490
    },
    {
      "epoch": 2.18,
      "grad_norm": 2.4642207622528076,
      "learning_rate": 1.3758389261744966e-05,
      "loss": 0.4827,
      "step": 54500
    },
    {
      "epoch": 2.1804,
      "grad_norm": 2.885525703430176,
      "learning_rate": 1.3751677852348993e-05,
      "loss": 0.5093,
      "step": 54510
    },
    {
      "epoch": 2.1808,
      "grad_norm": 2.7578437328338623,
      "learning_rate": 1.374496644295302e-05,
      "loss": 0.5384,
      "step": 54520
    },
    {
      "epoch": 2.1812,
      "grad_norm": 2.8114826679229736,
      "learning_rate": 1.3738255033557049e-05,
      "loss": 0.4657,
      "step": 54530
    },
    {
      "epoch": 2.1816,
      "grad_norm": 3.422314167022705,
      "learning_rate": 1.3731543624161076e-05,
      "loss": 0.5316,
      "step": 54540
    },
    {
      "epoch": 2.182,
      "grad_norm": 1.8198747634887695,
      "learning_rate": 1.37248322147651e-05,
      "loss": 0.518,
      "step": 54550
    },
    {
      "epoch": 2.1824,
      "grad_norm": 2.4830384254455566,
      "learning_rate": 1.3718120805369128e-05,
      "loss": 0.4899,
      "step": 54560
    },
    {
      "epoch": 2.1828,
      "grad_norm": 2.532060384750366,
      "learning_rate": 1.3711409395973154e-05,
      "loss": 0.4048,
      "step": 54570
    },
    {
      "epoch": 2.1832,
      "grad_norm": 2.099384069442749,
      "learning_rate": 1.3704697986577181e-05,
      "loss": 0.5117,
      "step": 54580
    },
    {
      "epoch": 2.1836,
      "grad_norm": 2.7322325706481934,
      "learning_rate": 1.369798657718121e-05,
      "loss": 0.55,
      "step": 54590
    },
    {
      "epoch": 2.184,
      "grad_norm": 2.1685640811920166,
      "learning_rate": 1.3691275167785237e-05,
      "loss": 0.4988,
      "step": 54600
    },
    {
      "epoch": 2.1844,
      "grad_norm": 2.7050483226776123,
      "learning_rate": 1.3684563758389264e-05,
      "loss": 0.5344,
      "step": 54610
    },
    {
      "epoch": 2.1848,
      "grad_norm": 2.673276901245117,
      "learning_rate": 1.3677852348993289e-05,
      "loss": 0.4902,
      "step": 54620
    },
    {
      "epoch": 2.1852,
      "grad_norm": 3.243504285812378,
      "learning_rate": 1.3671140939597316e-05,
      "loss": 0.5719,
      "step": 54630
    },
    {
      "epoch": 2.1856,
      "grad_norm": 2.656465768814087,
      "learning_rate": 1.3664429530201342e-05,
      "loss": 0.4894,
      "step": 54640
    },
    {
      "epoch": 2.186,
      "grad_norm": 3.2551801204681396,
      "learning_rate": 1.3657718120805371e-05,
      "loss": 0.5448,
      "step": 54650
    },
    {
      "epoch": 2.1864,
      "grad_norm": 3.330230951309204,
      "learning_rate": 1.3651006711409398e-05,
      "loss": 0.4624,
      "step": 54660
    },
    {
      "epoch": 2.1868,
      "grad_norm": 2.628798246383667,
      "learning_rate": 1.3644295302013425e-05,
      "loss": 0.5241,
      "step": 54670
    },
    {
      "epoch": 2.1872,
      "grad_norm": 2.6023244857788086,
      "learning_rate": 1.363758389261745e-05,
      "loss": 0.472,
      "step": 54680
    },
    {
      "epoch": 2.1875999999999998,
      "grad_norm": 2.9201481342315674,
      "learning_rate": 1.3630872483221477e-05,
      "loss": 0.4989,
      "step": 54690
    },
    {
      "epoch": 2.188,
      "grad_norm": 3.010852336883545,
      "learning_rate": 1.3624161073825504e-05,
      "loss": 0.5195,
      "step": 54700
    },
    {
      "epoch": 2.1884,
      "grad_norm": 2.900222063064575,
      "learning_rate": 1.361744966442953e-05,
      "loss": 0.5063,
      "step": 54710
    },
    {
      "epoch": 2.1888,
      "grad_norm": 2.442269802093506,
      "learning_rate": 1.3610738255033559e-05,
      "loss": 0.5937,
      "step": 54720
    },
    {
      "epoch": 2.1892,
      "grad_norm": 2.3304665088653564,
      "learning_rate": 1.3604026845637586e-05,
      "loss": 0.4173,
      "step": 54730
    },
    {
      "epoch": 2.1896,
      "grad_norm": 2.6519358158111572,
      "learning_rate": 1.3597315436241611e-05,
      "loss": 0.4409,
      "step": 54740
    },
    {
      "epoch": 2.19,
      "grad_norm": 2.726106643676758,
      "learning_rate": 1.3590604026845638e-05,
      "loss": 0.4766,
      "step": 54750
    },
    {
      "epoch": 2.1904,
      "grad_norm": 3.0109171867370605,
      "learning_rate": 1.3583892617449665e-05,
      "loss": 0.4839,
      "step": 54760
    },
    {
      "epoch": 2.1908,
      "grad_norm": 1.7474602460861206,
      "learning_rate": 1.3577181208053692e-05,
      "loss": 0.5147,
      "step": 54770
    },
    {
      "epoch": 2.1912,
      "grad_norm": 2.6095664501190186,
      "learning_rate": 1.357046979865772e-05,
      "loss": 0.5133,
      "step": 54780
    },
    {
      "epoch": 2.1916,
      "grad_norm": 2.6828086376190186,
      "learning_rate": 1.3563758389261747e-05,
      "loss": 0.4998,
      "step": 54790
    },
    {
      "epoch": 2.192,
      "grad_norm": 2.2770283222198486,
      "learning_rate": 1.3557046979865772e-05,
      "loss": 0.5343,
      "step": 54800
    },
    {
      "epoch": 2.1924,
      "grad_norm": 2.5982561111450195,
      "learning_rate": 1.3550335570469799e-05,
      "loss": 0.5104,
      "step": 54810
    },
    {
      "epoch": 2.1928,
      "grad_norm": 3.194100856781006,
      "learning_rate": 1.3543624161073826e-05,
      "loss": 0.5605,
      "step": 54820
    },
    {
      "epoch": 2.1932,
      "grad_norm": 2.8563637733459473,
      "learning_rate": 1.3536912751677853e-05,
      "loss": 0.4869,
      "step": 54830
    },
    {
      "epoch": 2.1936,
      "grad_norm": 2.5900325775146484,
      "learning_rate": 1.3530201342281878e-05,
      "loss": 0.5948,
      "step": 54840
    },
    {
      "epoch": 2.194,
      "grad_norm": 3.1153409481048584,
      "learning_rate": 1.3523489932885908e-05,
      "loss": 0.5358,
      "step": 54850
    },
    {
      "epoch": 2.1944,
      "grad_norm": 2.7091989517211914,
      "learning_rate": 1.3516778523489935e-05,
      "loss": 0.498,
      "step": 54860
    },
    {
      "epoch": 2.1948,
      "grad_norm": 2.266893148422241,
      "learning_rate": 1.351006711409396e-05,
      "loss": 0.5074,
      "step": 54870
    },
    {
      "epoch": 2.1952,
      "grad_norm": 3.164515256881714,
      "learning_rate": 1.3503355704697987e-05,
      "loss": 0.4938,
      "step": 54880
    },
    {
      "epoch": 2.1955999999999998,
      "grad_norm": 2.5054497718811035,
      "learning_rate": 1.3496644295302014e-05,
      "loss": 0.5365,
      "step": 54890
    },
    {
      "epoch": 2.196,
      "grad_norm": 2.1863853931427,
      "learning_rate": 1.3489932885906039e-05,
      "loss": 0.5135,
      "step": 54900
    },
    {
      "epoch": 2.1964,
      "grad_norm": 2.316899538040161,
      "learning_rate": 1.348322147651007e-05,
      "loss": 0.5157,
      "step": 54910
    },
    {
      "epoch": 2.1968,
      "grad_norm": 5.152804374694824,
      "learning_rate": 1.3476510067114096e-05,
      "loss": 0.5128,
      "step": 54920
    },
    {
      "epoch": 2.1972,
      "grad_norm": 4.385404586791992,
      "learning_rate": 1.3469798657718121e-05,
      "loss": 0.4215,
      "step": 54930
    },
    {
      "epoch": 2.1976,
      "grad_norm": 4.370258331298828,
      "learning_rate": 1.3463087248322148e-05,
      "loss": 0.474,
      "step": 54940
    },
    {
      "epoch": 2.198,
      "grad_norm": 2.2525362968444824,
      "learning_rate": 1.3456375838926175e-05,
      "loss": 0.516,
      "step": 54950
    },
    {
      "epoch": 2.1984,
      "grad_norm": 3.2032675743103027,
      "learning_rate": 1.3449664429530202e-05,
      "loss": 0.463,
      "step": 54960
    },
    {
      "epoch": 2.1988,
      "grad_norm": 2.7442519664764404,
      "learning_rate": 1.344295302013423e-05,
      "loss": 0.4714,
      "step": 54970
    },
    {
      "epoch": 2.1992,
      "grad_norm": 1.8547908067703247,
      "learning_rate": 1.3436241610738257e-05,
      "loss": 0.4823,
      "step": 54980
    },
    {
      "epoch": 2.1996,
      "grad_norm": 2.7014992237091064,
      "learning_rate": 1.3429530201342282e-05,
      "loss": 0.5037,
      "step": 54990
    },
    {
      "epoch": 2.2,
      "grad_norm": 2.7169249057769775,
      "learning_rate": 1.3422818791946309e-05,
      "loss": 0.5465,
      "step": 55000
    },
    {
      "epoch": 2.2004,
      "grad_norm": 2.9140732288360596,
      "learning_rate": 1.3416107382550336e-05,
      "loss": 0.5323,
      "step": 55010
    },
    {
      "epoch": 2.2008,
      "grad_norm": 2.423151969909668,
      "learning_rate": 1.3409395973154363e-05,
      "loss": 0.4471,
      "step": 55020
    },
    {
      "epoch": 2.2012,
      "grad_norm": 1.791001558303833,
      "learning_rate": 1.3402684563758388e-05,
      "loss": 0.4643,
      "step": 55030
    },
    {
      "epoch": 2.2016,
      "grad_norm": 2.7836227416992188,
      "learning_rate": 1.3395973154362418e-05,
      "loss": 0.5138,
      "step": 55040
    },
    {
      "epoch": 2.202,
      "grad_norm": 2.7628653049468994,
      "learning_rate": 1.3389261744966443e-05,
      "loss": 0.5141,
      "step": 55050
    },
    {
      "epoch": 2.2024,
      "grad_norm": 1.9959689378738403,
      "learning_rate": 1.338255033557047e-05,
      "loss": 0.4676,
      "step": 55060
    },
    {
      "epoch": 2.2028,
      "grad_norm": 3.0926756858825684,
      "learning_rate": 1.3375838926174497e-05,
      "loss": 0.5282,
      "step": 55070
    },
    {
      "epoch": 2.2032,
      "grad_norm": 2.4451324939727783,
      "learning_rate": 1.3369127516778524e-05,
      "loss": 0.4642,
      "step": 55080
    },
    {
      "epoch": 2.2036,
      "grad_norm": 3.2805733680725098,
      "learning_rate": 1.3362416107382549e-05,
      "loss": 0.517,
      "step": 55090
    },
    {
      "epoch": 2.204,
      "grad_norm": 2.4455113410949707,
      "learning_rate": 1.335570469798658e-05,
      "loss": 0.5172,
      "step": 55100
    },
    {
      "epoch": 2.2044,
      "grad_norm": 3.09167742729187,
      "learning_rate": 1.3348993288590606e-05,
      "loss": 0.5126,
      "step": 55110
    },
    {
      "epoch": 2.2048,
      "grad_norm": 2.1890037059783936,
      "learning_rate": 1.3342281879194631e-05,
      "loss": 0.4448,
      "step": 55120
    },
    {
      "epoch": 2.2052,
      "grad_norm": 2.5416367053985596,
      "learning_rate": 1.3335570469798658e-05,
      "loss": 0.5255,
      "step": 55130
    },
    {
      "epoch": 2.2056,
      "grad_norm": 3.0851383209228516,
      "learning_rate": 1.3328859060402685e-05,
      "loss": 0.5295,
      "step": 55140
    },
    {
      "epoch": 2.206,
      "grad_norm": 2.332151412963867,
      "learning_rate": 1.3322147651006712e-05,
      "loss": 0.4311,
      "step": 55150
    },
    {
      "epoch": 2.2064,
      "grad_norm": 2.1543848514556885,
      "learning_rate": 1.3315436241610737e-05,
      "loss": 0.445,
      "step": 55160
    },
    {
      "epoch": 2.2068,
      "grad_norm": 3.220362901687622,
      "learning_rate": 1.3308724832214767e-05,
      "loss": 0.5055,
      "step": 55170
    },
    {
      "epoch": 2.2072,
      "grad_norm": 2.9563517570495605,
      "learning_rate": 1.3302013422818793e-05,
      "loss": 0.5065,
      "step": 55180
    },
    {
      "epoch": 2.2076000000000002,
      "grad_norm": 2.004751205444336,
      "learning_rate": 1.329530201342282e-05,
      "loss": 0.5001,
      "step": 55190
    },
    {
      "epoch": 2.208,
      "grad_norm": 2.5994629859924316,
      "learning_rate": 1.3288590604026846e-05,
      "loss": 0.4958,
      "step": 55200
    },
    {
      "epoch": 2.2084,
      "grad_norm": 2.6660103797912598,
      "learning_rate": 1.3281879194630873e-05,
      "loss": 0.3868,
      "step": 55210
    },
    {
      "epoch": 2.2088,
      "grad_norm": 2.3511412143707275,
      "learning_rate": 1.3275167785234898e-05,
      "loss": 0.569,
      "step": 55220
    },
    {
      "epoch": 2.2092,
      "grad_norm": 3.3430209159851074,
      "learning_rate": 1.3268456375838928e-05,
      "loss": 0.5508,
      "step": 55230
    },
    {
      "epoch": 2.2096,
      "grad_norm": 2.369142770767212,
      "learning_rate": 1.3261744966442954e-05,
      "loss": 0.457,
      "step": 55240
    },
    {
      "epoch": 2.21,
      "grad_norm": 3.204162359237671,
      "learning_rate": 1.325503355704698e-05,
      "loss": 0.4951,
      "step": 55250
    },
    {
      "epoch": 2.2104,
      "grad_norm": 2.587233781814575,
      "learning_rate": 1.3248322147651007e-05,
      "loss": 0.5668,
      "step": 55260
    },
    {
      "epoch": 2.2108,
      "grad_norm": 2.243170738220215,
      "learning_rate": 1.3241610738255034e-05,
      "loss": 0.527,
      "step": 55270
    },
    {
      "epoch": 2.2112,
      "grad_norm": 2.4221408367156982,
      "learning_rate": 1.323489932885906e-05,
      "loss": 0.459,
      "step": 55280
    },
    {
      "epoch": 2.2116,
      "grad_norm": 2.8147377967834473,
      "learning_rate": 1.322818791946309e-05,
      "loss": 0.5392,
      "step": 55290
    },
    {
      "epoch": 2.212,
      "grad_norm": 2.846829652786255,
      "learning_rate": 1.3221476510067116e-05,
      "loss": 0.5348,
      "step": 55300
    },
    {
      "epoch": 2.2124,
      "grad_norm": 1.8528048992156982,
      "learning_rate": 1.3214765100671142e-05,
      "loss": 0.4915,
      "step": 55310
    },
    {
      "epoch": 2.2128,
      "grad_norm": 3.057922124862671,
      "learning_rate": 1.3208053691275168e-05,
      "loss": 0.4698,
      "step": 55320
    },
    {
      "epoch": 2.2132,
      "grad_norm": 2.3337626457214355,
      "learning_rate": 1.3201342281879195e-05,
      "loss": 0.4954,
      "step": 55330
    },
    {
      "epoch": 2.2136,
      "grad_norm": 2.6788413524627686,
      "learning_rate": 1.319463087248322e-05,
      "loss": 0.5124,
      "step": 55340
    },
    {
      "epoch": 2.214,
      "grad_norm": 2.9683094024658203,
      "learning_rate": 1.3187919463087247e-05,
      "loss": 0.6025,
      "step": 55350
    },
    {
      "epoch": 2.2144,
      "grad_norm": 2.7604000568389893,
      "learning_rate": 1.3181208053691278e-05,
      "loss": 0.4652,
      "step": 55360
    },
    {
      "epoch": 2.2148,
      "grad_norm": 2.709094762802124,
      "learning_rate": 1.3174496644295303e-05,
      "loss": 0.5425,
      "step": 55370
    },
    {
      "epoch": 2.2152,
      "grad_norm": 2.726839065551758,
      "learning_rate": 1.316778523489933e-05,
      "loss": 0.5562,
      "step": 55380
    },
    {
      "epoch": 2.2156000000000002,
      "grad_norm": 3.7709386348724365,
      "learning_rate": 1.3161073825503356e-05,
      "loss": 0.5182,
      "step": 55390
    },
    {
      "epoch": 2.216,
      "grad_norm": 2.323275327682495,
      "learning_rate": 1.3154362416107383e-05,
      "loss": 0.5577,
      "step": 55400
    },
    {
      "epoch": 2.2164,
      "grad_norm": 2.8669466972351074,
      "learning_rate": 1.3147651006711408e-05,
      "loss": 0.4878,
      "step": 55410
    },
    {
      "epoch": 2.2168,
      "grad_norm": 2.6297621726989746,
      "learning_rate": 1.3140939597315439e-05,
      "loss": 0.4942,
      "step": 55420
    },
    {
      "epoch": 2.2172,
      "grad_norm": 2.892216682434082,
      "learning_rate": 1.3134228187919464e-05,
      "loss": 0.5175,
      "step": 55430
    },
    {
      "epoch": 2.2176,
      "grad_norm": 2.5821168422698975,
      "learning_rate": 1.312751677852349e-05,
      "loss": 0.4992,
      "step": 55440
    },
    {
      "epoch": 2.218,
      "grad_norm": 2.661090612411499,
      "learning_rate": 1.3120805369127518e-05,
      "loss": 0.4903,
      "step": 55450
    },
    {
      "epoch": 2.2184,
      "grad_norm": 1.9117144346237183,
      "learning_rate": 1.3114093959731544e-05,
      "loss": 0.5039,
      "step": 55460
    },
    {
      "epoch": 2.2188,
      "grad_norm": 2.232764959335327,
      "learning_rate": 1.310738255033557e-05,
      "loss": 0.474,
      "step": 55470
    },
    {
      "epoch": 2.2192,
      "grad_norm": 2.165580987930298,
      "learning_rate": 1.3100671140939596e-05,
      "loss": 0.5263,
      "step": 55480
    },
    {
      "epoch": 2.2196,
      "grad_norm": 2.9459574222564697,
      "learning_rate": 1.3093959731543625e-05,
      "loss": 0.4295,
      "step": 55490
    },
    {
      "epoch": 2.22,
      "grad_norm": 3.3170623779296875,
      "learning_rate": 1.3087248322147652e-05,
      "loss": 0.5645,
      "step": 55500
    },
    {
      "epoch": 2.2204,
      "grad_norm": 2.572568416595459,
      "learning_rate": 1.3080536912751679e-05,
      "loss": 0.4887,
      "step": 55510
    },
    {
      "epoch": 2.2208,
      "grad_norm": 2.880460023880005,
      "learning_rate": 1.3073825503355706e-05,
      "loss": 0.522,
      "step": 55520
    },
    {
      "epoch": 2.2212,
      "grad_norm": 3.1527678966522217,
      "learning_rate": 1.306711409395973e-05,
      "loss": 0.5333,
      "step": 55530
    },
    {
      "epoch": 2.2216,
      "grad_norm": 2.5040225982666016,
      "learning_rate": 1.3060402684563758e-05,
      "loss": 0.4576,
      "step": 55540
    },
    {
      "epoch": 2.222,
      "grad_norm": 1.9138388633728027,
      "learning_rate": 1.3053691275167788e-05,
      "loss": 0.4025,
      "step": 55550
    },
    {
      "epoch": 2.2224,
      "grad_norm": 2.181845188140869,
      "learning_rate": 1.3046979865771813e-05,
      "loss": 0.535,
      "step": 55560
    },
    {
      "epoch": 2.2228,
      "grad_norm": 1.7703360319137573,
      "learning_rate": 1.304026845637584e-05,
      "loss": 0.492,
      "step": 55570
    },
    {
      "epoch": 2.2232,
      "grad_norm": 2.550398826599121,
      "learning_rate": 1.3033557046979867e-05,
      "loss": 0.5185,
      "step": 55580
    },
    {
      "epoch": 2.2236,
      "grad_norm": 2.425032377243042,
      "learning_rate": 1.3026845637583893e-05,
      "loss": 0.4175,
      "step": 55590
    },
    {
      "epoch": 2.224,
      "grad_norm": 2.9456424713134766,
      "learning_rate": 1.3020134228187919e-05,
      "loss": 0.5838,
      "step": 55600
    },
    {
      "epoch": 2.2244,
      "grad_norm": 2.9001636505126953,
      "learning_rate": 1.3013422818791949e-05,
      "loss": 0.5225,
      "step": 55610
    },
    {
      "epoch": 2.2248,
      "grad_norm": 3.4961512088775635,
      "learning_rate": 1.3006711409395974e-05,
      "loss": 0.539,
      "step": 55620
    },
    {
      "epoch": 2.2252,
      "grad_norm": 3.362079620361328,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 0.6489,
      "step": 55630
    },
    {
      "epoch": 2.2256,
      "grad_norm": 2.7474491596221924,
      "learning_rate": 1.2993288590604028e-05,
      "loss": 0.4119,
      "step": 55640
    },
    {
      "epoch": 2.226,
      "grad_norm": 2.4076859951019287,
      "learning_rate": 1.2986577181208055e-05,
      "loss": 0.507,
      "step": 55650
    },
    {
      "epoch": 2.2264,
      "grad_norm": 2.814030647277832,
      "learning_rate": 1.297986577181208e-05,
      "loss": 0.554,
      "step": 55660
    },
    {
      "epoch": 2.2268,
      "grad_norm": 2.9237747192382812,
      "learning_rate": 1.2973154362416107e-05,
      "loss": 0.4616,
      "step": 55670
    },
    {
      "epoch": 2.2272,
      "grad_norm": 2.257134199142456,
      "learning_rate": 1.2966442953020135e-05,
      "loss": 0.4966,
      "step": 55680
    },
    {
      "epoch": 2.2276,
      "grad_norm": 3.010258674621582,
      "learning_rate": 1.2959731543624162e-05,
      "loss": 0.5785,
      "step": 55690
    },
    {
      "epoch": 2.228,
      "grad_norm": 3.4067912101745605,
      "learning_rate": 1.2953020134228189e-05,
      "loss": 0.5637,
      "step": 55700
    },
    {
      "epoch": 2.2284,
      "grad_norm": 1.8512403964996338,
      "learning_rate": 1.2946308724832216e-05,
      "loss": 0.4335,
      "step": 55710
    },
    {
      "epoch": 2.2288,
      "grad_norm": 3.695441722869873,
      "learning_rate": 1.2939597315436241e-05,
      "loss": 0.55,
      "step": 55720
    },
    {
      "epoch": 2.2292,
      "grad_norm": 2.567817211151123,
      "learning_rate": 1.2932885906040268e-05,
      "loss": 0.4377,
      "step": 55730
    },
    {
      "epoch": 2.2296,
      "grad_norm": 3.282529592514038,
      "learning_rate": 1.2926174496644298e-05,
      "loss": 0.5454,
      "step": 55740
    },
    {
      "epoch": 2.23,
      "grad_norm": 2.6060972213745117,
      "learning_rate": 1.2919463087248323e-05,
      "loss": 0.5463,
      "step": 55750
    },
    {
      "epoch": 2.2304,
      "grad_norm": 2.5866787433624268,
      "learning_rate": 1.291275167785235e-05,
      "loss": 0.4863,
      "step": 55760
    },
    {
      "epoch": 2.2308,
      "grad_norm": 3.05385160446167,
      "learning_rate": 1.2906040268456377e-05,
      "loss": 0.4874,
      "step": 55770
    },
    {
      "epoch": 2.2312,
      "grad_norm": 2.671940565109253,
      "learning_rate": 1.2899328859060402e-05,
      "loss": 0.4748,
      "step": 55780
    },
    {
      "epoch": 2.2316,
      "grad_norm": 3.6975350379943848,
      "learning_rate": 1.2892617449664429e-05,
      "loss": 0.5147,
      "step": 55790
    },
    {
      "epoch": 2.232,
      "grad_norm": 3.084784507751465,
      "learning_rate": 1.2885906040268459e-05,
      "loss": 0.566,
      "step": 55800
    },
    {
      "epoch": 2.2324,
      "grad_norm": 2.5997791290283203,
      "learning_rate": 1.2879194630872484e-05,
      "loss": 0.554,
      "step": 55810
    },
    {
      "epoch": 2.2328,
      "grad_norm": 2.66513729095459,
      "learning_rate": 1.2872483221476511e-05,
      "loss": 0.4699,
      "step": 55820
    },
    {
      "epoch": 2.2332,
      "grad_norm": 2.290090322494507,
      "learning_rate": 1.2865771812080538e-05,
      "loss": 0.4831,
      "step": 55830
    },
    {
      "epoch": 2.2336,
      "grad_norm": 2.8751304149627686,
      "learning_rate": 1.2859060402684565e-05,
      "loss": 0.4524,
      "step": 55840
    },
    {
      "epoch": 2.234,
      "grad_norm": 2.7357890605926514,
      "learning_rate": 1.285234899328859e-05,
      "loss": 0.5374,
      "step": 55850
    },
    {
      "epoch": 2.2344,
      "grad_norm": 3.7541022300720215,
      "learning_rate": 1.2845637583892617e-05,
      "loss": 0.4933,
      "step": 55860
    },
    {
      "epoch": 2.2348,
      "grad_norm": 2.507723569869995,
      "learning_rate": 1.2838926174496645e-05,
      "loss": 0.5101,
      "step": 55870
    },
    {
      "epoch": 2.2352,
      "grad_norm": 2.4510557651519775,
      "learning_rate": 1.2832214765100672e-05,
      "loss": 0.4898,
      "step": 55880
    },
    {
      "epoch": 2.2356,
      "grad_norm": 2.761045455932617,
      "learning_rate": 1.2825503355704699e-05,
      "loss": 0.4917,
      "step": 55890
    },
    {
      "epoch": 2.2359999999999998,
      "grad_norm": 2.9566447734832764,
      "learning_rate": 1.2818791946308726e-05,
      "loss": 0.5311,
      "step": 55900
    },
    {
      "epoch": 2.2364,
      "grad_norm": 2.5141115188598633,
      "learning_rate": 1.2812080536912751e-05,
      "loss": 0.4625,
      "step": 55910
    },
    {
      "epoch": 2.2368,
      "grad_norm": 2.494431734085083,
      "learning_rate": 1.2805369127516778e-05,
      "loss": 0.4178,
      "step": 55920
    },
    {
      "epoch": 2.2372,
      "grad_norm": 2.9044055938720703,
      "learning_rate": 1.2798657718120806e-05,
      "loss": 0.4817,
      "step": 55930
    },
    {
      "epoch": 2.2376,
      "grad_norm": 2.478820562362671,
      "learning_rate": 1.2791946308724833e-05,
      "loss": 0.4913,
      "step": 55940
    },
    {
      "epoch": 2.238,
      "grad_norm": 3.1135871410369873,
      "learning_rate": 1.278523489932886e-05,
      "loss": 0.4284,
      "step": 55950
    },
    {
      "epoch": 2.2384,
      "grad_norm": 3.226807117462158,
      "learning_rate": 1.2778523489932887e-05,
      "loss": 0.5017,
      "step": 55960
    },
    {
      "epoch": 2.2388,
      "grad_norm": 3.454139232635498,
      "learning_rate": 1.2771812080536912e-05,
      "loss": 0.4221,
      "step": 55970
    },
    {
      "epoch": 2.2392,
      "grad_norm": 2.5816047191619873,
      "learning_rate": 1.2765100671140939e-05,
      "loss": 0.5069,
      "step": 55980
    },
    {
      "epoch": 2.2396,
      "grad_norm": 2.5593669414520264,
      "learning_rate": 1.2758389261744966e-05,
      "loss": 0.4332,
      "step": 55990
    },
    {
      "epoch": 2.24,
      "grad_norm": 2.8090145587921143,
      "learning_rate": 1.2751677852348994e-05,
      "loss": 0.4464,
      "step": 56000
    },
    {
      "epoch": 2.2404,
      "grad_norm": 2.497530937194824,
      "learning_rate": 1.2744966442953021e-05,
      "loss": 0.3923,
      "step": 56010
    },
    {
      "epoch": 2.2408,
      "grad_norm": 2.215888261795044,
      "learning_rate": 1.2738255033557048e-05,
      "loss": 0.4807,
      "step": 56020
    },
    {
      "epoch": 2.2412,
      "grad_norm": 2.675844669342041,
      "learning_rate": 1.2731543624161073e-05,
      "loss": 0.5203,
      "step": 56030
    },
    {
      "epoch": 2.2416,
      "grad_norm": 2.646493673324585,
      "learning_rate": 1.27248322147651e-05,
      "loss": 0.4962,
      "step": 56040
    },
    {
      "epoch": 2.242,
      "grad_norm": 2.413825750350952,
      "learning_rate": 1.2718120805369127e-05,
      "loss": 0.4491,
      "step": 56050
    },
    {
      "epoch": 2.2424,
      "grad_norm": 2.4491891860961914,
      "learning_rate": 1.2711409395973156e-05,
      "loss": 0.4509,
      "step": 56060
    },
    {
      "epoch": 2.2428,
      "grad_norm": 2.7341511249542236,
      "learning_rate": 1.2704697986577182e-05,
      "loss": 0.4864,
      "step": 56070
    },
    {
      "epoch": 2.2432,
      "grad_norm": 2.5396833419799805,
      "learning_rate": 1.269798657718121e-05,
      "loss": 0.4905,
      "step": 56080
    },
    {
      "epoch": 2.2436,
      "grad_norm": 2.8559765815734863,
      "learning_rate": 1.2691275167785236e-05,
      "loss": 0.5439,
      "step": 56090
    },
    {
      "epoch": 2.2439999999999998,
      "grad_norm": 2.6122381687164307,
      "learning_rate": 1.2684563758389261e-05,
      "loss": 0.492,
      "step": 56100
    },
    {
      "epoch": 2.2444,
      "grad_norm": 2.695255994796753,
      "learning_rate": 1.2677852348993288e-05,
      "loss": 0.4477,
      "step": 56110
    },
    {
      "epoch": 2.2448,
      "grad_norm": 3.0257210731506348,
      "learning_rate": 1.2671140939597317e-05,
      "loss": 0.5526,
      "step": 56120
    },
    {
      "epoch": 2.2452,
      "grad_norm": 3.229065418243408,
      "learning_rate": 1.2664429530201344e-05,
      "loss": 0.4877,
      "step": 56130
    },
    {
      "epoch": 2.2456,
      "grad_norm": 1.520435094833374,
      "learning_rate": 1.265771812080537e-05,
      "loss": 0.4615,
      "step": 56140
    },
    {
      "epoch": 2.246,
      "grad_norm": 2.4220495223999023,
      "learning_rate": 1.2651006711409397e-05,
      "loss": 0.4924,
      "step": 56150
    },
    {
      "epoch": 2.2464,
      "grad_norm": 2.7943873405456543,
      "learning_rate": 1.2644295302013422e-05,
      "loss": 0.5586,
      "step": 56160
    },
    {
      "epoch": 2.2468,
      "grad_norm": 3.0199127197265625,
      "learning_rate": 1.263758389261745e-05,
      "loss": 0.4998,
      "step": 56170
    },
    {
      "epoch": 2.2472,
      "grad_norm": 2.4340083599090576,
      "learning_rate": 1.2630872483221476e-05,
      "loss": 0.5278,
      "step": 56180
    },
    {
      "epoch": 2.2476,
      "grad_norm": 2.7998428344726562,
      "learning_rate": 1.2624161073825505e-05,
      "loss": 0.5108,
      "step": 56190
    },
    {
      "epoch": 2.248,
      "grad_norm": 2.4372785091400146,
      "learning_rate": 1.2617449664429532e-05,
      "loss": 0.4636,
      "step": 56200
    },
    {
      "epoch": 2.2484,
      "grad_norm": 2.6153528690338135,
      "learning_rate": 1.2610738255033558e-05,
      "loss": 0.484,
      "step": 56210
    },
    {
      "epoch": 2.2488,
      "grad_norm": 2.822685956954956,
      "learning_rate": 1.2604026845637584e-05,
      "loss": 0.5059,
      "step": 56220
    },
    {
      "epoch": 2.2492,
      "grad_norm": 2.291299343109131,
      "learning_rate": 1.259731543624161e-05,
      "loss": 0.4288,
      "step": 56230
    },
    {
      "epoch": 2.2496,
      "grad_norm": 2.8826138973236084,
      "learning_rate": 1.2590604026845637e-05,
      "loss": 0.4764,
      "step": 56240
    },
    {
      "epoch": 2.25,
      "grad_norm": 3.0809485912323,
      "learning_rate": 1.2583892617449666e-05,
      "loss": 0.4641,
      "step": 56250
    },
    {
      "epoch": 2.2504,
      "grad_norm": 2.5368690490722656,
      "learning_rate": 1.2577181208053693e-05,
      "loss": 0.5044,
      "step": 56260
    },
    {
      "epoch": 2.2508,
      "grad_norm": 2.40995717048645,
      "learning_rate": 1.257046979865772e-05,
      "loss": 0.4371,
      "step": 56270
    },
    {
      "epoch": 2.2512,
      "grad_norm": 2.2417941093444824,
      "learning_rate": 1.2563758389261746e-05,
      "loss": 0.5103,
      "step": 56280
    },
    {
      "epoch": 2.2516,
      "grad_norm": 2.961486577987671,
      "learning_rate": 1.2557046979865772e-05,
      "loss": 0.4503,
      "step": 56290
    },
    {
      "epoch": 2.252,
      "grad_norm": 3.0668485164642334,
      "learning_rate": 1.2550335570469798e-05,
      "loss": 0.5056,
      "step": 56300
    },
    {
      "epoch": 2.2524,
      "grad_norm": 2.8479528427124023,
      "learning_rate": 1.2543624161073825e-05,
      "loss": 0.4708,
      "step": 56310
    },
    {
      "epoch": 2.2528,
      "grad_norm": 2.6969261169433594,
      "learning_rate": 1.2536912751677854e-05,
      "loss": 0.5568,
      "step": 56320
    },
    {
      "epoch": 2.2532,
      "grad_norm": 2.5908420085906982,
      "learning_rate": 1.253020134228188e-05,
      "loss": 0.6149,
      "step": 56330
    },
    {
      "epoch": 2.2536,
      "grad_norm": 2.2977335453033447,
      "learning_rate": 1.2523489932885907e-05,
      "loss": 0.5426,
      "step": 56340
    },
    {
      "epoch": 2.254,
      "grad_norm": 2.884453535079956,
      "learning_rate": 1.2516778523489933e-05,
      "loss": 0.5001,
      "step": 56350
    },
    {
      "epoch": 2.2544,
      "grad_norm": 2.7926297187805176,
      "learning_rate": 1.251006711409396e-05,
      "loss": 0.5392,
      "step": 56360
    },
    {
      "epoch": 2.2548,
      "grad_norm": 2.73504900932312,
      "learning_rate": 1.2503355704697986e-05,
      "loss": 0.4787,
      "step": 56370
    },
    {
      "epoch": 2.2552,
      "grad_norm": 2.7345409393310547,
      "learning_rate": 1.2496644295302013e-05,
      "loss": 0.5041,
      "step": 56380
    },
    {
      "epoch": 2.2556,
      "grad_norm": 2.7389326095581055,
      "learning_rate": 1.248993288590604e-05,
      "loss": 0.531,
      "step": 56390
    },
    {
      "epoch": 2.2560000000000002,
      "grad_norm": 2.7878613471984863,
      "learning_rate": 1.2483221476510069e-05,
      "loss": 0.5331,
      "step": 56400
    },
    {
      "epoch": 2.2564,
      "grad_norm": 2.73274827003479,
      "learning_rate": 1.2476510067114094e-05,
      "loss": 0.5522,
      "step": 56410
    },
    {
      "epoch": 2.2568,
      "grad_norm": 2.981625556945801,
      "learning_rate": 1.246979865771812e-05,
      "loss": 0.5329,
      "step": 56420
    },
    {
      "epoch": 2.2572,
      "grad_norm": 2.1612770557403564,
      "learning_rate": 1.2463087248322149e-05,
      "loss": 0.4465,
      "step": 56430
    },
    {
      "epoch": 2.2576,
      "grad_norm": 2.732198715209961,
      "learning_rate": 1.2456375838926174e-05,
      "loss": 0.5467,
      "step": 56440
    },
    {
      "epoch": 2.258,
      "grad_norm": 2.5300467014312744,
      "learning_rate": 1.2449664429530201e-05,
      "loss": 0.5033,
      "step": 56450
    },
    {
      "epoch": 2.2584,
      "grad_norm": 3.2174127101898193,
      "learning_rate": 1.244295302013423e-05,
      "loss": 0.5901,
      "step": 56460
    },
    {
      "epoch": 2.2588,
      "grad_norm": 2.1501781940460205,
      "learning_rate": 1.2436241610738255e-05,
      "loss": 0.5332,
      "step": 56470
    },
    {
      "epoch": 2.2592,
      "grad_norm": 3.0975778102874756,
      "learning_rate": 1.2429530201342282e-05,
      "loss": 0.5095,
      "step": 56480
    },
    {
      "epoch": 2.2596,
      "grad_norm": 2.476954221725464,
      "learning_rate": 1.242281879194631e-05,
      "loss": 0.4994,
      "step": 56490
    },
    {
      "epoch": 2.26,
      "grad_norm": 2.2411673069000244,
      "learning_rate": 1.2416107382550337e-05,
      "loss": 0.4511,
      "step": 56500
    },
    {
      "epoch": 2.2604,
      "grad_norm": 2.1387972831726074,
      "learning_rate": 1.2409395973154362e-05,
      "loss": 0.5014,
      "step": 56510
    },
    {
      "epoch": 2.2608,
      "grad_norm": 3.45788836479187,
      "learning_rate": 1.240268456375839e-05,
      "loss": 0.4986,
      "step": 56520
    },
    {
      "epoch": 2.2612,
      "grad_norm": 2.8033504486083984,
      "learning_rate": 1.2395973154362418e-05,
      "loss": 0.4937,
      "step": 56530
    },
    {
      "epoch": 2.2616,
      "grad_norm": 2.7793476581573486,
      "learning_rate": 1.2389261744966443e-05,
      "loss": 0.6159,
      "step": 56540
    },
    {
      "epoch": 2.262,
      "grad_norm": 2.6886751651763916,
      "learning_rate": 1.238255033557047e-05,
      "loss": 0.5144,
      "step": 56550
    },
    {
      "epoch": 2.2624,
      "grad_norm": 2.428375720977783,
      "learning_rate": 1.2375838926174498e-05,
      "loss": 0.4967,
      "step": 56560
    },
    {
      "epoch": 2.2628,
      "grad_norm": 3.063516855239868,
      "learning_rate": 1.2369127516778523e-05,
      "loss": 0.4984,
      "step": 56570
    },
    {
      "epoch": 2.2632,
      "grad_norm": 2.518873929977417,
      "learning_rate": 1.236241610738255e-05,
      "loss": 0.4522,
      "step": 56580
    },
    {
      "epoch": 2.2636,
      "grad_norm": 2.3079967498779297,
      "learning_rate": 1.2355704697986579e-05,
      "loss": 0.5605,
      "step": 56590
    },
    {
      "epoch": 2.2640000000000002,
      "grad_norm": 2.2943384647369385,
      "learning_rate": 1.2348993288590604e-05,
      "loss": 0.4551,
      "step": 56600
    },
    {
      "epoch": 2.2644,
      "grad_norm": 2.8028440475463867,
      "learning_rate": 1.234228187919463e-05,
      "loss": 0.488,
      "step": 56610
    },
    {
      "epoch": 2.2648,
      "grad_norm": 2.1058249473571777,
      "learning_rate": 1.233557046979866e-05,
      "loss": 0.4907,
      "step": 56620
    },
    {
      "epoch": 2.2652,
      "grad_norm": 3.1559128761291504,
      "learning_rate": 1.2328859060402685e-05,
      "loss": 0.5422,
      "step": 56630
    },
    {
      "epoch": 2.2656,
      "grad_norm": 2.5313150882720947,
      "learning_rate": 1.2322147651006711e-05,
      "loss": 0.5062,
      "step": 56640
    },
    {
      "epoch": 2.266,
      "grad_norm": 2.0935258865356445,
      "learning_rate": 1.231543624161074e-05,
      "loss": 0.4228,
      "step": 56650
    },
    {
      "epoch": 2.2664,
      "grad_norm": 10.272217750549316,
      "learning_rate": 1.2308724832214765e-05,
      "loss": 0.5127,
      "step": 56660
    },
    {
      "epoch": 2.2668,
      "grad_norm": 2.6483824253082275,
      "learning_rate": 1.2302013422818792e-05,
      "loss": 0.522,
      "step": 56670
    },
    {
      "epoch": 2.2672,
      "grad_norm": 2.0985121726989746,
      "learning_rate": 1.229530201342282e-05,
      "loss": 0.4834,
      "step": 56680
    },
    {
      "epoch": 2.2676,
      "grad_norm": 2.6790857315063477,
      "learning_rate": 1.2288590604026846e-05,
      "loss": 0.454,
      "step": 56690
    },
    {
      "epoch": 2.268,
      "grad_norm": 2.018732786178589,
      "learning_rate": 1.2281879194630872e-05,
      "loss": 0.4889,
      "step": 56700
    },
    {
      "epoch": 2.2684,
      "grad_norm": 2.4563519954681396,
      "learning_rate": 1.22751677852349e-05,
      "loss": 0.447,
      "step": 56710
    },
    {
      "epoch": 2.2688,
      "grad_norm": 2.9629101753234863,
      "learning_rate": 1.2268456375838928e-05,
      "loss": 0.5574,
      "step": 56720
    },
    {
      "epoch": 2.2692,
      "grad_norm": 2.381652355194092,
      "learning_rate": 1.2261744966442953e-05,
      "loss": 0.4567,
      "step": 56730
    },
    {
      "epoch": 2.2696,
      "grad_norm": 2.651546001434326,
      "learning_rate": 1.225503355704698e-05,
      "loss": 0.5699,
      "step": 56740
    },
    {
      "epoch": 2.27,
      "grad_norm": 2.5258171558380127,
      "learning_rate": 1.2248322147651008e-05,
      "loss": 0.5262,
      "step": 56750
    },
    {
      "epoch": 2.2704,
      "grad_norm": 2.6430273056030273,
      "learning_rate": 1.2241610738255034e-05,
      "loss": 0.5445,
      "step": 56760
    },
    {
      "epoch": 2.2708,
      "grad_norm": 1.6517692804336548,
      "learning_rate": 1.223489932885906e-05,
      "loss": 0.4505,
      "step": 56770
    },
    {
      "epoch": 2.2712,
      "grad_norm": 2.3481554985046387,
      "learning_rate": 1.2228187919463089e-05,
      "loss": 0.4838,
      "step": 56780
    },
    {
      "epoch": 2.2716,
      "grad_norm": 2.3956592082977295,
      "learning_rate": 1.2221476510067114e-05,
      "loss": 0.5613,
      "step": 56790
    },
    {
      "epoch": 2.2720000000000002,
      "grad_norm": 2.105008840560913,
      "learning_rate": 1.2214765100671141e-05,
      "loss": 0.4759,
      "step": 56800
    },
    {
      "epoch": 2.2724,
      "grad_norm": 2.837359666824341,
      "learning_rate": 1.220805369127517e-05,
      "loss": 0.5226,
      "step": 56810
    },
    {
      "epoch": 2.2728,
      "grad_norm": 2.682934045791626,
      "learning_rate": 1.2201342281879195e-05,
      "loss": 0.4732,
      "step": 56820
    },
    {
      "epoch": 2.2732,
      "grad_norm": 2.4061074256896973,
      "learning_rate": 1.2194630872483222e-05,
      "loss": 0.5111,
      "step": 56830
    },
    {
      "epoch": 2.2736,
      "grad_norm": 3.023711919784546,
      "learning_rate": 1.218791946308725e-05,
      "loss": 0.5072,
      "step": 56840
    },
    {
      "epoch": 2.274,
      "grad_norm": 2.3412671089172363,
      "learning_rate": 1.2181208053691275e-05,
      "loss": 0.486,
      "step": 56850
    },
    {
      "epoch": 2.2744,
      "grad_norm": 2.7089123725891113,
      "learning_rate": 1.2174496644295302e-05,
      "loss": 0.5276,
      "step": 56860
    },
    {
      "epoch": 2.2748,
      "grad_norm": 3.1654551029205322,
      "learning_rate": 1.2167785234899329e-05,
      "loss": 0.5208,
      "step": 56870
    },
    {
      "epoch": 2.2752,
      "grad_norm": 2.9762074947357178,
      "learning_rate": 1.2161073825503356e-05,
      "loss": 0.5142,
      "step": 56880
    },
    {
      "epoch": 2.2756,
      "grad_norm": 2.396901845932007,
      "learning_rate": 1.2154362416107383e-05,
      "loss": 0.517,
      "step": 56890
    },
    {
      "epoch": 2.276,
      "grad_norm": 2.869380235671997,
      "learning_rate": 1.214765100671141e-05,
      "loss": 0.5472,
      "step": 56900
    },
    {
      "epoch": 2.2763999999999998,
      "grad_norm": 2.7845232486724854,
      "learning_rate": 1.2140939597315436e-05,
      "loss": 0.5093,
      "step": 56910
    },
    {
      "epoch": 2.2768,
      "grad_norm": 2.622020959854126,
      "learning_rate": 1.2134228187919463e-05,
      "loss": 0.5354,
      "step": 56920
    },
    {
      "epoch": 2.2772,
      "grad_norm": 2.5044236183166504,
      "learning_rate": 1.212751677852349e-05,
      "loss": 0.4818,
      "step": 56930
    },
    {
      "epoch": 2.2776,
      "grad_norm": 2.5099799633026123,
      "learning_rate": 1.2120805369127517e-05,
      "loss": 0.5017,
      "step": 56940
    },
    {
      "epoch": 2.278,
      "grad_norm": 2.71246337890625,
      "learning_rate": 1.2114093959731544e-05,
      "loss": 0.4756,
      "step": 56950
    },
    {
      "epoch": 2.2784,
      "grad_norm": 2.934584856033325,
      "learning_rate": 1.210738255033557e-05,
      "loss": 0.5993,
      "step": 56960
    },
    {
      "epoch": 2.2788,
      "grad_norm": 2.4771413803100586,
      "learning_rate": 1.21006711409396e-05,
      "loss": 0.522,
      "step": 56970
    },
    {
      "epoch": 2.2792,
      "grad_norm": 2.658618211746216,
      "learning_rate": 1.2093959731543624e-05,
      "loss": 0.5171,
      "step": 56980
    },
    {
      "epoch": 2.2796,
      "grad_norm": 2.5170321464538574,
      "learning_rate": 1.2087248322147651e-05,
      "loss": 0.5826,
      "step": 56990
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 2.4834232330322266,
      "learning_rate": 1.208053691275168e-05,
      "loss": 0.5187,
      "step": 57000
    },
    {
      "epoch": 2.2804,
      "grad_norm": 2.6070749759674072,
      "learning_rate": 1.2073825503355705e-05,
      "loss": 0.4602,
      "step": 57010
    },
    {
      "epoch": 2.2808,
      "grad_norm": 2.2059237957000732,
      "learning_rate": 1.2067114093959732e-05,
      "loss": 0.4193,
      "step": 57020
    },
    {
      "epoch": 2.2812,
      "grad_norm": 2.166097402572632,
      "learning_rate": 1.206040268456376e-05,
      "loss": 0.5716,
      "step": 57030
    },
    {
      "epoch": 2.2816,
      "grad_norm": 2.795383930206299,
      "learning_rate": 1.2053691275167785e-05,
      "loss": 0.4262,
      "step": 57040
    },
    {
      "epoch": 2.282,
      "grad_norm": 2.744455575942993,
      "learning_rate": 1.2046979865771812e-05,
      "loss": 0.4146,
      "step": 57050
    },
    {
      "epoch": 2.2824,
      "grad_norm": 2.711920976638794,
      "learning_rate": 1.204026845637584e-05,
      "loss": 0.5264,
      "step": 57060
    },
    {
      "epoch": 2.2828,
      "grad_norm": 2.171614646911621,
      "learning_rate": 1.2033557046979866e-05,
      "loss": 0.5349,
      "step": 57070
    },
    {
      "epoch": 2.2832,
      "grad_norm": 2.1833178997039795,
      "learning_rate": 1.2026845637583893e-05,
      "loss": 0.4904,
      "step": 57080
    },
    {
      "epoch": 2.2836,
      "grad_norm": 2.160057306289673,
      "learning_rate": 1.202013422818792e-05,
      "loss": 0.5232,
      "step": 57090
    },
    {
      "epoch": 2.284,
      "grad_norm": 2.446711778640747,
      "learning_rate": 1.2013422818791947e-05,
      "loss": 0.4383,
      "step": 57100
    },
    {
      "epoch": 2.2843999999999998,
      "grad_norm": 2.3762662410736084,
      "learning_rate": 1.2006711409395973e-05,
      "loss": 0.4453,
      "step": 57110
    },
    {
      "epoch": 2.2848,
      "grad_norm": 3.0485329627990723,
      "learning_rate": 1.2e-05,
      "loss": 0.5204,
      "step": 57120
    },
    {
      "epoch": 2.2852,
      "grad_norm": 2.3922080993652344,
      "learning_rate": 1.1993288590604027e-05,
      "loss": 0.5251,
      "step": 57130
    },
    {
      "epoch": 2.2856,
      "grad_norm": 2.7339842319488525,
      "learning_rate": 1.1986577181208054e-05,
      "loss": 0.4865,
      "step": 57140
    },
    {
      "epoch": 2.286,
      "grad_norm": 3.302473545074463,
      "learning_rate": 1.1979865771812081e-05,
      "loss": 0.4988,
      "step": 57150
    },
    {
      "epoch": 2.2864,
      "grad_norm": 2.258479595184326,
      "learning_rate": 1.1973154362416108e-05,
      "loss": 0.4723,
      "step": 57160
    },
    {
      "epoch": 2.2868,
      "grad_norm": 2.231011390686035,
      "learning_rate": 1.1966442953020135e-05,
      "loss": 0.4952,
      "step": 57170
    },
    {
      "epoch": 2.2872,
      "grad_norm": 2.567067861557007,
      "learning_rate": 1.1959731543624161e-05,
      "loss": 0.4766,
      "step": 57180
    },
    {
      "epoch": 2.2876,
      "grad_norm": 2.450150966644287,
      "learning_rate": 1.195302013422819e-05,
      "loss": 0.4832,
      "step": 57190
    },
    {
      "epoch": 2.288,
      "grad_norm": 2.185262680053711,
      "learning_rate": 1.1946308724832215e-05,
      "loss": 0.5151,
      "step": 57200
    },
    {
      "epoch": 2.2884,
      "grad_norm": 2.5910139083862305,
      "learning_rate": 1.1939597315436242e-05,
      "loss": 0.4582,
      "step": 57210
    },
    {
      "epoch": 2.2888,
      "grad_norm": 2.9381372928619385,
      "learning_rate": 1.1932885906040269e-05,
      "loss": 0.5542,
      "step": 57220
    },
    {
      "epoch": 2.2892,
      "grad_norm": 3.0564796924591064,
      "learning_rate": 1.1926174496644296e-05,
      "loss": 0.473,
      "step": 57230
    },
    {
      "epoch": 2.2896,
      "grad_norm": 2.707825183868408,
      "learning_rate": 1.1919463087248323e-05,
      "loss": 0.4661,
      "step": 57240
    },
    {
      "epoch": 2.29,
      "grad_norm": 2.01932430267334,
      "learning_rate": 1.191275167785235e-05,
      "loss": 0.4913,
      "step": 57250
    },
    {
      "epoch": 2.2904,
      "grad_norm": 2.4544241428375244,
      "learning_rate": 1.1906040268456376e-05,
      "loss": 0.5046,
      "step": 57260
    },
    {
      "epoch": 2.2908,
      "grad_norm": 3.441546678543091,
      "learning_rate": 1.1899328859060403e-05,
      "loss": 0.5572,
      "step": 57270
    },
    {
      "epoch": 2.2912,
      "grad_norm": 3.522178888320923,
      "learning_rate": 1.189261744966443e-05,
      "loss": 0.4828,
      "step": 57280
    },
    {
      "epoch": 2.2916,
      "grad_norm": 2.2037551403045654,
      "learning_rate": 1.1885906040268457e-05,
      "loss": 0.4779,
      "step": 57290
    },
    {
      "epoch": 2.292,
      "grad_norm": 2.8172740936279297,
      "learning_rate": 1.1879194630872484e-05,
      "loss": 0.5453,
      "step": 57300
    },
    {
      "epoch": 2.2923999999999998,
      "grad_norm": 3.0060319900512695,
      "learning_rate": 1.187248322147651e-05,
      "loss": 0.4592,
      "step": 57310
    },
    {
      "epoch": 2.2928,
      "grad_norm": 3.3527421951293945,
      "learning_rate": 1.1865771812080537e-05,
      "loss": 0.5111,
      "step": 57320
    },
    {
      "epoch": 2.2932,
      "grad_norm": 2.4832329750061035,
      "learning_rate": 1.1859060402684564e-05,
      "loss": 0.5365,
      "step": 57330
    },
    {
      "epoch": 2.2936,
      "grad_norm": 3.110203981399536,
      "learning_rate": 1.1852348993288591e-05,
      "loss": 0.4601,
      "step": 57340
    },
    {
      "epoch": 2.294,
      "grad_norm": 2.8071823120117188,
      "learning_rate": 1.1845637583892618e-05,
      "loss": 0.5924,
      "step": 57350
    },
    {
      "epoch": 2.2944,
      "grad_norm": 1.7803205251693726,
      "learning_rate": 1.1838926174496645e-05,
      "loss": 0.4584,
      "step": 57360
    },
    {
      "epoch": 2.2948,
      "grad_norm": 2.048112392425537,
      "learning_rate": 1.1832214765100672e-05,
      "loss": 0.4415,
      "step": 57370
    },
    {
      "epoch": 2.2952,
      "grad_norm": 2.440742015838623,
      "learning_rate": 1.1825503355704698e-05,
      "loss": 0.5359,
      "step": 57380
    },
    {
      "epoch": 2.2956,
      "grad_norm": 2.8989458084106445,
      "learning_rate": 1.1818791946308725e-05,
      "loss": 0.4799,
      "step": 57390
    },
    {
      "epoch": 2.296,
      "grad_norm": 1.8099652528762817,
      "learning_rate": 1.1812080536912752e-05,
      "loss": 0.4643,
      "step": 57400
    },
    {
      "epoch": 2.2964,
      "grad_norm": 2.693411350250244,
      "learning_rate": 1.1805369127516779e-05,
      "loss": 0.4362,
      "step": 57410
    },
    {
      "epoch": 2.2968,
      "grad_norm": 2.4692869186401367,
      "learning_rate": 1.1798657718120806e-05,
      "loss": 0.5389,
      "step": 57420
    },
    {
      "epoch": 2.2972,
      "grad_norm": 2.06345534324646,
      "learning_rate": 1.1791946308724833e-05,
      "loss": 0.5074,
      "step": 57430
    },
    {
      "epoch": 2.2976,
      "grad_norm": 2.1992692947387695,
      "learning_rate": 1.178523489932886e-05,
      "loss": 0.4486,
      "step": 57440
    },
    {
      "epoch": 2.298,
      "grad_norm": 2.1878151893615723,
      "learning_rate": 1.1778523489932886e-05,
      "loss": 0.4764,
      "step": 57450
    },
    {
      "epoch": 2.2984,
      "grad_norm": 2.403825044631958,
      "learning_rate": 1.1771812080536913e-05,
      "loss": 0.5319,
      "step": 57460
    },
    {
      "epoch": 2.2988,
      "grad_norm": 2.1926937103271484,
      "learning_rate": 1.176510067114094e-05,
      "loss": 0.5129,
      "step": 57470
    },
    {
      "epoch": 2.2992,
      "grad_norm": 2.3905029296875,
      "learning_rate": 1.1758389261744967e-05,
      "loss": 0.4661,
      "step": 57480
    },
    {
      "epoch": 2.2996,
      "grad_norm": 2.7301974296569824,
      "learning_rate": 1.1751677852348994e-05,
      "loss": 0.5904,
      "step": 57490
    },
    {
      "epoch": 2.3,
      "grad_norm": 2.6736507415771484,
      "learning_rate": 1.174496644295302e-05,
      "loss": 0.4912,
      "step": 57500
    },
    {
      "epoch": 2.3004,
      "grad_norm": 3.1409919261932373,
      "learning_rate": 1.1738255033557048e-05,
      "loss": 0.6095,
      "step": 57510
    },
    {
      "epoch": 2.3008,
      "grad_norm": 2.6325368881225586,
      "learning_rate": 1.1731543624161074e-05,
      "loss": 0.4345,
      "step": 57520
    },
    {
      "epoch": 2.3012,
      "grad_norm": 2.0691442489624023,
      "learning_rate": 1.1724832214765101e-05,
      "loss": 0.5237,
      "step": 57530
    },
    {
      "epoch": 2.3016,
      "grad_norm": 2.382894515991211,
      "learning_rate": 1.1718120805369128e-05,
      "loss": 0.4666,
      "step": 57540
    },
    {
      "epoch": 2.302,
      "grad_norm": 3.078002691268921,
      "learning_rate": 1.1711409395973155e-05,
      "loss": 0.5872,
      "step": 57550
    },
    {
      "epoch": 2.3024,
      "grad_norm": 3.080235719680786,
      "learning_rate": 1.1704697986577182e-05,
      "loss": 0.5099,
      "step": 57560
    },
    {
      "epoch": 2.3028,
      "grad_norm": 2.583730697631836,
      "learning_rate": 1.1697986577181209e-05,
      "loss": 0.4886,
      "step": 57570
    },
    {
      "epoch": 2.3032,
      "grad_norm": 2.979851722717285,
      "learning_rate": 1.1691275167785236e-05,
      "loss": 0.5753,
      "step": 57580
    },
    {
      "epoch": 2.3036,
      "grad_norm": 2.5932087898254395,
      "learning_rate": 1.1684563758389262e-05,
      "loss": 0.469,
      "step": 57590
    },
    {
      "epoch": 2.304,
      "grad_norm": 3.3123111724853516,
      "learning_rate": 1.167785234899329e-05,
      "loss": 0.4714,
      "step": 57600
    },
    {
      "epoch": 2.3044000000000002,
      "grad_norm": 1.7840458154678345,
      "learning_rate": 1.1671140939597316e-05,
      "loss": 0.4509,
      "step": 57610
    },
    {
      "epoch": 2.3048,
      "grad_norm": 2.7460546493530273,
      "learning_rate": 1.1664429530201343e-05,
      "loss": 0.4936,
      "step": 57620
    },
    {
      "epoch": 2.3052,
      "grad_norm": 2.689941644668579,
      "learning_rate": 1.165771812080537e-05,
      "loss": 0.4624,
      "step": 57630
    },
    {
      "epoch": 2.3056,
      "grad_norm": 2.3118979930877686,
      "learning_rate": 1.1651006711409397e-05,
      "loss": 0.5188,
      "step": 57640
    },
    {
      "epoch": 2.306,
      "grad_norm": 2.708113193511963,
      "learning_rate": 1.1644295302013424e-05,
      "loss": 0.4424,
      "step": 57650
    },
    {
      "epoch": 2.3064,
      "grad_norm": 2.638197898864746,
      "learning_rate": 1.163758389261745e-05,
      "loss": 0.4683,
      "step": 57660
    },
    {
      "epoch": 2.3068,
      "grad_norm": 3.302776575088501,
      "learning_rate": 1.1630872483221477e-05,
      "loss": 0.5273,
      "step": 57670
    },
    {
      "epoch": 2.3072,
      "grad_norm": 2.139352798461914,
      "learning_rate": 1.1624161073825504e-05,
      "loss": 0.4944,
      "step": 57680
    },
    {
      "epoch": 2.3076,
      "grad_norm": 3.3420162200927734,
      "learning_rate": 1.1617449664429531e-05,
      "loss": 0.5106,
      "step": 57690
    },
    {
      "epoch": 2.308,
      "grad_norm": 3.2143588066101074,
      "learning_rate": 1.1610738255033558e-05,
      "loss": 0.4636,
      "step": 57700
    },
    {
      "epoch": 2.3084,
      "grad_norm": 3.377310276031494,
      "learning_rate": 1.1604026845637585e-05,
      "loss": 0.519,
      "step": 57710
    },
    {
      "epoch": 2.3088,
      "grad_norm": 3.121575355529785,
      "learning_rate": 1.1597315436241611e-05,
      "loss": 0.5519,
      "step": 57720
    },
    {
      "epoch": 2.3092,
      "grad_norm": 2.75986647605896,
      "learning_rate": 1.1590604026845638e-05,
      "loss": 0.4613,
      "step": 57730
    },
    {
      "epoch": 2.3096,
      "grad_norm": 2.9279980659484863,
      "learning_rate": 1.1583892617449665e-05,
      "loss": 0.5632,
      "step": 57740
    },
    {
      "epoch": 2.31,
      "grad_norm": 2.5943922996520996,
      "learning_rate": 1.1577181208053692e-05,
      "loss": 0.5038,
      "step": 57750
    },
    {
      "epoch": 2.3104,
      "grad_norm": 3.0212106704711914,
      "learning_rate": 1.1570469798657719e-05,
      "loss": 0.476,
      "step": 57760
    },
    {
      "epoch": 2.3108,
      "grad_norm": 2.0776705741882324,
      "learning_rate": 1.1563758389261746e-05,
      "loss": 0.4134,
      "step": 57770
    },
    {
      "epoch": 2.3112,
      "grad_norm": 2.5571670532226562,
      "learning_rate": 1.1557046979865773e-05,
      "loss": 0.4766,
      "step": 57780
    },
    {
      "epoch": 2.3116,
      "grad_norm": 2.8420023918151855,
      "learning_rate": 1.15503355704698e-05,
      "loss": 0.5225,
      "step": 57790
    },
    {
      "epoch": 2.312,
      "grad_norm": 3.032400369644165,
      "learning_rate": 1.1543624161073826e-05,
      "loss": 0.563,
      "step": 57800
    },
    {
      "epoch": 2.3124000000000002,
      "grad_norm": 2.330120086669922,
      "learning_rate": 1.1536912751677853e-05,
      "loss": 0.5007,
      "step": 57810
    },
    {
      "epoch": 2.3128,
      "grad_norm": 2.8261921405792236,
      "learning_rate": 1.153020134228188e-05,
      "loss": 0.4694,
      "step": 57820
    },
    {
      "epoch": 2.3132,
      "grad_norm": 2.305433988571167,
      "learning_rate": 1.1523489932885907e-05,
      "loss": 0.4856,
      "step": 57830
    },
    {
      "epoch": 2.3136,
      "grad_norm": 2.102893352508545,
      "learning_rate": 1.1516778523489934e-05,
      "loss": 0.5036,
      "step": 57840
    },
    {
      "epoch": 2.314,
      "grad_norm": 2.9809770584106445,
      "learning_rate": 1.151006711409396e-05,
      "loss": 0.4576,
      "step": 57850
    },
    {
      "epoch": 2.3144,
      "grad_norm": 2.3614795207977295,
      "learning_rate": 1.1503355704697986e-05,
      "loss": 0.4885,
      "step": 57860
    },
    {
      "epoch": 2.3148,
      "grad_norm": 2.7780983448028564,
      "learning_rate": 1.1496644295302014e-05,
      "loss": 0.4646,
      "step": 57870
    },
    {
      "epoch": 2.3152,
      "grad_norm": 2.2300760746002197,
      "learning_rate": 1.1489932885906041e-05,
      "loss": 0.5201,
      "step": 57880
    },
    {
      "epoch": 2.3156,
      "grad_norm": 3.00472354888916,
      "learning_rate": 1.1483221476510066e-05,
      "loss": 0.4842,
      "step": 57890
    },
    {
      "epoch": 2.316,
      "grad_norm": 2.70979380607605,
      "learning_rate": 1.1476510067114095e-05,
      "loss": 0.4982,
      "step": 57900
    },
    {
      "epoch": 2.3164,
      "grad_norm": 2.7271907329559326,
      "learning_rate": 1.1469798657718122e-05,
      "loss": 0.5134,
      "step": 57910
    },
    {
      "epoch": 2.3168,
      "grad_norm": 2.91174054145813,
      "learning_rate": 1.1463087248322147e-05,
      "loss": 0.5223,
      "step": 57920
    },
    {
      "epoch": 2.3172,
      "grad_norm": 2.4971506595611572,
      "learning_rate": 1.1456375838926175e-05,
      "loss": 0.5063,
      "step": 57930
    },
    {
      "epoch": 2.3176,
      "grad_norm": 3.2312564849853516,
      "learning_rate": 1.1449664429530202e-05,
      "loss": 0.5472,
      "step": 57940
    },
    {
      "epoch": 2.318,
      "grad_norm": 2.7431750297546387,
      "learning_rate": 1.1442953020134229e-05,
      "loss": 0.4519,
      "step": 57950
    },
    {
      "epoch": 2.3184,
      "grad_norm": 2.557117462158203,
      "learning_rate": 1.1436241610738256e-05,
      "loss": 0.4845,
      "step": 57960
    },
    {
      "epoch": 2.3188,
      "grad_norm": 2.964407444000244,
      "learning_rate": 1.1429530201342283e-05,
      "loss": 0.5241,
      "step": 57970
    },
    {
      "epoch": 2.3192,
      "grad_norm": 2.6283204555511475,
      "learning_rate": 1.142281879194631e-05,
      "loss": 0.4605,
      "step": 57980
    },
    {
      "epoch": 2.3196,
      "grad_norm": 3.0918993949890137,
      "learning_rate": 1.1416107382550337e-05,
      "loss": 0.5162,
      "step": 57990
    },
    {
      "epoch": 2.32,
      "grad_norm": 1.7566674947738647,
      "learning_rate": 1.1409395973154363e-05,
      "loss": 0.4257,
      "step": 58000
    },
    {
      "epoch": 2.3204000000000002,
      "grad_norm": 3.3632922172546387,
      "learning_rate": 1.140268456375839e-05,
      "loss": 0.4984,
      "step": 58010
    },
    {
      "epoch": 2.3208,
      "grad_norm": 2.8991565704345703,
      "learning_rate": 1.1395973154362415e-05,
      "loss": 0.5063,
      "step": 58020
    },
    {
      "epoch": 2.3212,
      "grad_norm": 2.7818894386291504,
      "learning_rate": 1.1389261744966444e-05,
      "loss": 0.4824,
      "step": 58030
    },
    {
      "epoch": 2.3216,
      "grad_norm": 2.4632506370544434,
      "learning_rate": 1.138255033557047e-05,
      "loss": 0.4555,
      "step": 58040
    },
    {
      "epoch": 2.322,
      "grad_norm": 2.1980209350585938,
      "learning_rate": 1.1375838926174496e-05,
      "loss": 0.4817,
      "step": 58050
    },
    {
      "epoch": 2.3224,
      "grad_norm": 2.530137300491333,
      "learning_rate": 1.1369127516778524e-05,
      "loss": 0.4464,
      "step": 58060
    },
    {
      "epoch": 2.3228,
      "grad_norm": 2.8922042846679688,
      "learning_rate": 1.1362416107382551e-05,
      "loss": 0.5618,
      "step": 58070
    },
    {
      "epoch": 2.3232,
      "grad_norm": 2.3365776538848877,
      "learning_rate": 1.1355704697986577e-05,
      "loss": 0.4514,
      "step": 58080
    },
    {
      "epoch": 2.3236,
      "grad_norm": 2.8478567600250244,
      "learning_rate": 1.1348993288590605e-05,
      "loss": 0.5306,
      "step": 58090
    },
    {
      "epoch": 2.324,
      "grad_norm": 2.3020918369293213,
      "learning_rate": 1.1342281879194632e-05,
      "loss": 0.5148,
      "step": 58100
    },
    {
      "epoch": 2.3244,
      "grad_norm": 2.058014154434204,
      "learning_rate": 1.1335570469798657e-05,
      "loss": 0.4369,
      "step": 58110
    },
    {
      "epoch": 2.3247999999999998,
      "grad_norm": 3.120988607406616,
      "learning_rate": 1.1328859060402686e-05,
      "loss": 0.4769,
      "step": 58120
    },
    {
      "epoch": 2.3252,
      "grad_norm": 1.95710027217865,
      "learning_rate": 1.1322147651006712e-05,
      "loss": 0.433,
      "step": 58130
    },
    {
      "epoch": 2.3256,
      "grad_norm": 2.9990668296813965,
      "learning_rate": 1.1315436241610738e-05,
      "loss": 0.5348,
      "step": 58140
    },
    {
      "epoch": 2.326,
      "grad_norm": 3.216684341430664,
      "learning_rate": 1.1308724832214766e-05,
      "loss": 0.5191,
      "step": 58150
    },
    {
      "epoch": 2.3264,
      "grad_norm": 2.4589712619781494,
      "learning_rate": 1.1302013422818793e-05,
      "loss": 0.4853,
      "step": 58160
    },
    {
      "epoch": 2.3268,
      "grad_norm": 2.9451372623443604,
      "learning_rate": 1.129530201342282e-05,
      "loss": 0.559,
      "step": 58170
    },
    {
      "epoch": 2.3272,
      "grad_norm": 2.3176488876342773,
      "learning_rate": 1.1288590604026847e-05,
      "loss": 0.4726,
      "step": 58180
    },
    {
      "epoch": 2.3276,
      "grad_norm": 2.945371150970459,
      "learning_rate": 1.1281879194630874e-05,
      "loss": 0.5741,
      "step": 58190
    },
    {
      "epoch": 2.328,
      "grad_norm": 2.208801031112671,
      "learning_rate": 1.12751677852349e-05,
      "loss": 0.4964,
      "step": 58200
    },
    {
      "epoch": 2.3284000000000002,
      "grad_norm": 2.437340021133423,
      "learning_rate": 1.1268456375838926e-05,
      "loss": 0.5159,
      "step": 58210
    },
    {
      "epoch": 2.3288,
      "grad_norm": 2.1923305988311768,
      "learning_rate": 1.1261744966442954e-05,
      "loss": 0.4274,
      "step": 58220
    },
    {
      "epoch": 2.3292,
      "grad_norm": 3.3724420070648193,
      "learning_rate": 1.1255033557046981e-05,
      "loss": 0.5447,
      "step": 58230
    },
    {
      "epoch": 2.3296,
      "grad_norm": 2.6986238956451416,
      "learning_rate": 1.1248322147651006e-05,
      "loss": 0.5408,
      "step": 58240
    },
    {
      "epoch": 2.33,
      "grad_norm": 2.96030592918396,
      "learning_rate": 1.1241610738255035e-05,
      "loss": 0.5212,
      "step": 58250
    },
    {
      "epoch": 2.3304,
      "grad_norm": 1.8579332828521729,
      "learning_rate": 1.1234899328859062e-05,
      "loss": 0.4688,
      "step": 58260
    },
    {
      "epoch": 2.3308,
      "grad_norm": 2.6522016525268555,
      "learning_rate": 1.1228187919463087e-05,
      "loss": 0.471,
      "step": 58270
    },
    {
      "epoch": 2.3312,
      "grad_norm": 2.7940361499786377,
      "learning_rate": 1.1221476510067115e-05,
      "loss": 0.5457,
      "step": 58280
    },
    {
      "epoch": 2.3316,
      "grad_norm": 2.4878766536712646,
      "learning_rate": 1.1214765100671142e-05,
      "loss": 0.4871,
      "step": 58290
    },
    {
      "epoch": 2.332,
      "grad_norm": 2.459960699081421,
      "learning_rate": 1.1208053691275167e-05,
      "loss": 0.5065,
      "step": 58300
    },
    {
      "epoch": 2.3324,
      "grad_norm": 2.472508192062378,
      "learning_rate": 1.1201342281879196e-05,
      "loss": 0.4813,
      "step": 58310
    },
    {
      "epoch": 2.3327999999999998,
      "grad_norm": 2.6802868843078613,
      "learning_rate": 1.1194630872483223e-05,
      "loss": 0.5124,
      "step": 58320
    },
    {
      "epoch": 2.3332,
      "grad_norm": 2.8655292987823486,
      "learning_rate": 1.1187919463087248e-05,
      "loss": 0.5319,
      "step": 58330
    },
    {
      "epoch": 2.3336,
      "grad_norm": 2.7378416061401367,
      "learning_rate": 1.1181208053691276e-05,
      "loss": 0.4983,
      "step": 58340
    },
    {
      "epoch": 2.334,
      "grad_norm": 2.510063886642456,
      "learning_rate": 1.1174496644295303e-05,
      "loss": 0.4694,
      "step": 58350
    },
    {
      "epoch": 2.3344,
      "grad_norm": 3.0928282737731934,
      "learning_rate": 1.1167785234899328e-05,
      "loss": 0.5073,
      "step": 58360
    },
    {
      "epoch": 2.3348,
      "grad_norm": 1.8982731103897095,
      "learning_rate": 1.1161073825503355e-05,
      "loss": 0.4755,
      "step": 58370
    },
    {
      "epoch": 2.3352,
      "grad_norm": 2.1622979640960693,
      "learning_rate": 1.1154362416107384e-05,
      "loss": 0.4848,
      "step": 58380
    },
    {
      "epoch": 2.3356,
      "grad_norm": 2.3342533111572266,
      "learning_rate": 1.114765100671141e-05,
      "loss": 0.4561,
      "step": 58390
    },
    {
      "epoch": 2.336,
      "grad_norm": 3.074993848800659,
      "learning_rate": 1.1140939597315436e-05,
      "loss": 0.5629,
      "step": 58400
    },
    {
      "epoch": 2.3364,
      "grad_norm": 3.019723892211914,
      "learning_rate": 1.1134228187919464e-05,
      "loss": 0.4726,
      "step": 58410
    },
    {
      "epoch": 2.3368,
      "grad_norm": 2.287360668182373,
      "learning_rate": 1.1127516778523491e-05,
      "loss": 0.5091,
      "step": 58420
    },
    {
      "epoch": 2.3372,
      "grad_norm": 2.7170181274414062,
      "learning_rate": 1.1120805369127516e-05,
      "loss": 0.5008,
      "step": 58430
    },
    {
      "epoch": 2.3376,
      "grad_norm": 3.071579933166504,
      "learning_rate": 1.1114093959731545e-05,
      "loss": 0.4845,
      "step": 58440
    },
    {
      "epoch": 2.338,
      "grad_norm": 2.073657751083374,
      "learning_rate": 1.1107382550335572e-05,
      "loss": 0.4595,
      "step": 58450
    },
    {
      "epoch": 2.3384,
      "grad_norm": 3.610285997390747,
      "learning_rate": 1.1100671140939597e-05,
      "loss": 0.5152,
      "step": 58460
    },
    {
      "epoch": 2.3388,
      "grad_norm": 2.0789592266082764,
      "learning_rate": 1.1093959731543625e-05,
      "loss": 0.533,
      "step": 58470
    },
    {
      "epoch": 2.3392,
      "grad_norm": 2.459859848022461,
      "learning_rate": 1.1087248322147652e-05,
      "loss": 0.5002,
      "step": 58480
    },
    {
      "epoch": 2.3396,
      "grad_norm": 2.4106733798980713,
      "learning_rate": 1.1080536912751677e-05,
      "loss": 0.4843,
      "step": 58490
    },
    {
      "epoch": 2.34,
      "grad_norm": 2.7324767112731934,
      "learning_rate": 1.1073825503355706e-05,
      "loss": 0.4911,
      "step": 58500
    },
    {
      "epoch": 2.3404,
      "grad_norm": 3.2445621490478516,
      "learning_rate": 1.1067114093959733e-05,
      "loss": 0.4838,
      "step": 58510
    },
    {
      "epoch": 2.3407999999999998,
      "grad_norm": 2.3678665161132812,
      "learning_rate": 1.1060402684563758e-05,
      "loss": 0.5276,
      "step": 58520
    },
    {
      "epoch": 2.3412,
      "grad_norm": 3.0396969318389893,
      "learning_rate": 1.1053691275167785e-05,
      "loss": 0.4413,
      "step": 58530
    },
    {
      "epoch": 2.3416,
      "grad_norm": 3.2409372329711914,
      "learning_rate": 1.1046979865771813e-05,
      "loss": 0.4718,
      "step": 58540
    },
    {
      "epoch": 2.342,
      "grad_norm": 2.7928061485290527,
      "learning_rate": 1.1040268456375839e-05,
      "loss": 0.5462,
      "step": 58550
    },
    {
      "epoch": 2.3424,
      "grad_norm": 1.9839372634887695,
      "learning_rate": 1.1033557046979865e-05,
      "loss": 0.4685,
      "step": 58560
    },
    {
      "epoch": 2.3428,
      "grad_norm": 2.3011505603790283,
      "learning_rate": 1.1026845637583894e-05,
      "loss": 0.4961,
      "step": 58570
    },
    {
      "epoch": 2.3432,
      "grad_norm": 2.7936248779296875,
      "learning_rate": 1.102013422818792e-05,
      "loss": 0.5116,
      "step": 58580
    },
    {
      "epoch": 2.3436,
      "grad_norm": 2.4590766429901123,
      "learning_rate": 1.1013422818791946e-05,
      "loss": 0.4428,
      "step": 58590
    },
    {
      "epoch": 2.344,
      "grad_norm": 2.6652889251708984,
      "learning_rate": 1.1006711409395975e-05,
      "loss": 0.5218,
      "step": 58600
    },
    {
      "epoch": 2.3444,
      "grad_norm": 2.7026586532592773,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 0.5359,
      "step": 58610
    },
    {
      "epoch": 2.3448,
      "grad_norm": 2.9184718132019043,
      "learning_rate": 1.0993288590604027e-05,
      "loss": 0.4821,
      "step": 58620
    },
    {
      "epoch": 2.3452,
      "grad_norm": 3.1729166507720947,
      "learning_rate": 1.0986577181208055e-05,
      "loss": 0.5019,
      "step": 58630
    },
    {
      "epoch": 2.3456,
      "grad_norm": 2.4702672958374023,
      "learning_rate": 1.0979865771812082e-05,
      "loss": 0.5054,
      "step": 58640
    },
    {
      "epoch": 2.346,
      "grad_norm": 2.153506278991699,
      "learning_rate": 1.0973154362416107e-05,
      "loss": 0.5198,
      "step": 58650
    },
    {
      "epoch": 2.3464,
      "grad_norm": 2.924522876739502,
      "learning_rate": 1.0966442953020136e-05,
      "loss": 0.4833,
      "step": 58660
    },
    {
      "epoch": 2.3468,
      "grad_norm": 2.3852763175964355,
      "learning_rate": 1.0959731543624163e-05,
      "loss": 0.4895,
      "step": 58670
    },
    {
      "epoch": 2.3472,
      "grad_norm": 3.1227622032165527,
      "learning_rate": 1.0953020134228188e-05,
      "loss": 0.5427,
      "step": 58680
    },
    {
      "epoch": 2.3476,
      "grad_norm": 2.966952323913574,
      "learning_rate": 1.0946308724832215e-05,
      "loss": 0.5299,
      "step": 58690
    },
    {
      "epoch": 2.348,
      "grad_norm": 3.0699384212493896,
      "learning_rate": 1.0939597315436243e-05,
      "loss": 0.4741,
      "step": 58700
    },
    {
      "epoch": 2.3484,
      "grad_norm": 2.443397283554077,
      "learning_rate": 1.0932885906040268e-05,
      "loss": 0.4976,
      "step": 58710
    },
    {
      "epoch": 2.3487999999999998,
      "grad_norm": 2.383606433868408,
      "learning_rate": 1.0926174496644295e-05,
      "loss": 0.5422,
      "step": 58720
    },
    {
      "epoch": 2.3492,
      "grad_norm": 2.1184823513031006,
      "learning_rate": 1.0919463087248324e-05,
      "loss": 0.5495,
      "step": 58730
    },
    {
      "epoch": 2.3496,
      "grad_norm": 2.4802956581115723,
      "learning_rate": 1.0912751677852349e-05,
      "loss": 0.5299,
      "step": 58740
    },
    {
      "epoch": 2.35,
      "grad_norm": 3.4806602001190186,
      "learning_rate": 1.0906040268456376e-05,
      "loss": 0.5614,
      "step": 58750
    },
    {
      "epoch": 2.3504,
      "grad_norm": 2.4687533378601074,
      "learning_rate": 1.0899328859060404e-05,
      "loss": 0.379,
      "step": 58760
    },
    {
      "epoch": 2.3508,
      "grad_norm": 2.8543543815612793,
      "learning_rate": 1.089261744966443e-05,
      "loss": 0.4848,
      "step": 58770
    },
    {
      "epoch": 2.3512,
      "grad_norm": 2.763047695159912,
      "learning_rate": 1.0885906040268456e-05,
      "loss": 0.5121,
      "step": 58780
    },
    {
      "epoch": 2.3516,
      "grad_norm": 3.379326820373535,
      "learning_rate": 1.0879194630872485e-05,
      "loss": 0.5435,
      "step": 58790
    },
    {
      "epoch": 2.352,
      "grad_norm": 1.9411367177963257,
      "learning_rate": 1.087248322147651e-05,
      "loss": 0.4982,
      "step": 58800
    },
    {
      "epoch": 2.3524,
      "grad_norm": 2.5685882568359375,
      "learning_rate": 1.0865771812080537e-05,
      "loss": 0.525,
      "step": 58810
    },
    {
      "epoch": 2.3528000000000002,
      "grad_norm": 2.7913801670074463,
      "learning_rate": 1.0859060402684565e-05,
      "loss": 0.5061,
      "step": 58820
    },
    {
      "epoch": 2.3532,
      "grad_norm": 2.7020833492279053,
      "learning_rate": 1.0852348993288592e-05,
      "loss": 0.4638,
      "step": 58830
    },
    {
      "epoch": 2.3536,
      "grad_norm": 2.759225368499756,
      "learning_rate": 1.0845637583892617e-05,
      "loss": 0.4064,
      "step": 58840
    },
    {
      "epoch": 2.354,
      "grad_norm": 2.660130500793457,
      "learning_rate": 1.0838926174496644e-05,
      "loss": 0.4774,
      "step": 58850
    },
    {
      "epoch": 2.3544,
      "grad_norm": 2.6716997623443604,
      "learning_rate": 1.0832214765100673e-05,
      "loss": 0.5496,
      "step": 58860
    },
    {
      "epoch": 2.3548,
      "grad_norm": 3.042015790939331,
      "learning_rate": 1.0825503355704698e-05,
      "loss": 0.5908,
      "step": 58870
    },
    {
      "epoch": 2.3552,
      "grad_norm": 2.619981527328491,
      "learning_rate": 1.0818791946308725e-05,
      "loss": 0.4484,
      "step": 58880
    },
    {
      "epoch": 2.3556,
      "grad_norm": 2.270500421524048,
      "learning_rate": 1.0812080536912753e-05,
      "loss": 0.4595,
      "step": 58890
    },
    {
      "epoch": 2.356,
      "grad_norm": 2.8772060871124268,
      "learning_rate": 1.0805369127516778e-05,
      "loss": 0.4882,
      "step": 58900
    },
    {
      "epoch": 2.3564,
      "grad_norm": 2.25325870513916,
      "learning_rate": 1.0798657718120805e-05,
      "loss": 0.5044,
      "step": 58910
    },
    {
      "epoch": 2.3568,
      "grad_norm": 2.8047547340393066,
      "learning_rate": 1.0791946308724834e-05,
      "loss": 0.4638,
      "step": 58920
    },
    {
      "epoch": 2.3572,
      "grad_norm": 2.2756552696228027,
      "learning_rate": 1.0785234899328859e-05,
      "loss": 0.5274,
      "step": 58930
    },
    {
      "epoch": 2.3576,
      "grad_norm": 2.798198699951172,
      "learning_rate": 1.0778523489932886e-05,
      "loss": 0.4535,
      "step": 58940
    },
    {
      "epoch": 2.358,
      "grad_norm": 2.522460460662842,
      "learning_rate": 1.0771812080536914e-05,
      "loss": 0.5967,
      "step": 58950
    },
    {
      "epoch": 2.3584,
      "grad_norm": 2.8788881301879883,
      "learning_rate": 1.076510067114094e-05,
      "loss": 0.4607,
      "step": 58960
    },
    {
      "epoch": 2.3588,
      "grad_norm": 3.13828706741333,
      "learning_rate": 1.0758389261744966e-05,
      "loss": 0.4615,
      "step": 58970
    },
    {
      "epoch": 2.3592,
      "grad_norm": 2.4354653358459473,
      "learning_rate": 1.0751677852348995e-05,
      "loss": 0.5366,
      "step": 58980
    },
    {
      "epoch": 2.3596,
      "grad_norm": 2.1300461292266846,
      "learning_rate": 1.074496644295302e-05,
      "loss": 0.5543,
      "step": 58990
    },
    {
      "epoch": 2.36,
      "grad_norm": 1.847882866859436,
      "learning_rate": 1.0738255033557047e-05,
      "loss": 0.4787,
      "step": 59000
    },
    {
      "epoch": 2.3604,
      "grad_norm": 2.1562302112579346,
      "learning_rate": 1.0731543624161074e-05,
      "loss": 0.5197,
      "step": 59010
    },
    {
      "epoch": 2.3608000000000002,
      "grad_norm": 2.906277894973755,
      "learning_rate": 1.07248322147651e-05,
      "loss": 0.4846,
      "step": 59020
    },
    {
      "epoch": 2.3612,
      "grad_norm": 2.3240535259246826,
      "learning_rate": 1.0718120805369128e-05,
      "loss": 0.5249,
      "step": 59030
    },
    {
      "epoch": 2.3616,
      "grad_norm": 2.0211946964263916,
      "learning_rate": 1.0711409395973154e-05,
      "loss": 0.4607,
      "step": 59040
    },
    {
      "epoch": 2.362,
      "grad_norm": 2.6005680561065674,
      "learning_rate": 1.0704697986577181e-05,
      "loss": 0.4957,
      "step": 59050
    },
    {
      "epoch": 2.3624,
      "grad_norm": 2.9964895248413086,
      "learning_rate": 1.0697986577181208e-05,
      "loss": 0.4352,
      "step": 59060
    },
    {
      "epoch": 2.3628,
      "grad_norm": 2.434661388397217,
      "learning_rate": 1.0691275167785235e-05,
      "loss": 0.5201,
      "step": 59070
    },
    {
      "epoch": 2.3632,
      "grad_norm": 2.361452102661133,
      "learning_rate": 1.0684563758389264e-05,
      "loss": 0.4623,
      "step": 59080
    },
    {
      "epoch": 2.3636,
      "grad_norm": 2.6993818283081055,
      "learning_rate": 1.0677852348993289e-05,
      "loss": 0.5047,
      "step": 59090
    },
    {
      "epoch": 2.364,
      "grad_norm": 2.897272825241089,
      "learning_rate": 1.0671140939597316e-05,
      "loss": 0.5436,
      "step": 59100
    },
    {
      "epoch": 2.3644,
      "grad_norm": 2.9270317554473877,
      "learning_rate": 1.0664429530201344e-05,
      "loss": 0.5175,
      "step": 59110
    },
    {
      "epoch": 2.3648,
      "grad_norm": 2.7706260681152344,
      "learning_rate": 1.065771812080537e-05,
      "loss": 0.5195,
      "step": 59120
    },
    {
      "epoch": 2.3652,
      "grad_norm": 3.018831491470337,
      "learning_rate": 1.0651006711409396e-05,
      "loss": 0.4749,
      "step": 59130
    },
    {
      "epoch": 2.3656,
      "grad_norm": 1.8672477006912231,
      "learning_rate": 1.0644295302013425e-05,
      "loss": 0.4416,
      "step": 59140
    },
    {
      "epoch": 2.366,
      "grad_norm": 2.6446471214294434,
      "learning_rate": 1.063758389261745e-05,
      "loss": 0.4997,
      "step": 59150
    },
    {
      "epoch": 2.3664,
      "grad_norm": 2.667368173599243,
      "learning_rate": 1.0630872483221477e-05,
      "loss": 0.5608,
      "step": 59160
    },
    {
      "epoch": 2.3668,
      "grad_norm": 2.9609110355377197,
      "learning_rate": 1.0624161073825503e-05,
      "loss": 0.4837,
      "step": 59170
    },
    {
      "epoch": 2.3672,
      "grad_norm": 2.2889609336853027,
      "learning_rate": 1.061744966442953e-05,
      "loss": 0.4966,
      "step": 59180
    },
    {
      "epoch": 2.3676,
      "grad_norm": 2.6756277084350586,
      "learning_rate": 1.0610738255033557e-05,
      "loss": 0.5415,
      "step": 59190
    },
    {
      "epoch": 2.368,
      "grad_norm": 2.4045138359069824,
      "learning_rate": 1.0604026845637584e-05,
      "loss": 0.4377,
      "step": 59200
    },
    {
      "epoch": 2.3684,
      "grad_norm": 2.886075019836426,
      "learning_rate": 1.0597315436241611e-05,
      "loss": 0.4902,
      "step": 59210
    },
    {
      "epoch": 2.3688000000000002,
      "grad_norm": 2.6525702476501465,
      "learning_rate": 1.0590604026845638e-05,
      "loss": 0.4977,
      "step": 59220
    },
    {
      "epoch": 2.3692,
      "grad_norm": 2.496523141860962,
      "learning_rate": 1.0583892617449665e-05,
      "loss": 0.5334,
      "step": 59230
    },
    {
      "epoch": 2.3696,
      "grad_norm": 2.958561658859253,
      "learning_rate": 1.0577181208053691e-05,
      "loss": 0.5417,
      "step": 59240
    },
    {
      "epoch": 2.37,
      "grad_norm": 3.3345372676849365,
      "learning_rate": 1.0570469798657718e-05,
      "loss": 0.481,
      "step": 59250
    },
    {
      "epoch": 2.3704,
      "grad_norm": 2.8957858085632324,
      "learning_rate": 1.0563758389261745e-05,
      "loss": 0.4424,
      "step": 59260
    },
    {
      "epoch": 2.3708,
      "grad_norm": 2.796437978744507,
      "learning_rate": 1.0557046979865772e-05,
      "loss": 0.5251,
      "step": 59270
    },
    {
      "epoch": 2.3712,
      "grad_norm": 2.5440280437469482,
      "learning_rate": 1.0550335570469799e-05,
      "loss": 0.531,
      "step": 59280
    },
    {
      "epoch": 2.3716,
      "grad_norm": 2.073096752166748,
      "learning_rate": 1.0543624161073826e-05,
      "loss": 0.4678,
      "step": 59290
    },
    {
      "epoch": 2.372,
      "grad_norm": 2.7206404209136963,
      "learning_rate": 1.0536912751677854e-05,
      "loss": 0.5213,
      "step": 59300
    },
    {
      "epoch": 2.3724,
      "grad_norm": 3.328611135482788,
      "learning_rate": 1.053020134228188e-05,
      "loss": 0.5521,
      "step": 59310
    },
    {
      "epoch": 2.3728,
      "grad_norm": 2.178957939147949,
      "learning_rate": 1.0523489932885906e-05,
      "loss": 0.4859,
      "step": 59320
    },
    {
      "epoch": 2.3731999999999998,
      "grad_norm": 2.69510555267334,
      "learning_rate": 1.0516778523489933e-05,
      "loss": 0.4573,
      "step": 59330
    },
    {
      "epoch": 2.3736,
      "grad_norm": 2.897862672805786,
      "learning_rate": 1.051006711409396e-05,
      "loss": 0.6097,
      "step": 59340
    },
    {
      "epoch": 2.374,
      "grad_norm": 2.0638773441314697,
      "learning_rate": 1.0503355704697987e-05,
      "loss": 0.4048,
      "step": 59350
    },
    {
      "epoch": 2.3744,
      "grad_norm": 3.1840615272521973,
      "learning_rate": 1.0496644295302014e-05,
      "loss": 0.4815,
      "step": 59360
    },
    {
      "epoch": 2.3748,
      "grad_norm": 2.123439311981201,
      "learning_rate": 1.048993288590604e-05,
      "loss": 0.4417,
      "step": 59370
    },
    {
      "epoch": 2.3752,
      "grad_norm": 3.3725318908691406,
      "learning_rate": 1.0483221476510067e-05,
      "loss": 0.4493,
      "step": 59380
    },
    {
      "epoch": 2.3756,
      "grad_norm": 2.57671856880188,
      "learning_rate": 1.0476510067114094e-05,
      "loss": 0.5668,
      "step": 59390
    },
    {
      "epoch": 2.376,
      "grad_norm": 3.442404270172119,
      "learning_rate": 1.0469798657718121e-05,
      "loss": 0.4386,
      "step": 59400
    },
    {
      "epoch": 2.3764,
      "grad_norm": 2.9378652572631836,
      "learning_rate": 1.0463087248322148e-05,
      "loss": 0.5199,
      "step": 59410
    },
    {
      "epoch": 2.3768000000000002,
      "grad_norm": 2.862678289413452,
      "learning_rate": 1.0456375838926175e-05,
      "loss": 0.4614,
      "step": 59420
    },
    {
      "epoch": 2.3772,
      "grad_norm": 2.664740800857544,
      "learning_rate": 1.0449664429530202e-05,
      "loss": 0.4618,
      "step": 59430
    },
    {
      "epoch": 2.3776,
      "grad_norm": 2.744469165802002,
      "learning_rate": 1.0442953020134229e-05,
      "loss": 0.5494,
      "step": 59440
    },
    {
      "epoch": 2.378,
      "grad_norm": 2.288259744644165,
      "learning_rate": 1.0436241610738255e-05,
      "loss": 0.5328,
      "step": 59450
    },
    {
      "epoch": 2.3784,
      "grad_norm": 2.0800981521606445,
      "learning_rate": 1.0429530201342282e-05,
      "loss": 0.4903,
      "step": 59460
    },
    {
      "epoch": 2.3788,
      "grad_norm": 2.864990711212158,
      "learning_rate": 1.0422818791946309e-05,
      "loss": 0.5472,
      "step": 59470
    },
    {
      "epoch": 2.3792,
      "grad_norm": 2.401827335357666,
      "learning_rate": 1.0416107382550336e-05,
      "loss": 0.5121,
      "step": 59480
    },
    {
      "epoch": 2.3796,
      "grad_norm": 2.2891933917999268,
      "learning_rate": 1.0409395973154363e-05,
      "loss": 0.4911,
      "step": 59490
    },
    {
      "epoch": 2.38,
      "grad_norm": 3.285789728164673,
      "learning_rate": 1.040268456375839e-05,
      "loss": 0.4736,
      "step": 59500
    },
    {
      "epoch": 2.3804,
      "grad_norm": 2.553178548812866,
      "learning_rate": 1.0395973154362416e-05,
      "loss": 0.5538,
      "step": 59510
    },
    {
      "epoch": 2.3808,
      "grad_norm": 3.152290105819702,
      "learning_rate": 1.0389261744966443e-05,
      "loss": 0.5755,
      "step": 59520
    },
    {
      "epoch": 2.3811999999999998,
      "grad_norm": 1.9131734371185303,
      "learning_rate": 1.038255033557047e-05,
      "loss": 0.5445,
      "step": 59530
    },
    {
      "epoch": 2.3816,
      "grad_norm": 2.901195526123047,
      "learning_rate": 1.0375838926174497e-05,
      "loss": 0.5183,
      "step": 59540
    },
    {
      "epoch": 2.382,
      "grad_norm": 2.686659574508667,
      "learning_rate": 1.0369127516778524e-05,
      "loss": 0.4893,
      "step": 59550
    },
    {
      "epoch": 2.3824,
      "grad_norm": 2.2911665439605713,
      "learning_rate": 1.036241610738255e-05,
      "loss": 0.5219,
      "step": 59560
    },
    {
      "epoch": 2.3828,
      "grad_norm": 2.8084218502044678,
      "learning_rate": 1.0355704697986578e-05,
      "loss": 0.5537,
      "step": 59570
    },
    {
      "epoch": 2.3832,
      "grad_norm": 2.3636348247528076,
      "learning_rate": 1.0348993288590604e-05,
      "loss": 0.5624,
      "step": 59580
    },
    {
      "epoch": 2.3836,
      "grad_norm": 2.424349308013916,
      "learning_rate": 1.0342281879194631e-05,
      "loss": 0.5229,
      "step": 59590
    },
    {
      "epoch": 2.384,
      "grad_norm": 2.927337646484375,
      "learning_rate": 1.0335570469798658e-05,
      "loss": 0.525,
      "step": 59600
    },
    {
      "epoch": 2.3844,
      "grad_norm": 2.7199556827545166,
      "learning_rate": 1.0328859060402685e-05,
      "loss": 0.5341,
      "step": 59610
    },
    {
      "epoch": 2.3848,
      "grad_norm": 3.0093834400177,
      "learning_rate": 1.0322147651006712e-05,
      "loss": 0.4506,
      "step": 59620
    },
    {
      "epoch": 2.3852,
      "grad_norm": 3.6049511432647705,
      "learning_rate": 1.0315436241610739e-05,
      "loss": 0.5373,
      "step": 59630
    },
    {
      "epoch": 2.3856,
      "grad_norm": 2.7381701469421387,
      "learning_rate": 1.0308724832214766e-05,
      "loss": 0.468,
      "step": 59640
    },
    {
      "epoch": 2.386,
      "grad_norm": 2.0674633979797363,
      "learning_rate": 1.0302013422818792e-05,
      "loss": 0.4618,
      "step": 59650
    },
    {
      "epoch": 2.3864,
      "grad_norm": 2.527522325515747,
      "learning_rate": 1.029530201342282e-05,
      "loss": 0.49,
      "step": 59660
    },
    {
      "epoch": 2.3868,
      "grad_norm": 2.455479145050049,
      "learning_rate": 1.0288590604026846e-05,
      "loss": 0.5966,
      "step": 59670
    },
    {
      "epoch": 2.3872,
      "grad_norm": 2.9926486015319824,
      "learning_rate": 1.0281879194630873e-05,
      "loss": 0.5284,
      "step": 59680
    },
    {
      "epoch": 2.3876,
      "grad_norm": 2.6057257652282715,
      "learning_rate": 1.02751677852349e-05,
      "loss": 0.5628,
      "step": 59690
    },
    {
      "epoch": 2.388,
      "grad_norm": 2.6203298568725586,
      "learning_rate": 1.0268456375838927e-05,
      "loss": 0.594,
      "step": 59700
    },
    {
      "epoch": 2.3884,
      "grad_norm": 3.525430202484131,
      "learning_rate": 1.0261744966442954e-05,
      "loss": 0.4813,
      "step": 59710
    },
    {
      "epoch": 2.3888,
      "grad_norm": 2.726392984390259,
      "learning_rate": 1.025503355704698e-05,
      "loss": 0.5266,
      "step": 59720
    },
    {
      "epoch": 2.3891999999999998,
      "grad_norm": 2.3525350093841553,
      "learning_rate": 1.0248322147651007e-05,
      "loss": 0.5174,
      "step": 59730
    },
    {
      "epoch": 2.3896,
      "grad_norm": 3.10542368888855,
      "learning_rate": 1.0241610738255034e-05,
      "loss": 0.4998,
      "step": 59740
    },
    {
      "epoch": 2.39,
      "grad_norm": 2.7578611373901367,
      "learning_rate": 1.0234899328859061e-05,
      "loss": 0.5789,
      "step": 59750
    },
    {
      "epoch": 2.3904,
      "grad_norm": 1.8787904977798462,
      "learning_rate": 1.0228187919463088e-05,
      "loss": 0.5091,
      "step": 59760
    },
    {
      "epoch": 2.3908,
      "grad_norm": 2.866473913192749,
      "learning_rate": 1.0221476510067115e-05,
      "loss": 0.5301,
      "step": 59770
    },
    {
      "epoch": 2.3912,
      "grad_norm": 2.672210454940796,
      "learning_rate": 1.0214765100671142e-05,
      "loss": 0.4651,
      "step": 59780
    },
    {
      "epoch": 2.3916,
      "grad_norm": 3.2196240425109863,
      "learning_rate": 1.0208053691275168e-05,
      "loss": 0.5242,
      "step": 59790
    },
    {
      "epoch": 2.392,
      "grad_norm": 2.034015417098999,
      "learning_rate": 1.0201342281879195e-05,
      "loss": 0.5236,
      "step": 59800
    },
    {
      "epoch": 2.3924,
      "grad_norm": 2.0692036151885986,
      "learning_rate": 1.0194630872483222e-05,
      "loss": 0.5406,
      "step": 59810
    },
    {
      "epoch": 2.3928,
      "grad_norm": 2.7039830684661865,
      "learning_rate": 1.0187919463087249e-05,
      "loss": 0.5171,
      "step": 59820
    },
    {
      "epoch": 2.3932,
      "grad_norm": 3.0046257972717285,
      "learning_rate": 1.0181208053691276e-05,
      "loss": 0.559,
      "step": 59830
    },
    {
      "epoch": 2.3936,
      "grad_norm": 2.416189193725586,
      "learning_rate": 1.0174496644295303e-05,
      "loss": 0.4457,
      "step": 59840
    },
    {
      "epoch": 2.394,
      "grad_norm": 2.3762667179107666,
      "learning_rate": 1.016778523489933e-05,
      "loss": 0.5095,
      "step": 59850
    },
    {
      "epoch": 2.3944,
      "grad_norm": 2.6524925231933594,
      "learning_rate": 1.0161073825503356e-05,
      "loss": 0.46,
      "step": 59860
    },
    {
      "epoch": 2.3948,
      "grad_norm": 1.6660041809082031,
      "learning_rate": 1.0154362416107383e-05,
      "loss": 0.4165,
      "step": 59870
    },
    {
      "epoch": 2.3952,
      "grad_norm": 2.8176016807556152,
      "learning_rate": 1.014765100671141e-05,
      "loss": 0.4649,
      "step": 59880
    },
    {
      "epoch": 2.3956,
      "grad_norm": 2.724160671234131,
      "learning_rate": 1.0140939597315437e-05,
      "loss": 0.4362,
      "step": 59890
    },
    {
      "epoch": 2.396,
      "grad_norm": 3.2096078395843506,
      "learning_rate": 1.0134228187919464e-05,
      "loss": 0.4718,
      "step": 59900
    },
    {
      "epoch": 2.3964,
      "grad_norm": 2.8875792026519775,
      "learning_rate": 1.012751677852349e-05,
      "loss": 0.4697,
      "step": 59910
    },
    {
      "epoch": 2.3968,
      "grad_norm": 1.4306620359420776,
      "learning_rate": 1.0120805369127517e-05,
      "loss": 0.4448,
      "step": 59920
    },
    {
      "epoch": 2.3971999999999998,
      "grad_norm": 3.1056647300720215,
      "learning_rate": 1.0114093959731544e-05,
      "loss": 0.4552,
      "step": 59930
    },
    {
      "epoch": 2.3976,
      "grad_norm": 2.88836932182312,
      "learning_rate": 1.0107382550335571e-05,
      "loss": 0.4792,
      "step": 59940
    },
    {
      "epoch": 2.398,
      "grad_norm": 2.2838797569274902,
      "learning_rate": 1.0100671140939598e-05,
      "loss": 0.479,
      "step": 59950
    },
    {
      "epoch": 2.3984,
      "grad_norm": 2.858707904815674,
      "learning_rate": 1.0093959731543625e-05,
      "loss": 0.4478,
      "step": 59960
    },
    {
      "epoch": 2.3988,
      "grad_norm": 2.4244537353515625,
      "learning_rate": 1.0087248322147652e-05,
      "loss": 0.5056,
      "step": 59970
    },
    {
      "epoch": 2.3992,
      "grad_norm": 2.942333221435547,
      "learning_rate": 1.0080536912751679e-05,
      "loss": 0.4844,
      "step": 59980
    },
    {
      "epoch": 2.3996,
      "grad_norm": 2.9878478050231934,
      "learning_rate": 1.0073825503355705e-05,
      "loss": 0.5327,
      "step": 59990
    },
    {
      "epoch": 2.4,
      "grad_norm": 2.384176731109619,
      "learning_rate": 1.006711409395973e-05,
      "loss": 0.4798,
      "step": 60000
    },
    {
      "epoch": 2.4004,
      "grad_norm": 2.7221734523773193,
      "learning_rate": 1.0060402684563759e-05,
      "loss": 0.5752,
      "step": 60010
    },
    {
      "epoch": 2.4008,
      "grad_norm": 2.78037166595459,
      "learning_rate": 1.0053691275167786e-05,
      "loss": 0.4513,
      "step": 60020
    },
    {
      "epoch": 2.4012000000000002,
      "grad_norm": 2.1703295707702637,
      "learning_rate": 1.0046979865771811e-05,
      "loss": 0.508,
      "step": 60030
    },
    {
      "epoch": 2.4016,
      "grad_norm": 2.341083288192749,
      "learning_rate": 1.004026845637584e-05,
      "loss": 0.5097,
      "step": 60040
    },
    {
      "epoch": 2.402,
      "grad_norm": 3.1369717121124268,
      "learning_rate": 1.0033557046979867e-05,
      "loss": 0.469,
      "step": 60050
    },
    {
      "epoch": 2.4024,
      "grad_norm": 2.6829450130462646,
      "learning_rate": 1.0026845637583893e-05,
      "loss": 0.4708,
      "step": 60060
    },
    {
      "epoch": 2.4028,
      "grad_norm": 3.3159029483795166,
      "learning_rate": 1.002013422818792e-05,
      "loss": 0.5257,
      "step": 60070
    },
    {
      "epoch": 2.4032,
      "grad_norm": 2.5608513355255127,
      "learning_rate": 1.0013422818791947e-05,
      "loss": 0.4932,
      "step": 60080
    },
    {
      "epoch": 2.4036,
      "grad_norm": 3.298168659210205,
      "learning_rate": 1.0006711409395974e-05,
      "loss": 0.5198,
      "step": 60090
    },
    {
      "epoch": 2.404,
      "grad_norm": 2.58596134185791,
      "learning_rate": 1e-05,
      "loss": 0.4015,
      "step": 60100
    },
    {
      "epoch": 2.4044,
      "grad_norm": 3.142395496368408,
      "learning_rate": 9.993288590604028e-06,
      "loss": 0.4291,
      "step": 60110
    },
    {
      "epoch": 2.4048,
      "grad_norm": 2.020188331604004,
      "learning_rate": 9.986577181208055e-06,
      "loss": 0.3852,
      "step": 60120
    },
    {
      "epoch": 2.4052,
      "grad_norm": 2.3941075801849365,
      "learning_rate": 9.979865771812081e-06,
      "loss": 0.4699,
      "step": 60130
    },
    {
      "epoch": 2.4056,
      "grad_norm": 2.635852098464966,
      "learning_rate": 9.973154362416108e-06,
      "loss": 0.6003,
      "step": 60140
    },
    {
      "epoch": 2.406,
      "grad_norm": 3.011704206466675,
      "learning_rate": 9.966442953020135e-06,
      "loss": 0.4864,
      "step": 60150
    },
    {
      "epoch": 2.4064,
      "grad_norm": 2.8095903396606445,
      "learning_rate": 9.95973154362416e-06,
      "loss": 0.4511,
      "step": 60160
    },
    {
      "epoch": 2.4068,
      "grad_norm": 3.8414127826690674,
      "learning_rate": 9.953020134228189e-06,
      "loss": 0.4879,
      "step": 60170
    },
    {
      "epoch": 2.4072,
      "grad_norm": 1.9864081144332886,
      "learning_rate": 9.946308724832216e-06,
      "loss": 0.4726,
      "step": 60180
    },
    {
      "epoch": 2.4076,
      "grad_norm": 3.037993907928467,
      "learning_rate": 9.93959731543624e-06,
      "loss": 0.4352,
      "step": 60190
    },
    {
      "epoch": 2.408,
      "grad_norm": 2.453946113586426,
      "learning_rate": 9.93288590604027e-06,
      "loss": 0.5191,
      "step": 60200
    },
    {
      "epoch": 2.4084,
      "grad_norm": 2.35322642326355,
      "learning_rate": 9.926174496644296e-06,
      "loss": 0.4604,
      "step": 60210
    },
    {
      "epoch": 2.4088,
      "grad_norm": 1.9268770217895508,
      "learning_rate": 9.919463087248321e-06,
      "loss": 0.4629,
      "step": 60220
    },
    {
      "epoch": 2.4092000000000002,
      "grad_norm": 3.249279737472534,
      "learning_rate": 9.91275167785235e-06,
      "loss": 0.4966,
      "step": 60230
    },
    {
      "epoch": 2.4096,
      "grad_norm": 2.3813741207122803,
      "learning_rate": 9.906040268456377e-06,
      "loss": 0.4723,
      "step": 60240
    },
    {
      "epoch": 2.41,
      "grad_norm": 2.937211751937866,
      "learning_rate": 9.899328859060402e-06,
      "loss": 0.5198,
      "step": 60250
    },
    {
      "epoch": 2.4104,
      "grad_norm": 2.747697114944458,
      "learning_rate": 9.89261744966443e-06,
      "loss": 0.5262,
      "step": 60260
    },
    {
      "epoch": 2.4108,
      "grad_norm": 2.536306381225586,
      "learning_rate": 9.885906040268457e-06,
      "loss": 0.4536,
      "step": 60270
    },
    {
      "epoch": 2.4112,
      "grad_norm": 2.486809492111206,
      "learning_rate": 9.879194630872484e-06,
      "loss": 0.5042,
      "step": 60280
    },
    {
      "epoch": 2.4116,
      "grad_norm": 2.669306516647339,
      "learning_rate": 9.872483221476511e-06,
      "loss": 0.5391,
      "step": 60290
    },
    {
      "epoch": 2.412,
      "grad_norm": 2.273123264312744,
      "learning_rate": 9.865771812080538e-06,
      "loss": 0.4861,
      "step": 60300
    },
    {
      "epoch": 2.4124,
      "grad_norm": 3.166712522506714,
      "learning_rate": 9.859060402684565e-06,
      "loss": 0.5817,
      "step": 60310
    },
    {
      "epoch": 2.4128,
      "grad_norm": 2.4094607830047607,
      "learning_rate": 9.85234899328859e-06,
      "loss": 0.4304,
      "step": 60320
    },
    {
      "epoch": 2.4132,
      "grad_norm": 2.1163549423217773,
      "learning_rate": 9.845637583892618e-06,
      "loss": 0.5659,
      "step": 60330
    },
    {
      "epoch": 2.4136,
      "grad_norm": 2.980376958847046,
      "learning_rate": 9.838926174496645e-06,
      "loss": 0.5301,
      "step": 60340
    },
    {
      "epoch": 2.414,
      "grad_norm": 2.6714367866516113,
      "learning_rate": 9.83221476510067e-06,
      "loss": 0.5145,
      "step": 60350
    },
    {
      "epoch": 2.4144,
      "grad_norm": 2.136761426925659,
      "learning_rate": 9.825503355704699e-06,
      "loss": 0.4973,
      "step": 60360
    },
    {
      "epoch": 2.4148,
      "grad_norm": 2.1950836181640625,
      "learning_rate": 9.818791946308726e-06,
      "loss": 0.4869,
      "step": 60370
    },
    {
      "epoch": 2.4152,
      "grad_norm": 2.8113558292388916,
      "learning_rate": 9.812080536912751e-06,
      "loss": 0.5756,
      "step": 60380
    },
    {
      "epoch": 2.4156,
      "grad_norm": 3.4056224822998047,
      "learning_rate": 9.80536912751678e-06,
      "loss": 0.5403,
      "step": 60390
    },
    {
      "epoch": 2.416,
      "grad_norm": 2.4332873821258545,
      "learning_rate": 9.798657718120806e-06,
      "loss": 0.4367,
      "step": 60400
    },
    {
      "epoch": 2.4164,
      "grad_norm": 2.421267032623291,
      "learning_rate": 9.791946308724832e-06,
      "loss": 0.5443,
      "step": 60410
    },
    {
      "epoch": 2.4168,
      "grad_norm": 2.3399062156677246,
      "learning_rate": 9.78523489932886e-06,
      "loss": 0.4339,
      "step": 60420
    },
    {
      "epoch": 2.4172000000000002,
      "grad_norm": 2.1492624282836914,
      "learning_rate": 9.778523489932887e-06,
      "loss": 0.5942,
      "step": 60430
    },
    {
      "epoch": 2.4176,
      "grad_norm": 2.864495038986206,
      "learning_rate": 9.771812080536912e-06,
      "loss": 0.5125,
      "step": 60440
    },
    {
      "epoch": 2.418,
      "grad_norm": 2.648146390914917,
      "learning_rate": 9.76510067114094e-06,
      "loss": 0.5473,
      "step": 60450
    },
    {
      "epoch": 2.4184,
      "grad_norm": 2.1886179447174072,
      "learning_rate": 9.758389261744968e-06,
      "loss": 0.4567,
      "step": 60460
    },
    {
      "epoch": 2.4188,
      "grad_norm": 2.4058570861816406,
      "learning_rate": 9.751677852348993e-06,
      "loss": 0.5326,
      "step": 60470
    },
    {
      "epoch": 2.4192,
      "grad_norm": 3.3993537425994873,
      "learning_rate": 9.74496644295302e-06,
      "loss": 0.4997,
      "step": 60480
    },
    {
      "epoch": 2.4196,
      "grad_norm": 2.289336681365967,
      "learning_rate": 9.738255033557048e-06,
      "loss": 0.4507,
      "step": 60490
    },
    {
      "epoch": 2.42,
      "grad_norm": 2.8751604557037354,
      "learning_rate": 9.731543624161075e-06,
      "loss": 0.5527,
      "step": 60500
    },
    {
      "epoch": 2.4204,
      "grad_norm": 2.494508981704712,
      "learning_rate": 9.7248322147651e-06,
      "loss": 0.5126,
      "step": 60510
    },
    {
      "epoch": 2.4208,
      "grad_norm": 2.016207695007324,
      "learning_rate": 9.718120805369129e-06,
      "loss": 0.428,
      "step": 60520
    },
    {
      "epoch": 2.4212,
      "grad_norm": 2.3932042121887207,
      "learning_rate": 9.711409395973155e-06,
      "loss": 0.463,
      "step": 60530
    },
    {
      "epoch": 2.4215999999999998,
      "grad_norm": 2.5024142265319824,
      "learning_rate": 9.70469798657718e-06,
      "loss": 0.4954,
      "step": 60540
    },
    {
      "epoch": 2.422,
      "grad_norm": 2.5243959426879883,
      "learning_rate": 9.69798657718121e-06,
      "loss": 0.5039,
      "step": 60550
    },
    {
      "epoch": 2.4224,
      "grad_norm": 2.301464557647705,
      "learning_rate": 9.691275167785236e-06,
      "loss": 0.5244,
      "step": 60560
    },
    {
      "epoch": 2.4228,
      "grad_norm": 2.402061700820923,
      "learning_rate": 9.684563758389261e-06,
      "loss": 0.4922,
      "step": 60570
    },
    {
      "epoch": 2.4232,
      "grad_norm": 1.7215971946716309,
      "learning_rate": 9.67785234899329e-06,
      "loss": 0.4898,
      "step": 60580
    },
    {
      "epoch": 2.4236,
      "grad_norm": 2.858504295349121,
      "learning_rate": 9.671140939597317e-06,
      "loss": 0.5363,
      "step": 60590
    },
    {
      "epoch": 2.424,
      "grad_norm": 2.288999319076538,
      "learning_rate": 9.664429530201342e-06,
      "loss": 0.4684,
      "step": 60600
    },
    {
      "epoch": 2.4244,
      "grad_norm": 2.1221742630004883,
      "learning_rate": 9.65771812080537e-06,
      "loss": 0.4301,
      "step": 60610
    },
    {
      "epoch": 2.4248,
      "grad_norm": 2.4564168453216553,
      "learning_rate": 9.651006711409397e-06,
      "loss": 0.512,
      "step": 60620
    },
    {
      "epoch": 2.4252000000000002,
      "grad_norm": 2.628225326538086,
      "learning_rate": 9.644295302013422e-06,
      "loss": 0.5961,
      "step": 60630
    },
    {
      "epoch": 2.4256,
      "grad_norm": 2.1062543392181396,
      "learning_rate": 9.637583892617451e-06,
      "loss": 0.491,
      "step": 60640
    },
    {
      "epoch": 2.426,
      "grad_norm": 2.096306085586548,
      "learning_rate": 9.630872483221478e-06,
      "loss": 0.4731,
      "step": 60650
    },
    {
      "epoch": 2.4264,
      "grad_norm": 1.8023717403411865,
      "learning_rate": 9.624161073825503e-06,
      "loss": 0.4327,
      "step": 60660
    },
    {
      "epoch": 2.4268,
      "grad_norm": 3.7009150981903076,
      "learning_rate": 9.61744966442953e-06,
      "loss": 0.5041,
      "step": 60670
    },
    {
      "epoch": 2.4272,
      "grad_norm": 2.4755921363830566,
      "learning_rate": 9.610738255033558e-06,
      "loss": 0.4622,
      "step": 60680
    },
    {
      "epoch": 2.4276,
      "grad_norm": 2.116010904312134,
      "learning_rate": 9.604026845637583e-06,
      "loss": 0.5169,
      "step": 60690
    },
    {
      "epoch": 2.428,
      "grad_norm": 2.6520299911499023,
      "learning_rate": 9.59731543624161e-06,
      "loss": 0.5094,
      "step": 60700
    },
    {
      "epoch": 2.4284,
      "grad_norm": 2.465139865875244,
      "learning_rate": 9.590604026845639e-06,
      "loss": 0.4558,
      "step": 60710
    },
    {
      "epoch": 2.4288,
      "grad_norm": 2.600207805633545,
      "learning_rate": 9.583892617449666e-06,
      "loss": 0.4836,
      "step": 60720
    },
    {
      "epoch": 2.4292,
      "grad_norm": 2.684965133666992,
      "learning_rate": 9.577181208053691e-06,
      "loss": 0.549,
      "step": 60730
    },
    {
      "epoch": 2.4295999999999998,
      "grad_norm": 2.16719126701355,
      "learning_rate": 9.57046979865772e-06,
      "loss": 0.5207,
      "step": 60740
    },
    {
      "epoch": 2.43,
      "grad_norm": 2.339707612991333,
      "learning_rate": 9.563758389261746e-06,
      "loss": 0.5135,
      "step": 60750
    },
    {
      "epoch": 2.4304,
      "grad_norm": 2.728135585784912,
      "learning_rate": 9.557046979865771e-06,
      "loss": 0.5932,
      "step": 60760
    },
    {
      "epoch": 2.4308,
      "grad_norm": 2.425441265106201,
      "learning_rate": 9.5503355704698e-06,
      "loss": 0.4297,
      "step": 60770
    },
    {
      "epoch": 2.4312,
      "grad_norm": 2.651693105697632,
      "learning_rate": 9.543624161073827e-06,
      "loss": 0.4911,
      "step": 60780
    },
    {
      "epoch": 2.4316,
      "grad_norm": 2.7187225818634033,
      "learning_rate": 9.536912751677852e-06,
      "loss": 0.4228,
      "step": 60790
    },
    {
      "epoch": 2.432,
      "grad_norm": 3.1111669540405273,
      "learning_rate": 9.53020134228188e-06,
      "loss": 0.5229,
      "step": 60800
    },
    {
      "epoch": 2.4324,
      "grad_norm": 2.867526054382324,
      "learning_rate": 9.523489932885907e-06,
      "loss": 0.4657,
      "step": 60810
    },
    {
      "epoch": 2.4328,
      "grad_norm": 2.766771078109741,
      "learning_rate": 9.516778523489933e-06,
      "loss": 0.5439,
      "step": 60820
    },
    {
      "epoch": 2.4332,
      "grad_norm": 3.099569797515869,
      "learning_rate": 9.51006711409396e-06,
      "loss": 0.5768,
      "step": 60830
    },
    {
      "epoch": 2.4336,
      "grad_norm": 2.9910130500793457,
      "learning_rate": 9.503355704697988e-06,
      "loss": 0.5402,
      "step": 60840
    },
    {
      "epoch": 2.434,
      "grad_norm": 3.136528968811035,
      "learning_rate": 9.496644295302013e-06,
      "loss": 0.5238,
      "step": 60850
    },
    {
      "epoch": 2.4344,
      "grad_norm": 2.774008274078369,
      "learning_rate": 9.48993288590604e-06,
      "loss": 0.5567,
      "step": 60860
    },
    {
      "epoch": 2.4348,
      "grad_norm": 2.3934402465820312,
      "learning_rate": 9.483221476510069e-06,
      "loss": 0.4959,
      "step": 60870
    },
    {
      "epoch": 2.4352,
      "grad_norm": 3.349851608276367,
      "learning_rate": 9.476510067114094e-06,
      "loss": 0.5032,
      "step": 60880
    },
    {
      "epoch": 2.4356,
      "grad_norm": 2.408870220184326,
      "learning_rate": 9.46979865771812e-06,
      "loss": 0.5517,
      "step": 60890
    },
    {
      "epoch": 2.436,
      "grad_norm": 3.0934276580810547,
      "learning_rate": 9.463087248322149e-06,
      "loss": 0.5214,
      "step": 60900
    },
    {
      "epoch": 2.4364,
      "grad_norm": 2.4025704860687256,
      "learning_rate": 9.456375838926174e-06,
      "loss": 0.4727,
      "step": 60910
    },
    {
      "epoch": 2.4368,
      "grad_norm": 3.375152349472046,
      "learning_rate": 9.449664429530201e-06,
      "loss": 0.5759,
      "step": 60920
    },
    {
      "epoch": 2.4372,
      "grad_norm": 2.546456813812256,
      "learning_rate": 9.44295302013423e-06,
      "loss": 0.4935,
      "step": 60930
    },
    {
      "epoch": 2.4375999999999998,
      "grad_norm": 2.9536614418029785,
      "learning_rate": 9.436241610738256e-06,
      "loss": 0.5498,
      "step": 60940
    },
    {
      "epoch": 2.438,
      "grad_norm": 3.0901801586151123,
      "learning_rate": 9.429530201342282e-06,
      "loss": 0.4715,
      "step": 60950
    },
    {
      "epoch": 2.4384,
      "grad_norm": 3.385223388671875,
      "learning_rate": 9.42281879194631e-06,
      "loss": 0.4661,
      "step": 60960
    },
    {
      "epoch": 2.4388,
      "grad_norm": 2.6716372966766357,
      "learning_rate": 9.416107382550337e-06,
      "loss": 0.5756,
      "step": 60970
    },
    {
      "epoch": 2.4392,
      "grad_norm": 2.7339229583740234,
      "learning_rate": 9.409395973154362e-06,
      "loss": 0.4743,
      "step": 60980
    },
    {
      "epoch": 2.4396,
      "grad_norm": 2.437952756881714,
      "learning_rate": 9.402684563758389e-06,
      "loss": 0.4857,
      "step": 60990
    },
    {
      "epoch": 2.44,
      "grad_norm": 3.3475358486175537,
      "learning_rate": 9.395973154362418e-06,
      "loss": 0.5025,
      "step": 61000
    },
    {
      "epoch": 2.4404,
      "grad_norm": 2.6005141735076904,
      "learning_rate": 9.389261744966443e-06,
      "loss": 0.4984,
      "step": 61010
    },
    {
      "epoch": 2.4408,
      "grad_norm": 2.406705379486084,
      "learning_rate": 9.38255033557047e-06,
      "loss": 0.5193,
      "step": 61020
    },
    {
      "epoch": 2.4412,
      "grad_norm": 2.8384552001953125,
      "learning_rate": 9.375838926174498e-06,
      "loss": 0.5129,
      "step": 61030
    },
    {
      "epoch": 2.4416,
      "grad_norm": 2.32796049118042,
      "learning_rate": 9.369127516778523e-06,
      "loss": 0.4598,
      "step": 61040
    },
    {
      "epoch": 2.442,
      "grad_norm": 2.578639268875122,
      "learning_rate": 9.36241610738255e-06,
      "loss": 0.5137,
      "step": 61050
    },
    {
      "epoch": 2.4424,
      "grad_norm": 3.2859957218170166,
      "learning_rate": 9.355704697986579e-06,
      "loss": 0.5034,
      "step": 61060
    },
    {
      "epoch": 2.4428,
      "grad_norm": 2.640133857727051,
      "learning_rate": 9.348993288590604e-06,
      "loss": 0.4545,
      "step": 61070
    },
    {
      "epoch": 2.4432,
      "grad_norm": 3.0561716556549072,
      "learning_rate": 9.34228187919463e-06,
      "loss": 0.493,
      "step": 61080
    },
    {
      "epoch": 2.4436,
      "grad_norm": 2.99764084815979,
      "learning_rate": 9.33557046979866e-06,
      "loss": 0.5039,
      "step": 61090
    },
    {
      "epoch": 2.444,
      "grad_norm": 2.0870003700256348,
      "learning_rate": 9.328859060402684e-06,
      "loss": 0.4495,
      "step": 61100
    },
    {
      "epoch": 2.4444,
      "grad_norm": 2.940706491470337,
      "learning_rate": 9.322147651006711e-06,
      "loss": 0.5006,
      "step": 61110
    },
    {
      "epoch": 2.4448,
      "grad_norm": 2.941842555999756,
      "learning_rate": 9.31543624161074e-06,
      "loss": 0.5727,
      "step": 61120
    },
    {
      "epoch": 2.4452,
      "grad_norm": 3.099616765975952,
      "learning_rate": 9.308724832214765e-06,
      "loss": 0.4967,
      "step": 61130
    },
    {
      "epoch": 2.4455999999999998,
      "grad_norm": 3.224271297454834,
      "learning_rate": 9.302013422818792e-06,
      "loss": 0.5132,
      "step": 61140
    },
    {
      "epoch": 2.446,
      "grad_norm": 2.7021453380584717,
      "learning_rate": 9.295302013422819e-06,
      "loss": 0.4845,
      "step": 61150
    },
    {
      "epoch": 2.4464,
      "grad_norm": 2.406545877456665,
      "learning_rate": 9.288590604026846e-06,
      "loss": 0.4837,
      "step": 61160
    },
    {
      "epoch": 2.4468,
      "grad_norm": 2.601785659790039,
      "learning_rate": 9.281879194630872e-06,
      "loss": 0.5267,
      "step": 61170
    },
    {
      "epoch": 2.4472,
      "grad_norm": 2.946377992630005,
      "learning_rate": 9.2751677852349e-06,
      "loss": 0.5146,
      "step": 61180
    },
    {
      "epoch": 2.4476,
      "grad_norm": 2.509317398071289,
      "learning_rate": 9.268456375838928e-06,
      "loss": 0.5356,
      "step": 61190
    },
    {
      "epoch": 2.448,
      "grad_norm": 2.6829683780670166,
      "learning_rate": 9.261744966442953e-06,
      "loss": 0.5057,
      "step": 61200
    },
    {
      "epoch": 2.4484,
      "grad_norm": 3.2771260738372803,
      "learning_rate": 9.25503355704698e-06,
      "loss": 0.5873,
      "step": 61210
    },
    {
      "epoch": 2.4488,
      "grad_norm": 2.6208181381225586,
      "learning_rate": 9.248322147651008e-06,
      "loss": 0.523,
      "step": 61220
    },
    {
      "epoch": 2.4492,
      "grad_norm": 2.337467670440674,
      "learning_rate": 9.241610738255034e-06,
      "loss": 0.4663,
      "step": 61230
    },
    {
      "epoch": 2.4496,
      "grad_norm": 2.407383441925049,
      "learning_rate": 9.23489932885906e-06,
      "loss": 0.4877,
      "step": 61240
    },
    {
      "epoch": 2.45,
      "grad_norm": 2.21828031539917,
      "learning_rate": 9.228187919463089e-06,
      "loss": 0.4978,
      "step": 61250
    },
    {
      "epoch": 2.4504,
      "grad_norm": 1.9495532512664795,
      "learning_rate": 9.221476510067114e-06,
      "loss": 0.487,
      "step": 61260
    },
    {
      "epoch": 2.4508,
      "grad_norm": 2.667201519012451,
      "learning_rate": 9.214765100671141e-06,
      "loss": 0.5433,
      "step": 61270
    },
    {
      "epoch": 2.4512,
      "grad_norm": 2.604428291320801,
      "learning_rate": 9.20805369127517e-06,
      "loss": 0.4957,
      "step": 61280
    },
    {
      "epoch": 2.4516,
      "grad_norm": 2.6277639865875244,
      "learning_rate": 9.201342281879195e-06,
      "loss": 0.4676,
      "step": 61290
    },
    {
      "epoch": 2.452,
      "grad_norm": 2.3151464462280273,
      "learning_rate": 9.194630872483221e-06,
      "loss": 0.452,
      "step": 61300
    },
    {
      "epoch": 2.4524,
      "grad_norm": 3.008427143096924,
      "learning_rate": 9.187919463087248e-06,
      "loss": 0.4948,
      "step": 61310
    },
    {
      "epoch": 2.4528,
      "grad_norm": 3.1073226928710938,
      "learning_rate": 9.181208053691275e-06,
      "loss": 0.5466,
      "step": 61320
    },
    {
      "epoch": 2.4532,
      "grad_norm": 3.3167507648468018,
      "learning_rate": 9.174496644295302e-06,
      "loss": 0.5097,
      "step": 61330
    },
    {
      "epoch": 2.4536,
      "grad_norm": 3.2412924766540527,
      "learning_rate": 9.167785234899329e-06,
      "loss": 0.5102,
      "step": 61340
    },
    {
      "epoch": 2.454,
      "grad_norm": 1.6788756847381592,
      "learning_rate": 9.161073825503356e-06,
      "loss": 0.4547,
      "step": 61350
    },
    {
      "epoch": 2.4544,
      "grad_norm": 2.4267659187316895,
      "learning_rate": 9.154362416107383e-06,
      "loss": 0.5043,
      "step": 61360
    },
    {
      "epoch": 2.4548,
      "grad_norm": 1.9269819259643555,
      "learning_rate": 9.14765100671141e-06,
      "loss": 0.5236,
      "step": 61370
    },
    {
      "epoch": 2.4552,
      "grad_norm": 3.3487930297851562,
      "learning_rate": 9.140939597315436e-06,
      "loss": 0.5698,
      "step": 61380
    },
    {
      "epoch": 2.4556,
      "grad_norm": 2.234598398208618,
      "learning_rate": 9.134228187919463e-06,
      "loss": 0.5033,
      "step": 61390
    },
    {
      "epoch": 2.456,
      "grad_norm": 2.490143299102783,
      "learning_rate": 9.12751677852349e-06,
      "loss": 0.6148,
      "step": 61400
    },
    {
      "epoch": 2.4564,
      "grad_norm": 2.2487125396728516,
      "learning_rate": 9.120805369127519e-06,
      "loss": 0.4397,
      "step": 61410
    },
    {
      "epoch": 2.4568,
      "grad_norm": 3.182183027267456,
      "learning_rate": 9.114093959731544e-06,
      "loss": 0.5094,
      "step": 61420
    },
    {
      "epoch": 2.4572,
      "grad_norm": 1.943238377571106,
      "learning_rate": 9.10738255033557e-06,
      "loss": 0.4472,
      "step": 61430
    },
    {
      "epoch": 2.4576000000000002,
      "grad_norm": 2.215083599090576,
      "learning_rate": 9.100671140939599e-06,
      "loss": 0.421,
      "step": 61440
    },
    {
      "epoch": 2.458,
      "grad_norm": 3.164557933807373,
      "learning_rate": 9.093959731543624e-06,
      "loss": 0.4921,
      "step": 61450
    },
    {
      "epoch": 2.4584,
      "grad_norm": 2.649160623550415,
      "learning_rate": 9.087248322147651e-06,
      "loss": 0.424,
      "step": 61460
    },
    {
      "epoch": 2.4588,
      "grad_norm": 2.553464651107788,
      "learning_rate": 9.080536912751678e-06,
      "loss": 0.4877,
      "step": 61470
    },
    {
      "epoch": 2.4592,
      "grad_norm": 3.0138838291168213,
      "learning_rate": 9.073825503355705e-06,
      "loss": 0.5038,
      "step": 61480
    },
    {
      "epoch": 2.4596,
      "grad_norm": 2.534935235977173,
      "learning_rate": 9.067114093959732e-06,
      "loss": 0.4777,
      "step": 61490
    },
    {
      "epoch": 2.46,
      "grad_norm": 3.112119674682617,
      "learning_rate": 9.060402684563759e-06,
      "loss": 0.4216,
      "step": 61500
    },
    {
      "epoch": 2.4604,
      "grad_norm": 2.1738147735595703,
      "learning_rate": 9.053691275167785e-06,
      "loss": 0.463,
      "step": 61510
    },
    {
      "epoch": 2.4608,
      "grad_norm": 2.604823350906372,
      "learning_rate": 9.046979865771812e-06,
      "loss": 0.496,
      "step": 61520
    },
    {
      "epoch": 2.4612,
      "grad_norm": 2.0584185123443604,
      "learning_rate": 9.040268456375839e-06,
      "loss": 0.4663,
      "step": 61530
    },
    {
      "epoch": 2.4616,
      "grad_norm": 2.50895094871521,
      "learning_rate": 9.033557046979866e-06,
      "loss": 0.4928,
      "step": 61540
    },
    {
      "epoch": 2.462,
      "grad_norm": 3.0782737731933594,
      "learning_rate": 9.026845637583893e-06,
      "loss": 0.536,
      "step": 61550
    },
    {
      "epoch": 2.4624,
      "grad_norm": 2.826723098754883,
      "learning_rate": 9.02013422818792e-06,
      "loss": 0.4807,
      "step": 61560
    },
    {
      "epoch": 2.4628,
      "grad_norm": 2.730576276779175,
      "learning_rate": 9.013422818791947e-06,
      "loss": 0.4785,
      "step": 61570
    },
    {
      "epoch": 2.4632,
      "grad_norm": 2.5102298259735107,
      "learning_rate": 9.006711409395973e-06,
      "loss": 0.5137,
      "step": 61580
    },
    {
      "epoch": 2.4636,
      "grad_norm": 3.3491368293762207,
      "learning_rate": 9e-06,
      "loss": 0.4913,
      "step": 61590
    },
    {
      "epoch": 2.464,
      "grad_norm": 2.9357755184173584,
      "learning_rate": 8.993288590604027e-06,
      "loss": 0.5157,
      "step": 61600
    },
    {
      "epoch": 2.4644,
      "grad_norm": 3.1579344272613525,
      "learning_rate": 8.986577181208054e-06,
      "loss": 0.5032,
      "step": 61610
    },
    {
      "epoch": 2.4648,
      "grad_norm": 2.0096473693847656,
      "learning_rate": 8.97986577181208e-06,
      "loss": 0.4432,
      "step": 61620
    },
    {
      "epoch": 2.4652,
      "grad_norm": 3.0124917030334473,
      "learning_rate": 8.973154362416108e-06,
      "loss": 0.4821,
      "step": 61630
    },
    {
      "epoch": 2.4656000000000002,
      "grad_norm": 2.1627354621887207,
      "learning_rate": 8.966442953020134e-06,
      "loss": 0.4976,
      "step": 61640
    },
    {
      "epoch": 2.466,
      "grad_norm": 2.603863477706909,
      "learning_rate": 8.959731543624161e-06,
      "loss": 0.4508,
      "step": 61650
    },
    {
      "epoch": 2.4664,
      "grad_norm": 2.80633282661438,
      "learning_rate": 8.953020134228188e-06,
      "loss": 0.5348,
      "step": 61660
    },
    {
      "epoch": 2.4668,
      "grad_norm": 2.264091968536377,
      "learning_rate": 8.946308724832215e-06,
      "loss": 0.4643,
      "step": 61670
    },
    {
      "epoch": 2.4672,
      "grad_norm": 2.2255001068115234,
      "learning_rate": 8.939597315436242e-06,
      "loss": 0.4763,
      "step": 61680
    },
    {
      "epoch": 2.4676,
      "grad_norm": 3.098752737045288,
      "learning_rate": 8.932885906040269e-06,
      "loss": 0.4594,
      "step": 61690
    },
    {
      "epoch": 2.468,
      "grad_norm": 2.763707399368286,
      "learning_rate": 8.926174496644296e-06,
      "loss": 0.5145,
      "step": 61700
    },
    {
      "epoch": 2.4684,
      "grad_norm": 2.300736904144287,
      "learning_rate": 8.919463087248322e-06,
      "loss": 0.4566,
      "step": 61710
    },
    {
      "epoch": 2.4688,
      "grad_norm": 3.389782190322876,
      "learning_rate": 8.91275167785235e-06,
      "loss": 0.5386,
      "step": 61720
    },
    {
      "epoch": 2.4692,
      "grad_norm": 1.9628498554229736,
      "learning_rate": 8.906040268456376e-06,
      "loss": 0.51,
      "step": 61730
    },
    {
      "epoch": 2.4696,
      "grad_norm": 2.9444375038146973,
      "learning_rate": 8.899328859060403e-06,
      "loss": 0.4489,
      "step": 61740
    },
    {
      "epoch": 2.4699999999999998,
      "grad_norm": 3.1234853267669678,
      "learning_rate": 8.89261744966443e-06,
      "loss": 0.521,
      "step": 61750
    },
    {
      "epoch": 2.4704,
      "grad_norm": 3.1234993934631348,
      "learning_rate": 8.885906040268457e-06,
      "loss": 0.5397,
      "step": 61760
    },
    {
      "epoch": 2.4708,
      "grad_norm": 3.276486396789551,
      "learning_rate": 8.879194630872484e-06,
      "loss": 0.5524,
      "step": 61770
    },
    {
      "epoch": 2.4712,
      "grad_norm": 2.869173288345337,
      "learning_rate": 8.87248322147651e-06,
      "loss": 0.5405,
      "step": 61780
    },
    {
      "epoch": 2.4716,
      "grad_norm": 2.3037397861480713,
      "learning_rate": 8.865771812080537e-06,
      "loss": 0.5255,
      "step": 61790
    },
    {
      "epoch": 2.472,
      "grad_norm": 3.0427777767181396,
      "learning_rate": 8.859060402684564e-06,
      "loss": 0.5195,
      "step": 61800
    },
    {
      "epoch": 2.4724,
      "grad_norm": 2.1639645099639893,
      "learning_rate": 8.852348993288591e-06,
      "loss": 0.4328,
      "step": 61810
    },
    {
      "epoch": 2.4728,
      "grad_norm": 3.0072295665740967,
      "learning_rate": 8.845637583892618e-06,
      "loss": 0.4965,
      "step": 61820
    },
    {
      "epoch": 2.4732,
      "grad_norm": 2.939525842666626,
      "learning_rate": 8.838926174496645e-06,
      "loss": 0.5381,
      "step": 61830
    },
    {
      "epoch": 2.4736000000000002,
      "grad_norm": 3.2451322078704834,
      "learning_rate": 8.832214765100672e-06,
      "loss": 0.5525,
      "step": 61840
    },
    {
      "epoch": 2.474,
      "grad_norm": 3.9273359775543213,
      "learning_rate": 8.825503355704698e-06,
      "loss": 0.488,
      "step": 61850
    },
    {
      "epoch": 2.4744,
      "grad_norm": 2.4310905933380127,
      "learning_rate": 8.818791946308725e-06,
      "loss": 0.5314,
      "step": 61860
    },
    {
      "epoch": 2.4748,
      "grad_norm": 2.032844066619873,
      "learning_rate": 8.812080536912752e-06,
      "loss": 0.4842,
      "step": 61870
    },
    {
      "epoch": 2.4752,
      "grad_norm": 2.5628597736358643,
      "learning_rate": 8.805369127516779e-06,
      "loss": 0.4569,
      "step": 61880
    },
    {
      "epoch": 2.4756,
      "grad_norm": 3.1327407360076904,
      "learning_rate": 8.798657718120806e-06,
      "loss": 0.5655,
      "step": 61890
    },
    {
      "epoch": 2.476,
      "grad_norm": 2.4264416694641113,
      "learning_rate": 8.791946308724833e-06,
      "loss": 0.431,
      "step": 61900
    },
    {
      "epoch": 2.4764,
      "grad_norm": 2.1182239055633545,
      "learning_rate": 8.78523489932886e-06,
      "loss": 0.5665,
      "step": 61910
    },
    {
      "epoch": 2.4768,
      "grad_norm": 2.8939342498779297,
      "learning_rate": 8.778523489932886e-06,
      "loss": 0.475,
      "step": 61920
    },
    {
      "epoch": 2.4772,
      "grad_norm": 3.213528633117676,
      "learning_rate": 8.771812080536913e-06,
      "loss": 0.5477,
      "step": 61930
    },
    {
      "epoch": 2.4776,
      "grad_norm": 3.1129915714263916,
      "learning_rate": 8.76510067114094e-06,
      "loss": 0.5022,
      "step": 61940
    },
    {
      "epoch": 2.4779999999999998,
      "grad_norm": 2.5371615886688232,
      "learning_rate": 8.758389261744967e-06,
      "loss": 0.4724,
      "step": 61950
    },
    {
      "epoch": 2.4784,
      "grad_norm": 2.7193963527679443,
      "learning_rate": 8.751677852348994e-06,
      "loss": 0.471,
      "step": 61960
    },
    {
      "epoch": 2.4788,
      "grad_norm": 2.3636677265167236,
      "learning_rate": 8.74496644295302e-06,
      "loss": 0.4765,
      "step": 61970
    },
    {
      "epoch": 2.4792,
      "grad_norm": 2.6346282958984375,
      "learning_rate": 8.738255033557047e-06,
      "loss": 0.4953,
      "step": 61980
    },
    {
      "epoch": 2.4796,
      "grad_norm": 2.06447434425354,
      "learning_rate": 8.731543624161074e-06,
      "loss": 0.4274,
      "step": 61990
    },
    {
      "epoch": 2.48,
      "grad_norm": 2.4721529483795166,
      "learning_rate": 8.724832214765101e-06,
      "loss": 0.4462,
      "step": 62000
    },
    {
      "epoch": 2.4804,
      "grad_norm": 2.1612184047698975,
      "learning_rate": 8.718120805369128e-06,
      "loss": 0.4888,
      "step": 62010
    },
    {
      "epoch": 2.4808,
      "grad_norm": 1.9692589044570923,
      "learning_rate": 8.711409395973155e-06,
      "loss": 0.4638,
      "step": 62020
    },
    {
      "epoch": 2.4812,
      "grad_norm": 2.8327059745788574,
      "learning_rate": 8.704697986577182e-06,
      "loss": 0.5515,
      "step": 62030
    },
    {
      "epoch": 2.4816,
      "grad_norm": 2.6759204864501953,
      "learning_rate": 8.697986577181209e-06,
      "loss": 0.4578,
      "step": 62040
    },
    {
      "epoch": 2.482,
      "grad_norm": 2.9007391929626465,
      "learning_rate": 8.691275167785235e-06,
      "loss": 0.4252,
      "step": 62050
    },
    {
      "epoch": 2.4824,
      "grad_norm": 2.7062864303588867,
      "learning_rate": 8.684563758389262e-06,
      "loss": 0.4667,
      "step": 62060
    },
    {
      "epoch": 2.4828,
      "grad_norm": 2.8880279064178467,
      "learning_rate": 8.67785234899329e-06,
      "loss": 0.5854,
      "step": 62070
    },
    {
      "epoch": 2.4832,
      "grad_norm": 3.18475341796875,
      "learning_rate": 8.671140939597316e-06,
      "loss": 0.4688,
      "step": 62080
    },
    {
      "epoch": 2.4836,
      "grad_norm": 2.633880376815796,
      "learning_rate": 8.664429530201343e-06,
      "loss": 0.5779,
      "step": 62090
    },
    {
      "epoch": 2.484,
      "grad_norm": 2.6740965843200684,
      "learning_rate": 8.65771812080537e-06,
      "loss": 0.4592,
      "step": 62100
    },
    {
      "epoch": 2.4844,
      "grad_norm": 2.012610912322998,
      "learning_rate": 8.651006711409397e-06,
      "loss": 0.5714,
      "step": 62110
    },
    {
      "epoch": 2.4848,
      "grad_norm": 2.8950681686401367,
      "learning_rate": 8.644295302013423e-06,
      "loss": 0.5284,
      "step": 62120
    },
    {
      "epoch": 2.4852,
      "grad_norm": 2.5523223876953125,
      "learning_rate": 8.63758389261745e-06,
      "loss": 0.5335,
      "step": 62130
    },
    {
      "epoch": 2.4856,
      "grad_norm": 2.739464282989502,
      "learning_rate": 8.630872483221475e-06,
      "loss": 0.5101,
      "step": 62140
    },
    {
      "epoch": 2.4859999999999998,
      "grad_norm": 2.6580092906951904,
      "learning_rate": 8.624161073825504e-06,
      "loss": 0.4841,
      "step": 62150
    },
    {
      "epoch": 2.4864,
      "grad_norm": 1.9602309465408325,
      "learning_rate": 8.617449664429531e-06,
      "loss": 0.4658,
      "step": 62160
    },
    {
      "epoch": 2.4868,
      "grad_norm": 2.5811405181884766,
      "learning_rate": 8.610738255033558e-06,
      "loss": 0.4704,
      "step": 62170
    },
    {
      "epoch": 2.4872,
      "grad_norm": 2.280649423599243,
      "learning_rate": 8.604026845637585e-06,
      "loss": 0.6032,
      "step": 62180
    },
    {
      "epoch": 2.4876,
      "grad_norm": 2.8539929389953613,
      "learning_rate": 8.597315436241611e-06,
      "loss": 0.5175,
      "step": 62190
    },
    {
      "epoch": 2.488,
      "grad_norm": 2.11737060546875,
      "learning_rate": 8.590604026845638e-06,
      "loss": 0.5154,
      "step": 62200
    },
    {
      "epoch": 2.4884,
      "grad_norm": 2.692222833633423,
      "learning_rate": 8.583892617449665e-06,
      "loss": 0.5224,
      "step": 62210
    },
    {
      "epoch": 2.4888,
      "grad_norm": 2.7291064262390137,
      "learning_rate": 8.577181208053692e-06,
      "loss": 0.4079,
      "step": 62220
    },
    {
      "epoch": 2.4892,
      "grad_norm": 3.2859132289886475,
      "learning_rate": 8.570469798657719e-06,
      "loss": 0.4361,
      "step": 62230
    },
    {
      "epoch": 2.4896,
      "grad_norm": 2.8672327995300293,
      "learning_rate": 8.563758389261746e-06,
      "loss": 0.4406,
      "step": 62240
    },
    {
      "epoch": 2.49,
      "grad_norm": 2.150087594985962,
      "learning_rate": 8.557046979865773e-06,
      "loss": 0.4702,
      "step": 62250
    },
    {
      "epoch": 2.4904,
      "grad_norm": 3.0298593044281006,
      "learning_rate": 8.5503355704698e-06,
      "loss": 0.6061,
      "step": 62260
    },
    {
      "epoch": 2.4908,
      "grad_norm": 2.610023260116577,
      "learning_rate": 8.543624161073826e-06,
      "loss": 0.5175,
      "step": 62270
    },
    {
      "epoch": 2.4912,
      "grad_norm": 1.9889897108078003,
      "learning_rate": 8.536912751677853e-06,
      "loss": 0.5536,
      "step": 62280
    },
    {
      "epoch": 2.4916,
      "grad_norm": 2.0631096363067627,
      "learning_rate": 8.53020134228188e-06,
      "loss": 0.5043,
      "step": 62290
    },
    {
      "epoch": 2.492,
      "grad_norm": 2.993717908859253,
      "learning_rate": 8.523489932885905e-06,
      "loss": 0.501,
      "step": 62300
    },
    {
      "epoch": 2.4924,
      "grad_norm": 2.5087027549743652,
      "learning_rate": 8.516778523489934e-06,
      "loss": 0.5282,
      "step": 62310
    },
    {
      "epoch": 2.4928,
      "grad_norm": 2.831388235092163,
      "learning_rate": 8.51006711409396e-06,
      "loss": 0.5504,
      "step": 62320
    },
    {
      "epoch": 2.4932,
      "grad_norm": 2.3442342281341553,
      "learning_rate": 8.503355704697986e-06,
      "loss": 0.4466,
      "step": 62330
    },
    {
      "epoch": 2.4936,
      "grad_norm": 2.9942145347595215,
      "learning_rate": 8.496644295302014e-06,
      "loss": 0.6062,
      "step": 62340
    },
    {
      "epoch": 2.4939999999999998,
      "grad_norm": 2.5850982666015625,
      "learning_rate": 8.489932885906041e-06,
      "loss": 0.544,
      "step": 62350
    },
    {
      "epoch": 2.4944,
      "grad_norm": 1.9562101364135742,
      "learning_rate": 8.483221476510066e-06,
      "loss": 0.4751,
      "step": 62360
    },
    {
      "epoch": 2.4948,
      "grad_norm": 2.8208088874816895,
      "learning_rate": 8.476510067114095e-06,
      "loss": 0.4615,
      "step": 62370
    },
    {
      "epoch": 2.4952,
      "grad_norm": 2.3790900707244873,
      "learning_rate": 8.469798657718122e-06,
      "loss": 0.4986,
      "step": 62380
    },
    {
      "epoch": 2.4956,
      "grad_norm": 2.9329187870025635,
      "learning_rate": 8.463087248322148e-06,
      "loss": 0.5551,
      "step": 62390
    },
    {
      "epoch": 2.496,
      "grad_norm": 2.7002527713775635,
      "learning_rate": 8.456375838926175e-06,
      "loss": 0.4761,
      "step": 62400
    },
    {
      "epoch": 2.4964,
      "grad_norm": 2.5761773586273193,
      "learning_rate": 8.449664429530202e-06,
      "loss": 0.4442,
      "step": 62410
    },
    {
      "epoch": 2.4968,
      "grad_norm": 2.8412792682647705,
      "learning_rate": 8.442953020134229e-06,
      "loss": 0.4921,
      "step": 62420
    },
    {
      "epoch": 2.4972,
      "grad_norm": 2.666289806365967,
      "learning_rate": 8.436241610738256e-06,
      "loss": 0.5698,
      "step": 62430
    },
    {
      "epoch": 2.4976,
      "grad_norm": 2.695931911468506,
      "learning_rate": 8.429530201342283e-06,
      "loss": 0.5328,
      "step": 62440
    },
    {
      "epoch": 2.498,
      "grad_norm": 2.7599432468414307,
      "learning_rate": 8.42281879194631e-06,
      "loss": 0.4436,
      "step": 62450
    },
    {
      "epoch": 2.4984,
      "grad_norm": 1.7447402477264404,
      "learning_rate": 8.416107382550335e-06,
      "loss": 0.4497,
      "step": 62460
    },
    {
      "epoch": 2.4988,
      "grad_norm": 2.834765911102295,
      "learning_rate": 8.409395973154363e-06,
      "loss": 0.4632,
      "step": 62470
    },
    {
      "epoch": 2.4992,
      "grad_norm": 2.7174110412597656,
      "learning_rate": 8.40268456375839e-06,
      "loss": 0.491,
      "step": 62480
    },
    {
      "epoch": 2.4996,
      "grad_norm": 2.723796844482422,
      "learning_rate": 8.395973154362415e-06,
      "loss": 0.4654,
      "step": 62490
    },
    {
      "epoch": 2.5,
      "grad_norm": 3.4378325939178467,
      "learning_rate": 8.389261744966444e-06,
      "loss": 0.5319,
      "step": 62500
    },
    {
      "epoch": 2.5004,
      "grad_norm": 2.0812487602233887,
      "learning_rate": 8.38255033557047e-06,
      "loss": 0.4852,
      "step": 62510
    },
    {
      "epoch": 2.5008,
      "grad_norm": 2.8981521129608154,
      "learning_rate": 8.375838926174496e-06,
      "loss": 0.5518,
      "step": 62520
    },
    {
      "epoch": 2.5012,
      "grad_norm": 2.408172607421875,
      "learning_rate": 8.369127516778524e-06,
      "loss": 0.5193,
      "step": 62530
    },
    {
      "epoch": 2.5016,
      "grad_norm": 1.949396014213562,
      "learning_rate": 8.362416107382551e-06,
      "loss": 0.4352,
      "step": 62540
    },
    {
      "epoch": 2.502,
      "grad_norm": 2.8275017738342285,
      "learning_rate": 8.355704697986576e-06,
      "loss": 0.551,
      "step": 62550
    },
    {
      "epoch": 2.5023999999999997,
      "grad_norm": 2.624732255935669,
      "learning_rate": 8.348993288590605e-06,
      "loss": 0.4482,
      "step": 62560
    },
    {
      "epoch": 2.5028,
      "grad_norm": 3.0863306522369385,
      "learning_rate": 8.342281879194632e-06,
      "loss": 0.499,
      "step": 62570
    },
    {
      "epoch": 2.5032,
      "grad_norm": 2.5569815635681152,
      "learning_rate": 8.335570469798657e-06,
      "loss": 0.5591,
      "step": 62580
    },
    {
      "epoch": 2.5036,
      "grad_norm": 2.2525417804718018,
      "learning_rate": 8.328859060402686e-06,
      "loss": 0.4815,
      "step": 62590
    },
    {
      "epoch": 2.504,
      "grad_norm": 2.317270040512085,
      "learning_rate": 8.322147651006712e-06,
      "loss": 0.4566,
      "step": 62600
    },
    {
      "epoch": 2.5044,
      "grad_norm": 2.4980416297912598,
      "learning_rate": 8.31543624161074e-06,
      "loss": 0.5058,
      "step": 62610
    },
    {
      "epoch": 2.5048,
      "grad_norm": 2.993290662765503,
      "learning_rate": 8.308724832214764e-06,
      "loss": 0.4932,
      "step": 62620
    },
    {
      "epoch": 2.5052,
      "grad_norm": 2.672431468963623,
      "learning_rate": 8.302013422818793e-06,
      "loss": 0.55,
      "step": 62630
    },
    {
      "epoch": 2.5056000000000003,
      "grad_norm": 1.8873039484024048,
      "learning_rate": 8.29530201342282e-06,
      "loss": 0.4178,
      "step": 62640
    },
    {
      "epoch": 2.5060000000000002,
      "grad_norm": 2.1683671474456787,
      "learning_rate": 8.288590604026845e-06,
      "loss": 0.5439,
      "step": 62650
    },
    {
      "epoch": 2.5064,
      "grad_norm": 2.5946085453033447,
      "learning_rate": 8.281879194630874e-06,
      "loss": 0.4427,
      "step": 62660
    },
    {
      "epoch": 2.5068,
      "grad_norm": 2.6401596069335938,
      "learning_rate": 8.2751677852349e-06,
      "loss": 0.4608,
      "step": 62670
    },
    {
      "epoch": 2.5072,
      "grad_norm": 1.8749762773513794,
      "learning_rate": 8.268456375838926e-06,
      "loss": 0.4423,
      "step": 62680
    },
    {
      "epoch": 2.5076,
      "grad_norm": 3.197601795196533,
      "learning_rate": 8.261744966442954e-06,
      "loss": 0.5424,
      "step": 62690
    },
    {
      "epoch": 2.508,
      "grad_norm": 2.5311403274536133,
      "learning_rate": 8.255033557046981e-06,
      "loss": 0.4824,
      "step": 62700
    },
    {
      "epoch": 2.5084,
      "grad_norm": 1.7651344537734985,
      "learning_rate": 8.248322147651006e-06,
      "loss": 0.4127,
      "step": 62710
    },
    {
      "epoch": 2.5088,
      "grad_norm": 1.9082404375076294,
      "learning_rate": 8.241610738255035e-06,
      "loss": 0.4648,
      "step": 62720
    },
    {
      "epoch": 2.5092,
      "grad_norm": 3.4884283542633057,
      "learning_rate": 8.234899328859061e-06,
      "loss": 0.5137,
      "step": 62730
    },
    {
      "epoch": 2.5096,
      "grad_norm": 2.270717144012451,
      "learning_rate": 8.228187919463087e-06,
      "loss": 0.4948,
      "step": 62740
    },
    {
      "epoch": 2.51,
      "grad_norm": 2.619788646697998,
      "learning_rate": 8.221476510067115e-06,
      "loss": 0.4944,
      "step": 62750
    },
    {
      "epoch": 2.5103999999999997,
      "grad_norm": 2.8826844692230225,
      "learning_rate": 8.214765100671142e-06,
      "loss": 0.5995,
      "step": 62760
    },
    {
      "epoch": 2.5108,
      "grad_norm": 2.064056158065796,
      "learning_rate": 8.208053691275167e-06,
      "loss": 0.4115,
      "step": 62770
    },
    {
      "epoch": 2.5112,
      "grad_norm": 2.9626595973968506,
      "learning_rate": 8.201342281879194e-06,
      "loss": 0.5501,
      "step": 62780
    },
    {
      "epoch": 2.5116,
      "grad_norm": 3.227841854095459,
      "learning_rate": 8.194630872483223e-06,
      "loss": 0.568,
      "step": 62790
    },
    {
      "epoch": 2.512,
      "grad_norm": 2.745659351348877,
      "learning_rate": 8.187919463087248e-06,
      "loss": 0.5658,
      "step": 62800
    },
    {
      "epoch": 2.5124,
      "grad_norm": 2.495959520339966,
      "learning_rate": 8.181208053691275e-06,
      "loss": 0.5186,
      "step": 62810
    },
    {
      "epoch": 2.5128,
      "grad_norm": 2.9137158393859863,
      "learning_rate": 8.174496644295303e-06,
      "loss": 0.4934,
      "step": 62820
    },
    {
      "epoch": 2.5132,
      "grad_norm": 3.242767810821533,
      "learning_rate": 8.16778523489933e-06,
      "loss": 0.5573,
      "step": 62830
    },
    {
      "epoch": 2.5136,
      "grad_norm": 3.6306636333465576,
      "learning_rate": 8.161073825503355e-06,
      "loss": 0.5304,
      "step": 62840
    },
    {
      "epoch": 2.5140000000000002,
      "grad_norm": 2.163712978363037,
      "learning_rate": 8.154362416107384e-06,
      "loss": 0.4591,
      "step": 62850
    },
    {
      "epoch": 2.5144,
      "grad_norm": 2.211991786956787,
      "learning_rate": 8.14765100671141e-06,
      "loss": 0.4909,
      "step": 62860
    },
    {
      "epoch": 2.5148,
      "grad_norm": 2.9446113109588623,
      "learning_rate": 8.140939597315436e-06,
      "loss": 0.5144,
      "step": 62870
    },
    {
      "epoch": 2.5152,
      "grad_norm": 2.440572500228882,
      "learning_rate": 8.134228187919464e-06,
      "loss": 0.5134,
      "step": 62880
    },
    {
      "epoch": 2.5156,
      "grad_norm": 2.1711535453796387,
      "learning_rate": 8.127516778523491e-06,
      "loss": 0.4408,
      "step": 62890
    },
    {
      "epoch": 2.516,
      "grad_norm": 2.628821611404419,
      "learning_rate": 8.120805369127516e-06,
      "loss": 0.5853,
      "step": 62900
    },
    {
      "epoch": 2.5164,
      "grad_norm": 2.5030808448791504,
      "learning_rate": 8.114093959731545e-06,
      "loss": 0.4966,
      "step": 62910
    },
    {
      "epoch": 2.5168,
      "grad_norm": 3.0319859981536865,
      "learning_rate": 8.107382550335572e-06,
      "loss": 0.4822,
      "step": 62920
    },
    {
      "epoch": 2.5172,
      "grad_norm": 2.8001303672790527,
      "learning_rate": 8.100671140939597e-06,
      "loss": 0.4762,
      "step": 62930
    },
    {
      "epoch": 2.5176,
      "grad_norm": 2.2858035564422607,
      "learning_rate": 8.093959731543624e-06,
      "loss": 0.5711,
      "step": 62940
    },
    {
      "epoch": 2.518,
      "grad_norm": 2.349118709564209,
      "learning_rate": 8.087248322147652e-06,
      "loss": 0.5018,
      "step": 62950
    },
    {
      "epoch": 2.5183999999999997,
      "grad_norm": 2.8305108547210693,
      "learning_rate": 8.080536912751677e-06,
      "loss": 0.4988,
      "step": 62960
    },
    {
      "epoch": 2.5188,
      "grad_norm": 2.5949506759643555,
      "learning_rate": 8.073825503355704e-06,
      "loss": 0.4824,
      "step": 62970
    },
    {
      "epoch": 2.5192,
      "grad_norm": 1.9263118505477905,
      "learning_rate": 8.067114093959733e-06,
      "loss": 0.4209,
      "step": 62980
    },
    {
      "epoch": 2.5196,
      "grad_norm": 2.80572509765625,
      "learning_rate": 8.060402684563758e-06,
      "loss": 0.5548,
      "step": 62990
    },
    {
      "epoch": 2.52,
      "grad_norm": 2.2535529136657715,
      "learning_rate": 8.053691275167785e-06,
      "loss": 0.4992,
      "step": 63000
    },
    {
      "epoch": 2.5204,
      "grad_norm": 2.8093178272247314,
      "learning_rate": 8.046979865771813e-06,
      "loss": 0.5079,
      "step": 63010
    },
    {
      "epoch": 2.5208,
      "grad_norm": 2.488163709640503,
      "learning_rate": 8.040268456375839e-06,
      "loss": 0.4542,
      "step": 63020
    },
    {
      "epoch": 2.5212,
      "grad_norm": 2.4299306869506836,
      "learning_rate": 8.033557046979865e-06,
      "loss": 0.4974,
      "step": 63030
    },
    {
      "epoch": 2.5216,
      "grad_norm": 2.745948076248169,
      "learning_rate": 8.026845637583894e-06,
      "loss": 0.4949,
      "step": 63040
    },
    {
      "epoch": 2.5220000000000002,
      "grad_norm": 3.104255437850952,
      "learning_rate": 8.02013422818792e-06,
      "loss": 0.4662,
      "step": 63050
    },
    {
      "epoch": 2.5224,
      "grad_norm": 2.2634389400482178,
      "learning_rate": 8.013422818791946e-06,
      "loss": 0.4862,
      "step": 63060
    },
    {
      "epoch": 2.5228,
      "grad_norm": 2.613471269607544,
      "learning_rate": 8.006711409395974e-06,
      "loss": 0.5058,
      "step": 63070
    },
    {
      "epoch": 2.5232,
      "grad_norm": 2.738896369934082,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.5008,
      "step": 63080
    },
    {
      "epoch": 2.5236,
      "grad_norm": 3.003282070159912,
      "learning_rate": 7.993288590604026e-06,
      "loss": 0.5647,
      "step": 63090
    },
    {
      "epoch": 2.524,
      "grad_norm": 2.261841058731079,
      "learning_rate": 7.986577181208055e-06,
      "loss": 0.4729,
      "step": 63100
    },
    {
      "epoch": 2.5244,
      "grad_norm": 2.547581195831299,
      "learning_rate": 7.979865771812082e-06,
      "loss": 0.4266,
      "step": 63110
    },
    {
      "epoch": 2.5248,
      "grad_norm": 3.649127960205078,
      "learning_rate": 7.973154362416107e-06,
      "loss": 0.574,
      "step": 63120
    },
    {
      "epoch": 2.5252,
      "grad_norm": 1.9630968570709229,
      "learning_rate": 7.966442953020134e-06,
      "loss": 0.4273,
      "step": 63130
    },
    {
      "epoch": 2.5256,
      "grad_norm": 2.6117959022521973,
      "learning_rate": 7.959731543624162e-06,
      "loss": 0.5064,
      "step": 63140
    },
    {
      "epoch": 2.526,
      "grad_norm": 2.387591600418091,
      "learning_rate": 7.953020134228188e-06,
      "loss": 0.4902,
      "step": 63150
    },
    {
      "epoch": 2.5263999999999998,
      "grad_norm": 3.1135008335113525,
      "learning_rate": 7.946308724832214e-06,
      "loss": 0.5262,
      "step": 63160
    },
    {
      "epoch": 2.5268,
      "grad_norm": 2.660019636154175,
      "learning_rate": 7.939597315436243e-06,
      "loss": 0.5387,
      "step": 63170
    },
    {
      "epoch": 2.5272,
      "grad_norm": 2.3732712268829346,
      "learning_rate": 7.932885906040268e-06,
      "loss": 0.4737,
      "step": 63180
    },
    {
      "epoch": 2.5276,
      "grad_norm": 2.4751198291778564,
      "learning_rate": 7.926174496644295e-06,
      "loss": 0.4959,
      "step": 63190
    },
    {
      "epoch": 2.528,
      "grad_norm": 3.2717270851135254,
      "learning_rate": 7.919463087248324e-06,
      "loss": 0.5032,
      "step": 63200
    },
    {
      "epoch": 2.5284,
      "grad_norm": 2.276320695877075,
      "learning_rate": 7.912751677852349e-06,
      "loss": 0.5075,
      "step": 63210
    },
    {
      "epoch": 2.5288,
      "grad_norm": 2.483736276626587,
      "learning_rate": 7.906040268456376e-06,
      "loss": 0.5912,
      "step": 63220
    },
    {
      "epoch": 2.5292,
      "grad_norm": 3.387434959411621,
      "learning_rate": 7.899328859060404e-06,
      "loss": 0.5687,
      "step": 63230
    },
    {
      "epoch": 2.5296,
      "grad_norm": 2.4647247791290283,
      "learning_rate": 7.89261744966443e-06,
      "loss": 0.4292,
      "step": 63240
    },
    {
      "epoch": 2.5300000000000002,
      "grad_norm": 2.4109303951263428,
      "learning_rate": 7.885906040268456e-06,
      "loss": 0.4799,
      "step": 63250
    },
    {
      "epoch": 2.5304,
      "grad_norm": 2.3085403442382812,
      "learning_rate": 7.879194630872485e-06,
      "loss": 0.569,
      "step": 63260
    },
    {
      "epoch": 2.5308,
      "grad_norm": 2.259775400161743,
      "learning_rate": 7.87248322147651e-06,
      "loss": 0.54,
      "step": 63270
    },
    {
      "epoch": 2.5312,
      "grad_norm": 2.6836788654327393,
      "learning_rate": 7.865771812080537e-06,
      "loss": 0.531,
      "step": 63280
    },
    {
      "epoch": 2.5316,
      "grad_norm": 2.5936787128448486,
      "learning_rate": 7.859060402684564e-06,
      "loss": 0.4702,
      "step": 63290
    },
    {
      "epoch": 2.532,
      "grad_norm": 2.540891408920288,
      "learning_rate": 7.852348993288592e-06,
      "loss": 0.5092,
      "step": 63300
    },
    {
      "epoch": 2.5324,
      "grad_norm": 3.311372995376587,
      "learning_rate": 7.845637583892617e-06,
      "loss": 0.5615,
      "step": 63310
    },
    {
      "epoch": 2.5328,
      "grad_norm": 4.540698051452637,
      "learning_rate": 7.838926174496644e-06,
      "loss": 0.4797,
      "step": 63320
    },
    {
      "epoch": 2.5332,
      "grad_norm": 3.1885743141174316,
      "learning_rate": 7.832214765100673e-06,
      "loss": 0.5558,
      "step": 63330
    },
    {
      "epoch": 2.5336,
      "grad_norm": 2.380882740020752,
      "learning_rate": 7.825503355704698e-06,
      "loss": 0.4693,
      "step": 63340
    },
    {
      "epoch": 2.534,
      "grad_norm": 2.5127902030944824,
      "learning_rate": 7.818791946308725e-06,
      "loss": 0.5028,
      "step": 63350
    },
    {
      "epoch": 2.5343999999999998,
      "grad_norm": 2.0435588359832764,
      "learning_rate": 7.812080536912753e-06,
      "loss": 0.4356,
      "step": 63360
    },
    {
      "epoch": 2.5348,
      "grad_norm": 2.239194869995117,
      "learning_rate": 7.805369127516778e-06,
      "loss": 0.5043,
      "step": 63370
    },
    {
      "epoch": 2.5352,
      "grad_norm": 2.3190221786499023,
      "learning_rate": 7.798657718120805e-06,
      "loss": 0.4742,
      "step": 63380
    },
    {
      "epoch": 2.5356,
      "grad_norm": 2.363800525665283,
      "learning_rate": 7.791946308724834e-06,
      "loss": 0.3883,
      "step": 63390
    },
    {
      "epoch": 2.536,
      "grad_norm": 2.652009963989258,
      "learning_rate": 7.785234899328859e-06,
      "loss": 0.4575,
      "step": 63400
    },
    {
      "epoch": 2.5364,
      "grad_norm": 3.3781731128692627,
      "learning_rate": 7.778523489932886e-06,
      "loss": 0.556,
      "step": 63410
    },
    {
      "epoch": 2.5368,
      "grad_norm": 3.198890447616577,
      "learning_rate": 7.771812080536914e-06,
      "loss": 0.5097,
      "step": 63420
    },
    {
      "epoch": 2.5372,
      "grad_norm": 3.799112319946289,
      "learning_rate": 7.76510067114094e-06,
      "loss": 0.4185,
      "step": 63430
    },
    {
      "epoch": 2.5376,
      "grad_norm": 2.9128215312957764,
      "learning_rate": 7.758389261744966e-06,
      "loss": 0.4994,
      "step": 63440
    },
    {
      "epoch": 2.5380000000000003,
      "grad_norm": 2.9234893321990967,
      "learning_rate": 7.751677852348993e-06,
      "loss": 0.5219,
      "step": 63450
    },
    {
      "epoch": 2.5384,
      "grad_norm": 2.6694908142089844,
      "learning_rate": 7.74496644295302e-06,
      "loss": 0.483,
      "step": 63460
    },
    {
      "epoch": 2.5388,
      "grad_norm": 2.6668860912323,
      "learning_rate": 7.738255033557047e-06,
      "loss": 0.5367,
      "step": 63470
    },
    {
      "epoch": 2.5392,
      "grad_norm": 2.63016939163208,
      "learning_rate": 7.731543624161074e-06,
      "loss": 0.4678,
      "step": 63480
    },
    {
      "epoch": 2.5396,
      "grad_norm": 3.0330569744110107,
      "learning_rate": 7.7248322147651e-06,
      "loss": 0.4277,
      "step": 63490
    },
    {
      "epoch": 2.54,
      "grad_norm": 2.5853002071380615,
      "learning_rate": 7.718120805369127e-06,
      "loss": 0.5157,
      "step": 63500
    },
    {
      "epoch": 2.5404,
      "grad_norm": 3.265575408935547,
      "learning_rate": 7.711409395973154e-06,
      "loss": 0.5288,
      "step": 63510
    },
    {
      "epoch": 2.5408,
      "grad_norm": 2.66975998878479,
      "learning_rate": 7.704697986577183e-06,
      "loss": 0.4972,
      "step": 63520
    },
    {
      "epoch": 2.5412,
      "grad_norm": 2.3909363746643066,
      "learning_rate": 7.697986577181208e-06,
      "loss": 0.489,
      "step": 63530
    },
    {
      "epoch": 2.5416,
      "grad_norm": 2.391270637512207,
      "learning_rate": 7.691275167785235e-06,
      "loss": 0.4406,
      "step": 63540
    },
    {
      "epoch": 2.542,
      "grad_norm": 2.490771770477295,
      "learning_rate": 7.684563758389263e-06,
      "loss": 0.4748,
      "step": 63550
    },
    {
      "epoch": 2.5423999999999998,
      "grad_norm": 2.806262493133545,
      "learning_rate": 7.677852348993289e-06,
      "loss": 0.4646,
      "step": 63560
    },
    {
      "epoch": 2.5427999999999997,
      "grad_norm": 3.1744039058685303,
      "learning_rate": 7.671140939597315e-06,
      "loss": 0.4729,
      "step": 63570
    },
    {
      "epoch": 2.5432,
      "grad_norm": 1.9533482789993286,
      "learning_rate": 7.664429530201344e-06,
      "loss": 0.4753,
      "step": 63580
    },
    {
      "epoch": 2.5436,
      "grad_norm": 2.9044389724731445,
      "learning_rate": 7.657718120805369e-06,
      "loss": 0.5808,
      "step": 63590
    },
    {
      "epoch": 2.544,
      "grad_norm": 3.2967965602874756,
      "learning_rate": 7.651006711409396e-06,
      "loss": 0.4852,
      "step": 63600
    },
    {
      "epoch": 2.5444,
      "grad_norm": 2.8334834575653076,
      "learning_rate": 7.644295302013423e-06,
      "loss": 0.4916,
      "step": 63610
    },
    {
      "epoch": 2.5448,
      "grad_norm": 2.854401111602783,
      "learning_rate": 7.63758389261745e-06,
      "loss": 0.4735,
      "step": 63620
    },
    {
      "epoch": 2.5452,
      "grad_norm": 2.7949225902557373,
      "learning_rate": 7.630872483221477e-06,
      "loss": 0.5314,
      "step": 63630
    },
    {
      "epoch": 2.5456,
      "grad_norm": 3.159679651260376,
      "learning_rate": 7.624161073825503e-06,
      "loss": 0.5261,
      "step": 63640
    },
    {
      "epoch": 2.5460000000000003,
      "grad_norm": 2.098470687866211,
      "learning_rate": 7.617449664429531e-06,
      "loss": 0.4606,
      "step": 63650
    },
    {
      "epoch": 2.5464,
      "grad_norm": 2.4637176990509033,
      "learning_rate": 7.610738255033557e-06,
      "loss": 0.5026,
      "step": 63660
    },
    {
      "epoch": 2.5468,
      "grad_norm": 2.7354533672332764,
      "learning_rate": 7.604026845637584e-06,
      "loss": 0.6133,
      "step": 63670
    },
    {
      "epoch": 2.5472,
      "grad_norm": 2.800999164581299,
      "learning_rate": 7.597315436241612e-06,
      "loss": 0.5258,
      "step": 63680
    },
    {
      "epoch": 2.5476,
      "grad_norm": 2.770787477493286,
      "learning_rate": 7.590604026845638e-06,
      "loss": 0.482,
      "step": 63690
    },
    {
      "epoch": 2.548,
      "grad_norm": 2.7263710498809814,
      "learning_rate": 7.5838926174496645e-06,
      "loss": 0.4971,
      "step": 63700
    },
    {
      "epoch": 2.5484,
      "grad_norm": 2.3309710025787354,
      "learning_rate": 7.577181208053692e-06,
      "loss": 0.5295,
      "step": 63710
    },
    {
      "epoch": 2.5488,
      "grad_norm": 3.0160458087921143,
      "learning_rate": 7.570469798657718e-06,
      "loss": 0.5748,
      "step": 63720
    },
    {
      "epoch": 2.5492,
      "grad_norm": 2.509427309036255,
      "learning_rate": 7.563758389261745e-06,
      "loss": 0.4836,
      "step": 63730
    },
    {
      "epoch": 2.5496,
      "grad_norm": 2.6810011863708496,
      "learning_rate": 7.557046979865773e-06,
      "loss": 0.4634,
      "step": 63740
    },
    {
      "epoch": 2.55,
      "grad_norm": 2.629124879837036,
      "learning_rate": 7.5503355704698e-06,
      "loss": 0.5369,
      "step": 63750
    },
    {
      "epoch": 2.5504,
      "grad_norm": 2.7262394428253174,
      "learning_rate": 7.543624161073826e-06,
      "loss": 0.489,
      "step": 63760
    },
    {
      "epoch": 2.5507999999999997,
      "grad_norm": 2.444514036178589,
      "learning_rate": 7.536912751677852e-06,
      "loss": 0.5428,
      "step": 63770
    },
    {
      "epoch": 2.5512,
      "grad_norm": 1.7099761962890625,
      "learning_rate": 7.53020134228188e-06,
      "loss": 0.4353,
      "step": 63780
    },
    {
      "epoch": 2.5516,
      "grad_norm": 2.8953561782836914,
      "learning_rate": 7.523489932885906e-06,
      "loss": 0.5074,
      "step": 63790
    },
    {
      "epoch": 2.552,
      "grad_norm": 3.205728769302368,
      "learning_rate": 7.516778523489933e-06,
      "loss": 0.592,
      "step": 63800
    },
    {
      "epoch": 2.5524,
      "grad_norm": 2.8505280017852783,
      "learning_rate": 7.510067114093961e-06,
      "loss": 0.5464,
      "step": 63810
    },
    {
      "epoch": 2.5528,
      "grad_norm": 2.009831428527832,
      "learning_rate": 7.503355704697987e-06,
      "loss": 0.547,
      "step": 63820
    },
    {
      "epoch": 2.5532,
      "grad_norm": 2.2997593879699707,
      "learning_rate": 7.496644295302014e-06,
      "loss": 0.4919,
      "step": 63830
    },
    {
      "epoch": 2.5536,
      "grad_norm": 2.7085752487182617,
      "learning_rate": 7.489932885906041e-06,
      "loss": 0.432,
      "step": 63840
    },
    {
      "epoch": 2.5540000000000003,
      "grad_norm": 2.6698734760284424,
      "learning_rate": 7.483221476510067e-06,
      "loss": 0.4758,
      "step": 63850
    },
    {
      "epoch": 2.5544000000000002,
      "grad_norm": 2.808011293411255,
      "learning_rate": 7.476510067114094e-06,
      "loss": 0.4417,
      "step": 63860
    },
    {
      "epoch": 2.5548,
      "grad_norm": 2.973984718322754,
      "learning_rate": 7.469798657718122e-06,
      "loss": 0.5044,
      "step": 63870
    },
    {
      "epoch": 2.5552,
      "grad_norm": 3.218881607055664,
      "learning_rate": 7.463087248322148e-06,
      "loss": 0.5167,
      "step": 63880
    },
    {
      "epoch": 2.5556,
      "grad_norm": 2.3116068840026855,
      "learning_rate": 7.456375838926175e-06,
      "loss": 0.4969,
      "step": 63890
    },
    {
      "epoch": 2.556,
      "grad_norm": 2.9805335998535156,
      "learning_rate": 7.4496644295302024e-06,
      "loss": 0.472,
      "step": 63900
    },
    {
      "epoch": 2.5564,
      "grad_norm": 2.3469104766845703,
      "learning_rate": 7.4429530201342284e-06,
      "loss": 0.4796,
      "step": 63910
    },
    {
      "epoch": 2.5568,
      "grad_norm": 2.2781929969787598,
      "learning_rate": 7.436241610738255e-06,
      "loss": 0.4728,
      "step": 63920
    },
    {
      "epoch": 2.5572,
      "grad_norm": 3.065156936645508,
      "learning_rate": 7.429530201342281e-06,
      "loss": 0.4444,
      "step": 63930
    },
    {
      "epoch": 2.5576,
      "grad_norm": 2.6325390338897705,
      "learning_rate": 7.422818791946309e-06,
      "loss": 0.4995,
      "step": 63940
    },
    {
      "epoch": 2.558,
      "grad_norm": 2.5362131595611572,
      "learning_rate": 7.416107382550336e-06,
      "loss": 0.5203,
      "step": 63950
    },
    {
      "epoch": 2.5584,
      "grad_norm": 3.648458957672119,
      "learning_rate": 7.409395973154362e-06,
      "loss": 0.4279,
      "step": 63960
    },
    {
      "epoch": 2.5587999999999997,
      "grad_norm": 3.1990790367126465,
      "learning_rate": 7.4026845637583896e-06,
      "loss": 0.5272,
      "step": 63970
    },
    {
      "epoch": 2.5592,
      "grad_norm": 2.749952793121338,
      "learning_rate": 7.395973154362416e-06,
      "loss": 0.4941,
      "step": 63980
    },
    {
      "epoch": 2.5596,
      "grad_norm": 2.7619941234588623,
      "learning_rate": 7.389261744966442e-06,
      "loss": 0.4755,
      "step": 63990
    },
    {
      "epoch": 2.56,
      "grad_norm": 2.6018874645233154,
      "learning_rate": 7.382550335570471e-06,
      "loss": 0.5344,
      "step": 64000
    },
    {
      "epoch": 2.5604,
      "grad_norm": 2.5530130863189697,
      "learning_rate": 7.375838926174497e-06,
      "loss": 0.5029,
      "step": 64010
    },
    {
      "epoch": 2.5608,
      "grad_norm": 3.204686164855957,
      "learning_rate": 7.369127516778524e-06,
      "loss": 0.5961,
      "step": 64020
    },
    {
      "epoch": 2.5612,
      "grad_norm": 3.7060136795043945,
      "learning_rate": 7.3624161073825515e-06,
      "loss": 0.5231,
      "step": 64030
    },
    {
      "epoch": 2.5616,
      "grad_norm": 2.1408960819244385,
      "learning_rate": 7.3557046979865775e-06,
      "loss": 0.4252,
      "step": 64040
    },
    {
      "epoch": 2.5620000000000003,
      "grad_norm": 2.914853572845459,
      "learning_rate": 7.348993288590604e-06,
      "loss": 0.5092,
      "step": 64050
    },
    {
      "epoch": 2.5624000000000002,
      "grad_norm": 2.9392058849334717,
      "learning_rate": 7.342281879194632e-06,
      "loss": 0.5041,
      "step": 64060
    },
    {
      "epoch": 2.5628,
      "grad_norm": 2.3649821281433105,
      "learning_rate": 7.335570469798658e-06,
      "loss": 0.5268,
      "step": 64070
    },
    {
      "epoch": 2.5632,
      "grad_norm": 2.705263137817383,
      "learning_rate": 7.328859060402685e-06,
      "loss": 0.5298,
      "step": 64080
    },
    {
      "epoch": 2.5636,
      "grad_norm": 3.1222047805786133,
      "learning_rate": 7.322147651006711e-06,
      "loss": 0.4751,
      "step": 64090
    },
    {
      "epoch": 2.564,
      "grad_norm": 2.835784435272217,
      "learning_rate": 7.315436241610739e-06,
      "loss": 0.4593,
      "step": 64100
    },
    {
      "epoch": 2.5644,
      "grad_norm": 2.681892156600952,
      "learning_rate": 7.3087248322147655e-06,
      "loss": 0.4428,
      "step": 64110
    },
    {
      "epoch": 2.5648,
      "grad_norm": 1.9290857315063477,
      "learning_rate": 7.3020134228187915e-06,
      "loss": 0.5039,
      "step": 64120
    },
    {
      "epoch": 2.5652,
      "grad_norm": 2.816406011581421,
      "learning_rate": 7.295302013422819e-06,
      "loss": 0.5573,
      "step": 64130
    },
    {
      "epoch": 2.5656,
      "grad_norm": 2.8895037174224854,
      "learning_rate": 7.288590604026846e-06,
      "loss": 0.4662,
      "step": 64140
    },
    {
      "epoch": 2.566,
      "grad_norm": 3.826488494873047,
      "learning_rate": 7.281879194630872e-06,
      "loss": 0.5744,
      "step": 64150
    },
    {
      "epoch": 2.5664,
      "grad_norm": 2.5255186557769775,
      "learning_rate": 7.2751677852349e-06,
      "loss": 0.5193,
      "step": 64160
    },
    {
      "epoch": 2.5667999999999997,
      "grad_norm": 2.8236725330352783,
      "learning_rate": 7.268456375838927e-06,
      "loss": 0.4676,
      "step": 64170
    },
    {
      "epoch": 2.5672,
      "grad_norm": 2.8070032596588135,
      "learning_rate": 7.261744966442953e-06,
      "loss": 0.5218,
      "step": 64180
    },
    {
      "epoch": 2.5676,
      "grad_norm": 2.6642632484436035,
      "learning_rate": 7.25503355704698e-06,
      "loss": 0.4773,
      "step": 64190
    },
    {
      "epoch": 2.568,
      "grad_norm": 2.240293025970459,
      "learning_rate": 7.248322147651007e-06,
      "loss": 0.4369,
      "step": 64200
    },
    {
      "epoch": 2.5684,
      "grad_norm": 2.3031270503997803,
      "learning_rate": 7.241610738255033e-06,
      "loss": 0.5115,
      "step": 64210
    },
    {
      "epoch": 2.5688,
      "grad_norm": 2.441715717315674,
      "learning_rate": 7.234899328859062e-06,
      "loss": 0.5514,
      "step": 64220
    },
    {
      "epoch": 2.5692,
      "grad_norm": 1.9557840824127197,
      "learning_rate": 7.228187919463088e-06,
      "loss": 0.4041,
      "step": 64230
    },
    {
      "epoch": 2.5696,
      "grad_norm": 1.75407075881958,
      "learning_rate": 7.221476510067115e-06,
      "loss": 0.4081,
      "step": 64240
    },
    {
      "epoch": 2.57,
      "grad_norm": 2.138430118560791,
      "learning_rate": 7.214765100671142e-06,
      "loss": 0.5142,
      "step": 64250
    },
    {
      "epoch": 2.5704000000000002,
      "grad_norm": 2.7817702293395996,
      "learning_rate": 7.208053691275168e-06,
      "loss": 0.4793,
      "step": 64260
    },
    {
      "epoch": 2.5708,
      "grad_norm": 2.485466718673706,
      "learning_rate": 7.201342281879195e-06,
      "loss": 0.4309,
      "step": 64270
    },
    {
      "epoch": 2.5712,
      "grad_norm": 2.5801408290863037,
      "learning_rate": 7.194630872483221e-06,
      "loss": 0.5239,
      "step": 64280
    },
    {
      "epoch": 2.5716,
      "grad_norm": 2.9023213386535645,
      "learning_rate": 7.187919463087249e-06,
      "loss": 0.537,
      "step": 64290
    },
    {
      "epoch": 2.572,
      "grad_norm": 2.5583457946777344,
      "learning_rate": 7.181208053691276e-06,
      "loss": 0.4686,
      "step": 64300
    },
    {
      "epoch": 2.5724,
      "grad_norm": 2.3383755683898926,
      "learning_rate": 7.174496644295302e-06,
      "loss": 0.4379,
      "step": 64310
    },
    {
      "epoch": 2.5728,
      "grad_norm": 3.125988721847534,
      "learning_rate": 7.167785234899329e-06,
      "loss": 0.4856,
      "step": 64320
    },
    {
      "epoch": 2.5732,
      "grad_norm": 2.231175184249878,
      "learning_rate": 7.161073825503356e-06,
      "loss": 0.5059,
      "step": 64330
    },
    {
      "epoch": 2.5736,
      "grad_norm": 2.4084365367889404,
      "learning_rate": 7.154362416107382e-06,
      "loss": 0.432,
      "step": 64340
    },
    {
      "epoch": 2.574,
      "grad_norm": 2.917724370956421,
      "learning_rate": 7.14765100671141e-06,
      "loss": 0.5266,
      "step": 64350
    },
    {
      "epoch": 2.5744,
      "grad_norm": 2.7833471298217773,
      "learning_rate": 7.140939597315437e-06,
      "loss": 0.5242,
      "step": 64360
    },
    {
      "epoch": 2.5747999999999998,
      "grad_norm": 3.0557117462158203,
      "learning_rate": 7.134228187919463e-06,
      "loss": 0.506,
      "step": 64370
    },
    {
      "epoch": 2.5752,
      "grad_norm": 2.3856563568115234,
      "learning_rate": 7.1275167785234905e-06,
      "loss": 0.4772,
      "step": 64380
    },
    {
      "epoch": 2.5756,
      "grad_norm": 2.709291934967041,
      "learning_rate": 7.120805369127517e-06,
      "loss": 0.4557,
      "step": 64390
    },
    {
      "epoch": 2.576,
      "grad_norm": 2.833341598510742,
      "learning_rate": 7.114093959731543e-06,
      "loss": 0.4983,
      "step": 64400
    },
    {
      "epoch": 2.5764,
      "grad_norm": 2.0254549980163574,
      "learning_rate": 7.107382550335571e-06,
      "loss": 0.505,
      "step": 64410
    },
    {
      "epoch": 2.5768,
      "grad_norm": 2.3015692234039307,
      "learning_rate": 7.100671140939598e-06,
      "loss": 0.506,
      "step": 64420
    },
    {
      "epoch": 2.5772,
      "grad_norm": 2.6718363761901855,
      "learning_rate": 7.093959731543624e-06,
      "loss": 0.5282,
      "step": 64430
    },
    {
      "epoch": 2.5776,
      "grad_norm": 2.727804660797119,
      "learning_rate": 7.087248322147651e-06,
      "loss": 0.536,
      "step": 64440
    },
    {
      "epoch": 2.578,
      "grad_norm": 2.2972216606140137,
      "learning_rate": 7.0805369127516785e-06,
      "loss": 0.4219,
      "step": 64450
    },
    {
      "epoch": 2.5784000000000002,
      "grad_norm": 3.059218645095825,
      "learning_rate": 7.0738255033557045e-06,
      "loss": 0.4775,
      "step": 64460
    },
    {
      "epoch": 2.5788,
      "grad_norm": 3.0729575157165527,
      "learning_rate": 7.067114093959731e-06,
      "loss": 0.5227,
      "step": 64470
    },
    {
      "epoch": 2.5792,
      "grad_norm": 2.99326229095459,
      "learning_rate": 7.060402684563759e-06,
      "loss": 0.5611,
      "step": 64480
    },
    {
      "epoch": 2.5796,
      "grad_norm": 2.268049955368042,
      "learning_rate": 7.053691275167786e-06,
      "loss": 0.4556,
      "step": 64490
    },
    {
      "epoch": 2.58,
      "grad_norm": 3.060366153717041,
      "learning_rate": 7.046979865771812e-06,
      "loss": 0.5432,
      "step": 64500
    },
    {
      "epoch": 2.5804,
      "grad_norm": 2.8013641834259033,
      "learning_rate": 7.04026845637584e-06,
      "loss": 0.5948,
      "step": 64510
    },
    {
      "epoch": 2.5808,
      "grad_norm": 2.216635227203369,
      "learning_rate": 7.0335570469798665e-06,
      "loss": 0.5347,
      "step": 64520
    },
    {
      "epoch": 2.5812,
      "grad_norm": 1.986992359161377,
      "learning_rate": 7.0268456375838925e-06,
      "loss": 0.5619,
      "step": 64530
    },
    {
      "epoch": 2.5816,
      "grad_norm": 2.570223093032837,
      "learning_rate": 7.02013422818792e-06,
      "loss": 0.5238,
      "step": 64540
    },
    {
      "epoch": 2.582,
      "grad_norm": 2.411543846130371,
      "learning_rate": 7.013422818791947e-06,
      "loss": 0.5174,
      "step": 64550
    },
    {
      "epoch": 2.5824,
      "grad_norm": 2.9331002235412598,
      "learning_rate": 7.006711409395973e-06,
      "loss": 0.5033,
      "step": 64560
    },
    {
      "epoch": 2.5827999999999998,
      "grad_norm": 2.406256675720215,
      "learning_rate": 7.000000000000001e-06,
      "loss": 0.5148,
      "step": 64570
    },
    {
      "epoch": 2.5832,
      "grad_norm": 2.794563055038452,
      "learning_rate": 6.993288590604028e-06,
      "loss": 0.4451,
      "step": 64580
    },
    {
      "epoch": 2.5836,
      "grad_norm": 2.8408405780792236,
      "learning_rate": 6.986577181208054e-06,
      "loss": 0.5207,
      "step": 64590
    },
    {
      "epoch": 2.584,
      "grad_norm": 2.3125314712524414,
      "learning_rate": 6.9798657718120805e-06,
      "loss": 0.5493,
      "step": 64600
    },
    {
      "epoch": 2.5844,
      "grad_norm": 2.8751721382141113,
      "learning_rate": 6.973154362416108e-06,
      "loss": 0.4829,
      "step": 64610
    },
    {
      "epoch": 2.5848,
      "grad_norm": 2.637424945831299,
      "learning_rate": 6.966442953020134e-06,
      "loss": 0.4805,
      "step": 64620
    },
    {
      "epoch": 2.5852,
      "grad_norm": 2.715078830718994,
      "learning_rate": 6.959731543624161e-06,
      "loss": 0.5165,
      "step": 64630
    },
    {
      "epoch": 2.5856,
      "grad_norm": 2.656261920928955,
      "learning_rate": 6.953020134228189e-06,
      "loss": 0.4902,
      "step": 64640
    },
    {
      "epoch": 2.586,
      "grad_norm": 2.409282684326172,
      "learning_rate": 6.946308724832215e-06,
      "loss": 0.4212,
      "step": 64650
    },
    {
      "epoch": 2.5864000000000003,
      "grad_norm": 2.339721918106079,
      "learning_rate": 6.9395973154362416e-06,
      "loss": 0.4899,
      "step": 64660
    },
    {
      "epoch": 2.5868,
      "grad_norm": 2.7496683597564697,
      "learning_rate": 6.932885906040269e-06,
      "loss": 0.4877,
      "step": 64670
    },
    {
      "epoch": 2.5872,
      "grad_norm": 2.4232730865478516,
      "learning_rate": 6.926174496644295e-06,
      "loss": 0.4855,
      "step": 64680
    },
    {
      "epoch": 2.5876,
      "grad_norm": 2.9838197231292725,
      "learning_rate": 6.919463087248322e-06,
      "loss": 0.5185,
      "step": 64690
    },
    {
      "epoch": 2.588,
      "grad_norm": 2.8346493244171143,
      "learning_rate": 6.91275167785235e-06,
      "loss": 0.5053,
      "step": 64700
    },
    {
      "epoch": 2.5884,
      "grad_norm": 2.6773855686187744,
      "learning_rate": 6.906040268456377e-06,
      "loss": 0.5371,
      "step": 64710
    },
    {
      "epoch": 2.5888,
      "grad_norm": 1.8477458953857422,
      "learning_rate": 6.899328859060403e-06,
      "loss": 0.4708,
      "step": 64720
    },
    {
      "epoch": 2.5892,
      "grad_norm": 2.9136548042297363,
      "learning_rate": 6.89261744966443e-06,
      "loss": 0.506,
      "step": 64730
    },
    {
      "epoch": 2.5896,
      "grad_norm": 2.6025726795196533,
      "learning_rate": 6.885906040268457e-06,
      "loss": 0.5342,
      "step": 64740
    },
    {
      "epoch": 2.59,
      "grad_norm": 2.187588930130005,
      "learning_rate": 6.879194630872483e-06,
      "loss": 0.4794,
      "step": 64750
    },
    {
      "epoch": 2.5904,
      "grad_norm": 2.5435266494750977,
      "learning_rate": 6.87248322147651e-06,
      "loss": 0.5268,
      "step": 64760
    },
    {
      "epoch": 2.5907999999999998,
      "grad_norm": 2.9758007526397705,
      "learning_rate": 6.865771812080538e-06,
      "loss": 0.4216,
      "step": 64770
    },
    {
      "epoch": 2.5911999999999997,
      "grad_norm": 2.290693521499634,
      "learning_rate": 6.859060402684564e-06,
      "loss": 0.4311,
      "step": 64780
    },
    {
      "epoch": 2.5916,
      "grad_norm": 2.757037878036499,
      "learning_rate": 6.852348993288591e-06,
      "loss": 0.4704,
      "step": 64790
    },
    {
      "epoch": 2.592,
      "grad_norm": 1.9142197370529175,
      "learning_rate": 6.845637583892618e-06,
      "loss": 0.4078,
      "step": 64800
    },
    {
      "epoch": 2.5924,
      "grad_norm": 2.5791094303131104,
      "learning_rate": 6.838926174496644e-06,
      "loss": 0.4912,
      "step": 64810
    },
    {
      "epoch": 2.5928,
      "grad_norm": 2.005391836166382,
      "learning_rate": 6.832214765100671e-06,
      "loss": 0.5069,
      "step": 64820
    },
    {
      "epoch": 2.5932,
      "grad_norm": 2.33915376663208,
      "learning_rate": 6.825503355704699e-06,
      "loss": 0.4608,
      "step": 64830
    },
    {
      "epoch": 2.5936,
      "grad_norm": 2.4200990200042725,
      "learning_rate": 6.818791946308725e-06,
      "loss": 0.5025,
      "step": 64840
    },
    {
      "epoch": 2.594,
      "grad_norm": 2.5994575023651123,
      "learning_rate": 6.812080536912752e-06,
      "loss": 0.5438,
      "step": 64850
    },
    {
      "epoch": 2.5944000000000003,
      "grad_norm": 2.585036516189575,
      "learning_rate": 6.8053691275167795e-06,
      "loss": 0.5369,
      "step": 64860
    },
    {
      "epoch": 2.5948,
      "grad_norm": 3.2679169178009033,
      "learning_rate": 6.7986577181208055e-06,
      "loss": 0.4938,
      "step": 64870
    },
    {
      "epoch": 2.5952,
      "grad_norm": 4.562999725341797,
      "learning_rate": 6.791946308724832e-06,
      "loss": 0.4818,
      "step": 64880
    },
    {
      "epoch": 2.5956,
      "grad_norm": 2.296677827835083,
      "learning_rate": 6.78523489932886e-06,
      "loss": 0.4601,
      "step": 64890
    },
    {
      "epoch": 2.596,
      "grad_norm": 2.889139413833618,
      "learning_rate": 6.778523489932886e-06,
      "loss": 0.4475,
      "step": 64900
    },
    {
      "epoch": 2.5964,
      "grad_norm": 2.0350379943847656,
      "learning_rate": 6.771812080536913e-06,
      "loss": 0.4729,
      "step": 64910
    },
    {
      "epoch": 2.5968,
      "grad_norm": 2.564112663269043,
      "learning_rate": 6.765100671140939e-06,
      "loss": 0.5388,
      "step": 64920
    },
    {
      "epoch": 2.5972,
      "grad_norm": 2.4764328002929688,
      "learning_rate": 6.7583892617449675e-06,
      "loss": 0.5003,
      "step": 64930
    },
    {
      "epoch": 2.5976,
      "grad_norm": 2.8276543617248535,
      "learning_rate": 6.7516778523489935e-06,
      "loss": 0.5367,
      "step": 64940
    },
    {
      "epoch": 2.598,
      "grad_norm": 2.5122694969177246,
      "learning_rate": 6.7449664429530195e-06,
      "loss": 0.4523,
      "step": 64950
    },
    {
      "epoch": 2.5984,
      "grad_norm": 2.936098098754883,
      "learning_rate": 6.738255033557048e-06,
      "loss": 0.5289,
      "step": 64960
    },
    {
      "epoch": 2.5987999999999998,
      "grad_norm": 2.722001314163208,
      "learning_rate": 6.731543624161074e-06,
      "loss": 0.5563,
      "step": 64970
    },
    {
      "epoch": 2.5991999999999997,
      "grad_norm": 2.5360515117645264,
      "learning_rate": 6.724832214765101e-06,
      "loss": 0.4471,
      "step": 64980
    },
    {
      "epoch": 2.5996,
      "grad_norm": 2.9086310863494873,
      "learning_rate": 6.7181208053691286e-06,
      "loss": 0.4871,
      "step": 64990
    },
    {
      "epoch": 2.6,
      "grad_norm": 3.299988031387329,
      "learning_rate": 6.7114093959731546e-06,
      "loss": 0.4409,
      "step": 65000
    },
    {
      "epoch": 2.6004,
      "grad_norm": 2.407522201538086,
      "learning_rate": 6.7046979865771814e-06,
      "loss": 0.4589,
      "step": 65010
    },
    {
      "epoch": 2.6008,
      "grad_norm": 3.7812256813049316,
      "learning_rate": 6.697986577181209e-06,
      "loss": 0.4901,
      "step": 65020
    },
    {
      "epoch": 2.6012,
      "grad_norm": 2.3655691146850586,
      "learning_rate": 6.691275167785235e-06,
      "loss": 0.5259,
      "step": 65030
    },
    {
      "epoch": 2.6016,
      "grad_norm": 3.0278024673461914,
      "learning_rate": 6.684563758389262e-06,
      "loss": 0.3838,
      "step": 65040
    },
    {
      "epoch": 2.602,
      "grad_norm": 2.6344714164733887,
      "learning_rate": 6.67785234899329e-06,
      "loss": 0.6224,
      "step": 65050
    },
    {
      "epoch": 2.6024000000000003,
      "grad_norm": 2.395681858062744,
      "learning_rate": 6.671140939597316e-06,
      "loss": 0.4636,
      "step": 65060
    },
    {
      "epoch": 2.6028000000000002,
      "grad_norm": 2.3173444271087646,
      "learning_rate": 6.6644295302013425e-06,
      "loss": 0.5755,
      "step": 65070
    },
    {
      "epoch": 2.6032,
      "grad_norm": 2.396111488342285,
      "learning_rate": 6.6577181208053686e-06,
      "loss": 0.5016,
      "step": 65080
    },
    {
      "epoch": 2.6036,
      "grad_norm": 3.0755436420440674,
      "learning_rate": 6.651006711409396e-06,
      "loss": 0.5748,
      "step": 65090
    },
    {
      "epoch": 2.604,
      "grad_norm": 2.440516948699951,
      "learning_rate": 6.644295302013423e-06,
      "loss": 0.5133,
      "step": 65100
    },
    {
      "epoch": 2.6044,
      "grad_norm": 3.0108230113983154,
      "learning_rate": 6.637583892617449e-06,
      "loss": 0.4864,
      "step": 65110
    },
    {
      "epoch": 2.6048,
      "grad_norm": 3.1967005729675293,
      "learning_rate": 6.630872483221477e-06,
      "loss": 0.5436,
      "step": 65120
    },
    {
      "epoch": 2.6052,
      "grad_norm": 2.942854642868042,
      "learning_rate": 6.624161073825504e-06,
      "loss": 0.5621,
      "step": 65130
    },
    {
      "epoch": 2.6056,
      "grad_norm": 2.14804744720459,
      "learning_rate": 6.61744966442953e-06,
      "loss": 0.4667,
      "step": 65140
    },
    {
      "epoch": 2.606,
      "grad_norm": 2.6430935859680176,
      "learning_rate": 6.610738255033558e-06,
      "loss": 0.5038,
      "step": 65150
    },
    {
      "epoch": 2.6064,
      "grad_norm": 3.1784167289733887,
      "learning_rate": 6.604026845637584e-06,
      "loss": 0.4947,
      "step": 65160
    },
    {
      "epoch": 2.6068,
      "grad_norm": 2.4475255012512207,
      "learning_rate": 6.59731543624161e-06,
      "loss": 0.5041,
      "step": 65170
    },
    {
      "epoch": 2.6071999999999997,
      "grad_norm": 2.4827322959899902,
      "learning_rate": 6.590604026845639e-06,
      "loss": 0.4936,
      "step": 65180
    },
    {
      "epoch": 2.6076,
      "grad_norm": 2.613741397857666,
      "learning_rate": 6.583892617449665e-06,
      "loss": 0.5088,
      "step": 65190
    },
    {
      "epoch": 2.608,
      "grad_norm": 2.2125802040100098,
      "learning_rate": 6.577181208053692e-06,
      "loss": 0.4815,
      "step": 65200
    },
    {
      "epoch": 2.6084,
      "grad_norm": 2.343768835067749,
      "learning_rate": 6.570469798657719e-06,
      "loss": 0.4168,
      "step": 65210
    },
    {
      "epoch": 2.6088,
      "grad_norm": 2.6161537170410156,
      "learning_rate": 6.563758389261745e-06,
      "loss": 0.4956,
      "step": 65220
    },
    {
      "epoch": 2.6092,
      "grad_norm": 2.577206611633301,
      "learning_rate": 6.557046979865772e-06,
      "loss": 0.5262,
      "step": 65230
    },
    {
      "epoch": 2.6096,
      "grad_norm": 2.3585329055786133,
      "learning_rate": 6.550335570469798e-06,
      "loss": 0.5168,
      "step": 65240
    },
    {
      "epoch": 2.61,
      "grad_norm": 2.6074461936950684,
      "learning_rate": 6.543624161073826e-06,
      "loss": 0.591,
      "step": 65250
    },
    {
      "epoch": 2.6104000000000003,
      "grad_norm": 3.1708574295043945,
      "learning_rate": 6.536912751677853e-06,
      "loss": 0.4993,
      "step": 65260
    },
    {
      "epoch": 2.6108000000000002,
      "grad_norm": 2.3833930492401123,
      "learning_rate": 6.530201342281879e-06,
      "loss": 0.4692,
      "step": 65270
    },
    {
      "epoch": 2.6112,
      "grad_norm": 2.772888422012329,
      "learning_rate": 6.5234899328859065e-06,
      "loss": 0.3987,
      "step": 65280
    },
    {
      "epoch": 2.6116,
      "grad_norm": 2.719130754470825,
      "learning_rate": 6.516778523489933e-06,
      "loss": 0.5262,
      "step": 65290
    },
    {
      "epoch": 2.612,
      "grad_norm": 2.6296889781951904,
      "learning_rate": 6.510067114093959e-06,
      "loss": 0.4781,
      "step": 65300
    },
    {
      "epoch": 2.6124,
      "grad_norm": 2.5620839595794678,
      "learning_rate": 6.503355704697987e-06,
      "loss": 0.5513,
      "step": 65310
    },
    {
      "epoch": 2.6128,
      "grad_norm": 2.592787981033325,
      "learning_rate": 6.496644295302014e-06,
      "loss": 0.5059,
      "step": 65320
    },
    {
      "epoch": 2.6132,
      "grad_norm": 2.702657699584961,
      "learning_rate": 6.48993288590604e-06,
      "loss": 0.4774,
      "step": 65330
    },
    {
      "epoch": 2.6136,
      "grad_norm": 2.7511789798736572,
      "learning_rate": 6.483221476510068e-06,
      "loss": 0.4848,
      "step": 65340
    },
    {
      "epoch": 2.614,
      "grad_norm": 2.64217472076416,
      "learning_rate": 6.4765100671140944e-06,
      "loss": 0.5303,
      "step": 65350
    },
    {
      "epoch": 2.6144,
      "grad_norm": 2.581214427947998,
      "learning_rate": 6.4697986577181204e-06,
      "loss": 0.5175,
      "step": 65360
    },
    {
      "epoch": 2.6148,
      "grad_norm": 3.028980255126953,
      "learning_rate": 6.463087248322149e-06,
      "loss": 0.5018,
      "step": 65370
    },
    {
      "epoch": 2.6151999999999997,
      "grad_norm": 3.2939488887786865,
      "learning_rate": 6.456375838926175e-06,
      "loss": 0.553,
      "step": 65380
    },
    {
      "epoch": 2.6156,
      "grad_norm": 2.330613851547241,
      "learning_rate": 6.449664429530201e-06,
      "loss": 0.4874,
      "step": 65390
    },
    {
      "epoch": 2.616,
      "grad_norm": 2.2802488803863525,
      "learning_rate": 6.4429530201342295e-06,
      "loss": 0.5399,
      "step": 65400
    },
    {
      "epoch": 2.6164,
      "grad_norm": 2.836301565170288,
      "learning_rate": 6.4362416107382556e-06,
      "loss": 0.5384,
      "step": 65410
    },
    {
      "epoch": 2.6168,
      "grad_norm": 2.9434783458709717,
      "learning_rate": 6.429530201342282e-06,
      "loss": 0.5757,
      "step": 65420
    },
    {
      "epoch": 2.6172,
      "grad_norm": 2.5861411094665527,
      "learning_rate": 6.422818791946308e-06,
      "loss": 0.4831,
      "step": 65430
    },
    {
      "epoch": 2.6176,
      "grad_norm": 2.2169363498687744,
      "learning_rate": 6.416107382550336e-06,
      "loss": 0.5249,
      "step": 65440
    },
    {
      "epoch": 2.618,
      "grad_norm": 2.435452461242676,
      "learning_rate": 6.409395973154363e-06,
      "loss": 0.4726,
      "step": 65450
    },
    {
      "epoch": 2.6184,
      "grad_norm": 2.1417758464813232,
      "learning_rate": 6.402684563758389e-06,
      "loss": 0.4605,
      "step": 65460
    },
    {
      "epoch": 2.6188000000000002,
      "grad_norm": 2.775247573852539,
      "learning_rate": 6.395973154362417e-06,
      "loss": 0.4894,
      "step": 65470
    },
    {
      "epoch": 2.6192,
      "grad_norm": 2.6065990924835205,
      "learning_rate": 6.3892617449664435e-06,
      "loss": 0.4697,
      "step": 65480
    },
    {
      "epoch": 2.6196,
      "grad_norm": 2.171098470687866,
      "learning_rate": 6.3825503355704695e-06,
      "loss": 0.5324,
      "step": 65490
    },
    {
      "epoch": 2.62,
      "grad_norm": 2.556300163269043,
      "learning_rate": 6.375838926174497e-06,
      "loss": 0.5059,
      "step": 65500
    },
    {
      "epoch": 2.6204,
      "grad_norm": 2.228248357772827,
      "learning_rate": 6.369127516778524e-06,
      "loss": 0.4862,
      "step": 65510
    },
    {
      "epoch": 2.6208,
      "grad_norm": 2.750821828842163,
      "learning_rate": 6.36241610738255e-06,
      "loss": 0.5248,
      "step": 65520
    },
    {
      "epoch": 2.6212,
      "grad_norm": 2.5388622283935547,
      "learning_rate": 6.355704697986578e-06,
      "loss": 0.54,
      "step": 65530
    },
    {
      "epoch": 2.6216,
      "grad_norm": 3.042506217956543,
      "learning_rate": 6.348993288590605e-06,
      "loss": 0.4798,
      "step": 65540
    },
    {
      "epoch": 2.622,
      "grad_norm": 3.5421698093414307,
      "learning_rate": 6.342281879194631e-06,
      "loss": 0.4455,
      "step": 65550
    },
    {
      "epoch": 2.6224,
      "grad_norm": 2.9172306060791016,
      "learning_rate": 6.335570469798658e-06,
      "loss": 0.4672,
      "step": 65560
    },
    {
      "epoch": 2.6228,
      "grad_norm": 2.5935211181640625,
      "learning_rate": 6.328859060402685e-06,
      "loss": 0.5073,
      "step": 65570
    },
    {
      "epoch": 2.6231999999999998,
      "grad_norm": 2.526244878768921,
      "learning_rate": 6.322147651006711e-06,
      "loss": 0.5204,
      "step": 65580
    },
    {
      "epoch": 2.6236,
      "grad_norm": 3.302213430404663,
      "learning_rate": 6.315436241610738e-06,
      "loss": 0.4641,
      "step": 65590
    },
    {
      "epoch": 2.624,
      "grad_norm": 2.477774143218994,
      "learning_rate": 6.308724832214766e-06,
      "loss": 0.4953,
      "step": 65600
    },
    {
      "epoch": 2.6244,
      "grad_norm": 2.50929594039917,
      "learning_rate": 6.302013422818792e-06,
      "loss": 0.4939,
      "step": 65610
    },
    {
      "epoch": 2.6248,
      "grad_norm": 2.2857892513275146,
      "learning_rate": 6.295302013422819e-06,
      "loss": 0.5162,
      "step": 65620
    },
    {
      "epoch": 2.6252,
      "grad_norm": 2.939716339111328,
      "learning_rate": 6.288590604026846e-06,
      "loss": 0.61,
      "step": 65630
    },
    {
      "epoch": 2.6256,
      "grad_norm": 1.9183908700942993,
      "learning_rate": 6.281879194630873e-06,
      "loss": 0.4546,
      "step": 65640
    },
    {
      "epoch": 2.626,
      "grad_norm": 2.7978501319885254,
      "learning_rate": 6.275167785234899e-06,
      "loss": 0.4896,
      "step": 65650
    },
    {
      "epoch": 2.6264,
      "grad_norm": 2.392845869064331,
      "learning_rate": 6.268456375838927e-06,
      "loss": 0.475,
      "step": 65660
    },
    {
      "epoch": 2.6268000000000002,
      "grad_norm": 2.6937739849090576,
      "learning_rate": 6.261744966442954e-06,
      "loss": 0.4799,
      "step": 65670
    },
    {
      "epoch": 2.6272,
      "grad_norm": 2.6892459392547607,
      "learning_rate": 6.25503355704698e-06,
      "loss": 0.5763,
      "step": 65680
    },
    {
      "epoch": 2.6276,
      "grad_norm": 2.061680316925049,
      "learning_rate": 6.248322147651007e-06,
      "loss": 0.5989,
      "step": 65690
    },
    {
      "epoch": 2.628,
      "grad_norm": 2.729187488555908,
      "learning_rate": 6.241610738255034e-06,
      "loss": 0.5113,
      "step": 65700
    },
    {
      "epoch": 2.6284,
      "grad_norm": 2.737618923187256,
      "learning_rate": 6.23489932885906e-06,
      "loss": 0.395,
      "step": 65710
    },
    {
      "epoch": 2.6288,
      "grad_norm": 2.344459295272827,
      "learning_rate": 6.228187919463087e-06,
      "loss": 0.452,
      "step": 65720
    },
    {
      "epoch": 2.6292,
      "grad_norm": 3.237497091293335,
      "learning_rate": 6.221476510067115e-06,
      "loss": 0.5368,
      "step": 65730
    },
    {
      "epoch": 2.6296,
      "grad_norm": 2.898026943206787,
      "learning_rate": 6.214765100671141e-06,
      "loss": 0.4854,
      "step": 65740
    },
    {
      "epoch": 2.63,
      "grad_norm": 2.458723306655884,
      "learning_rate": 6.2080536912751686e-06,
      "loss": 0.4934,
      "step": 65750
    },
    {
      "epoch": 2.6304,
      "grad_norm": 2.7457878589630127,
      "learning_rate": 6.201342281879195e-06,
      "loss": 0.5614,
      "step": 65760
    },
    {
      "epoch": 2.6308,
      "grad_norm": 3.172720432281494,
      "learning_rate": 6.194630872483221e-06,
      "loss": 0.4857,
      "step": 65770
    },
    {
      "epoch": 2.6311999999999998,
      "grad_norm": 2.277430534362793,
      "learning_rate": 6.187919463087249e-06,
      "loss": 0.508,
      "step": 65780
    },
    {
      "epoch": 2.6316,
      "grad_norm": 2.471900463104248,
      "learning_rate": 6.181208053691275e-06,
      "loss": 0.5314,
      "step": 65790
    },
    {
      "epoch": 2.632,
      "grad_norm": 2.276482105255127,
      "learning_rate": 6.174496644295302e-06,
      "loss": 0.4182,
      "step": 65800
    },
    {
      "epoch": 2.6324,
      "grad_norm": 3.6091439723968506,
      "learning_rate": 6.16778523489933e-06,
      "loss": 0.5408,
      "step": 65810
    },
    {
      "epoch": 2.6328,
      "grad_norm": 2.422722339630127,
      "learning_rate": 6.161073825503356e-06,
      "loss": 0.5444,
      "step": 65820
    },
    {
      "epoch": 2.6332,
      "grad_norm": 2.9854094982147217,
      "learning_rate": 6.1543624161073825e-06,
      "loss": 0.5083,
      "step": 65830
    },
    {
      "epoch": 2.6336,
      "grad_norm": 2.8186275959014893,
      "learning_rate": 6.14765100671141e-06,
      "loss": 0.482,
      "step": 65840
    },
    {
      "epoch": 2.634,
      "grad_norm": 2.263411521911621,
      "learning_rate": 6.140939597315436e-06,
      "loss": 0.464,
      "step": 65850
    },
    {
      "epoch": 2.6344,
      "grad_norm": 2.2541096210479736,
      "learning_rate": 6.134228187919464e-06,
      "loss": 0.4522,
      "step": 65860
    },
    {
      "epoch": 2.6348000000000003,
      "grad_norm": 3.0887160301208496,
      "learning_rate": 6.12751677852349e-06,
      "loss": 0.528,
      "step": 65870
    },
    {
      "epoch": 2.6352,
      "grad_norm": 2.143967628479004,
      "learning_rate": 6.120805369127517e-06,
      "loss": 0.5646,
      "step": 65880
    },
    {
      "epoch": 2.6356,
      "grad_norm": 2.174311637878418,
      "learning_rate": 6.1140939597315445e-06,
      "loss": 0.4827,
      "step": 65890
    },
    {
      "epoch": 2.636,
      "grad_norm": 2.4574458599090576,
      "learning_rate": 6.1073825503355705e-06,
      "loss": 0.4866,
      "step": 65900
    },
    {
      "epoch": 2.6364,
      "grad_norm": 2.841813564300537,
      "learning_rate": 6.100671140939597e-06,
      "loss": 0.4347,
      "step": 65910
    },
    {
      "epoch": 2.6368,
      "grad_norm": 2.532081365585327,
      "learning_rate": 6.093959731543625e-06,
      "loss": 0.4844,
      "step": 65920
    },
    {
      "epoch": 2.6372,
      "grad_norm": 2.5191030502319336,
      "learning_rate": 6.087248322147651e-06,
      "loss": 0.4579,
      "step": 65930
    },
    {
      "epoch": 2.6376,
      "grad_norm": 3.0342319011688232,
      "learning_rate": 6.080536912751678e-06,
      "loss": 0.5337,
      "step": 65940
    },
    {
      "epoch": 2.638,
      "grad_norm": 2.738330602645874,
      "learning_rate": 6.073825503355705e-06,
      "loss": 0.5297,
      "step": 65950
    },
    {
      "epoch": 2.6384,
      "grad_norm": 2.7467803955078125,
      "learning_rate": 6.067114093959732e-06,
      "loss": 0.4559,
      "step": 65960
    },
    {
      "epoch": 2.6388,
      "grad_norm": 2.9642341136932373,
      "learning_rate": 6.0604026845637585e-06,
      "loss": 0.5171,
      "step": 65970
    },
    {
      "epoch": 2.6391999999999998,
      "grad_norm": 2.835758924484253,
      "learning_rate": 6.053691275167785e-06,
      "loss": 0.4958,
      "step": 65980
    },
    {
      "epoch": 2.6395999999999997,
      "grad_norm": 2.666490316390991,
      "learning_rate": 6.046979865771812e-06,
      "loss": 0.4598,
      "step": 65990
    },
    {
      "epoch": 2.64,
      "grad_norm": 2.936650276184082,
      "learning_rate": 6.04026845637584e-06,
      "loss": 0.5374,
      "step": 66000
    },
    {
      "epoch": 2.6404,
      "grad_norm": 2.6182663440704346,
      "learning_rate": 6.033557046979866e-06,
      "loss": 0.4464,
      "step": 66010
    },
    {
      "epoch": 2.6408,
      "grad_norm": 4.247682094573975,
      "learning_rate": 6.026845637583893e-06,
      "loss": 0.5188,
      "step": 66020
    },
    {
      "epoch": 2.6412,
      "grad_norm": 2.646571159362793,
      "learning_rate": 6.02013422818792e-06,
      "loss": 0.4843,
      "step": 66030
    },
    {
      "epoch": 2.6416,
      "grad_norm": 2.268836736679077,
      "learning_rate": 6.0134228187919464e-06,
      "loss": 0.5052,
      "step": 66040
    },
    {
      "epoch": 2.642,
      "grad_norm": 2.444312334060669,
      "learning_rate": 6.006711409395973e-06,
      "loss": 0.4761,
      "step": 66050
    },
    {
      "epoch": 2.6424,
      "grad_norm": 2.4614739418029785,
      "learning_rate": 6e-06,
      "loss": 0.4839,
      "step": 66060
    },
    {
      "epoch": 2.6428000000000003,
      "grad_norm": 2.5509965419769287,
      "learning_rate": 5.993288590604027e-06,
      "loss": 0.5057,
      "step": 66070
    },
    {
      "epoch": 2.6432,
      "grad_norm": 2.5431931018829346,
      "learning_rate": 5.986577181208054e-06,
      "loss": 0.4948,
      "step": 66080
    },
    {
      "epoch": 2.6436,
      "grad_norm": 3.1887295246124268,
      "learning_rate": 5.979865771812081e-06,
      "loss": 0.5409,
      "step": 66090
    },
    {
      "epoch": 2.644,
      "grad_norm": 3.0277156829833984,
      "learning_rate": 5.9731543624161076e-06,
      "loss": 0.4903,
      "step": 66100
    },
    {
      "epoch": 2.6444,
      "grad_norm": 2.7532567977905273,
      "learning_rate": 5.966442953020134e-06,
      "loss": 0.4191,
      "step": 66110
    },
    {
      "epoch": 2.6448,
      "grad_norm": 3.145634412765503,
      "learning_rate": 5.959731543624161e-06,
      "loss": 0.5253,
      "step": 66120
    },
    {
      "epoch": 2.6452,
      "grad_norm": 2.5813779830932617,
      "learning_rate": 5.953020134228188e-06,
      "loss": 0.5881,
      "step": 66130
    },
    {
      "epoch": 2.6456,
      "grad_norm": 2.618647813796997,
      "learning_rate": 5.946308724832215e-06,
      "loss": 0.4408,
      "step": 66140
    },
    {
      "epoch": 2.646,
      "grad_norm": 2.7100791931152344,
      "learning_rate": 5.939597315436242e-06,
      "loss": 0.4858,
      "step": 66150
    },
    {
      "epoch": 2.6464,
      "grad_norm": 2.9154837131500244,
      "learning_rate": 5.932885906040269e-06,
      "loss": 0.5153,
      "step": 66160
    },
    {
      "epoch": 2.6468,
      "grad_norm": 2.8754830360412598,
      "learning_rate": 5.9261744966442955e-06,
      "loss": 0.5339,
      "step": 66170
    },
    {
      "epoch": 2.6471999999999998,
      "grad_norm": 3.2449018955230713,
      "learning_rate": 5.919463087248322e-06,
      "loss": 0.5421,
      "step": 66180
    },
    {
      "epoch": 2.6475999999999997,
      "grad_norm": 3.2780544757843018,
      "learning_rate": 5.912751677852349e-06,
      "loss": 0.5049,
      "step": 66190
    },
    {
      "epoch": 2.648,
      "grad_norm": 2.7007458209991455,
      "learning_rate": 5.906040268456376e-06,
      "loss": 0.4569,
      "step": 66200
    },
    {
      "epoch": 2.6484,
      "grad_norm": 2.5106117725372314,
      "learning_rate": 5.899328859060403e-06,
      "loss": 0.4794,
      "step": 66210
    },
    {
      "epoch": 2.6488,
      "grad_norm": 2.4226577281951904,
      "learning_rate": 5.89261744966443e-06,
      "loss": 0.4731,
      "step": 66220
    },
    {
      "epoch": 2.6492,
      "grad_norm": 2.967957019805908,
      "learning_rate": 5.885906040268457e-06,
      "loss": 0.4715,
      "step": 66230
    },
    {
      "epoch": 2.6496,
      "grad_norm": 3.198392391204834,
      "learning_rate": 5.8791946308724835e-06,
      "loss": 0.5522,
      "step": 66240
    },
    {
      "epoch": 2.65,
      "grad_norm": 3.13283109664917,
      "learning_rate": 5.87248322147651e-06,
      "loss": 0.529,
      "step": 66250
    },
    {
      "epoch": 2.6504,
      "grad_norm": 2.255188465118408,
      "learning_rate": 5.865771812080537e-06,
      "loss": 0.4703,
      "step": 66260
    },
    {
      "epoch": 2.6508000000000003,
      "grad_norm": 2.517719030380249,
      "learning_rate": 5.859060402684564e-06,
      "loss": 0.5757,
      "step": 66270
    },
    {
      "epoch": 2.6512000000000002,
      "grad_norm": 2.5894224643707275,
      "learning_rate": 5.852348993288591e-06,
      "loss": 0.4175,
      "step": 66280
    },
    {
      "epoch": 2.6516,
      "grad_norm": 2.643202543258667,
      "learning_rate": 5.845637583892618e-06,
      "loss": 0.3906,
      "step": 66290
    },
    {
      "epoch": 2.652,
      "grad_norm": 2.0921781063079834,
      "learning_rate": 5.838926174496645e-06,
      "loss": 0.5131,
      "step": 66300
    },
    {
      "epoch": 2.6524,
      "grad_norm": 2.095979928970337,
      "learning_rate": 5.8322147651006715e-06,
      "loss": 0.4563,
      "step": 66310
    },
    {
      "epoch": 2.6528,
      "grad_norm": 3.417469024658203,
      "learning_rate": 5.825503355704698e-06,
      "loss": 0.498,
      "step": 66320
    },
    {
      "epoch": 2.6532,
      "grad_norm": 2.730804920196533,
      "learning_rate": 5.818791946308725e-06,
      "loss": 0.4452,
      "step": 66330
    },
    {
      "epoch": 2.6536,
      "grad_norm": 2.3009471893310547,
      "learning_rate": 5.812080536912752e-06,
      "loss": 0.438,
      "step": 66340
    },
    {
      "epoch": 2.654,
      "grad_norm": 2.2476556301116943,
      "learning_rate": 5.805369127516779e-06,
      "loss": 0.4772,
      "step": 66350
    },
    {
      "epoch": 2.6544,
      "grad_norm": 2.4458022117614746,
      "learning_rate": 5.798657718120806e-06,
      "loss": 0.4906,
      "step": 66360
    },
    {
      "epoch": 2.6548,
      "grad_norm": 3.2444381713867188,
      "learning_rate": 5.791946308724833e-06,
      "loss": 0.4972,
      "step": 66370
    },
    {
      "epoch": 2.6552,
      "grad_norm": 2.833728790283203,
      "learning_rate": 5.7852348993288594e-06,
      "loss": 0.4333,
      "step": 66380
    },
    {
      "epoch": 2.6555999999999997,
      "grad_norm": 2.4810500144958496,
      "learning_rate": 5.778523489932886e-06,
      "loss": 0.5265,
      "step": 66390
    },
    {
      "epoch": 2.656,
      "grad_norm": 2.9203429222106934,
      "learning_rate": 5.771812080536913e-06,
      "loss": 0.526,
      "step": 66400
    },
    {
      "epoch": 2.6564,
      "grad_norm": 2.6328375339508057,
      "learning_rate": 5.76510067114094e-06,
      "loss": 0.473,
      "step": 66410
    },
    {
      "epoch": 2.6568,
      "grad_norm": 2.6764943599700928,
      "learning_rate": 5.758389261744967e-06,
      "loss": 0.4858,
      "step": 66420
    },
    {
      "epoch": 2.6572,
      "grad_norm": 2.1138863563537598,
      "learning_rate": 5.751677852348993e-06,
      "loss": 0.5197,
      "step": 66430
    },
    {
      "epoch": 2.6576,
      "grad_norm": 3.032259225845337,
      "learning_rate": 5.7449664429530206e-06,
      "loss": 0.4784,
      "step": 66440
    },
    {
      "epoch": 2.658,
      "grad_norm": 2.1699604988098145,
      "learning_rate": 5.738255033557047e-06,
      "loss": 0.5314,
      "step": 66450
    },
    {
      "epoch": 2.6584,
      "grad_norm": 2.249046564102173,
      "learning_rate": 5.7315436241610734e-06,
      "loss": 0.4439,
      "step": 66460
    },
    {
      "epoch": 2.6588000000000003,
      "grad_norm": 3.061382532119751,
      "learning_rate": 5.724832214765101e-06,
      "loss": 0.5482,
      "step": 66470
    },
    {
      "epoch": 2.6592000000000002,
      "grad_norm": 2.3779897689819336,
      "learning_rate": 5.718120805369128e-06,
      "loss": 0.5151,
      "step": 66480
    },
    {
      "epoch": 2.6596,
      "grad_norm": 2.390367031097412,
      "learning_rate": 5.711409395973155e-06,
      "loss": 0.4092,
      "step": 66490
    },
    {
      "epoch": 2.66,
      "grad_norm": 2.469072103500366,
      "learning_rate": 5.704697986577182e-06,
      "loss": 0.5407,
      "step": 66500
    },
    {
      "epoch": 2.6604,
      "grad_norm": 2.568638563156128,
      "learning_rate": 5.697986577181208e-06,
      "loss": 0.4372,
      "step": 66510
    },
    {
      "epoch": 2.6608,
      "grad_norm": 3.2251393795013428,
      "learning_rate": 5.691275167785235e-06,
      "loss": 0.5096,
      "step": 66520
    },
    {
      "epoch": 2.6612,
      "grad_norm": 2.547311544418335,
      "learning_rate": 5.684563758389262e-06,
      "loss": 0.4965,
      "step": 66530
    },
    {
      "epoch": 2.6616,
      "grad_norm": 2.4128201007843018,
      "learning_rate": 5.677852348993288e-06,
      "loss": 0.4567,
      "step": 66540
    },
    {
      "epoch": 2.662,
      "grad_norm": 2.9983255863189697,
      "learning_rate": 5.671140939597316e-06,
      "loss": 0.451,
      "step": 66550
    },
    {
      "epoch": 2.6624,
      "grad_norm": 2.5531575679779053,
      "learning_rate": 5.664429530201343e-06,
      "loss": 0.4408,
      "step": 66560
    },
    {
      "epoch": 2.6628,
      "grad_norm": 2.5800981521606445,
      "learning_rate": 5.657718120805369e-06,
      "loss": 0.4989,
      "step": 66570
    },
    {
      "epoch": 2.6632,
      "grad_norm": 2.4787771701812744,
      "learning_rate": 5.6510067114093965e-06,
      "loss": 0.488,
      "step": 66580
    },
    {
      "epoch": 2.6635999999999997,
      "grad_norm": 2.558908700942993,
      "learning_rate": 5.644295302013423e-06,
      "loss": 0.5485,
      "step": 66590
    },
    {
      "epoch": 2.664,
      "grad_norm": 2.4295709133148193,
      "learning_rate": 5.63758389261745e-06,
      "loss": 0.5349,
      "step": 66600
    },
    {
      "epoch": 2.6644,
      "grad_norm": 2.382225275039673,
      "learning_rate": 5.630872483221477e-06,
      "loss": 0.4644,
      "step": 66610
    },
    {
      "epoch": 2.6648,
      "grad_norm": 2.6167919635772705,
      "learning_rate": 5.624161073825503e-06,
      "loss": 0.4687,
      "step": 66620
    },
    {
      "epoch": 2.6652,
      "grad_norm": 1.8430343866348267,
      "learning_rate": 5.617449664429531e-06,
      "loss": 0.4526,
      "step": 66630
    },
    {
      "epoch": 2.6656,
      "grad_norm": 2.366490602493286,
      "learning_rate": 5.610738255033558e-06,
      "loss": 0.4565,
      "step": 66640
    },
    {
      "epoch": 2.666,
      "grad_norm": 2.108405590057373,
      "learning_rate": 5.604026845637584e-06,
      "loss": 0.4381,
      "step": 66650
    },
    {
      "epoch": 2.6664,
      "grad_norm": 2.8334250450134277,
      "learning_rate": 5.597315436241611e-06,
      "loss": 0.5088,
      "step": 66660
    },
    {
      "epoch": 2.6668,
      "grad_norm": 2.411651372909546,
      "learning_rate": 5.590604026845638e-06,
      "loss": 0.4843,
      "step": 66670
    },
    {
      "epoch": 2.6672000000000002,
      "grad_norm": 2.8149807453155518,
      "learning_rate": 5.583892617449664e-06,
      "loss": 0.5321,
      "step": 66680
    },
    {
      "epoch": 2.6676,
      "grad_norm": 3.2283637523651123,
      "learning_rate": 5.577181208053692e-06,
      "loss": 0.5302,
      "step": 66690
    },
    {
      "epoch": 2.668,
      "grad_norm": 2.783478260040283,
      "learning_rate": 5.570469798657718e-06,
      "loss": 0.5269,
      "step": 66700
    },
    {
      "epoch": 2.6684,
      "grad_norm": 1.5681697130203247,
      "learning_rate": 5.563758389261746e-06,
      "loss": 0.4756,
      "step": 66710
    },
    {
      "epoch": 2.6688,
      "grad_norm": 2.7782323360443115,
      "learning_rate": 5.5570469798657725e-06,
      "loss": 0.4823,
      "step": 66720
    },
    {
      "epoch": 2.6692,
      "grad_norm": 3.736180543899536,
      "learning_rate": 5.5503355704697985e-06,
      "loss": 0.6071,
      "step": 66730
    },
    {
      "epoch": 2.6696,
      "grad_norm": 2.764683961868286,
      "learning_rate": 5.543624161073826e-06,
      "loss": 0.4735,
      "step": 66740
    },
    {
      "epoch": 2.67,
      "grad_norm": 3.1154887676239014,
      "learning_rate": 5.536912751677853e-06,
      "loss": 0.5771,
      "step": 66750
    },
    {
      "epoch": 2.6704,
      "grad_norm": 3.646355628967285,
      "learning_rate": 5.530201342281879e-06,
      "loss": 0.5225,
      "step": 66760
    },
    {
      "epoch": 2.6708,
      "grad_norm": 2.584787607192993,
      "learning_rate": 5.523489932885907e-06,
      "loss": 0.4911,
      "step": 66770
    },
    {
      "epoch": 2.6712,
      "grad_norm": 3.1789262294769287,
      "learning_rate": 5.516778523489933e-06,
      "loss": 0.5818,
      "step": 66780
    },
    {
      "epoch": 2.6715999999999998,
      "grad_norm": 2.98077654838562,
      "learning_rate": 5.51006711409396e-06,
      "loss": 0.4955,
      "step": 66790
    },
    {
      "epoch": 2.672,
      "grad_norm": 2.2267744541168213,
      "learning_rate": 5.503355704697987e-06,
      "loss": 0.4485,
      "step": 66800
    },
    {
      "epoch": 2.6724,
      "grad_norm": 2.6857893466949463,
      "learning_rate": 5.496644295302013e-06,
      "loss": 0.4723,
      "step": 66810
    },
    {
      "epoch": 2.6728,
      "grad_norm": 1.8212461471557617,
      "learning_rate": 5.489932885906041e-06,
      "loss": 0.4304,
      "step": 66820
    },
    {
      "epoch": 2.6732,
      "grad_norm": 3.2800068855285645,
      "learning_rate": 5.483221476510068e-06,
      "loss": 0.5013,
      "step": 66830
    },
    {
      "epoch": 2.6736,
      "grad_norm": 2.3281776905059814,
      "learning_rate": 5.476510067114094e-06,
      "loss": 0.5786,
      "step": 66840
    },
    {
      "epoch": 2.674,
      "grad_norm": 2.475306987762451,
      "learning_rate": 5.4697986577181215e-06,
      "loss": 0.5234,
      "step": 66850
    },
    {
      "epoch": 2.6744,
      "grad_norm": 2.298199415206909,
      "learning_rate": 5.4630872483221475e-06,
      "loss": 0.5279,
      "step": 66860
    },
    {
      "epoch": 2.6748,
      "grad_norm": 2.499016284942627,
      "learning_rate": 5.456375838926174e-06,
      "loss": 0.4639,
      "step": 66870
    },
    {
      "epoch": 2.6752000000000002,
      "grad_norm": 2.965090036392212,
      "learning_rate": 5.449664429530202e-06,
      "loss": 0.4552,
      "step": 66880
    },
    {
      "epoch": 2.6756,
      "grad_norm": 2.5321872234344482,
      "learning_rate": 5.442953020134228e-06,
      "loss": 0.5447,
      "step": 66890
    },
    {
      "epoch": 2.676,
      "grad_norm": 2.527681827545166,
      "learning_rate": 5.436241610738255e-06,
      "loss": 0.4788,
      "step": 66900
    },
    {
      "epoch": 2.6764,
      "grad_norm": 2.066835403442383,
      "learning_rate": 5.429530201342283e-06,
      "loss": 0.4952,
      "step": 66910
    },
    {
      "epoch": 2.6768,
      "grad_norm": 3.0839877128601074,
      "learning_rate": 5.422818791946309e-06,
      "loss": 0.4471,
      "step": 66920
    },
    {
      "epoch": 2.6772,
      "grad_norm": 2.3629164695739746,
      "learning_rate": 5.416107382550336e-06,
      "loss": 0.4454,
      "step": 66930
    },
    {
      "epoch": 2.6776,
      "grad_norm": 2.511082410812378,
      "learning_rate": 5.409395973154362e-06,
      "loss": 0.4764,
      "step": 66940
    },
    {
      "epoch": 2.678,
      "grad_norm": 2.38893723487854,
      "learning_rate": 5.402684563758389e-06,
      "loss": 0.5245,
      "step": 66950
    },
    {
      "epoch": 2.6784,
      "grad_norm": 2.4155826568603516,
      "learning_rate": 5.395973154362417e-06,
      "loss": 0.493,
      "step": 66960
    },
    {
      "epoch": 2.6788,
      "grad_norm": 2.5546131134033203,
      "learning_rate": 5.389261744966443e-06,
      "loss": 0.5027,
      "step": 66970
    },
    {
      "epoch": 2.6792,
      "grad_norm": 1.9774463176727295,
      "learning_rate": 5.38255033557047e-06,
      "loss": 0.4881,
      "step": 66980
    },
    {
      "epoch": 2.6795999999999998,
      "grad_norm": 3.0417191982269287,
      "learning_rate": 5.3758389261744975e-06,
      "loss": 0.5385,
      "step": 66990
    },
    {
      "epoch": 2.68,
      "grad_norm": 3.437424421310425,
      "learning_rate": 5.3691275167785235e-06,
      "loss": 0.4813,
      "step": 67000
    },
    {
      "epoch": 2.6804,
      "grad_norm": 2.7865166664123535,
      "learning_rate": 5.36241610738255e-06,
      "loss": 0.4831,
      "step": 67010
    },
    {
      "epoch": 2.6808,
      "grad_norm": 2.905521869659424,
      "learning_rate": 5.355704697986577e-06,
      "loss": 0.5295,
      "step": 67020
    },
    {
      "epoch": 2.6812,
      "grad_norm": 2.2331597805023193,
      "learning_rate": 5.348993288590604e-06,
      "loss": 0.4452,
      "step": 67030
    },
    {
      "epoch": 2.6816,
      "grad_norm": 3.5515666007995605,
      "learning_rate": 5.342281879194632e-06,
      "loss": 0.5213,
      "step": 67040
    },
    {
      "epoch": 2.682,
      "grad_norm": 2.538909912109375,
      "learning_rate": 5.335570469798658e-06,
      "loss": 0.5342,
      "step": 67050
    },
    {
      "epoch": 2.6824,
      "grad_norm": 3.009618043899536,
      "learning_rate": 5.328859060402685e-06,
      "loss": 0.4587,
      "step": 67060
    },
    {
      "epoch": 2.6828,
      "grad_norm": 2.627688407897949,
      "learning_rate": 5.322147651006712e-06,
      "loss": 0.5336,
      "step": 67070
    },
    {
      "epoch": 2.6832000000000003,
      "grad_norm": 2.7183656692504883,
      "learning_rate": 5.315436241610738e-06,
      "loss": 0.5286,
      "step": 67080
    },
    {
      "epoch": 2.6836,
      "grad_norm": 3.0009589195251465,
      "learning_rate": 5.308724832214765e-06,
      "loss": 0.5373,
      "step": 67090
    },
    {
      "epoch": 2.684,
      "grad_norm": 2.6253838539123535,
      "learning_rate": 5.302013422818792e-06,
      "loss": 0.5147,
      "step": 67100
    },
    {
      "epoch": 2.6844,
      "grad_norm": 2.8478927612304688,
      "learning_rate": 5.295302013422819e-06,
      "loss": 0.4675,
      "step": 67110
    },
    {
      "epoch": 2.6848,
      "grad_norm": 2.313692569732666,
      "learning_rate": 5.288590604026846e-06,
      "loss": 0.4808,
      "step": 67120
    },
    {
      "epoch": 2.6852,
      "grad_norm": 2.881890296936035,
      "learning_rate": 5.281879194630873e-06,
      "loss": 0.4449,
      "step": 67130
    },
    {
      "epoch": 2.6856,
      "grad_norm": 2.740628242492676,
      "learning_rate": 5.2751677852348994e-06,
      "loss": 0.5289,
      "step": 67140
    },
    {
      "epoch": 2.686,
      "grad_norm": 3.0179190635681152,
      "learning_rate": 5.268456375838927e-06,
      "loss": 0.4995,
      "step": 67150
    },
    {
      "epoch": 2.6864,
      "grad_norm": 2.39481258392334,
      "learning_rate": 5.261744966442953e-06,
      "loss": 0.4987,
      "step": 67160
    },
    {
      "epoch": 2.6868,
      "grad_norm": 2.1427199840545654,
      "learning_rate": 5.25503355704698e-06,
      "loss": 0.4551,
      "step": 67170
    },
    {
      "epoch": 2.6872,
      "grad_norm": 2.36313796043396,
      "learning_rate": 5.248322147651007e-06,
      "loss": 0.4287,
      "step": 67180
    },
    {
      "epoch": 2.6875999999999998,
      "grad_norm": 3.0418541431427,
      "learning_rate": 5.241610738255034e-06,
      "loss": 0.521,
      "step": 67190
    },
    {
      "epoch": 2.6879999999999997,
      "grad_norm": 2.2127552032470703,
      "learning_rate": 5.2348993288590606e-06,
      "loss": 0.4286,
      "step": 67200
    },
    {
      "epoch": 2.6884,
      "grad_norm": 3.2914907932281494,
      "learning_rate": 5.228187919463087e-06,
      "loss": 0.5185,
      "step": 67210
    },
    {
      "epoch": 2.6888,
      "grad_norm": 2.918409824371338,
      "learning_rate": 5.221476510067114e-06,
      "loss": 0.5292,
      "step": 67220
    },
    {
      "epoch": 2.6892,
      "grad_norm": 3.0630476474761963,
      "learning_rate": 5.214765100671141e-06,
      "loss": 0.5287,
      "step": 67230
    },
    {
      "epoch": 2.6896,
      "grad_norm": 3.45918607711792,
      "learning_rate": 5.208053691275168e-06,
      "loss": 0.4869,
      "step": 67240
    },
    {
      "epoch": 2.69,
      "grad_norm": 3.178018808364868,
      "learning_rate": 5.201342281879195e-06,
      "loss": 0.577,
      "step": 67250
    },
    {
      "epoch": 2.6904,
      "grad_norm": 2.812971830368042,
      "learning_rate": 5.194630872483222e-06,
      "loss": 0.5458,
      "step": 67260
    },
    {
      "epoch": 2.6908,
      "grad_norm": 2.5453107357025146,
      "learning_rate": 5.1879194630872485e-06,
      "loss": 0.5368,
      "step": 67270
    },
    {
      "epoch": 2.6912000000000003,
      "grad_norm": 3.049698829650879,
      "learning_rate": 5.181208053691275e-06,
      "loss": 0.4446,
      "step": 67280
    },
    {
      "epoch": 2.6916,
      "grad_norm": 3.2090539932250977,
      "learning_rate": 5.174496644295302e-06,
      "loss": 0.4588,
      "step": 67290
    },
    {
      "epoch": 2.692,
      "grad_norm": 2.957157611846924,
      "learning_rate": 5.167785234899329e-06,
      "loss": 0.4694,
      "step": 67300
    },
    {
      "epoch": 2.6924,
      "grad_norm": 3.341688871383667,
      "learning_rate": 5.161073825503356e-06,
      "loss": 0.514,
      "step": 67310
    },
    {
      "epoch": 2.6928,
      "grad_norm": 2.6364188194274902,
      "learning_rate": 5.154362416107383e-06,
      "loss": 0.4706,
      "step": 67320
    },
    {
      "epoch": 2.6932,
      "grad_norm": 2.9017701148986816,
      "learning_rate": 5.14765100671141e-06,
      "loss": 0.5297,
      "step": 67330
    },
    {
      "epoch": 2.6936,
      "grad_norm": 2.607689619064331,
      "learning_rate": 5.1409395973154365e-06,
      "loss": 0.5255,
      "step": 67340
    },
    {
      "epoch": 2.694,
      "grad_norm": 2.3219075202941895,
      "learning_rate": 5.134228187919463e-06,
      "loss": 0.4265,
      "step": 67350
    },
    {
      "epoch": 2.6944,
      "grad_norm": 2.544220447540283,
      "learning_rate": 5.12751677852349e-06,
      "loss": 0.4993,
      "step": 67360
    },
    {
      "epoch": 2.6948,
      "grad_norm": 2.7209818363189697,
      "learning_rate": 5.120805369127517e-06,
      "loss": 0.4356,
      "step": 67370
    },
    {
      "epoch": 2.6952,
      "grad_norm": 2.7837717533111572,
      "learning_rate": 5.114093959731544e-06,
      "loss": 0.4784,
      "step": 67380
    },
    {
      "epoch": 2.6955999999999998,
      "grad_norm": 3.313751220703125,
      "learning_rate": 5.107382550335571e-06,
      "loss": 0.5261,
      "step": 67390
    },
    {
      "epoch": 2.6959999999999997,
      "grad_norm": 2.938617467880249,
      "learning_rate": 5.100671140939598e-06,
      "loss": 0.4812,
      "step": 67400
    },
    {
      "epoch": 2.6964,
      "grad_norm": 2.655508041381836,
      "learning_rate": 5.0939597315436245e-06,
      "loss": 0.4969,
      "step": 67410
    },
    {
      "epoch": 2.6968,
      "grad_norm": 2.45281720161438,
      "learning_rate": 5.087248322147651e-06,
      "loss": 0.457,
      "step": 67420
    },
    {
      "epoch": 2.6972,
      "grad_norm": 2.125274896621704,
      "learning_rate": 5.080536912751678e-06,
      "loss": 0.4727,
      "step": 67430
    },
    {
      "epoch": 2.6976,
      "grad_norm": 3.1847386360168457,
      "learning_rate": 5.073825503355705e-06,
      "loss": 0.52,
      "step": 67440
    },
    {
      "epoch": 2.698,
      "grad_norm": 2.2129220962524414,
      "learning_rate": 5.067114093959732e-06,
      "loss": 0.5127,
      "step": 67450
    },
    {
      "epoch": 2.6984,
      "grad_norm": 2.2257635593414307,
      "learning_rate": 5.060402684563759e-06,
      "loss": 0.4507,
      "step": 67460
    },
    {
      "epoch": 2.6988,
      "grad_norm": 2.4618172645568848,
      "learning_rate": 5.053691275167786e-06,
      "loss": 0.4638,
      "step": 67470
    },
    {
      "epoch": 2.6992000000000003,
      "grad_norm": 2.5049028396606445,
      "learning_rate": 5.0469798657718124e-06,
      "loss": 0.4465,
      "step": 67480
    },
    {
      "epoch": 2.6996,
      "grad_norm": 3.133371114730835,
      "learning_rate": 5.040268456375839e-06,
      "loss": 0.463,
      "step": 67490
    },
    {
      "epoch": 2.7,
      "grad_norm": 3.0859861373901367,
      "learning_rate": 5.033557046979865e-06,
      "loss": 0.5411,
      "step": 67500
    },
    {
      "epoch": 2.7004,
      "grad_norm": 2.3531322479248047,
      "learning_rate": 5.026845637583893e-06,
      "loss": 0.5644,
      "step": 67510
    },
    {
      "epoch": 2.7008,
      "grad_norm": 3.2713139057159424,
      "learning_rate": 5.02013422818792e-06,
      "loss": 0.5961,
      "step": 67520
    },
    {
      "epoch": 2.7012,
      "grad_norm": 2.819242000579834,
      "learning_rate": 5.013422818791947e-06,
      "loss": 0.4795,
      "step": 67530
    },
    {
      "epoch": 2.7016,
      "grad_norm": 3.1685738563537598,
      "learning_rate": 5.0067114093959736e-06,
      "loss": 0.4932,
      "step": 67540
    },
    {
      "epoch": 2.702,
      "grad_norm": 3.380034923553467,
      "learning_rate": 5e-06,
      "loss": 0.5234,
      "step": 67550
    },
    {
      "epoch": 2.7024,
      "grad_norm": 2.4230234622955322,
      "learning_rate": 4.993288590604027e-06,
      "loss": 0.4598,
      "step": 67560
    },
    {
      "epoch": 2.7028,
      "grad_norm": 2.181478261947632,
      "learning_rate": 4.986577181208054e-06,
      "loss": 0.4765,
      "step": 67570
    },
    {
      "epoch": 2.7032,
      "grad_norm": 2.050417184829712,
      "learning_rate": 4.97986577181208e-06,
      "loss": 0.4769,
      "step": 67580
    },
    {
      "epoch": 2.7036,
      "grad_norm": 3.2709388732910156,
      "learning_rate": 4.973154362416108e-06,
      "loss": 0.4744,
      "step": 67590
    },
    {
      "epoch": 2.7039999999999997,
      "grad_norm": 2.721952438354492,
      "learning_rate": 4.966442953020135e-06,
      "loss": 0.475,
      "step": 67600
    },
    {
      "epoch": 2.7044,
      "grad_norm": 2.365433931350708,
      "learning_rate": 4.959731543624161e-06,
      "loss": 0.5068,
      "step": 67610
    },
    {
      "epoch": 2.7048,
      "grad_norm": 1.9041937589645386,
      "learning_rate": 4.953020134228188e-06,
      "loss": 0.4647,
      "step": 67620
    },
    {
      "epoch": 2.7052,
      "grad_norm": 3.0853567123413086,
      "learning_rate": 4.946308724832215e-06,
      "loss": 0.5188,
      "step": 67630
    },
    {
      "epoch": 2.7056,
      "grad_norm": 3.0902342796325684,
      "learning_rate": 4.939597315436242e-06,
      "loss": 0.5288,
      "step": 67640
    },
    {
      "epoch": 2.706,
      "grad_norm": 2.8141367435455322,
      "learning_rate": 4.932885906040269e-06,
      "loss": 0.6467,
      "step": 67650
    },
    {
      "epoch": 2.7064,
      "grad_norm": 2.8052706718444824,
      "learning_rate": 4.926174496644295e-06,
      "loss": 0.4676,
      "step": 67660
    },
    {
      "epoch": 2.7068,
      "grad_norm": 3.075850009918213,
      "learning_rate": 4.919463087248323e-06,
      "loss": 0.4675,
      "step": 67670
    },
    {
      "epoch": 2.7072000000000003,
      "grad_norm": 2.9321045875549316,
      "learning_rate": 4.9127516778523495e-06,
      "loss": 0.5646,
      "step": 67680
    },
    {
      "epoch": 2.7076000000000002,
      "grad_norm": 2.4502780437469482,
      "learning_rate": 4.9060402684563755e-06,
      "loss": 0.5003,
      "step": 67690
    },
    {
      "epoch": 2.708,
      "grad_norm": 2.121574640274048,
      "learning_rate": 4.899328859060403e-06,
      "loss": 0.4697,
      "step": 67700
    },
    {
      "epoch": 2.7084,
      "grad_norm": 2.1205248832702637,
      "learning_rate": 4.89261744966443e-06,
      "loss": 0.4215,
      "step": 67710
    },
    {
      "epoch": 2.7088,
      "grad_norm": 2.9619295597076416,
      "learning_rate": 4.885906040268456e-06,
      "loss": 0.4656,
      "step": 67720
    },
    {
      "epoch": 2.7092,
      "grad_norm": 2.8410611152648926,
      "learning_rate": 4.879194630872484e-06,
      "loss": 0.4695,
      "step": 67730
    },
    {
      "epoch": 2.7096,
      "grad_norm": 3.0780465602874756,
      "learning_rate": 4.87248322147651e-06,
      "loss": 0.5209,
      "step": 67740
    },
    {
      "epoch": 2.71,
      "grad_norm": 2.620899200439453,
      "learning_rate": 4.8657718120805375e-06,
      "loss": 0.5497,
      "step": 67750
    },
    {
      "epoch": 2.7104,
      "grad_norm": 2.936323642730713,
      "learning_rate": 4.859060402684564e-06,
      "loss": 0.5232,
      "step": 67760
    },
    {
      "epoch": 2.7108,
      "grad_norm": 2.106501579284668,
      "learning_rate": 4.85234899328859e-06,
      "loss": 0.501,
      "step": 67770
    },
    {
      "epoch": 2.7112,
      "grad_norm": 3.174440383911133,
      "learning_rate": 4.845637583892618e-06,
      "loss": 0.5243,
      "step": 67780
    },
    {
      "epoch": 2.7116,
      "grad_norm": 3.5962538719177246,
      "learning_rate": 4.838926174496645e-06,
      "loss": 0.5311,
      "step": 67790
    },
    {
      "epoch": 2.7119999999999997,
      "grad_norm": 2.5886125564575195,
      "learning_rate": 4.832214765100671e-06,
      "loss": 0.494,
      "step": 67800
    },
    {
      "epoch": 2.7124,
      "grad_norm": 2.8449628353118896,
      "learning_rate": 4.825503355704699e-06,
      "loss": 0.4931,
      "step": 67810
    },
    {
      "epoch": 2.7128,
      "grad_norm": 2.853959321975708,
      "learning_rate": 4.8187919463087254e-06,
      "loss": 0.547,
      "step": 67820
    },
    {
      "epoch": 2.7132,
      "grad_norm": 2.301754951477051,
      "learning_rate": 4.8120805369127514e-06,
      "loss": 0.4895,
      "step": 67830
    },
    {
      "epoch": 2.7136,
      "grad_norm": 2.5715482234954834,
      "learning_rate": 4.805369127516779e-06,
      "loss": 0.4643,
      "step": 67840
    },
    {
      "epoch": 2.714,
      "grad_norm": 2.8355724811553955,
      "learning_rate": 4.798657718120805e-06,
      "loss": 0.4995,
      "step": 67850
    },
    {
      "epoch": 2.7144,
      "grad_norm": 2.096611976623535,
      "learning_rate": 4.791946308724833e-06,
      "loss": 0.4887,
      "step": 67860
    },
    {
      "epoch": 2.7148,
      "grad_norm": 2.534593105316162,
      "learning_rate": 4.78523489932886e-06,
      "loss": 0.5307,
      "step": 67870
    },
    {
      "epoch": 2.7152,
      "grad_norm": 2.6361234188079834,
      "learning_rate": 4.778523489932886e-06,
      "loss": 0.4437,
      "step": 67880
    },
    {
      "epoch": 2.7156000000000002,
      "grad_norm": 2.744229555130005,
      "learning_rate": 4.771812080536913e-06,
      "loss": 0.4638,
      "step": 67890
    },
    {
      "epoch": 2.716,
      "grad_norm": 3.412940740585327,
      "learning_rate": 4.76510067114094e-06,
      "loss": 0.5038,
      "step": 67900
    },
    {
      "epoch": 2.7164,
      "grad_norm": 2.8959438800811768,
      "learning_rate": 4.758389261744966e-06,
      "loss": 0.508,
      "step": 67910
    },
    {
      "epoch": 2.7168,
      "grad_norm": 2.5182082653045654,
      "learning_rate": 4.751677852348994e-06,
      "loss": 0.4685,
      "step": 67920
    },
    {
      "epoch": 2.7172,
      "grad_norm": 3.0222513675689697,
      "learning_rate": 4.74496644295302e-06,
      "loss": 0.4576,
      "step": 67930
    },
    {
      "epoch": 2.7176,
      "grad_norm": 2.6333446502685547,
      "learning_rate": 4.738255033557047e-06,
      "loss": 0.5391,
      "step": 67940
    },
    {
      "epoch": 2.718,
      "grad_norm": 2.566965341567993,
      "learning_rate": 4.7315436241610745e-06,
      "loss": 0.5224,
      "step": 67950
    },
    {
      "epoch": 2.7184,
      "grad_norm": 2.3902435302734375,
      "learning_rate": 4.7248322147651005e-06,
      "loss": 0.4838,
      "step": 67960
    },
    {
      "epoch": 2.7188,
      "grad_norm": 2.9704458713531494,
      "learning_rate": 4.718120805369128e-06,
      "loss": 0.485,
      "step": 67970
    },
    {
      "epoch": 2.7192,
      "grad_norm": 2.4591445922851562,
      "learning_rate": 4.711409395973155e-06,
      "loss": 0.5071,
      "step": 67980
    },
    {
      "epoch": 2.7196,
      "grad_norm": 2.050870418548584,
      "learning_rate": 4.704697986577181e-06,
      "loss": 0.5384,
      "step": 67990
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 2.8509438037872314,
      "learning_rate": 4.697986577181209e-06,
      "loss": 0.4991,
      "step": 68000
    },
    {
      "epoch": 2.7204,
      "grad_norm": 2.8954334259033203,
      "learning_rate": 4.691275167785235e-06,
      "loss": 0.5575,
      "step": 68010
    },
    {
      "epoch": 2.7208,
      "grad_norm": 2.7414016723632812,
      "learning_rate": 4.684563758389262e-06,
      "loss": 0.579,
      "step": 68020
    },
    {
      "epoch": 2.7212,
      "grad_norm": 2.494145393371582,
      "learning_rate": 4.677852348993289e-06,
      "loss": 0.448,
      "step": 68030
    },
    {
      "epoch": 2.7216,
      "grad_norm": 3.3423640727996826,
      "learning_rate": 4.671140939597315e-06,
      "loss": 0.5214,
      "step": 68040
    },
    {
      "epoch": 2.722,
      "grad_norm": 2.6130805015563965,
      "learning_rate": 4.664429530201342e-06,
      "loss": 0.464,
      "step": 68050
    },
    {
      "epoch": 2.7224,
      "grad_norm": 2.864102840423584,
      "learning_rate": 4.65771812080537e-06,
      "loss": 0.5327,
      "step": 68060
    },
    {
      "epoch": 2.7228,
      "grad_norm": 2.5690300464630127,
      "learning_rate": 4.651006711409396e-06,
      "loss": 0.4986,
      "step": 68070
    },
    {
      "epoch": 2.7232,
      "grad_norm": 2.715520143508911,
      "learning_rate": 4.644295302013423e-06,
      "loss": 0.4719,
      "step": 68080
    },
    {
      "epoch": 2.7236000000000002,
      "grad_norm": 3.349308967590332,
      "learning_rate": 4.63758389261745e-06,
      "loss": 0.4856,
      "step": 68090
    },
    {
      "epoch": 2.724,
      "grad_norm": 2.833303213119507,
      "learning_rate": 4.6308724832214765e-06,
      "loss": 0.4715,
      "step": 68100
    },
    {
      "epoch": 2.7244,
      "grad_norm": 2.698452949523926,
      "learning_rate": 4.624161073825504e-06,
      "loss": 0.5513,
      "step": 68110
    },
    {
      "epoch": 2.7248,
      "grad_norm": 3.2504830360412598,
      "learning_rate": 4.61744966442953e-06,
      "loss": 0.5627,
      "step": 68120
    },
    {
      "epoch": 2.7252,
      "grad_norm": 3.2128055095672607,
      "learning_rate": 4.610738255033557e-06,
      "loss": 0.5848,
      "step": 68130
    },
    {
      "epoch": 2.7256,
      "grad_norm": 3.0724105834960938,
      "learning_rate": 4.604026845637585e-06,
      "loss": 0.4925,
      "step": 68140
    },
    {
      "epoch": 2.726,
      "grad_norm": 2.1846368312835693,
      "learning_rate": 4.597315436241611e-06,
      "loss": 0.5244,
      "step": 68150
    },
    {
      "epoch": 2.7264,
      "grad_norm": 3.2214550971984863,
      "learning_rate": 4.590604026845638e-06,
      "loss": 0.5232,
      "step": 68160
    },
    {
      "epoch": 2.7268,
      "grad_norm": 2.3496644496917725,
      "learning_rate": 4.5838926174496645e-06,
      "loss": 0.4522,
      "step": 68170
    },
    {
      "epoch": 2.7272,
      "grad_norm": 2.0742404460906982,
      "learning_rate": 4.577181208053691e-06,
      "loss": 0.5481,
      "step": 68180
    },
    {
      "epoch": 2.7276,
      "grad_norm": 2.286931037902832,
      "learning_rate": 4.570469798657718e-06,
      "loss": 0.4573,
      "step": 68190
    },
    {
      "epoch": 2.7279999999999998,
      "grad_norm": 3.0270743370056152,
      "learning_rate": 4.563758389261745e-06,
      "loss": 0.5264,
      "step": 68200
    },
    {
      "epoch": 2.7284,
      "grad_norm": 2.3908791542053223,
      "learning_rate": 4.557046979865772e-06,
      "loss": 0.491,
      "step": 68210
    },
    {
      "epoch": 2.7288,
      "grad_norm": 2.638808012008667,
      "learning_rate": 4.5503355704697996e-06,
      "loss": 0.4848,
      "step": 68220
    },
    {
      "epoch": 2.7292,
      "grad_norm": 1.6873242855072021,
      "learning_rate": 4.5436241610738256e-06,
      "loss": 0.4744,
      "step": 68230
    },
    {
      "epoch": 2.7296,
      "grad_norm": 3.199350595474243,
      "learning_rate": 4.536912751677852e-06,
      "loss": 0.4815,
      "step": 68240
    },
    {
      "epoch": 2.73,
      "grad_norm": 3.2066521644592285,
      "learning_rate": 4.530201342281879e-06,
      "loss": 0.537,
      "step": 68250
    },
    {
      "epoch": 2.7304,
      "grad_norm": 3.498629331588745,
      "learning_rate": 4.523489932885906e-06,
      "loss": 0.5388,
      "step": 68260
    },
    {
      "epoch": 2.7308,
      "grad_norm": 1.9166332483291626,
      "learning_rate": 4.516778523489933e-06,
      "loss": 0.4471,
      "step": 68270
    },
    {
      "epoch": 2.7312,
      "grad_norm": 2.7831649780273438,
      "learning_rate": 4.51006711409396e-06,
      "loss": 0.5373,
      "step": 68280
    },
    {
      "epoch": 2.7316000000000003,
      "grad_norm": 1.493489146232605,
      "learning_rate": 4.503355704697987e-06,
      "loss": 0.4655,
      "step": 68290
    },
    {
      "epoch": 2.732,
      "grad_norm": 2.11468505859375,
      "learning_rate": 4.4966442953020135e-06,
      "loss": 0.4739,
      "step": 68300
    },
    {
      "epoch": 2.7324,
      "grad_norm": 2.8691163063049316,
      "learning_rate": 4.48993288590604e-06,
      "loss": 0.5423,
      "step": 68310
    },
    {
      "epoch": 2.7328,
      "grad_norm": 1.765261173248291,
      "learning_rate": 4.483221476510067e-06,
      "loss": 0.4032,
      "step": 68320
    },
    {
      "epoch": 2.7332,
      "grad_norm": 2.4882044792175293,
      "learning_rate": 4.476510067114094e-06,
      "loss": 0.4925,
      "step": 68330
    },
    {
      "epoch": 2.7336,
      "grad_norm": 2.556537628173828,
      "learning_rate": 4.469798657718121e-06,
      "loss": 0.4149,
      "step": 68340
    },
    {
      "epoch": 2.734,
      "grad_norm": 2.206585168838501,
      "learning_rate": 4.463087248322148e-06,
      "loss": 0.4823,
      "step": 68350
    },
    {
      "epoch": 2.7344,
      "grad_norm": 2.6957154273986816,
      "learning_rate": 4.456375838926175e-06,
      "loss": 0.5343,
      "step": 68360
    },
    {
      "epoch": 2.7348,
      "grad_norm": 2.5787177085876465,
      "learning_rate": 4.4496644295302015e-06,
      "loss": 0.5782,
      "step": 68370
    },
    {
      "epoch": 2.7352,
      "grad_norm": 2.1640167236328125,
      "learning_rate": 4.442953020134228e-06,
      "loss": 0.5134,
      "step": 68380
    },
    {
      "epoch": 2.7356,
      "grad_norm": 2.994980812072754,
      "learning_rate": 4.436241610738255e-06,
      "loss": 0.4577,
      "step": 68390
    },
    {
      "epoch": 2.7359999999999998,
      "grad_norm": 3.223421573638916,
      "learning_rate": 4.429530201342282e-06,
      "loss": 0.5605,
      "step": 68400
    },
    {
      "epoch": 2.7364,
      "grad_norm": 2.057255268096924,
      "learning_rate": 4.422818791946309e-06,
      "loss": 0.4756,
      "step": 68410
    },
    {
      "epoch": 2.7368,
      "grad_norm": 2.8937363624572754,
      "learning_rate": 4.416107382550336e-06,
      "loss": 0.4836,
      "step": 68420
    },
    {
      "epoch": 2.7372,
      "grad_norm": 3.174132823944092,
      "learning_rate": 4.409395973154363e-06,
      "loss": 0.5312,
      "step": 68430
    },
    {
      "epoch": 2.7376,
      "grad_norm": 2.8060061931610107,
      "learning_rate": 4.4026845637583895e-06,
      "loss": 0.4994,
      "step": 68440
    },
    {
      "epoch": 2.738,
      "grad_norm": 2.2466161251068115,
      "learning_rate": 4.395973154362416e-06,
      "loss": 0.4726,
      "step": 68450
    },
    {
      "epoch": 2.7384,
      "grad_norm": 2.7136924266815186,
      "learning_rate": 4.389261744966443e-06,
      "loss": 0.4966,
      "step": 68460
    },
    {
      "epoch": 2.7388,
      "grad_norm": 2.3605599403381348,
      "learning_rate": 4.38255033557047e-06,
      "loss": 0.472,
      "step": 68470
    },
    {
      "epoch": 2.7392,
      "grad_norm": 2.5493814945220947,
      "learning_rate": 4.375838926174497e-06,
      "loss": 0.4599,
      "step": 68480
    },
    {
      "epoch": 2.7396000000000003,
      "grad_norm": 2.804591655731201,
      "learning_rate": 4.369127516778524e-06,
      "loss": 0.4781,
      "step": 68490
    },
    {
      "epoch": 2.74,
      "grad_norm": 2.19362735748291,
      "learning_rate": 4.362416107382551e-06,
      "loss": 0.486,
      "step": 68500
    },
    {
      "epoch": 2.7404,
      "grad_norm": 2.3664472103118896,
      "learning_rate": 4.3557046979865775e-06,
      "loss": 0.5345,
      "step": 68510
    },
    {
      "epoch": 2.7408,
      "grad_norm": 2.011234998703003,
      "learning_rate": 4.348993288590604e-06,
      "loss": 0.3989,
      "step": 68520
    },
    {
      "epoch": 2.7412,
      "grad_norm": 2.3667099475860596,
      "learning_rate": 4.342281879194631e-06,
      "loss": 0.452,
      "step": 68530
    },
    {
      "epoch": 2.7416,
      "grad_norm": 2.7304131984710693,
      "learning_rate": 4.335570469798658e-06,
      "loss": 0.5207,
      "step": 68540
    },
    {
      "epoch": 2.742,
      "grad_norm": 2.5138533115386963,
      "learning_rate": 4.328859060402685e-06,
      "loss": 0.477,
      "step": 68550
    },
    {
      "epoch": 2.7424,
      "grad_norm": 3.2746119499206543,
      "learning_rate": 4.322147651006712e-06,
      "loss": 0.5609,
      "step": 68560
    },
    {
      "epoch": 2.7428,
      "grad_norm": 2.8810646533966064,
      "learning_rate": 4.315436241610738e-06,
      "loss": 0.5356,
      "step": 68570
    },
    {
      "epoch": 2.7432,
      "grad_norm": 3.4245927333831787,
      "learning_rate": 4.3087248322147654e-06,
      "loss": 0.5229,
      "step": 68580
    },
    {
      "epoch": 2.7436,
      "grad_norm": 2.984710931777954,
      "learning_rate": 4.302013422818792e-06,
      "loss": 0.4957,
      "step": 68590
    },
    {
      "epoch": 2.7439999999999998,
      "grad_norm": 3.050981044769287,
      "learning_rate": 4.295302013422819e-06,
      "loss": 0.5,
      "step": 68600
    },
    {
      "epoch": 2.7443999999999997,
      "grad_norm": 2.2631540298461914,
      "learning_rate": 4.288590604026846e-06,
      "loss": 0.5487,
      "step": 68610
    },
    {
      "epoch": 2.7448,
      "grad_norm": 2.157562494277954,
      "learning_rate": 4.281879194630873e-06,
      "loss": 0.4586,
      "step": 68620
    },
    {
      "epoch": 2.7452,
      "grad_norm": 2.452014923095703,
      "learning_rate": 4.2751677852349e-06,
      "loss": 0.5206,
      "step": 68630
    },
    {
      "epoch": 2.7456,
      "grad_norm": 2.8392937183380127,
      "learning_rate": 4.2684563758389265e-06,
      "loss": 0.5476,
      "step": 68640
    },
    {
      "epoch": 2.746,
      "grad_norm": 2.3470542430877686,
      "learning_rate": 4.2617449664429526e-06,
      "loss": 0.4723,
      "step": 68650
    },
    {
      "epoch": 2.7464,
      "grad_norm": 2.3525984287261963,
      "learning_rate": 4.25503355704698e-06,
      "loss": 0.3951,
      "step": 68660
    },
    {
      "epoch": 2.7468,
      "grad_norm": 2.102034091949463,
      "learning_rate": 4.248322147651007e-06,
      "loss": 0.4357,
      "step": 68670
    },
    {
      "epoch": 2.7472,
      "grad_norm": 2.3571054935455322,
      "learning_rate": 4.241610738255033e-06,
      "loss": 0.4405,
      "step": 68680
    },
    {
      "epoch": 2.7476000000000003,
      "grad_norm": 2.5446317195892334,
      "learning_rate": 4.234899328859061e-06,
      "loss": 0.4918,
      "step": 68690
    },
    {
      "epoch": 2.748,
      "grad_norm": 3.2961533069610596,
      "learning_rate": 4.228187919463088e-06,
      "loss": 0.588,
      "step": 68700
    },
    {
      "epoch": 2.7484,
      "grad_norm": 3.0065531730651855,
      "learning_rate": 4.2214765100671145e-06,
      "loss": 0.5306,
      "step": 68710
    },
    {
      "epoch": 2.7488,
      "grad_norm": 3.0839691162109375,
      "learning_rate": 4.214765100671141e-06,
      "loss": 0.4593,
      "step": 68720
    },
    {
      "epoch": 2.7492,
      "grad_norm": 2.0949206352233887,
      "learning_rate": 4.208053691275167e-06,
      "loss": 0.4206,
      "step": 68730
    },
    {
      "epoch": 2.7496,
      "grad_norm": 2.8034422397613525,
      "learning_rate": 4.201342281879195e-06,
      "loss": 0.483,
      "step": 68740
    },
    {
      "epoch": 2.75,
      "grad_norm": 2.6445987224578857,
      "learning_rate": 4.194630872483222e-06,
      "loss": 0.5396,
      "step": 68750
    },
    {
      "epoch": 2.7504,
      "grad_norm": 2.497938394546509,
      "learning_rate": 4.187919463087248e-06,
      "loss": 0.4832,
      "step": 68760
    },
    {
      "epoch": 2.7508,
      "grad_norm": 3.027696132659912,
      "learning_rate": 4.181208053691276e-06,
      "loss": 0.4396,
      "step": 68770
    },
    {
      "epoch": 2.7512,
      "grad_norm": 2.6945624351501465,
      "learning_rate": 4.1744966442953025e-06,
      "loss": 0.4564,
      "step": 68780
    },
    {
      "epoch": 2.7516,
      "grad_norm": 2.1958534717559814,
      "learning_rate": 4.1677852348993285e-06,
      "loss": 0.4287,
      "step": 68790
    },
    {
      "epoch": 2.752,
      "grad_norm": 2.7633473873138428,
      "learning_rate": 4.161073825503356e-06,
      "loss": 0.5062,
      "step": 68800
    },
    {
      "epoch": 2.7523999999999997,
      "grad_norm": 3.619872570037842,
      "learning_rate": 4.154362416107382e-06,
      "loss": 0.4911,
      "step": 68810
    },
    {
      "epoch": 2.7528,
      "grad_norm": 3.254287004470825,
      "learning_rate": 4.14765100671141e-06,
      "loss": 0.5331,
      "step": 68820
    },
    {
      "epoch": 2.7532,
      "grad_norm": 2.5794475078582764,
      "learning_rate": 4.140939597315437e-06,
      "loss": 0.4362,
      "step": 68830
    },
    {
      "epoch": 2.7536,
      "grad_norm": 2.0886118412017822,
      "learning_rate": 4.134228187919463e-06,
      "loss": 0.5215,
      "step": 68840
    },
    {
      "epoch": 2.754,
      "grad_norm": 2.368154764175415,
      "learning_rate": 4.1275167785234905e-06,
      "loss": 0.5297,
      "step": 68850
    },
    {
      "epoch": 2.7544,
      "grad_norm": 2.3624467849731445,
      "learning_rate": 4.120805369127517e-06,
      "loss": 0.4908,
      "step": 68860
    },
    {
      "epoch": 2.7548,
      "grad_norm": 2.22381591796875,
      "learning_rate": 4.114093959731543e-06,
      "loss": 0.4333,
      "step": 68870
    },
    {
      "epoch": 2.7552,
      "grad_norm": 2.1681268215179443,
      "learning_rate": 4.107382550335571e-06,
      "loss": 0.5368,
      "step": 68880
    },
    {
      "epoch": 2.7556000000000003,
      "grad_norm": 2.9934730529785156,
      "learning_rate": 4.100671140939597e-06,
      "loss": 0.5279,
      "step": 68890
    },
    {
      "epoch": 2.7560000000000002,
      "grad_norm": 2.1007649898529053,
      "learning_rate": 4.093959731543624e-06,
      "loss": 0.456,
      "step": 68900
    },
    {
      "epoch": 2.7564,
      "grad_norm": 3.0259246826171875,
      "learning_rate": 4.0872483221476516e-06,
      "loss": 0.5336,
      "step": 68910
    },
    {
      "epoch": 2.7568,
      "grad_norm": 2.5286171436309814,
      "learning_rate": 4.080536912751678e-06,
      "loss": 0.4363,
      "step": 68920
    },
    {
      "epoch": 2.7572,
      "grad_norm": 2.5984580516815186,
      "learning_rate": 4.073825503355705e-06,
      "loss": 0.512,
      "step": 68930
    },
    {
      "epoch": 2.7576,
      "grad_norm": 2.775670051574707,
      "learning_rate": 4.067114093959732e-06,
      "loss": 0.5532,
      "step": 68940
    },
    {
      "epoch": 2.758,
      "grad_norm": 2.1733169555664062,
      "learning_rate": 4.060402684563758e-06,
      "loss": 0.4975,
      "step": 68950
    },
    {
      "epoch": 2.7584,
      "grad_norm": 2.1504933834075928,
      "learning_rate": 4.053691275167786e-06,
      "loss": 0.4527,
      "step": 68960
    },
    {
      "epoch": 2.7588,
      "grad_norm": 3.0939228534698486,
      "learning_rate": 4.046979865771812e-06,
      "loss": 0.533,
      "step": 68970
    },
    {
      "epoch": 2.7592,
      "grad_norm": 2.8035457134246826,
      "learning_rate": 4.040268456375839e-06,
      "loss": 0.6038,
      "step": 68980
    },
    {
      "epoch": 2.7596,
      "grad_norm": 2.6705322265625,
      "learning_rate": 4.033557046979866e-06,
      "loss": 0.5457,
      "step": 68990
    },
    {
      "epoch": 2.76,
      "grad_norm": 2.8459901809692383,
      "learning_rate": 4.026845637583892e-06,
      "loss": 0.4963,
      "step": 69000
    },
    {
      "epoch": 2.7603999999999997,
      "grad_norm": 2.7055320739746094,
      "learning_rate": 4.020134228187919e-06,
      "loss": 0.5266,
      "step": 69010
    },
    {
      "epoch": 2.7608,
      "grad_norm": 2.7536866664886475,
      "learning_rate": 4.013422818791947e-06,
      "loss": 0.4573,
      "step": 69020
    },
    {
      "epoch": 2.7612,
      "grad_norm": 2.7718989849090576,
      "learning_rate": 4.006711409395973e-06,
      "loss": 0.511,
      "step": 69030
    },
    {
      "epoch": 2.7616,
      "grad_norm": 2.443457841873169,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.4711,
      "step": 69040
    },
    {
      "epoch": 2.762,
      "grad_norm": 2.611152172088623,
      "learning_rate": 3.9932885906040275e-06,
      "loss": 0.4484,
      "step": 69050
    },
    {
      "epoch": 2.7624,
      "grad_norm": 3.0902671813964844,
      "learning_rate": 3.9865771812080535e-06,
      "loss": 0.5041,
      "step": 69060
    },
    {
      "epoch": 2.7628,
      "grad_norm": 2.6058011054992676,
      "learning_rate": 3.979865771812081e-06,
      "loss": 0.4756,
      "step": 69070
    },
    {
      "epoch": 2.7632,
      "grad_norm": 1.894348382949829,
      "learning_rate": 3.973154362416107e-06,
      "loss": 0.466,
      "step": 69080
    },
    {
      "epoch": 2.7636,
      "grad_norm": 2.844660758972168,
      "learning_rate": 3.966442953020134e-06,
      "loss": 0.5102,
      "step": 69090
    },
    {
      "epoch": 2.7640000000000002,
      "grad_norm": 2.2841572761535645,
      "learning_rate": 3.959731543624162e-06,
      "loss": 0.5594,
      "step": 69100
    },
    {
      "epoch": 2.7644,
      "grad_norm": 3.5047261714935303,
      "learning_rate": 3.953020134228188e-06,
      "loss": 0.4963,
      "step": 69110
    },
    {
      "epoch": 2.7648,
      "grad_norm": 2.6024582386016846,
      "learning_rate": 3.946308724832215e-06,
      "loss": 0.4854,
      "step": 69120
    },
    {
      "epoch": 2.7652,
      "grad_norm": 2.9336931705474854,
      "learning_rate": 3.939597315436242e-06,
      "loss": 0.6098,
      "step": 69130
    },
    {
      "epoch": 2.7656,
      "grad_norm": 2.2736711502075195,
      "learning_rate": 3.932885906040268e-06,
      "loss": 0.4936,
      "step": 69140
    },
    {
      "epoch": 2.766,
      "grad_norm": 3.0791945457458496,
      "learning_rate": 3.926174496644296e-06,
      "loss": 0.5266,
      "step": 69150
    },
    {
      "epoch": 2.7664,
      "grad_norm": 3.1267168521881104,
      "learning_rate": 3.919463087248322e-06,
      "loss": 0.5619,
      "step": 69160
    },
    {
      "epoch": 2.7668,
      "grad_norm": 3.300992727279663,
      "learning_rate": 3.912751677852349e-06,
      "loss": 0.5888,
      "step": 69170
    },
    {
      "epoch": 2.7672,
      "grad_norm": 2.432905673980713,
      "learning_rate": 3.906040268456377e-06,
      "loss": 0.5409,
      "step": 69180
    },
    {
      "epoch": 2.7676,
      "grad_norm": 2.6062214374542236,
      "learning_rate": 3.899328859060403e-06,
      "loss": 0.5209,
      "step": 69190
    },
    {
      "epoch": 2.768,
      "grad_norm": 2.444460391998291,
      "learning_rate": 3.8926174496644295e-06,
      "loss": 0.4645,
      "step": 69200
    },
    {
      "epoch": 2.7683999999999997,
      "grad_norm": 2.7382383346557617,
      "learning_rate": 3.885906040268457e-06,
      "loss": 0.5352,
      "step": 69210
    },
    {
      "epoch": 2.7688,
      "grad_norm": 2.6104469299316406,
      "learning_rate": 3.879194630872483e-06,
      "loss": 0.4951,
      "step": 69220
    },
    {
      "epoch": 2.7692,
      "grad_norm": 2.4055063724517822,
      "learning_rate": 3.87248322147651e-06,
      "loss": 0.3812,
      "step": 69230
    },
    {
      "epoch": 2.7696,
      "grad_norm": 2.564465045928955,
      "learning_rate": 3.865771812080537e-06,
      "loss": 0.5351,
      "step": 69240
    },
    {
      "epoch": 2.77,
      "grad_norm": 3.3201754093170166,
      "learning_rate": 3.859060402684564e-06,
      "loss": 0.512,
      "step": 69250
    },
    {
      "epoch": 2.7704,
      "grad_norm": 2.866010904312134,
      "learning_rate": 3.8523489932885914e-06,
      "loss": 0.5264,
      "step": 69260
    },
    {
      "epoch": 2.7708,
      "grad_norm": 2.784749746322632,
      "learning_rate": 3.8456375838926174e-06,
      "loss": 0.5985,
      "step": 69270
    },
    {
      "epoch": 2.7712,
      "grad_norm": 2.8630499839782715,
      "learning_rate": 3.838926174496644e-06,
      "loss": 0.4789,
      "step": 69280
    },
    {
      "epoch": 2.7716,
      "grad_norm": 3.5039539337158203,
      "learning_rate": 3.832214765100672e-06,
      "loss": 0.5396,
      "step": 69290
    },
    {
      "epoch": 2.7720000000000002,
      "grad_norm": 3.1915361881256104,
      "learning_rate": 3.825503355704698e-06,
      "loss": 0.4763,
      "step": 69300
    },
    {
      "epoch": 2.7724,
      "grad_norm": 2.740347385406494,
      "learning_rate": 3.818791946308725e-06,
      "loss": 0.5106,
      "step": 69310
    },
    {
      "epoch": 2.7728,
      "grad_norm": 2.2298052310943604,
      "learning_rate": 3.8120805369127517e-06,
      "loss": 0.516,
      "step": 69320
    },
    {
      "epoch": 2.7732,
      "grad_norm": 2.2299251556396484,
      "learning_rate": 3.8053691275167786e-06,
      "loss": 0.4295,
      "step": 69330
    },
    {
      "epoch": 2.7736,
      "grad_norm": 3.617361545562744,
      "learning_rate": 3.798657718120806e-06,
      "loss": 0.4818,
      "step": 69340
    },
    {
      "epoch": 2.774,
      "grad_norm": 2.1558871269226074,
      "learning_rate": 3.7919463087248323e-06,
      "loss": 0.5197,
      "step": 69350
    },
    {
      "epoch": 2.7744,
      "grad_norm": 2.842590808868408,
      "learning_rate": 3.785234899328859e-06,
      "loss": 0.4705,
      "step": 69360
    },
    {
      "epoch": 2.7748,
      "grad_norm": 2.7154581546783447,
      "learning_rate": 3.7785234899328864e-06,
      "loss": 0.4757,
      "step": 69370
    },
    {
      "epoch": 2.7752,
      "grad_norm": 2.807983875274658,
      "learning_rate": 3.771812080536913e-06,
      "loss": 0.5199,
      "step": 69380
    },
    {
      "epoch": 2.7756,
      "grad_norm": 2.535515069961548,
      "learning_rate": 3.76510067114094e-06,
      "loss": 0.5214,
      "step": 69390
    },
    {
      "epoch": 2.776,
      "grad_norm": 2.3799631595611572,
      "learning_rate": 3.7583892617449665e-06,
      "loss": 0.4488,
      "step": 69400
    },
    {
      "epoch": 2.7763999999999998,
      "grad_norm": 2.3799521923065186,
      "learning_rate": 3.7516778523489934e-06,
      "loss": 0.4457,
      "step": 69410
    },
    {
      "epoch": 2.7768,
      "grad_norm": 2.5501303672790527,
      "learning_rate": 3.7449664429530207e-06,
      "loss": 0.5311,
      "step": 69420
    },
    {
      "epoch": 2.7772,
      "grad_norm": 2.441514253616333,
      "learning_rate": 3.738255033557047e-06,
      "loss": 0.4586,
      "step": 69430
    },
    {
      "epoch": 2.7776,
      "grad_norm": 2.8413636684417725,
      "learning_rate": 3.731543624161074e-06,
      "loss": 0.482,
      "step": 69440
    },
    {
      "epoch": 2.778,
      "grad_norm": 2.922297239303589,
      "learning_rate": 3.7248322147651012e-06,
      "loss": 0.5071,
      "step": 69450
    },
    {
      "epoch": 2.7784,
      "grad_norm": 2.372013568878174,
      "learning_rate": 3.7181208053691276e-06,
      "loss": 0.569,
      "step": 69460
    },
    {
      "epoch": 2.7788,
      "grad_norm": 2.3772685527801514,
      "learning_rate": 3.7114093959731545e-06,
      "loss": 0.5372,
      "step": 69470
    },
    {
      "epoch": 2.7792,
      "grad_norm": 2.952913522720337,
      "learning_rate": 3.704697986577181e-06,
      "loss": 0.5353,
      "step": 69480
    },
    {
      "epoch": 2.7796,
      "grad_norm": 2.0179522037506104,
      "learning_rate": 3.697986577181208e-06,
      "loss": 0.4876,
      "step": 69490
    },
    {
      "epoch": 2.7800000000000002,
      "grad_norm": 2.205610990524292,
      "learning_rate": 3.6912751677852355e-06,
      "loss": 0.4879,
      "step": 69500
    },
    {
      "epoch": 2.7804,
      "grad_norm": 3.226588010787964,
      "learning_rate": 3.684563758389262e-06,
      "loss": 0.6057,
      "step": 69510
    },
    {
      "epoch": 2.7808,
      "grad_norm": 2.6789743900299072,
      "learning_rate": 3.6778523489932888e-06,
      "loss": 0.4647,
      "step": 69520
    },
    {
      "epoch": 2.7812,
      "grad_norm": 2.5765693187713623,
      "learning_rate": 3.671140939597316e-06,
      "loss": 0.5143,
      "step": 69530
    },
    {
      "epoch": 2.7816,
      "grad_norm": 2.7675273418426514,
      "learning_rate": 3.6644295302013425e-06,
      "loss": 0.4222,
      "step": 69540
    },
    {
      "epoch": 2.782,
      "grad_norm": 2.103769063949585,
      "learning_rate": 3.6577181208053693e-06,
      "loss": 0.5016,
      "step": 69550
    },
    {
      "epoch": 2.7824,
      "grad_norm": 2.449707269668579,
      "learning_rate": 3.6510067114093958e-06,
      "loss": 0.4753,
      "step": 69560
    },
    {
      "epoch": 2.7828,
      "grad_norm": 2.58416485786438,
      "learning_rate": 3.644295302013423e-06,
      "loss": 0.5163,
      "step": 69570
    },
    {
      "epoch": 2.7832,
      "grad_norm": 3.0795865058898926,
      "learning_rate": 3.63758389261745e-06,
      "loss": 0.5302,
      "step": 69580
    },
    {
      "epoch": 2.7836,
      "grad_norm": 2.9132449626922607,
      "learning_rate": 3.6308724832214763e-06,
      "loss": 0.4613,
      "step": 69590
    },
    {
      "epoch": 2.784,
      "grad_norm": 2.49031925201416,
      "learning_rate": 3.6241610738255036e-06,
      "loss": 0.5325,
      "step": 69600
    },
    {
      "epoch": 2.7843999999999998,
      "grad_norm": 2.557765483856201,
      "learning_rate": 3.617449664429531e-06,
      "loss": 0.4897,
      "step": 69610
    },
    {
      "epoch": 2.7848,
      "grad_norm": 2.5748915672302246,
      "learning_rate": 3.6107382550335573e-06,
      "loss": 0.6002,
      "step": 69620
    },
    {
      "epoch": 2.7852,
      "grad_norm": 2.706393241882324,
      "learning_rate": 3.604026845637584e-06,
      "loss": 0.4872,
      "step": 69630
    },
    {
      "epoch": 2.7856,
      "grad_norm": 2.4363696575164795,
      "learning_rate": 3.5973154362416106e-06,
      "loss": 0.5134,
      "step": 69640
    },
    {
      "epoch": 2.786,
      "grad_norm": 4.392805099487305,
      "learning_rate": 3.590604026845638e-06,
      "loss": 0.4945,
      "step": 69650
    },
    {
      "epoch": 2.7864,
      "grad_norm": 3.155812978744507,
      "learning_rate": 3.5838926174496647e-06,
      "loss": 0.512,
      "step": 69660
    },
    {
      "epoch": 2.7868,
      "grad_norm": 1.940804123878479,
      "learning_rate": 3.577181208053691e-06,
      "loss": 0.5038,
      "step": 69670
    },
    {
      "epoch": 2.7872,
      "grad_norm": 1.998203992843628,
      "learning_rate": 3.5704697986577184e-06,
      "loss": 0.4349,
      "step": 69680
    },
    {
      "epoch": 2.7876,
      "grad_norm": 2.675706386566162,
      "learning_rate": 3.5637583892617453e-06,
      "loss": 0.5057,
      "step": 69690
    },
    {
      "epoch": 2.7880000000000003,
      "grad_norm": 3.028435468673706,
      "learning_rate": 3.5570469798657717e-06,
      "loss": 0.5122,
      "step": 69700
    },
    {
      "epoch": 2.7884,
      "grad_norm": 2.3040366172790527,
      "learning_rate": 3.550335570469799e-06,
      "loss": 0.4588,
      "step": 69710
    },
    {
      "epoch": 2.7888,
      "grad_norm": 2.8432974815368652,
      "learning_rate": 3.5436241610738254e-06,
      "loss": 0.5359,
      "step": 69720
    },
    {
      "epoch": 2.7892,
      "grad_norm": 1.6464524269104004,
      "learning_rate": 3.5369127516778523e-06,
      "loss": 0.4527,
      "step": 69730
    },
    {
      "epoch": 2.7896,
      "grad_norm": 2.8757317066192627,
      "learning_rate": 3.5302013422818795e-06,
      "loss": 0.5244,
      "step": 69740
    },
    {
      "epoch": 2.79,
      "grad_norm": 2.4529237747192383,
      "learning_rate": 3.523489932885906e-06,
      "loss": 0.5246,
      "step": 69750
    },
    {
      "epoch": 2.7904,
      "grad_norm": 2.6982152462005615,
      "learning_rate": 3.5167785234899332e-06,
      "loss": 0.5197,
      "step": 69760
    },
    {
      "epoch": 2.7908,
      "grad_norm": 2.9222164154052734,
      "learning_rate": 3.51006711409396e-06,
      "loss": 0.556,
      "step": 69770
    },
    {
      "epoch": 2.7912,
      "grad_norm": 2.4658234119415283,
      "learning_rate": 3.5033557046979865e-06,
      "loss": 0.5135,
      "step": 69780
    },
    {
      "epoch": 2.7916,
      "grad_norm": 3.2116682529449463,
      "learning_rate": 3.496644295302014e-06,
      "loss": 0.5558,
      "step": 69790
    },
    {
      "epoch": 2.792,
      "grad_norm": 2.876851797103882,
      "learning_rate": 3.4899328859060402e-06,
      "loss": 0.4643,
      "step": 69800
    },
    {
      "epoch": 2.7923999999999998,
      "grad_norm": 2.9041709899902344,
      "learning_rate": 3.483221476510067e-06,
      "loss": 0.5103,
      "step": 69810
    },
    {
      "epoch": 2.7927999999999997,
      "grad_norm": 2.0721540451049805,
      "learning_rate": 3.4765100671140944e-06,
      "loss": 0.4685,
      "step": 69820
    },
    {
      "epoch": 2.7932,
      "grad_norm": 2.542621612548828,
      "learning_rate": 3.4697986577181208e-06,
      "loss": 0.4575,
      "step": 69830
    },
    {
      "epoch": 2.7936,
      "grad_norm": 3.8848042488098145,
      "learning_rate": 3.4630872483221476e-06,
      "loss": 0.5386,
      "step": 69840
    },
    {
      "epoch": 2.794,
      "grad_norm": 2.2437338829040527,
      "learning_rate": 3.456375838926175e-06,
      "loss": 0.5288,
      "step": 69850
    },
    {
      "epoch": 2.7944,
      "grad_norm": 2.4879565238952637,
      "learning_rate": 3.4496644295302013e-06,
      "loss": 0.5714,
      "step": 69860
    },
    {
      "epoch": 2.7948,
      "grad_norm": 2.5073795318603516,
      "learning_rate": 3.4429530201342286e-06,
      "loss": 0.4738,
      "step": 69870
    },
    {
      "epoch": 2.7952,
      "grad_norm": 2.7797727584838867,
      "learning_rate": 3.436241610738255e-06,
      "loss": 0.5166,
      "step": 69880
    },
    {
      "epoch": 2.7956,
      "grad_norm": 2.102605104446411,
      "learning_rate": 3.429530201342282e-06,
      "loss": 0.5808,
      "step": 69890
    },
    {
      "epoch": 2.7960000000000003,
      "grad_norm": 2.877002239227295,
      "learning_rate": 3.422818791946309e-06,
      "loss": 0.4995,
      "step": 69900
    },
    {
      "epoch": 2.7964,
      "grad_norm": 3.291486978530884,
      "learning_rate": 3.4161073825503356e-06,
      "loss": 0.5545,
      "step": 69910
    },
    {
      "epoch": 2.7968,
      "grad_norm": 2.1932730674743652,
      "learning_rate": 3.4093959731543625e-06,
      "loss": 0.4464,
      "step": 69920
    },
    {
      "epoch": 2.7972,
      "grad_norm": 2.334472417831421,
      "learning_rate": 3.4026845637583897e-06,
      "loss": 0.4842,
      "step": 69930
    },
    {
      "epoch": 2.7976,
      "grad_norm": 2.970170736312866,
      "learning_rate": 3.395973154362416e-06,
      "loss": 0.5319,
      "step": 69940
    },
    {
      "epoch": 2.798,
      "grad_norm": 2.6859474182128906,
      "learning_rate": 3.389261744966443e-06,
      "loss": 0.4912,
      "step": 69950
    },
    {
      "epoch": 2.7984,
      "grad_norm": 2.496821641921997,
      "learning_rate": 3.3825503355704695e-06,
      "loss": 0.4985,
      "step": 69960
    },
    {
      "epoch": 2.7988,
      "grad_norm": 2.276317596435547,
      "learning_rate": 3.3758389261744967e-06,
      "loss": 0.5165,
      "step": 69970
    },
    {
      "epoch": 2.7992,
      "grad_norm": 2.704108238220215,
      "learning_rate": 3.369127516778524e-06,
      "loss": 0.4407,
      "step": 69980
    },
    {
      "epoch": 2.7996,
      "grad_norm": 2.588991165161133,
      "learning_rate": 3.3624161073825504e-06,
      "loss": 0.4739,
      "step": 69990
    },
    {
      "epoch": 2.8,
      "grad_norm": 2.8672170639038086,
      "learning_rate": 3.3557046979865773e-06,
      "loss": 0.5157,
      "step": 70000
    },
    {
      "epoch": 2.8004,
      "grad_norm": 2.2524144649505615,
      "learning_rate": 3.3489932885906046e-06,
      "loss": 0.5052,
      "step": 70010
    },
    {
      "epoch": 2.8007999999999997,
      "grad_norm": 3.273449420928955,
      "learning_rate": 3.342281879194631e-06,
      "loss": 0.5062,
      "step": 70020
    },
    {
      "epoch": 2.8012,
      "grad_norm": 3.154240846633911,
      "learning_rate": 3.335570469798658e-06,
      "loss": 0.5254,
      "step": 70030
    },
    {
      "epoch": 2.8016,
      "grad_norm": 2.5614728927612305,
      "learning_rate": 3.3288590604026843e-06,
      "loss": 0.4217,
      "step": 70040
    },
    {
      "epoch": 2.802,
      "grad_norm": 2.7856078147888184,
      "learning_rate": 3.3221476510067116e-06,
      "loss": 0.4621,
      "step": 70050
    },
    {
      "epoch": 2.8024,
      "grad_norm": 1.738403558731079,
      "learning_rate": 3.3154362416107384e-06,
      "loss": 0.4801,
      "step": 70060
    },
    {
      "epoch": 2.8028,
      "grad_norm": 2.9309909343719482,
      "learning_rate": 3.308724832214765e-06,
      "loss": 0.4776,
      "step": 70070
    },
    {
      "epoch": 2.8032,
      "grad_norm": 2.09649395942688,
      "learning_rate": 3.302013422818792e-06,
      "loss": 0.4743,
      "step": 70080
    },
    {
      "epoch": 2.8036,
      "grad_norm": 3.1403024196624756,
      "learning_rate": 3.2953020134228194e-06,
      "loss": 0.5377,
      "step": 70090
    },
    {
      "epoch": 2.8040000000000003,
      "grad_norm": 3.0429046154022217,
      "learning_rate": 3.288590604026846e-06,
      "loss": 0.5379,
      "step": 70100
    },
    {
      "epoch": 2.8044000000000002,
      "grad_norm": 3.0797247886657715,
      "learning_rate": 3.2818791946308727e-06,
      "loss": 0.5373,
      "step": 70110
    },
    {
      "epoch": 2.8048,
      "grad_norm": 3.1420960426330566,
      "learning_rate": 3.275167785234899e-06,
      "loss": 0.4533,
      "step": 70120
    },
    {
      "epoch": 2.8052,
      "grad_norm": 2.542386770248413,
      "learning_rate": 3.2684563758389264e-06,
      "loss": 0.5203,
      "step": 70130
    },
    {
      "epoch": 2.8056,
      "grad_norm": 2.448493719100952,
      "learning_rate": 3.2617449664429532e-06,
      "loss": 0.5168,
      "step": 70140
    },
    {
      "epoch": 2.806,
      "grad_norm": 2.328460931777954,
      "learning_rate": 3.2550335570469797e-06,
      "loss": 0.5127,
      "step": 70150
    },
    {
      "epoch": 2.8064,
      "grad_norm": 2.438398599624634,
      "learning_rate": 3.248322147651007e-06,
      "loss": 0.5198,
      "step": 70160
    },
    {
      "epoch": 2.8068,
      "grad_norm": 1.919005036354065,
      "learning_rate": 3.241610738255034e-06,
      "loss": 0.453,
      "step": 70170
    },
    {
      "epoch": 2.8072,
      "grad_norm": 3.2770795822143555,
      "learning_rate": 3.2348993288590602e-06,
      "loss": 0.5151,
      "step": 70180
    },
    {
      "epoch": 2.8076,
      "grad_norm": 2.7279155254364014,
      "learning_rate": 3.2281879194630875e-06,
      "loss": 0.5093,
      "step": 70190
    },
    {
      "epoch": 2.808,
      "grad_norm": 2.806400775909424,
      "learning_rate": 3.2214765100671148e-06,
      "loss": 0.4953,
      "step": 70200
    },
    {
      "epoch": 2.8084,
      "grad_norm": 2.680701971054077,
      "learning_rate": 3.214765100671141e-06,
      "loss": 0.4956,
      "step": 70210
    },
    {
      "epoch": 2.8087999999999997,
      "grad_norm": 3.0853209495544434,
      "learning_rate": 3.208053691275168e-06,
      "loss": 0.4901,
      "step": 70220
    },
    {
      "epoch": 2.8092,
      "grad_norm": 2.699988842010498,
      "learning_rate": 3.2013422818791945e-06,
      "loss": 0.4999,
      "step": 70230
    },
    {
      "epoch": 2.8096,
      "grad_norm": 2.7794928550720215,
      "learning_rate": 3.1946308724832218e-06,
      "loss": 0.4061,
      "step": 70240
    },
    {
      "epoch": 2.81,
      "grad_norm": 3.0611369609832764,
      "learning_rate": 3.1879194630872486e-06,
      "loss": 0.4994,
      "step": 70250
    },
    {
      "epoch": 2.8104,
      "grad_norm": 2.8950648307800293,
      "learning_rate": 3.181208053691275e-06,
      "loss": 0.4696,
      "step": 70260
    },
    {
      "epoch": 2.8108,
      "grad_norm": 3.0793566703796387,
      "learning_rate": 3.1744966442953023e-06,
      "loss": 0.555,
      "step": 70270
    },
    {
      "epoch": 2.8112,
      "grad_norm": 2.489753246307373,
      "learning_rate": 3.167785234899329e-06,
      "loss": 0.4934,
      "step": 70280
    },
    {
      "epoch": 2.8116,
      "grad_norm": 2.648052930831909,
      "learning_rate": 3.1610738255033556e-06,
      "loss": 0.5153,
      "step": 70290
    },
    {
      "epoch": 2.8120000000000003,
      "grad_norm": 2.3419501781463623,
      "learning_rate": 3.154362416107383e-06,
      "loss": 0.4522,
      "step": 70300
    },
    {
      "epoch": 2.8124000000000002,
      "grad_norm": 2.1587672233581543,
      "learning_rate": 3.1476510067114093e-06,
      "loss": 0.4893,
      "step": 70310
    },
    {
      "epoch": 2.8128,
      "grad_norm": 2.5938682556152344,
      "learning_rate": 3.1409395973154366e-06,
      "loss": 0.5782,
      "step": 70320
    },
    {
      "epoch": 2.8132,
      "grad_norm": 2.4998435974121094,
      "learning_rate": 3.1342281879194634e-06,
      "loss": 0.4662,
      "step": 70330
    },
    {
      "epoch": 2.8136,
      "grad_norm": 2.127319097518921,
      "learning_rate": 3.12751677852349e-06,
      "loss": 0.459,
      "step": 70340
    },
    {
      "epoch": 2.814,
      "grad_norm": 2.8713552951812744,
      "learning_rate": 3.120805369127517e-06,
      "loss": 0.4626,
      "step": 70350
    },
    {
      "epoch": 2.8144,
      "grad_norm": 1.9454814195632935,
      "learning_rate": 3.1140939597315436e-06,
      "loss": 0.4966,
      "step": 70360
    },
    {
      "epoch": 2.8148,
      "grad_norm": 2.71574330329895,
      "learning_rate": 3.1073825503355704e-06,
      "loss": 0.4566,
      "step": 70370
    },
    {
      "epoch": 2.8152,
      "grad_norm": 2.0656802654266357,
      "learning_rate": 3.1006711409395977e-06,
      "loss": 0.4848,
      "step": 70380
    },
    {
      "epoch": 2.8156,
      "grad_norm": 2.279494285583496,
      "learning_rate": 3.0939597315436246e-06,
      "loss": 0.441,
      "step": 70390
    },
    {
      "epoch": 2.816,
      "grad_norm": 2.798725128173828,
      "learning_rate": 3.087248322147651e-06,
      "loss": 0.5088,
      "step": 70400
    },
    {
      "epoch": 2.8164,
      "grad_norm": 3.200080633163452,
      "learning_rate": 3.080536912751678e-06,
      "loss": 0.5382,
      "step": 70410
    },
    {
      "epoch": 2.8167999999999997,
      "grad_norm": 2.675072193145752,
      "learning_rate": 3.073825503355705e-06,
      "loss": 0.426,
      "step": 70420
    },
    {
      "epoch": 2.8172,
      "grad_norm": 1.9780642986297607,
      "learning_rate": 3.067114093959732e-06,
      "loss": 0.4323,
      "step": 70430
    },
    {
      "epoch": 2.8176,
      "grad_norm": 1.6062722206115723,
      "learning_rate": 3.0604026845637584e-06,
      "loss": 0.5199,
      "step": 70440
    },
    {
      "epoch": 2.818,
      "grad_norm": 2.086641550064087,
      "learning_rate": 3.0536912751677853e-06,
      "loss": 0.4717,
      "step": 70450
    },
    {
      "epoch": 2.8184,
      "grad_norm": 3.1037020683288574,
      "learning_rate": 3.0469798657718125e-06,
      "loss": 0.52,
      "step": 70460
    },
    {
      "epoch": 2.8188,
      "grad_norm": 2.228273391723633,
      "learning_rate": 3.040268456375839e-06,
      "loss": 0.4135,
      "step": 70470
    },
    {
      "epoch": 2.8192,
      "grad_norm": 2.039475202560425,
      "learning_rate": 3.033557046979866e-06,
      "loss": 0.4487,
      "step": 70480
    },
    {
      "epoch": 2.8196,
      "grad_norm": 3.00933837890625,
      "learning_rate": 3.0268456375838927e-06,
      "loss": 0.5646,
      "step": 70490
    },
    {
      "epoch": 2.82,
      "grad_norm": 2.315401077270508,
      "learning_rate": 3.02013422818792e-06,
      "loss": 0.4447,
      "step": 70500
    },
    {
      "epoch": 2.8204000000000002,
      "grad_norm": 3.2468204498291016,
      "learning_rate": 3.0134228187919464e-06,
      "loss": 0.5557,
      "step": 70510
    },
    {
      "epoch": 2.8208,
      "grad_norm": 2.7456438541412354,
      "learning_rate": 3.0067114093959732e-06,
      "loss": 0.4977,
      "step": 70520
    },
    {
      "epoch": 2.8212,
      "grad_norm": 2.606518507003784,
      "learning_rate": 3e-06,
      "loss": 0.4778,
      "step": 70530
    },
    {
      "epoch": 2.8216,
      "grad_norm": 2.583798408508301,
      "learning_rate": 2.993288590604027e-06,
      "loss": 0.4976,
      "step": 70540
    },
    {
      "epoch": 2.822,
      "grad_norm": 2.347747802734375,
      "learning_rate": 2.9865771812080538e-06,
      "loss": 0.4324,
      "step": 70550
    },
    {
      "epoch": 2.8224,
      "grad_norm": 2.477590560913086,
      "learning_rate": 2.9798657718120806e-06,
      "loss": 0.4415,
      "step": 70560
    },
    {
      "epoch": 2.8228,
      "grad_norm": 2.634845733642578,
      "learning_rate": 2.9731543624161075e-06,
      "loss": 0.5294,
      "step": 70570
    },
    {
      "epoch": 2.8232,
      "grad_norm": 2.1621463298797607,
      "learning_rate": 2.9664429530201343e-06,
      "loss": 0.4529,
      "step": 70580
    },
    {
      "epoch": 2.8236,
      "grad_norm": 3.0601770877838135,
      "learning_rate": 2.959731543624161e-06,
      "loss": 0.5174,
      "step": 70590
    },
    {
      "epoch": 2.824,
      "grad_norm": 1.9204330444335938,
      "learning_rate": 2.953020134228188e-06,
      "loss": 0.4568,
      "step": 70600
    },
    {
      "epoch": 2.8244,
      "grad_norm": 2.498856544494629,
      "learning_rate": 2.946308724832215e-06,
      "loss": 0.503,
      "step": 70610
    },
    {
      "epoch": 2.8247999999999998,
      "grad_norm": 2.141481876373291,
      "learning_rate": 2.9395973154362418e-06,
      "loss": 0.4802,
      "step": 70620
    },
    {
      "epoch": 2.8252,
      "grad_norm": 2.982966184616089,
      "learning_rate": 2.9328859060402686e-06,
      "loss": 0.5239,
      "step": 70630
    },
    {
      "epoch": 2.8256,
      "grad_norm": 3.0627524852752686,
      "learning_rate": 2.9261744966442955e-06,
      "loss": 0.4894,
      "step": 70640
    },
    {
      "epoch": 2.826,
      "grad_norm": 2.4563961029052734,
      "learning_rate": 2.9194630872483223e-06,
      "loss": 0.5092,
      "step": 70650
    },
    {
      "epoch": 2.8264,
      "grad_norm": 2.926225423812866,
      "learning_rate": 2.912751677852349e-06,
      "loss": 0.4627,
      "step": 70660
    },
    {
      "epoch": 2.8268,
      "grad_norm": 2.4457342624664307,
      "learning_rate": 2.906040268456376e-06,
      "loss": 0.5389,
      "step": 70670
    },
    {
      "epoch": 2.8272,
      "grad_norm": 2.6755363941192627,
      "learning_rate": 2.899328859060403e-06,
      "loss": 0.4792,
      "step": 70680
    },
    {
      "epoch": 2.8276,
      "grad_norm": 2.6120223999023438,
      "learning_rate": 2.8926174496644297e-06,
      "loss": 0.4814,
      "step": 70690
    },
    {
      "epoch": 2.828,
      "grad_norm": 2.507781505584717,
      "learning_rate": 2.8859060402684566e-06,
      "loss": 0.443,
      "step": 70700
    },
    {
      "epoch": 2.8284000000000002,
      "grad_norm": 2.971461057662964,
      "learning_rate": 2.8791946308724834e-06,
      "loss": 0.4945,
      "step": 70710
    },
    {
      "epoch": 2.8288,
      "grad_norm": 3.289224863052368,
      "learning_rate": 2.8724832214765103e-06,
      "loss": 0.5236,
      "step": 70720
    },
    {
      "epoch": 2.8292,
      "grad_norm": 2.07702898979187,
      "learning_rate": 2.8657718120805367e-06,
      "loss": 0.5592,
      "step": 70730
    },
    {
      "epoch": 2.8296,
      "grad_norm": 2.6407642364501953,
      "learning_rate": 2.859060402684564e-06,
      "loss": 0.5043,
      "step": 70740
    },
    {
      "epoch": 2.83,
      "grad_norm": 2.596257448196411,
      "learning_rate": 2.852348993288591e-06,
      "loss": 0.4671,
      "step": 70750
    },
    {
      "epoch": 2.8304,
      "grad_norm": 2.8701601028442383,
      "learning_rate": 2.8456375838926177e-06,
      "loss": 0.4264,
      "step": 70760
    },
    {
      "epoch": 2.8308,
      "grad_norm": 2.549708127975464,
      "learning_rate": 2.838926174496644e-06,
      "loss": 0.4838,
      "step": 70770
    },
    {
      "epoch": 2.8312,
      "grad_norm": 2.4086930751800537,
      "learning_rate": 2.8322147651006714e-06,
      "loss": 0.4691,
      "step": 70780
    },
    {
      "epoch": 2.8316,
      "grad_norm": 3.2445030212402344,
      "learning_rate": 2.8255033557046983e-06,
      "loss": 0.6275,
      "step": 70790
    },
    {
      "epoch": 2.832,
      "grad_norm": 2.625082015991211,
      "learning_rate": 2.818791946308725e-06,
      "loss": 0.5379,
      "step": 70800
    },
    {
      "epoch": 2.8324,
      "grad_norm": 3.2828571796417236,
      "learning_rate": 2.8120805369127515e-06,
      "loss": 0.4754,
      "step": 70810
    },
    {
      "epoch": 2.8327999999999998,
      "grad_norm": 3.1857094764709473,
      "learning_rate": 2.805369127516779e-06,
      "loss": 0.5402,
      "step": 70820
    },
    {
      "epoch": 2.8332,
      "grad_norm": 2.146958827972412,
      "learning_rate": 2.7986577181208057e-06,
      "loss": 0.3752,
      "step": 70830
    },
    {
      "epoch": 2.8336,
      "grad_norm": 2.986238718032837,
      "learning_rate": 2.791946308724832e-06,
      "loss": 0.4841,
      "step": 70840
    },
    {
      "epoch": 2.834,
      "grad_norm": 2.678765296936035,
      "learning_rate": 2.785234899328859e-06,
      "loss": 0.4916,
      "step": 70850
    },
    {
      "epoch": 2.8344,
      "grad_norm": 2.1448636054992676,
      "learning_rate": 2.7785234899328862e-06,
      "loss": 0.4805,
      "step": 70860
    },
    {
      "epoch": 2.8348,
      "grad_norm": 2.1824216842651367,
      "learning_rate": 2.771812080536913e-06,
      "loss": 0.4908,
      "step": 70870
    },
    {
      "epoch": 2.8352,
      "grad_norm": 2.628765344619751,
      "learning_rate": 2.7651006711409395e-06,
      "loss": 0.4867,
      "step": 70880
    },
    {
      "epoch": 2.8356,
      "grad_norm": 2.4247586727142334,
      "learning_rate": 2.7583892617449664e-06,
      "loss": 0.4831,
      "step": 70890
    },
    {
      "epoch": 2.836,
      "grad_norm": 3.2225165367126465,
      "learning_rate": 2.7516778523489936e-06,
      "loss": 0.5584,
      "step": 70900
    },
    {
      "epoch": 2.8364000000000003,
      "grad_norm": 2.3753254413604736,
      "learning_rate": 2.7449664429530205e-06,
      "loss": 0.5404,
      "step": 70910
    },
    {
      "epoch": 2.8368,
      "grad_norm": 2.1097981929779053,
      "learning_rate": 2.738255033557047e-06,
      "loss": 0.3758,
      "step": 70920
    },
    {
      "epoch": 2.8372,
      "grad_norm": 2.3331210613250732,
      "learning_rate": 2.7315436241610738e-06,
      "loss": 0.4354,
      "step": 70930
    },
    {
      "epoch": 2.8376,
      "grad_norm": 3.1597769260406494,
      "learning_rate": 2.724832214765101e-06,
      "loss": 0.5234,
      "step": 70940
    },
    {
      "epoch": 2.838,
      "grad_norm": 2.799898147583008,
      "learning_rate": 2.7181208053691275e-06,
      "loss": 0.6078,
      "step": 70950
    },
    {
      "epoch": 2.8384,
      "grad_norm": 2.442945957183838,
      "learning_rate": 2.7114093959731543e-06,
      "loss": 0.4757,
      "step": 70960
    },
    {
      "epoch": 2.8388,
      "grad_norm": 2.7474849224090576,
      "learning_rate": 2.704697986577181e-06,
      "loss": 0.5917,
      "step": 70970
    },
    {
      "epoch": 2.8392,
      "grad_norm": 2.519573211669922,
      "learning_rate": 2.6979865771812085e-06,
      "loss": 0.5378,
      "step": 70980
    },
    {
      "epoch": 2.8396,
      "grad_norm": 2.2257513999938965,
      "learning_rate": 2.691275167785235e-06,
      "loss": 0.5614,
      "step": 70990
    },
    {
      "epoch": 2.84,
      "grad_norm": 2.7522754669189453,
      "learning_rate": 2.6845637583892617e-06,
      "loss": 0.4655,
      "step": 71000
    },
    {
      "epoch": 2.8404,
      "grad_norm": 1.7994030714035034,
      "learning_rate": 2.6778523489932886e-06,
      "loss": 0.4373,
      "step": 71010
    },
    {
      "epoch": 2.8407999999999998,
      "grad_norm": 2.9509360790252686,
      "learning_rate": 2.671140939597316e-06,
      "loss": 0.472,
      "step": 71020
    },
    {
      "epoch": 2.8411999999999997,
      "grad_norm": 3.1527259349823,
      "learning_rate": 2.6644295302013423e-06,
      "loss": 0.5233,
      "step": 71030
    },
    {
      "epoch": 2.8416,
      "grad_norm": 2.590379476547241,
      "learning_rate": 2.657718120805369e-06,
      "loss": 0.5446,
      "step": 71040
    },
    {
      "epoch": 2.842,
      "grad_norm": 2.94063138961792,
      "learning_rate": 2.651006711409396e-06,
      "loss": 0.4623,
      "step": 71050
    },
    {
      "epoch": 2.8424,
      "grad_norm": 2.316333532333374,
      "learning_rate": 2.644295302013423e-06,
      "loss": 0.5477,
      "step": 71060
    },
    {
      "epoch": 2.8428,
      "grad_norm": 2.2828586101531982,
      "learning_rate": 2.6375838926174497e-06,
      "loss": 0.4709,
      "step": 71070
    },
    {
      "epoch": 2.8432,
      "grad_norm": 1.9650193452835083,
      "learning_rate": 2.6308724832214766e-06,
      "loss": 0.5143,
      "step": 71080
    },
    {
      "epoch": 2.8436,
      "grad_norm": 2.6790599822998047,
      "learning_rate": 2.6241610738255034e-06,
      "loss": 0.4895,
      "step": 71090
    },
    {
      "epoch": 2.844,
      "grad_norm": 2.9177474975585938,
      "learning_rate": 2.6174496644295303e-06,
      "loss": 0.4693,
      "step": 71100
    },
    {
      "epoch": 2.8444000000000003,
      "grad_norm": 2.994955539703369,
      "learning_rate": 2.610738255033557e-06,
      "loss": 0.5359,
      "step": 71110
    },
    {
      "epoch": 2.8448,
      "grad_norm": 2.434236526489258,
      "learning_rate": 2.604026845637584e-06,
      "loss": 0.5043,
      "step": 71120
    },
    {
      "epoch": 2.8452,
      "grad_norm": 2.721067190170288,
      "learning_rate": 2.597315436241611e-06,
      "loss": 0.5278,
      "step": 71130
    },
    {
      "epoch": 2.8456,
      "grad_norm": 2.316457986831665,
      "learning_rate": 2.5906040268456377e-06,
      "loss": 0.5056,
      "step": 71140
    },
    {
      "epoch": 2.846,
      "grad_norm": 2.3792126178741455,
      "learning_rate": 2.5838926174496645e-06,
      "loss": 0.4899,
      "step": 71150
    },
    {
      "epoch": 2.8464,
      "grad_norm": 2.6044747829437256,
      "learning_rate": 2.5771812080536914e-06,
      "loss": 0.4814,
      "step": 71160
    },
    {
      "epoch": 2.8468,
      "grad_norm": 2.9328339099884033,
      "learning_rate": 2.5704697986577182e-06,
      "loss": 0.4321,
      "step": 71170
    },
    {
      "epoch": 2.8472,
      "grad_norm": 3.0090365409851074,
      "learning_rate": 2.563758389261745e-06,
      "loss": 0.4826,
      "step": 71180
    },
    {
      "epoch": 2.8476,
      "grad_norm": 3.1853573322296143,
      "learning_rate": 2.557046979865772e-06,
      "loss": 0.5391,
      "step": 71190
    },
    {
      "epoch": 2.848,
      "grad_norm": 2.3564040660858154,
      "learning_rate": 2.550335570469799e-06,
      "loss": 0.4525,
      "step": 71200
    },
    {
      "epoch": 2.8484,
      "grad_norm": 2.8771326541900635,
      "learning_rate": 2.5436241610738257e-06,
      "loss": 0.4831,
      "step": 71210
    },
    {
      "epoch": 2.8487999999999998,
      "grad_norm": 3.0247740745544434,
      "learning_rate": 2.5369127516778525e-06,
      "loss": 0.4502,
      "step": 71220
    },
    {
      "epoch": 2.8491999999999997,
      "grad_norm": 2.6263530254364014,
      "learning_rate": 2.5302013422818794e-06,
      "loss": 0.5031,
      "step": 71230
    },
    {
      "epoch": 2.8496,
      "grad_norm": 2.5748374462127686,
      "learning_rate": 2.5234899328859062e-06,
      "loss": 0.4948,
      "step": 71240
    },
    {
      "epoch": 2.85,
      "grad_norm": 2.301213502883911,
      "learning_rate": 2.5167785234899326e-06,
      "loss": 0.5002,
      "step": 71250
    },
    {
      "epoch": 2.8504,
      "grad_norm": 2.2854411602020264,
      "learning_rate": 2.51006711409396e-06,
      "loss": 0.4754,
      "step": 71260
    },
    {
      "epoch": 2.8508,
      "grad_norm": 2.6950342655181885,
      "learning_rate": 2.5033557046979868e-06,
      "loss": 0.5312,
      "step": 71270
    },
    {
      "epoch": 2.8512,
      "grad_norm": 2.1481282711029053,
      "learning_rate": 2.4966442953020136e-06,
      "loss": 0.4796,
      "step": 71280
    },
    {
      "epoch": 2.8516,
      "grad_norm": 2.461422920227051,
      "learning_rate": 2.48993288590604e-06,
      "loss": 0.5017,
      "step": 71290
    },
    {
      "epoch": 2.852,
      "grad_norm": 2.407282829284668,
      "learning_rate": 2.4832214765100673e-06,
      "loss": 0.4728,
      "step": 71300
    },
    {
      "epoch": 2.8524000000000003,
      "grad_norm": 2.604377269744873,
      "learning_rate": 2.476510067114094e-06,
      "loss": 0.5064,
      "step": 71310
    },
    {
      "epoch": 2.8528000000000002,
      "grad_norm": 2.677866220474243,
      "learning_rate": 2.469798657718121e-06,
      "loss": 0.4481,
      "step": 71320
    },
    {
      "epoch": 2.8532,
      "grad_norm": 2.4924802780151367,
      "learning_rate": 2.4630872483221475e-06,
      "loss": 0.5056,
      "step": 71330
    },
    {
      "epoch": 2.8536,
      "grad_norm": 2.63520884513855,
      "learning_rate": 2.4563758389261747e-06,
      "loss": 0.4681,
      "step": 71340
    },
    {
      "epoch": 2.854,
      "grad_norm": 2.558666944503784,
      "learning_rate": 2.4496644295302016e-06,
      "loss": 0.4394,
      "step": 71350
    },
    {
      "epoch": 2.8544,
      "grad_norm": 2.79416561126709,
      "learning_rate": 2.442953020134228e-06,
      "loss": 0.4222,
      "step": 71360
    },
    {
      "epoch": 2.8548,
      "grad_norm": 2.1881296634674072,
      "learning_rate": 2.436241610738255e-06,
      "loss": 0.4124,
      "step": 71370
    },
    {
      "epoch": 2.8552,
      "grad_norm": 2.5715014934539795,
      "learning_rate": 2.429530201342282e-06,
      "loss": 0.3914,
      "step": 71380
    },
    {
      "epoch": 2.8556,
      "grad_norm": 2.74249267578125,
      "learning_rate": 2.422818791946309e-06,
      "loss": 0.5044,
      "step": 71390
    },
    {
      "epoch": 2.856,
      "grad_norm": 2.7496252059936523,
      "learning_rate": 2.4161073825503354e-06,
      "loss": 0.5767,
      "step": 71400
    },
    {
      "epoch": 2.8564,
      "grad_norm": 2.8952012062072754,
      "learning_rate": 2.4093959731543627e-06,
      "loss": 0.53,
      "step": 71410
    },
    {
      "epoch": 2.8568,
      "grad_norm": 2.611894369125366,
      "learning_rate": 2.4026845637583896e-06,
      "loss": 0.5371,
      "step": 71420
    },
    {
      "epoch": 2.8571999999999997,
      "grad_norm": 3.012728691101074,
      "learning_rate": 2.3959731543624164e-06,
      "loss": 0.6545,
      "step": 71430
    },
    {
      "epoch": 2.8576,
      "grad_norm": 2.7365922927856445,
      "learning_rate": 2.389261744966443e-06,
      "loss": 0.4028,
      "step": 71440
    },
    {
      "epoch": 2.858,
      "grad_norm": 2.5864782333374023,
      "learning_rate": 2.38255033557047e-06,
      "loss": 0.445,
      "step": 71450
    },
    {
      "epoch": 2.8584,
      "grad_norm": 2.880563735961914,
      "learning_rate": 2.375838926174497e-06,
      "loss": 0.5782,
      "step": 71460
    },
    {
      "epoch": 2.8588,
      "grad_norm": 3.0627713203430176,
      "learning_rate": 2.3691275167785234e-06,
      "loss": 0.5583,
      "step": 71470
    },
    {
      "epoch": 2.8592,
      "grad_norm": 2.3978054523468018,
      "learning_rate": 2.3624161073825503e-06,
      "loss": 0.5741,
      "step": 71480
    },
    {
      "epoch": 2.8596,
      "grad_norm": 2.52534556388855,
      "learning_rate": 2.3557046979865775e-06,
      "loss": 0.5213,
      "step": 71490
    },
    {
      "epoch": 2.86,
      "grad_norm": 3.1096303462982178,
      "learning_rate": 2.3489932885906044e-06,
      "loss": 0.4559,
      "step": 71500
    },
    {
      "epoch": 2.8604000000000003,
      "grad_norm": 2.3737447261810303,
      "learning_rate": 2.342281879194631e-06,
      "loss": 0.4965,
      "step": 71510
    },
    {
      "epoch": 2.8608000000000002,
      "grad_norm": 2.579552173614502,
      "learning_rate": 2.3355704697986577e-06,
      "loss": 0.4845,
      "step": 71520
    },
    {
      "epoch": 2.8612,
      "grad_norm": 2.6197474002838135,
      "learning_rate": 2.328859060402685e-06,
      "loss": 0.4404,
      "step": 71530
    },
    {
      "epoch": 2.8616,
      "grad_norm": 3.046736717224121,
      "learning_rate": 2.3221476510067114e-06,
      "loss": 0.4467,
      "step": 71540
    },
    {
      "epoch": 2.862,
      "grad_norm": 2.518359422683716,
      "learning_rate": 2.3154362416107382e-06,
      "loss": 0.5055,
      "step": 71550
    },
    {
      "epoch": 2.8624,
      "grad_norm": 2.2490761280059814,
      "learning_rate": 2.308724832214765e-06,
      "loss": 0.47,
      "step": 71560
    },
    {
      "epoch": 2.8628,
      "grad_norm": 2.463238000869751,
      "learning_rate": 2.3020134228187924e-06,
      "loss": 0.5032,
      "step": 71570
    },
    {
      "epoch": 2.8632,
      "grad_norm": 2.3562235832214355,
      "learning_rate": 2.295302013422819e-06,
      "loss": 0.4955,
      "step": 71580
    },
    {
      "epoch": 2.8636,
      "grad_norm": 2.6909263134002686,
      "learning_rate": 2.2885906040268457e-06,
      "loss": 0.423,
      "step": 71590
    },
    {
      "epoch": 2.864,
      "grad_norm": 2.0521810054779053,
      "learning_rate": 2.2818791946308725e-06,
      "loss": 0.4794,
      "step": 71600
    },
    {
      "epoch": 2.8644,
      "grad_norm": 3.03942608833313,
      "learning_rate": 2.2751677852348998e-06,
      "loss": 0.4866,
      "step": 71610
    },
    {
      "epoch": 2.8648,
      "grad_norm": 3.8527109622955322,
      "learning_rate": 2.268456375838926e-06,
      "loss": 0.5337,
      "step": 71620
    },
    {
      "epoch": 2.8651999999999997,
      "grad_norm": 2.9813902378082275,
      "learning_rate": 2.261744966442953e-06,
      "loss": 0.5025,
      "step": 71630
    },
    {
      "epoch": 2.8656,
      "grad_norm": 2.1870765686035156,
      "learning_rate": 2.25503355704698e-06,
      "loss": 0.4538,
      "step": 71640
    },
    {
      "epoch": 2.866,
      "grad_norm": 2.2847788333892822,
      "learning_rate": 2.2483221476510068e-06,
      "loss": 0.4111,
      "step": 71650
    },
    {
      "epoch": 2.8664,
      "grad_norm": 2.4036169052124023,
      "learning_rate": 2.2416107382550336e-06,
      "loss": 0.5531,
      "step": 71660
    },
    {
      "epoch": 2.8668,
      "grad_norm": 3.6720187664031982,
      "learning_rate": 2.2348993288590605e-06,
      "loss": 0.5819,
      "step": 71670
    },
    {
      "epoch": 2.8672,
      "grad_norm": 2.891287088394165,
      "learning_rate": 2.2281879194630873e-06,
      "loss": 0.4767,
      "step": 71680
    },
    {
      "epoch": 2.8676,
      "grad_norm": 3.6881637573242188,
      "learning_rate": 2.221476510067114e-06,
      "loss": 0.4492,
      "step": 71690
    },
    {
      "epoch": 2.868,
      "grad_norm": 3.008488416671753,
      "learning_rate": 2.214765100671141e-06,
      "loss": 0.4425,
      "step": 71700
    },
    {
      "epoch": 2.8684,
      "grad_norm": 2.9734671115875244,
      "learning_rate": 2.208053691275168e-06,
      "loss": 0.5101,
      "step": 71710
    },
    {
      "epoch": 2.8688000000000002,
      "grad_norm": 2.7612643241882324,
      "learning_rate": 2.2013422818791947e-06,
      "loss": 0.5796,
      "step": 71720
    },
    {
      "epoch": 2.8692,
      "grad_norm": 2.840956687927246,
      "learning_rate": 2.1946308724832216e-06,
      "loss": 0.4743,
      "step": 71730
    },
    {
      "epoch": 2.8696,
      "grad_norm": 2.6667232513427734,
      "learning_rate": 2.1879194630872484e-06,
      "loss": 0.5014,
      "step": 71740
    },
    {
      "epoch": 2.87,
      "grad_norm": 2.805069923400879,
      "learning_rate": 2.1812080536912753e-06,
      "loss": 0.4968,
      "step": 71750
    },
    {
      "epoch": 2.8704,
      "grad_norm": 2.4146218299865723,
      "learning_rate": 2.174496644295302e-06,
      "loss": 0.5126,
      "step": 71760
    },
    {
      "epoch": 2.8708,
      "grad_norm": 2.9403529167175293,
      "learning_rate": 2.167785234899329e-06,
      "loss": 0.5546,
      "step": 71770
    },
    {
      "epoch": 2.8712,
      "grad_norm": 3.3494322299957275,
      "learning_rate": 2.161073825503356e-06,
      "loss": 0.6066,
      "step": 71780
    },
    {
      "epoch": 2.8716,
      "grad_norm": 8.511622428894043,
      "learning_rate": 2.1543624161073827e-06,
      "loss": 0.5037,
      "step": 71790
    },
    {
      "epoch": 2.872,
      "grad_norm": 2.7572312355041504,
      "learning_rate": 2.1476510067114096e-06,
      "loss": 0.5396,
      "step": 71800
    },
    {
      "epoch": 2.8724,
      "grad_norm": 2.447598457336426,
      "learning_rate": 2.1409395973154364e-06,
      "loss": 0.5034,
      "step": 71810
    },
    {
      "epoch": 2.8728,
      "grad_norm": 2.290673017501831,
      "learning_rate": 2.1342281879194633e-06,
      "loss": 0.4299,
      "step": 71820
    },
    {
      "epoch": 2.8731999999999998,
      "grad_norm": 2.4802985191345215,
      "learning_rate": 2.12751677852349e-06,
      "loss": 0.4521,
      "step": 71830
    },
    {
      "epoch": 2.8736,
      "grad_norm": 2.319272994995117,
      "learning_rate": 2.1208053691275166e-06,
      "loss": 0.4967,
      "step": 71840
    },
    {
      "epoch": 2.874,
      "grad_norm": 2.2478902339935303,
      "learning_rate": 2.114093959731544e-06,
      "loss": 0.549,
      "step": 71850
    },
    {
      "epoch": 2.8744,
      "grad_norm": 2.820835590362549,
      "learning_rate": 2.1073825503355707e-06,
      "loss": 0.5614,
      "step": 71860
    },
    {
      "epoch": 2.8748,
      "grad_norm": 3.1915132999420166,
      "learning_rate": 2.1006711409395975e-06,
      "loss": 0.5259,
      "step": 71870
    },
    {
      "epoch": 2.8752,
      "grad_norm": 2.973102569580078,
      "learning_rate": 2.093959731543624e-06,
      "loss": 0.4608,
      "step": 71880
    },
    {
      "epoch": 2.8756,
      "grad_norm": 3.2306745052337646,
      "learning_rate": 2.0872483221476512e-06,
      "loss": 0.5602,
      "step": 71890
    },
    {
      "epoch": 2.876,
      "grad_norm": 2.891463279724121,
      "learning_rate": 2.080536912751678e-06,
      "loss": 0.4949,
      "step": 71900
    },
    {
      "epoch": 2.8764,
      "grad_norm": 2.420633554458618,
      "learning_rate": 2.073825503355705e-06,
      "loss": 0.5111,
      "step": 71910
    },
    {
      "epoch": 2.8768000000000002,
      "grad_norm": 2.7574241161346436,
      "learning_rate": 2.0671140939597314e-06,
      "loss": 0.4706,
      "step": 71920
    },
    {
      "epoch": 2.8772,
      "grad_norm": 1.9945753812789917,
      "learning_rate": 2.0604026845637587e-06,
      "loss": 0.4732,
      "step": 71930
    },
    {
      "epoch": 2.8776,
      "grad_norm": 2.1609318256378174,
      "learning_rate": 2.0536912751677855e-06,
      "loss": 0.4377,
      "step": 71940
    },
    {
      "epoch": 2.878,
      "grad_norm": 2.575505495071411,
      "learning_rate": 2.046979865771812e-06,
      "loss": 0.5594,
      "step": 71950
    },
    {
      "epoch": 2.8784,
      "grad_norm": 2.6241796016693115,
      "learning_rate": 2.040268456375839e-06,
      "loss": 0.4402,
      "step": 71960
    },
    {
      "epoch": 2.8788,
      "grad_norm": 2.6265299320220947,
      "learning_rate": 2.033557046979866e-06,
      "loss": 0.4902,
      "step": 71970
    },
    {
      "epoch": 2.8792,
      "grad_norm": 2.721834182739258,
      "learning_rate": 2.026845637583893e-06,
      "loss": 0.5287,
      "step": 71980
    },
    {
      "epoch": 2.8796,
      "grad_norm": 2.119084358215332,
      "learning_rate": 2.0201342281879194e-06,
      "loss": 0.5379,
      "step": 71990
    },
    {
      "epoch": 2.88,
      "grad_norm": 2.4300904273986816,
      "learning_rate": 2.013422818791946e-06,
      "loss": 0.5264,
      "step": 72000
    },
    {
      "epoch": 2.8804,
      "grad_norm": 2.327895402908325,
      "learning_rate": 2.0067114093959735e-06,
      "loss": 0.5005,
      "step": 72010
    },
    {
      "epoch": 2.8808,
      "grad_norm": 2.3236536979675293,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.4382,
      "step": 72020
    },
    {
      "epoch": 2.8811999999999998,
      "grad_norm": 3.542130947113037,
      "learning_rate": 1.9932885906040268e-06,
      "loss": 0.5327,
      "step": 72030
    },
    {
      "epoch": 2.8816,
      "grad_norm": 2.9882848262786865,
      "learning_rate": 1.9865771812080536e-06,
      "loss": 0.5124,
      "step": 72040
    },
    {
      "epoch": 2.882,
      "grad_norm": 2.760099172592163,
      "learning_rate": 1.979865771812081e-06,
      "loss": 0.5364,
      "step": 72050
    },
    {
      "epoch": 2.8824,
      "grad_norm": 2.398097276687622,
      "learning_rate": 1.9731543624161073e-06,
      "loss": 0.4937,
      "step": 72060
    },
    {
      "epoch": 2.8828,
      "grad_norm": 2.990408182144165,
      "learning_rate": 1.966442953020134e-06,
      "loss": 0.5126,
      "step": 72070
    },
    {
      "epoch": 2.8832,
      "grad_norm": 2.9054059982299805,
      "learning_rate": 1.959731543624161e-06,
      "loss": 0.4802,
      "step": 72080
    },
    {
      "epoch": 2.8836,
      "grad_norm": 1.9469681978225708,
      "learning_rate": 1.9530201342281883e-06,
      "loss": 0.4881,
      "step": 72090
    },
    {
      "epoch": 2.884,
      "grad_norm": 2.6028850078582764,
      "learning_rate": 1.9463087248322147e-06,
      "loss": 0.5339,
      "step": 72100
    },
    {
      "epoch": 2.8844,
      "grad_norm": 3.3814988136291504,
      "learning_rate": 1.9395973154362416e-06,
      "loss": 0.4591,
      "step": 72110
    },
    {
      "epoch": 2.8848000000000003,
      "grad_norm": 3.053799867630005,
      "learning_rate": 1.9328859060402684e-06,
      "loss": 0.5093,
      "step": 72120
    },
    {
      "epoch": 2.8852,
      "grad_norm": 2.599276542663574,
      "learning_rate": 1.9261744966442957e-06,
      "loss": 0.5336,
      "step": 72130
    },
    {
      "epoch": 2.8856,
      "grad_norm": 2.389866590499878,
      "learning_rate": 1.919463087248322e-06,
      "loss": 0.451,
      "step": 72140
    },
    {
      "epoch": 2.886,
      "grad_norm": 2.4293086528778076,
      "learning_rate": 1.912751677852349e-06,
      "loss": 0.4698,
      "step": 72150
    },
    {
      "epoch": 2.8864,
      "grad_norm": 2.618849515914917,
      "learning_rate": 1.9060402684563759e-06,
      "loss": 0.6111,
      "step": 72160
    },
    {
      "epoch": 2.8868,
      "grad_norm": 2.2078397274017334,
      "learning_rate": 1.899328859060403e-06,
      "loss": 0.5172,
      "step": 72170
    },
    {
      "epoch": 2.8872,
      "grad_norm": 2.399195909500122,
      "learning_rate": 1.8926174496644296e-06,
      "loss": 0.4471,
      "step": 72180
    },
    {
      "epoch": 2.8876,
      "grad_norm": 2.5749118328094482,
      "learning_rate": 1.8859060402684564e-06,
      "loss": 0.5071,
      "step": 72190
    },
    {
      "epoch": 2.888,
      "grad_norm": 2.4529290199279785,
      "learning_rate": 1.8791946308724833e-06,
      "loss": 0.5037,
      "step": 72200
    },
    {
      "epoch": 2.8884,
      "grad_norm": 2.9749934673309326,
      "learning_rate": 1.8724832214765103e-06,
      "loss": 0.4822,
      "step": 72210
    },
    {
      "epoch": 2.8888,
      "grad_norm": 2.3537240028381348,
      "learning_rate": 1.865771812080537e-06,
      "loss": 0.4668,
      "step": 72220
    },
    {
      "epoch": 2.8891999999999998,
      "grad_norm": 3.2769558429718018,
      "learning_rate": 1.8590604026845638e-06,
      "loss": 0.5559,
      "step": 72230
    },
    {
      "epoch": 2.8895999999999997,
      "grad_norm": 3.3878695964813232,
      "learning_rate": 1.8523489932885905e-06,
      "loss": 0.4888,
      "step": 72240
    },
    {
      "epoch": 2.89,
      "grad_norm": 2.395240068435669,
      "learning_rate": 1.8456375838926177e-06,
      "loss": 0.4589,
      "step": 72250
    },
    {
      "epoch": 2.8904,
      "grad_norm": 2.0810487270355225,
      "learning_rate": 1.8389261744966444e-06,
      "loss": 0.4271,
      "step": 72260
    },
    {
      "epoch": 2.8908,
      "grad_norm": 2.57814359664917,
      "learning_rate": 1.8322147651006712e-06,
      "loss": 0.5064,
      "step": 72270
    },
    {
      "epoch": 2.8912,
      "grad_norm": 2.7974324226379395,
      "learning_rate": 1.8255033557046979e-06,
      "loss": 0.5096,
      "step": 72280
    },
    {
      "epoch": 2.8916,
      "grad_norm": 2.604987382888794,
      "learning_rate": 1.818791946308725e-06,
      "loss": 0.5211,
      "step": 72290
    },
    {
      "epoch": 2.892,
      "grad_norm": 1.9921848773956299,
      "learning_rate": 1.8120805369127518e-06,
      "loss": 0.4388,
      "step": 72300
    },
    {
      "epoch": 2.8924,
      "grad_norm": 2.5689094066619873,
      "learning_rate": 1.8053691275167786e-06,
      "loss": 0.5136,
      "step": 72310
    },
    {
      "epoch": 2.8928000000000003,
      "grad_norm": 2.2372066974639893,
      "learning_rate": 1.7986577181208053e-06,
      "loss": 0.4395,
      "step": 72320
    },
    {
      "epoch": 2.8932,
      "grad_norm": 2.9389333724975586,
      "learning_rate": 1.7919463087248324e-06,
      "loss": 0.5149,
      "step": 72330
    },
    {
      "epoch": 2.8936,
      "grad_norm": 2.527104139328003,
      "learning_rate": 1.7852348993288592e-06,
      "loss": 0.4675,
      "step": 72340
    },
    {
      "epoch": 2.894,
      "grad_norm": 2.6450164318084717,
      "learning_rate": 1.7785234899328858e-06,
      "loss": 0.5016,
      "step": 72350
    },
    {
      "epoch": 2.8944,
      "grad_norm": 2.6002395153045654,
      "learning_rate": 1.7718120805369127e-06,
      "loss": 0.4635,
      "step": 72360
    },
    {
      "epoch": 2.8948,
      "grad_norm": 2.787790536880493,
      "learning_rate": 1.7651006711409398e-06,
      "loss": 0.5069,
      "step": 72370
    },
    {
      "epoch": 2.8952,
      "grad_norm": 2.3423588275909424,
      "learning_rate": 1.7583892617449666e-06,
      "loss": 0.4929,
      "step": 72380
    },
    {
      "epoch": 2.8956,
      "grad_norm": 2.301055431365967,
      "learning_rate": 1.7516778523489933e-06,
      "loss": 0.4623,
      "step": 72390
    },
    {
      "epoch": 2.896,
      "grad_norm": 2.9606850147247314,
      "learning_rate": 1.7449664429530201e-06,
      "loss": 0.4844,
      "step": 72400
    },
    {
      "epoch": 2.8964,
      "grad_norm": 2.4992644786834717,
      "learning_rate": 1.7382550335570472e-06,
      "loss": 0.4681,
      "step": 72410
    },
    {
      "epoch": 2.8968,
      "grad_norm": 2.284573554992676,
      "learning_rate": 1.7315436241610738e-06,
      "loss": 0.4776,
      "step": 72420
    },
    {
      "epoch": 2.8971999999999998,
      "grad_norm": 3.261007308959961,
      "learning_rate": 1.7248322147651007e-06,
      "loss": 0.5409,
      "step": 72430
    },
    {
      "epoch": 2.8975999999999997,
      "grad_norm": 2.1093263626098633,
      "learning_rate": 1.7181208053691275e-06,
      "loss": 0.5803,
      "step": 72440
    },
    {
      "epoch": 2.898,
      "grad_norm": 2.7673168182373047,
      "learning_rate": 1.7114093959731546e-06,
      "loss": 0.5494,
      "step": 72450
    },
    {
      "epoch": 2.8984,
      "grad_norm": 2.214141607284546,
      "learning_rate": 1.7046979865771812e-06,
      "loss": 0.4898,
      "step": 72460
    },
    {
      "epoch": 2.8988,
      "grad_norm": 3.9692962169647217,
      "learning_rate": 1.697986577181208e-06,
      "loss": 0.4763,
      "step": 72470
    },
    {
      "epoch": 2.8992,
      "grad_norm": 3.3493831157684326,
      "learning_rate": 1.6912751677852347e-06,
      "loss": 0.5739,
      "step": 72480
    },
    {
      "epoch": 2.8996,
      "grad_norm": 2.741551160812378,
      "learning_rate": 1.684563758389262e-06,
      "loss": 0.5382,
      "step": 72490
    },
    {
      "epoch": 2.9,
      "grad_norm": 1.9903512001037598,
      "learning_rate": 1.6778523489932886e-06,
      "loss": 0.4969,
      "step": 72500
    },
    {
      "epoch": 2.9004,
      "grad_norm": 2.5587780475616455,
      "learning_rate": 1.6711409395973155e-06,
      "loss": 0.4778,
      "step": 72510
    },
    {
      "epoch": 2.9008000000000003,
      "grad_norm": 3.0088067054748535,
      "learning_rate": 1.6644295302013421e-06,
      "loss": 0.5178,
      "step": 72520
    },
    {
      "epoch": 2.9012000000000002,
      "grad_norm": 2.9214038848876953,
      "learning_rate": 1.6577181208053692e-06,
      "loss": 0.4721,
      "step": 72530
    },
    {
      "epoch": 2.9016,
      "grad_norm": 3.696575403213501,
      "learning_rate": 1.651006711409396e-06,
      "loss": 0.5398,
      "step": 72540
    },
    {
      "epoch": 2.902,
      "grad_norm": 2.312588691711426,
      "learning_rate": 1.644295302013423e-06,
      "loss": 0.562,
      "step": 72550
    },
    {
      "epoch": 2.9024,
      "grad_norm": 2.7156448364257812,
      "learning_rate": 1.6375838926174496e-06,
      "loss": 0.4272,
      "step": 72560
    },
    {
      "epoch": 2.9028,
      "grad_norm": 2.6731855869293213,
      "learning_rate": 1.6308724832214766e-06,
      "loss": 0.5017,
      "step": 72570
    },
    {
      "epoch": 2.9032,
      "grad_norm": 2.10598087310791,
      "learning_rate": 1.6241610738255035e-06,
      "loss": 0.4787,
      "step": 72580
    },
    {
      "epoch": 2.9036,
      "grad_norm": 1.9016437530517578,
      "learning_rate": 1.6174496644295301e-06,
      "loss": 0.5218,
      "step": 72590
    },
    {
      "epoch": 2.904,
      "grad_norm": 2.0643951892852783,
      "learning_rate": 1.6107382550335574e-06,
      "loss": 0.3845,
      "step": 72600
    },
    {
      "epoch": 2.9044,
      "grad_norm": 2.616177797317505,
      "learning_rate": 1.604026845637584e-06,
      "loss": 0.5373,
      "step": 72610
    },
    {
      "epoch": 2.9048,
      "grad_norm": 2.1020092964172363,
      "learning_rate": 1.5973154362416109e-06,
      "loss": 0.4633,
      "step": 72620
    },
    {
      "epoch": 2.9052,
      "grad_norm": 2.5002691745758057,
      "learning_rate": 1.5906040268456375e-06,
      "loss": 0.5419,
      "step": 72630
    },
    {
      "epoch": 2.9055999999999997,
      "grad_norm": 2.9478743076324463,
      "learning_rate": 1.5838926174496646e-06,
      "loss": 0.4486,
      "step": 72640
    },
    {
      "epoch": 2.906,
      "grad_norm": 2.0324156284332275,
      "learning_rate": 1.5771812080536914e-06,
      "loss": 0.4569,
      "step": 72650
    },
    {
      "epoch": 2.9064,
      "grad_norm": 2.178589105606079,
      "learning_rate": 1.5704697986577183e-06,
      "loss": 0.4656,
      "step": 72660
    },
    {
      "epoch": 2.9068,
      "grad_norm": 2.6332004070281982,
      "learning_rate": 1.563758389261745e-06,
      "loss": 0.4951,
      "step": 72670
    },
    {
      "epoch": 2.9072,
      "grad_norm": 2.8954591751098633,
      "learning_rate": 1.5570469798657718e-06,
      "loss": 0.5239,
      "step": 72680
    },
    {
      "epoch": 2.9076,
      "grad_norm": 2.6525487899780273,
      "learning_rate": 1.5503355704697989e-06,
      "loss": 0.5731,
      "step": 72690
    },
    {
      "epoch": 2.908,
      "grad_norm": 2.3777220249176025,
      "learning_rate": 1.5436241610738255e-06,
      "loss": 0.4518,
      "step": 72700
    },
    {
      "epoch": 2.9084,
      "grad_norm": 3.0797319412231445,
      "learning_rate": 1.5369127516778526e-06,
      "loss": 0.4499,
      "step": 72710
    },
    {
      "epoch": 2.9088000000000003,
      "grad_norm": 1.716763973236084,
      "learning_rate": 1.5302013422818792e-06,
      "loss": 0.5025,
      "step": 72720
    },
    {
      "epoch": 2.9092000000000002,
      "grad_norm": 2.0853164196014404,
      "learning_rate": 1.5234899328859063e-06,
      "loss": 0.4488,
      "step": 72730
    },
    {
      "epoch": 2.9096,
      "grad_norm": 2.5821995735168457,
      "learning_rate": 1.516778523489933e-06,
      "loss": 0.4424,
      "step": 72740
    },
    {
      "epoch": 2.91,
      "grad_norm": 2.8320674896240234,
      "learning_rate": 1.51006711409396e-06,
      "loss": 0.6156,
      "step": 72750
    },
    {
      "epoch": 2.9104,
      "grad_norm": 2.3964128494262695,
      "learning_rate": 1.5033557046979866e-06,
      "loss": 0.4979,
      "step": 72760
    },
    {
      "epoch": 2.9108,
      "grad_norm": 2.941608190536499,
      "learning_rate": 1.4966442953020135e-06,
      "loss": 0.5313,
      "step": 72770
    },
    {
      "epoch": 2.9112,
      "grad_norm": 2.4698004722595215,
      "learning_rate": 1.4899328859060403e-06,
      "loss": 0.4971,
      "step": 72780
    },
    {
      "epoch": 2.9116,
      "grad_norm": 3.375272035598755,
      "learning_rate": 1.4832214765100672e-06,
      "loss": 0.5102,
      "step": 72790
    },
    {
      "epoch": 2.912,
      "grad_norm": 3.245575428009033,
      "learning_rate": 1.476510067114094e-06,
      "loss": 0.48,
      "step": 72800
    },
    {
      "epoch": 2.9124,
      "grad_norm": 2.252811908721924,
      "learning_rate": 1.4697986577181209e-06,
      "loss": 0.4456,
      "step": 72810
    },
    {
      "epoch": 2.9128,
      "grad_norm": 2.738804817199707,
      "learning_rate": 1.4630872483221477e-06,
      "loss": 0.4845,
      "step": 72820
    },
    {
      "epoch": 2.9132,
      "grad_norm": 2.6464321613311768,
      "learning_rate": 1.4563758389261746e-06,
      "loss": 0.441,
      "step": 72830
    },
    {
      "epoch": 2.9135999999999997,
      "grad_norm": 1.9499776363372803,
      "learning_rate": 1.4496644295302014e-06,
      "loss": 0.5163,
      "step": 72840
    },
    {
      "epoch": 2.914,
      "grad_norm": 2.6742939949035645,
      "learning_rate": 1.4429530201342283e-06,
      "loss": 0.5753,
      "step": 72850
    },
    {
      "epoch": 2.9144,
      "grad_norm": 2.5914065837860107,
      "learning_rate": 1.4362416107382551e-06,
      "loss": 0.5036,
      "step": 72860
    },
    {
      "epoch": 2.9148,
      "grad_norm": 4.411151885986328,
      "learning_rate": 1.429530201342282e-06,
      "loss": 0.4649,
      "step": 72870
    },
    {
      "epoch": 2.9152,
      "grad_norm": 2.376354217529297,
      "learning_rate": 1.4228187919463088e-06,
      "loss": 0.4788,
      "step": 72880
    },
    {
      "epoch": 2.9156,
      "grad_norm": 2.405669689178467,
      "learning_rate": 1.4161073825503357e-06,
      "loss": 0.4526,
      "step": 72890
    },
    {
      "epoch": 2.916,
      "grad_norm": 2.4433116912841797,
      "learning_rate": 1.4093959731543626e-06,
      "loss": 0.4716,
      "step": 72900
    },
    {
      "epoch": 2.9164,
      "grad_norm": 2.5094621181488037,
      "learning_rate": 1.4026845637583894e-06,
      "loss": 0.5332,
      "step": 72910
    },
    {
      "epoch": 2.9168,
      "grad_norm": 2.252518892288208,
      "learning_rate": 1.395973154362416e-06,
      "loss": 0.5299,
      "step": 72920
    },
    {
      "epoch": 2.9172000000000002,
      "grad_norm": 2.619145631790161,
      "learning_rate": 1.3892617449664431e-06,
      "loss": 0.4959,
      "step": 72930
    },
    {
      "epoch": 2.9176,
      "grad_norm": 2.7329273223876953,
      "learning_rate": 1.3825503355704698e-06,
      "loss": 0.4258,
      "step": 72940
    },
    {
      "epoch": 2.918,
      "grad_norm": 2.1915488243103027,
      "learning_rate": 1.3758389261744968e-06,
      "loss": 0.4952,
      "step": 72950
    },
    {
      "epoch": 2.9184,
      "grad_norm": 1.6699343919754028,
      "learning_rate": 1.3691275167785235e-06,
      "loss": 0.4099,
      "step": 72960
    },
    {
      "epoch": 2.9188,
      "grad_norm": 3.292832136154175,
      "learning_rate": 1.3624161073825505e-06,
      "loss": 0.4802,
      "step": 72970
    },
    {
      "epoch": 2.9192,
      "grad_norm": 3.0647284984588623,
      "learning_rate": 1.3557046979865772e-06,
      "loss": 0.5462,
      "step": 72980
    },
    {
      "epoch": 2.9196,
      "grad_norm": 3.0675389766693115,
      "learning_rate": 1.3489932885906042e-06,
      "loss": 0.5557,
      "step": 72990
    },
    {
      "epoch": 2.92,
      "grad_norm": 2.524553060531616,
      "learning_rate": 1.3422818791946309e-06,
      "loss": 0.5772,
      "step": 73000
    },
    {
      "epoch": 2.9204,
      "grad_norm": 2.731908082962036,
      "learning_rate": 1.335570469798658e-06,
      "loss": 0.4722,
      "step": 73010
    },
    {
      "epoch": 2.9208,
      "grad_norm": 2.160771131515503,
      "learning_rate": 1.3288590604026846e-06,
      "loss": 0.4428,
      "step": 73020
    },
    {
      "epoch": 2.9212,
      "grad_norm": 2.1892852783203125,
      "learning_rate": 1.3221476510067114e-06,
      "loss": 0.4884,
      "step": 73030
    },
    {
      "epoch": 2.9215999999999998,
      "grad_norm": 2.99355149269104,
      "learning_rate": 1.3154362416107383e-06,
      "loss": 0.4609,
      "step": 73040
    },
    {
      "epoch": 2.922,
      "grad_norm": 2.0983457565307617,
      "learning_rate": 1.3087248322147651e-06,
      "loss": 0.5279,
      "step": 73050
    },
    {
      "epoch": 2.9224,
      "grad_norm": 2.955247640609741,
      "learning_rate": 1.302013422818792e-06,
      "loss": 0.5106,
      "step": 73060
    },
    {
      "epoch": 2.9228,
      "grad_norm": 2.3171558380126953,
      "learning_rate": 1.2953020134228188e-06,
      "loss": 0.5607,
      "step": 73070
    },
    {
      "epoch": 2.9232,
      "grad_norm": 2.406982660293579,
      "learning_rate": 1.2885906040268457e-06,
      "loss": 0.4967,
      "step": 73080
    },
    {
      "epoch": 2.9236,
      "grad_norm": 2.5989739894866943,
      "learning_rate": 1.2818791946308726e-06,
      "loss": 0.4475,
      "step": 73090
    },
    {
      "epoch": 2.924,
      "grad_norm": 2.5127317905426025,
      "learning_rate": 1.2751677852348994e-06,
      "loss": 0.4828,
      "step": 73100
    },
    {
      "epoch": 2.9244,
      "grad_norm": 2.7769837379455566,
      "learning_rate": 1.2684563758389263e-06,
      "loss": 0.5065,
      "step": 73110
    },
    {
      "epoch": 2.9248,
      "grad_norm": 2.4398622512817383,
      "learning_rate": 1.2617449664429531e-06,
      "loss": 0.525,
      "step": 73120
    },
    {
      "epoch": 2.9252000000000002,
      "grad_norm": 2.097940683364868,
      "learning_rate": 1.25503355704698e-06,
      "loss": 0.5759,
      "step": 73130
    },
    {
      "epoch": 2.9256,
      "grad_norm": 2.0461161136627197,
      "learning_rate": 1.2483221476510068e-06,
      "loss": 0.4742,
      "step": 73140
    },
    {
      "epoch": 2.926,
      "grad_norm": 2.6040444374084473,
      "learning_rate": 1.2416107382550337e-06,
      "loss": 0.5266,
      "step": 73150
    },
    {
      "epoch": 2.9264,
      "grad_norm": 2.7885491847991943,
      "learning_rate": 1.2348993288590605e-06,
      "loss": 0.4883,
      "step": 73160
    },
    {
      "epoch": 2.9268,
      "grad_norm": 3.195591688156128,
      "learning_rate": 1.2281879194630874e-06,
      "loss": 0.5801,
      "step": 73170
    },
    {
      "epoch": 2.9272,
      "grad_norm": 2.6763408184051514,
      "learning_rate": 1.221476510067114e-06,
      "loss": 0.4814,
      "step": 73180
    },
    {
      "epoch": 2.9276,
      "grad_norm": 2.4720489978790283,
      "learning_rate": 1.214765100671141e-06,
      "loss": 0.5248,
      "step": 73190
    },
    {
      "epoch": 2.928,
      "grad_norm": 2.6698076725006104,
      "learning_rate": 1.2080536912751677e-06,
      "loss": 0.5597,
      "step": 73200
    },
    {
      "epoch": 2.9284,
      "grad_norm": 2.8992245197296143,
      "learning_rate": 1.2013422818791948e-06,
      "loss": 0.5243,
      "step": 73210
    },
    {
      "epoch": 2.9288,
      "grad_norm": 2.4549057483673096,
      "learning_rate": 1.1946308724832214e-06,
      "loss": 0.4567,
      "step": 73220
    },
    {
      "epoch": 2.9292,
      "grad_norm": 3.093764066696167,
      "learning_rate": 1.1879194630872485e-06,
      "loss": 0.4628,
      "step": 73230
    },
    {
      "epoch": 2.9295999999999998,
      "grad_norm": 2.3094847202301025,
      "learning_rate": 1.1812080536912751e-06,
      "loss": 0.4691,
      "step": 73240
    },
    {
      "epoch": 2.93,
      "grad_norm": 2.892076015472412,
      "learning_rate": 1.1744966442953022e-06,
      "loss": 0.497,
      "step": 73250
    },
    {
      "epoch": 2.9304,
      "grad_norm": 3.684598207473755,
      "learning_rate": 1.1677852348993288e-06,
      "loss": 0.5378,
      "step": 73260
    },
    {
      "epoch": 2.9308,
      "grad_norm": 2.3396928310394287,
      "learning_rate": 1.1610738255033557e-06,
      "loss": 0.4807,
      "step": 73270
    },
    {
      "epoch": 2.9312,
      "grad_norm": 2.6200637817382812,
      "learning_rate": 1.1543624161073825e-06,
      "loss": 0.5559,
      "step": 73280
    },
    {
      "epoch": 2.9316,
      "grad_norm": 2.708888530731201,
      "learning_rate": 1.1476510067114094e-06,
      "loss": 0.4785,
      "step": 73290
    },
    {
      "epoch": 2.932,
      "grad_norm": 2.6569623947143555,
      "learning_rate": 1.1409395973154363e-06,
      "loss": 0.492,
      "step": 73300
    },
    {
      "epoch": 2.9324,
      "grad_norm": 2.100990056991577,
      "learning_rate": 1.134228187919463e-06,
      "loss": 0.475,
      "step": 73310
    },
    {
      "epoch": 2.9328,
      "grad_norm": 3.2504937648773193,
      "learning_rate": 1.12751677852349e-06,
      "loss": 0.5272,
      "step": 73320
    },
    {
      "epoch": 2.9332000000000003,
      "grad_norm": 2.6127328872680664,
      "learning_rate": 1.1208053691275168e-06,
      "loss": 0.4929,
      "step": 73330
    },
    {
      "epoch": 2.9336,
      "grad_norm": 2.78562068939209,
      "learning_rate": 1.1140939597315437e-06,
      "loss": 0.504,
      "step": 73340
    },
    {
      "epoch": 2.934,
      "grad_norm": 2.384489059448242,
      "learning_rate": 1.1073825503355705e-06,
      "loss": 0.5208,
      "step": 73350
    },
    {
      "epoch": 2.9344,
      "grad_norm": 2.771713972091675,
      "learning_rate": 1.1006711409395974e-06,
      "loss": 0.4757,
      "step": 73360
    },
    {
      "epoch": 2.9348,
      "grad_norm": 2.620727777481079,
      "learning_rate": 1.0939597315436242e-06,
      "loss": 0.5086,
      "step": 73370
    },
    {
      "epoch": 2.9352,
      "grad_norm": 2.637232542037964,
      "learning_rate": 1.087248322147651e-06,
      "loss": 0.4876,
      "step": 73380
    },
    {
      "epoch": 2.9356,
      "grad_norm": 2.692762613296509,
      "learning_rate": 1.080536912751678e-06,
      "loss": 0.4662,
      "step": 73390
    },
    {
      "epoch": 2.936,
      "grad_norm": 2.4858779907226562,
      "learning_rate": 1.0738255033557048e-06,
      "loss": 0.4626,
      "step": 73400
    },
    {
      "epoch": 2.9364,
      "grad_norm": 3.168041706085205,
      "learning_rate": 1.0671140939597316e-06,
      "loss": 0.5027,
      "step": 73410
    },
    {
      "epoch": 2.9368,
      "grad_norm": 2.7598214149475098,
      "learning_rate": 1.0604026845637583e-06,
      "loss": 0.4905,
      "step": 73420
    },
    {
      "epoch": 2.9372,
      "grad_norm": 2.55765438079834,
      "learning_rate": 1.0536912751677853e-06,
      "loss": 0.5078,
      "step": 73430
    },
    {
      "epoch": 2.9375999999999998,
      "grad_norm": 2.2400314807891846,
      "learning_rate": 1.046979865771812e-06,
      "loss": 0.4893,
      "step": 73440
    },
    {
      "epoch": 2.9379999999999997,
      "grad_norm": 1.8048055171966553,
      "learning_rate": 1.040268456375839e-06,
      "loss": 0.5046,
      "step": 73450
    },
    {
      "epoch": 2.9384,
      "grad_norm": 2.762462854385376,
      "learning_rate": 1.0335570469798657e-06,
      "loss": 0.4947,
      "step": 73460
    },
    {
      "epoch": 2.9388,
      "grad_norm": 3.638429641723633,
      "learning_rate": 1.0268456375838928e-06,
      "loss": 0.5726,
      "step": 73470
    },
    {
      "epoch": 2.9392,
      "grad_norm": 2.0170862674713135,
      "learning_rate": 1.0201342281879194e-06,
      "loss": 0.4895,
      "step": 73480
    },
    {
      "epoch": 2.9396,
      "grad_norm": 2.6473426818847656,
      "learning_rate": 1.0134228187919465e-06,
      "loss": 0.4365,
      "step": 73490
    },
    {
      "epoch": 2.94,
      "grad_norm": 2.972752809524536,
      "learning_rate": 1.006711409395973e-06,
      "loss": 0.6211,
      "step": 73500
    },
    {
      "epoch": 2.9404,
      "grad_norm": 2.2007853984832764,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 0.4372,
      "step": 73510
    },
    {
      "epoch": 2.9408,
      "grad_norm": 2.8684241771698,
      "learning_rate": 9.932885906040268e-07,
      "loss": 0.4803,
      "step": 73520
    },
    {
      "epoch": 2.9412000000000003,
      "grad_norm": 2.5271198749542236,
      "learning_rate": 9.865771812080537e-07,
      "loss": 0.4795,
      "step": 73530
    },
    {
      "epoch": 2.9416,
      "grad_norm": 2.682004928588867,
      "learning_rate": 9.798657718120805e-07,
      "loss": 0.4433,
      "step": 73540
    },
    {
      "epoch": 2.942,
      "grad_norm": 2.196249485015869,
      "learning_rate": 9.731543624161074e-07,
      "loss": 0.3914,
      "step": 73550
    },
    {
      "epoch": 2.9424,
      "grad_norm": 2.1819069385528564,
      "learning_rate": 9.664429530201342e-07,
      "loss": 0.472,
      "step": 73560
    },
    {
      "epoch": 2.9428,
      "grad_norm": 2.2681944370269775,
      "learning_rate": 9.59731543624161e-07,
      "loss": 0.5211,
      "step": 73570
    },
    {
      "epoch": 2.9432,
      "grad_norm": 2.548685073852539,
      "learning_rate": 9.530201342281879e-07,
      "loss": 0.5334,
      "step": 73580
    },
    {
      "epoch": 2.9436,
      "grad_norm": 2.325418472290039,
      "learning_rate": 9.463087248322148e-07,
      "loss": 0.3929,
      "step": 73590
    },
    {
      "epoch": 2.944,
      "grad_norm": 2.7352352142333984,
      "learning_rate": 9.395973154362416e-07,
      "loss": 0.4872,
      "step": 73600
    },
    {
      "epoch": 2.9444,
      "grad_norm": 2.526885509490967,
      "learning_rate": 9.328859060402685e-07,
      "loss": 0.5336,
      "step": 73610
    },
    {
      "epoch": 2.9448,
      "grad_norm": 2.9832711219787598,
      "learning_rate": 9.261744966442952e-07,
      "loss": 0.4915,
      "step": 73620
    },
    {
      "epoch": 2.9452,
      "grad_norm": 3.3462069034576416,
      "learning_rate": 9.194630872483222e-07,
      "loss": 0.5468,
      "step": 73630
    },
    {
      "epoch": 2.9455999999999998,
      "grad_norm": 3.0689539909362793,
      "learning_rate": 9.127516778523489e-07,
      "loss": 0.6335,
      "step": 73640
    },
    {
      "epoch": 2.9459999999999997,
      "grad_norm": 3.5191805362701416,
      "learning_rate": 9.060402684563759e-07,
      "loss": 0.5227,
      "step": 73650
    },
    {
      "epoch": 2.9464,
      "grad_norm": 2.4247829914093018,
      "learning_rate": 8.993288590604026e-07,
      "loss": 0.4968,
      "step": 73660
    },
    {
      "epoch": 2.9468,
      "grad_norm": 2.306260585784912,
      "learning_rate": 8.926174496644296e-07,
      "loss": 0.4992,
      "step": 73670
    },
    {
      "epoch": 2.9472,
      "grad_norm": 2.8234403133392334,
      "learning_rate": 8.859060402684564e-07,
      "loss": 0.5454,
      "step": 73680
    },
    {
      "epoch": 2.9476,
      "grad_norm": 3.0080161094665527,
      "learning_rate": 8.791946308724833e-07,
      "loss": 0.5362,
      "step": 73690
    },
    {
      "epoch": 2.948,
      "grad_norm": 2.51227068901062,
      "learning_rate": 8.724832214765101e-07,
      "loss": 0.5276,
      "step": 73700
    },
    {
      "epoch": 2.9484,
      "grad_norm": 3.0356554985046387,
      "learning_rate": 8.657718120805369e-07,
      "loss": 0.4631,
      "step": 73710
    },
    {
      "epoch": 2.9488,
      "grad_norm": 2.3851118087768555,
      "learning_rate": 8.590604026845638e-07,
      "loss": 0.5366,
      "step": 73720
    },
    {
      "epoch": 2.9492000000000003,
      "grad_norm": 2.3111329078674316,
      "learning_rate": 8.523489932885906e-07,
      "loss": 0.4864,
      "step": 73730
    },
    {
      "epoch": 2.9496,
      "grad_norm": 2.830923080444336,
      "learning_rate": 8.456375838926174e-07,
      "loss": 0.5005,
      "step": 73740
    },
    {
      "epoch": 2.95,
      "grad_norm": 2.397709608078003,
      "learning_rate": 8.389261744966443e-07,
      "loss": 0.3648,
      "step": 73750
    },
    {
      "epoch": 2.9504,
      "grad_norm": 3.068971872329712,
      "learning_rate": 8.322147651006711e-07,
      "loss": 0.5725,
      "step": 73760
    },
    {
      "epoch": 2.9508,
      "grad_norm": 2.2789502143859863,
      "learning_rate": 8.25503355704698e-07,
      "loss": 0.4313,
      "step": 73770
    },
    {
      "epoch": 2.9512,
      "grad_norm": 2.474900484085083,
      "learning_rate": 8.187919463087248e-07,
      "loss": 0.49,
      "step": 73780
    },
    {
      "epoch": 2.9516,
      "grad_norm": 2.5649704933166504,
      "learning_rate": 8.120805369127517e-07,
      "loss": 0.4832,
      "step": 73790
    },
    {
      "epoch": 2.952,
      "grad_norm": 1.985520362854004,
      "learning_rate": 8.053691275167787e-07,
      "loss": 0.4254,
      "step": 73800
    },
    {
      "epoch": 2.9524,
      "grad_norm": 2.1449406147003174,
      "learning_rate": 7.986577181208054e-07,
      "loss": 0.5031,
      "step": 73810
    },
    {
      "epoch": 2.9528,
      "grad_norm": 2.2137155532836914,
      "learning_rate": 7.919463087248323e-07,
      "loss": 0.4904,
      "step": 73820
    },
    {
      "epoch": 2.9532,
      "grad_norm": 2.9636402130126953,
      "learning_rate": 7.852348993288591e-07,
      "loss": 0.4892,
      "step": 73830
    },
    {
      "epoch": 2.9536,
      "grad_norm": 3.181375741958618,
      "learning_rate": 7.785234899328859e-07,
      "loss": 0.4831,
      "step": 73840
    },
    {
      "epoch": 2.9539999999999997,
      "grad_norm": 3.1987364292144775,
      "learning_rate": 7.718120805369127e-07,
      "loss": 0.4816,
      "step": 73850
    },
    {
      "epoch": 2.9544,
      "grad_norm": 2.609842300415039,
      "learning_rate": 7.651006711409396e-07,
      "loss": 0.4255,
      "step": 73860
    },
    {
      "epoch": 2.9548,
      "grad_norm": 2.4356305599212646,
      "learning_rate": 7.583892617449665e-07,
      "loss": 0.4662,
      "step": 73870
    },
    {
      "epoch": 2.9552,
      "grad_norm": 2.7737715244293213,
      "learning_rate": 7.516778523489933e-07,
      "loss": 0.5382,
      "step": 73880
    },
    {
      "epoch": 2.9556,
      "grad_norm": 3.1040053367614746,
      "learning_rate": 7.449664429530202e-07,
      "loss": 0.541,
      "step": 73890
    },
    {
      "epoch": 2.956,
      "grad_norm": 2.8950743675231934,
      "learning_rate": 7.38255033557047e-07,
      "loss": 0.4468,
      "step": 73900
    },
    {
      "epoch": 2.9564,
      "grad_norm": 2.819718360900879,
      "learning_rate": 7.315436241610739e-07,
      "loss": 0.4178,
      "step": 73910
    },
    {
      "epoch": 2.9568,
      "grad_norm": 2.753627061843872,
      "learning_rate": 7.248322147651007e-07,
      "loss": 0.5307,
      "step": 73920
    },
    {
      "epoch": 2.9572000000000003,
      "grad_norm": 2.5228331089019775,
      "learning_rate": 7.181208053691276e-07,
      "loss": 0.4918,
      "step": 73930
    },
    {
      "epoch": 2.9576000000000002,
      "grad_norm": 2.689664363861084,
      "learning_rate": 7.114093959731544e-07,
      "loss": 0.5154,
      "step": 73940
    },
    {
      "epoch": 2.958,
      "grad_norm": 2.47043776512146,
      "learning_rate": 7.046979865771813e-07,
      "loss": 0.4389,
      "step": 73950
    },
    {
      "epoch": 2.9584,
      "grad_norm": 2.630645751953125,
      "learning_rate": 6.97986577181208e-07,
      "loss": 0.5402,
      "step": 73960
    },
    {
      "epoch": 2.9588,
      "grad_norm": 3.0274763107299805,
      "learning_rate": 6.912751677852349e-07,
      "loss": 0.4815,
      "step": 73970
    },
    {
      "epoch": 2.9592,
      "grad_norm": 2.111999988555908,
      "learning_rate": 6.845637583892617e-07,
      "loss": 0.4314,
      "step": 73980
    },
    {
      "epoch": 2.9596,
      "grad_norm": 2.8421502113342285,
      "learning_rate": 6.778523489932886e-07,
      "loss": 0.4935,
      "step": 73990
    },
    {
      "epoch": 2.96,
      "grad_norm": 2.655369520187378,
      "learning_rate": 6.711409395973154e-07,
      "loss": 0.5044,
      "step": 74000
    },
    {
      "epoch": 2.9604,
      "grad_norm": 3.25612473487854,
      "learning_rate": 6.644295302013423e-07,
      "loss": 0.534,
      "step": 74010
    },
    {
      "epoch": 2.9608,
      "grad_norm": 3.0548648834228516,
      "learning_rate": 6.577181208053691e-07,
      "loss": 0.5382,
      "step": 74020
    },
    {
      "epoch": 2.9612,
      "grad_norm": 2.3659417629241943,
      "learning_rate": 6.51006711409396e-07,
      "loss": 0.564,
      "step": 74030
    },
    {
      "epoch": 2.9616,
      "grad_norm": 2.5629122257232666,
      "learning_rate": 6.442953020134228e-07,
      "loss": 0.5077,
      "step": 74040
    },
    {
      "epoch": 2.9619999999999997,
      "grad_norm": 3.0459177494049072,
      "learning_rate": 6.375838926174497e-07,
      "loss": 0.4996,
      "step": 74050
    },
    {
      "epoch": 2.9624,
      "grad_norm": 2.409566640853882,
      "learning_rate": 6.308724832214766e-07,
      "loss": 0.4667,
      "step": 74060
    },
    {
      "epoch": 2.9628,
      "grad_norm": 2.617696762084961,
      "learning_rate": 6.241610738255034e-07,
      "loss": 0.488,
      "step": 74070
    },
    {
      "epoch": 2.9632,
      "grad_norm": 2.7039947509765625,
      "learning_rate": 6.174496644295303e-07,
      "loss": 0.5151,
      "step": 74080
    },
    {
      "epoch": 2.9636,
      "grad_norm": 2.9137160778045654,
      "learning_rate": 6.10738255033557e-07,
      "loss": 0.5013,
      "step": 74090
    },
    {
      "epoch": 2.964,
      "grad_norm": 3.5165276527404785,
      "learning_rate": 6.040268456375839e-07,
      "loss": 0.5811,
      "step": 74100
    },
    {
      "epoch": 2.9644,
      "grad_norm": 2.8497233390808105,
      "learning_rate": 5.973154362416107e-07,
      "loss": 0.4611,
      "step": 74110
    },
    {
      "epoch": 2.9648,
      "grad_norm": 2.5469369888305664,
      "learning_rate": 5.906040268456376e-07,
      "loss": 0.5244,
      "step": 74120
    },
    {
      "epoch": 2.9652,
      "grad_norm": 2.629696846008301,
      "learning_rate": 5.838926174496644e-07,
      "loss": 0.5158,
      "step": 74130
    },
    {
      "epoch": 2.9656000000000002,
      "grad_norm": 2.8512513637542725,
      "learning_rate": 5.771812080536913e-07,
      "loss": 0.4871,
      "step": 74140
    },
    {
      "epoch": 2.966,
      "grad_norm": 2.827697992324829,
      "learning_rate": 5.704697986577181e-07,
      "loss": 0.5194,
      "step": 74150
    },
    {
      "epoch": 2.9664,
      "grad_norm": 2.4900662899017334,
      "learning_rate": 5.63758389261745e-07,
      "loss": 0.4924,
      "step": 74160
    },
    {
      "epoch": 2.9668,
      "grad_norm": 2.8597118854522705,
      "learning_rate": 5.570469798657718e-07,
      "loss": 0.5333,
      "step": 74170
    },
    {
      "epoch": 2.9672,
      "grad_norm": 2.537717342376709,
      "learning_rate": 5.503355704697987e-07,
      "loss": 0.5348,
      "step": 74180
    },
    {
      "epoch": 2.9676,
      "grad_norm": 3.3223607540130615,
      "learning_rate": 5.436241610738255e-07,
      "loss": 0.5533,
      "step": 74190
    },
    {
      "epoch": 2.968,
      "grad_norm": 2.573275327682495,
      "learning_rate": 5.369127516778524e-07,
      "loss": 0.4264,
      "step": 74200
    },
    {
      "epoch": 2.9684,
      "grad_norm": 2.654644250869751,
      "learning_rate": 5.302013422818791e-07,
      "loss": 0.5073,
      "step": 74210
    },
    {
      "epoch": 2.9688,
      "grad_norm": 3.5706050395965576,
      "learning_rate": 5.23489932885906e-07,
      "loss": 0.5462,
      "step": 74220
    },
    {
      "epoch": 2.9692,
      "grad_norm": 3.293703317642212,
      "learning_rate": 5.167785234899328e-07,
      "loss": 0.5024,
      "step": 74230
    },
    {
      "epoch": 2.9696,
      "grad_norm": 2.4659624099731445,
      "learning_rate": 5.100671140939597e-07,
      "loss": 0.4445,
      "step": 74240
    },
    {
      "epoch": 2.9699999999999998,
      "grad_norm": 2.25107741355896,
      "learning_rate": 5.033557046979866e-07,
      "loss": 0.5119,
      "step": 74250
    },
    {
      "epoch": 2.9704,
      "grad_norm": 1.6698806285858154,
      "learning_rate": 4.966442953020134e-07,
      "loss": 0.516,
      "step": 74260
    },
    {
      "epoch": 2.9708,
      "grad_norm": 2.5788440704345703,
      "learning_rate": 4.899328859060403e-07,
      "loss": 0.478,
      "step": 74270
    },
    {
      "epoch": 2.9712,
      "grad_norm": 2.139402389526367,
      "learning_rate": 4.832214765100671e-07,
      "loss": 0.4022,
      "step": 74280
    },
    {
      "epoch": 2.9716,
      "grad_norm": 2.965183973312378,
      "learning_rate": 4.7651006711409396e-07,
      "loss": 0.4541,
      "step": 74290
    },
    {
      "epoch": 2.972,
      "grad_norm": 2.331404685974121,
      "learning_rate": 4.697986577181208e-07,
      "loss": 0.5028,
      "step": 74300
    },
    {
      "epoch": 2.9724,
      "grad_norm": 2.8502469062805176,
      "learning_rate": 4.630872483221476e-07,
      "loss": 0.5284,
      "step": 74310
    },
    {
      "epoch": 2.9728,
      "grad_norm": 2.7747244834899902,
      "learning_rate": 4.5637583892617447e-07,
      "loss": 0.5093,
      "step": 74320
    },
    {
      "epoch": 2.9732,
      "grad_norm": 2.9068119525909424,
      "learning_rate": 4.496644295302013e-07,
      "loss": 0.5366,
      "step": 74330
    },
    {
      "epoch": 2.9736000000000002,
      "grad_norm": 2.313555955886841,
      "learning_rate": 4.429530201342282e-07,
      "loss": 0.4678,
      "step": 74340
    },
    {
      "epoch": 2.974,
      "grad_norm": 2.3779118061065674,
      "learning_rate": 4.3624161073825503e-07,
      "loss": 0.5171,
      "step": 74350
    },
    {
      "epoch": 2.9744,
      "grad_norm": 3.1438076496124268,
      "learning_rate": 4.295302013422819e-07,
      "loss": 0.5247,
      "step": 74360
    },
    {
      "epoch": 2.9748,
      "grad_norm": 2.4440767765045166,
      "learning_rate": 4.228187919463087e-07,
      "loss": 0.4647,
      "step": 74370
    },
    {
      "epoch": 2.9752,
      "grad_norm": 2.879077196121216,
      "learning_rate": 4.1610738255033553e-07,
      "loss": 0.4872,
      "step": 74380
    },
    {
      "epoch": 2.9756,
      "grad_norm": 4.404960632324219,
      "learning_rate": 4.093959731543624e-07,
      "loss": 0.5618,
      "step": 74390
    },
    {
      "epoch": 2.976,
      "grad_norm": 2.665673017501831,
      "learning_rate": 4.0268456375838935e-07,
      "loss": 0.4496,
      "step": 74400
    },
    {
      "epoch": 2.9764,
      "grad_norm": 2.6896684169769287,
      "learning_rate": 3.9597315436241615e-07,
      "loss": 0.5047,
      "step": 74410
    },
    {
      "epoch": 2.9768,
      "grad_norm": 2.1261746883392334,
      "learning_rate": 3.8926174496644295e-07,
      "loss": 0.42,
      "step": 74420
    },
    {
      "epoch": 2.9772,
      "grad_norm": 2.956738233566284,
      "learning_rate": 3.825503355704698e-07,
      "loss": 0.5308,
      "step": 74430
    },
    {
      "epoch": 2.9776,
      "grad_norm": 2.55122971534729,
      "learning_rate": 3.7583892617449665e-07,
      "loss": 0.4448,
      "step": 74440
    },
    {
      "epoch": 2.9779999999999998,
      "grad_norm": 3.1283087730407715,
      "learning_rate": 3.691275167785235e-07,
      "loss": 0.4452,
      "step": 74450
    },
    {
      "epoch": 2.9784,
      "grad_norm": 2.5357916355133057,
      "learning_rate": 3.6241610738255036e-07,
      "loss": 0.6094,
      "step": 74460
    },
    {
      "epoch": 2.9788,
      "grad_norm": 2.563171625137329,
      "learning_rate": 3.557046979865772e-07,
      "loss": 0.5181,
      "step": 74470
    },
    {
      "epoch": 2.9792,
      "grad_norm": 2.375582218170166,
      "learning_rate": 3.48993288590604e-07,
      "loss": 0.4758,
      "step": 74480
    },
    {
      "epoch": 2.9796,
      "grad_norm": 3.5255126953125,
      "learning_rate": 3.4228187919463087e-07,
      "loss": 0.5066,
      "step": 74490
    },
    {
      "epoch": 2.98,
      "grad_norm": 2.724670171737671,
      "learning_rate": 3.355704697986577e-07,
      "loss": 0.4655,
      "step": 74500
    },
    {
      "epoch": 2.9804,
      "grad_norm": 2.6972365379333496,
      "learning_rate": 3.2885906040268457e-07,
      "loss": 0.6431,
      "step": 74510
    },
    {
      "epoch": 2.9808,
      "grad_norm": 2.0948290824890137,
      "learning_rate": 3.221476510067114e-07,
      "loss": 0.4536,
      "step": 74520
    },
    {
      "epoch": 2.9812,
      "grad_norm": 2.5671377182006836,
      "learning_rate": 3.154362416107383e-07,
      "loss": 0.4347,
      "step": 74530
    },
    {
      "epoch": 2.9816000000000003,
      "grad_norm": 2.363001585006714,
      "learning_rate": 3.0872483221476513e-07,
      "loss": 0.5444,
      "step": 74540
    },
    {
      "epoch": 2.982,
      "grad_norm": 2.132276773452759,
      "learning_rate": 3.0201342281879193e-07,
      "loss": 0.4826,
      "step": 74550
    },
    {
      "epoch": 2.9824,
      "grad_norm": 2.48162579536438,
      "learning_rate": 2.953020134228188e-07,
      "loss": 0.4441,
      "step": 74560
    },
    {
      "epoch": 2.9828,
      "grad_norm": 2.957505941390991,
      "learning_rate": 2.8859060402684564e-07,
      "loss": 0.5477,
      "step": 74570
    },
    {
      "epoch": 2.9832,
      "grad_norm": 2.3483800888061523,
      "learning_rate": 2.818791946308725e-07,
      "loss": 0.4798,
      "step": 74580
    },
    {
      "epoch": 2.9836,
      "grad_norm": 2.320951223373413,
      "learning_rate": 2.7516778523489934e-07,
      "loss": 0.4534,
      "step": 74590
    },
    {
      "epoch": 2.984,
      "grad_norm": 2.68769907951355,
      "learning_rate": 2.684563758389262e-07,
      "loss": 0.4915,
      "step": 74600
    },
    {
      "epoch": 2.9844,
      "grad_norm": 3.157731056213379,
      "learning_rate": 2.61744966442953e-07,
      "loss": 0.5198,
      "step": 74610
    },
    {
      "epoch": 2.9848,
      "grad_norm": 2.858440399169922,
      "learning_rate": 2.5503355704697985e-07,
      "loss": 0.5899,
      "step": 74620
    },
    {
      "epoch": 2.9852,
      "grad_norm": 1.900879144668579,
      "learning_rate": 2.483221476510067e-07,
      "loss": 0.4103,
      "step": 74630
    },
    {
      "epoch": 2.9856,
      "grad_norm": 2.232494354248047,
      "learning_rate": 2.4161073825503355e-07,
      "loss": 0.4239,
      "step": 74640
    },
    {
      "epoch": 2.9859999999999998,
      "grad_norm": 2.6804280281066895,
      "learning_rate": 2.348993288590604e-07,
      "loss": 0.5256,
      "step": 74650
    },
    {
      "epoch": 2.9864,
      "grad_norm": 1.854349970817566,
      "learning_rate": 2.2818791946308723e-07,
      "loss": 0.5482,
      "step": 74660
    },
    {
      "epoch": 2.9868,
      "grad_norm": 2.5776360034942627,
      "learning_rate": 2.214765100671141e-07,
      "loss": 0.4981,
      "step": 74670
    },
    {
      "epoch": 2.9872,
      "grad_norm": 2.7475929260253906,
      "learning_rate": 2.1476510067114094e-07,
      "loss": 0.448,
      "step": 74680
    },
    {
      "epoch": 2.9876,
      "grad_norm": 2.1065549850463867,
      "learning_rate": 2.0805369127516777e-07,
      "loss": 0.4785,
      "step": 74690
    },
    {
      "epoch": 2.988,
      "grad_norm": 3.790989398956299,
      "learning_rate": 2.0134228187919467e-07,
      "loss": 0.5157,
      "step": 74700
    },
    {
      "epoch": 2.9884,
      "grad_norm": 2.8988585472106934,
      "learning_rate": 1.9463087248322147e-07,
      "loss": 0.5242,
      "step": 74710
    },
    {
      "epoch": 2.9888,
      "grad_norm": 2.844578981399536,
      "learning_rate": 1.8791946308724833e-07,
      "loss": 0.5294,
      "step": 74720
    },
    {
      "epoch": 2.9892,
      "grad_norm": 1.9479838609695435,
      "learning_rate": 1.8120805369127518e-07,
      "loss": 0.4438,
      "step": 74730
    },
    {
      "epoch": 2.9896000000000003,
      "grad_norm": 2.3603687286376953,
      "learning_rate": 1.74496644295302e-07,
      "loss": 0.4623,
      "step": 74740
    },
    {
      "epoch": 2.99,
      "grad_norm": 2.986623525619507,
      "learning_rate": 1.6778523489932886e-07,
      "loss": 0.5178,
      "step": 74750
    },
    {
      "epoch": 2.9904,
      "grad_norm": 2.236358404159546,
      "learning_rate": 1.610738255033557e-07,
      "loss": 0.4622,
      "step": 74760
    },
    {
      "epoch": 2.9908,
      "grad_norm": 2.09285569190979,
      "learning_rate": 1.5436241610738257e-07,
      "loss": 0.5199,
      "step": 74770
    },
    {
      "epoch": 2.9912,
      "grad_norm": 2.7485439777374268,
      "learning_rate": 1.476510067114094e-07,
      "loss": 0.4992,
      "step": 74780
    },
    {
      "epoch": 2.9916,
      "grad_norm": 2.9364681243896484,
      "learning_rate": 1.4093959731543624e-07,
      "loss": 0.5371,
      "step": 74790
    },
    {
      "epoch": 2.992,
      "grad_norm": 2.3861188888549805,
      "learning_rate": 1.342281879194631e-07,
      "loss": 0.5241,
      "step": 74800
    },
    {
      "epoch": 2.9924,
      "grad_norm": 3.1731534004211426,
      "learning_rate": 1.2751677852348992e-07,
      "loss": 0.5099,
      "step": 74810
    },
    {
      "epoch": 2.9928,
      "grad_norm": 2.564797878265381,
      "learning_rate": 1.2080536912751678e-07,
      "loss": 0.4942,
      "step": 74820
    },
    {
      "epoch": 2.9932,
      "grad_norm": 2.811336040496826,
      "learning_rate": 1.1409395973154362e-07,
      "loss": 0.4471,
      "step": 74830
    },
    {
      "epoch": 2.9936,
      "grad_norm": 2.729571580886841,
      "learning_rate": 1.0738255033557047e-07,
      "loss": 0.4853,
      "step": 74840
    },
    {
      "epoch": 2.9939999999999998,
      "grad_norm": 3.0202412605285645,
      "learning_rate": 1.0067114093959734e-07,
      "loss": 0.5364,
      "step": 74850
    },
    {
      "epoch": 2.9943999999999997,
      "grad_norm": 3.202139139175415,
      "learning_rate": 9.395973154362416e-08,
      "loss": 0.5886,
      "step": 74860
    },
    {
      "epoch": 2.9948,
      "grad_norm": 2.5840678215026855,
      "learning_rate": 8.7248322147651e-08,
      "loss": 0.492,
      "step": 74870
    },
    {
      "epoch": 2.9952,
      "grad_norm": 2.824327230453491,
      "learning_rate": 8.053691275167786e-08,
      "loss": 0.5298,
      "step": 74880
    },
    {
      "epoch": 2.9956,
      "grad_norm": 2.284724712371826,
      "learning_rate": 7.38255033557047e-08,
      "loss": 0.5005,
      "step": 74890
    },
    {
      "epoch": 2.996,
      "grad_norm": 3.361865997314453,
      "learning_rate": 6.711409395973155e-08,
      "loss": 0.5617,
      "step": 74900
    },
    {
      "epoch": 2.9964,
      "grad_norm": 2.571132183074951,
      "learning_rate": 6.040268456375839e-08,
      "loss": 0.5972,
      "step": 74910
    },
    {
      "epoch": 2.9968,
      "grad_norm": 2.691502809524536,
      "learning_rate": 5.3691275167785235e-08,
      "loss": 0.5392,
      "step": 74920
    },
    {
      "epoch": 2.9972,
      "grad_norm": 2.60556697845459,
      "learning_rate": 4.697986577181208e-08,
      "loss": 0.5356,
      "step": 74930
    },
    {
      "epoch": 2.9976000000000003,
      "grad_norm": 3.3160033226013184,
      "learning_rate": 4.026845637583893e-08,
      "loss": 0.5044,
      "step": 74940
    },
    {
      "epoch": 2.998,
      "grad_norm": 2.656341075897217,
      "learning_rate": 3.3557046979865774e-08,
      "loss": 0.4909,
      "step": 74950
    },
    {
      "epoch": 2.9984,
      "grad_norm": 3.0200393199920654,
      "learning_rate": 2.6845637583892618e-08,
      "loss": 0.5118,
      "step": 74960
    },
    {
      "epoch": 2.9988,
      "grad_norm": 3.0350635051727295,
      "learning_rate": 2.0134228187919464e-08,
      "loss": 0.5387,
      "step": 74970
    },
    {
      "epoch": 2.9992,
      "grad_norm": 2.6045174598693848,
      "learning_rate": 1.3422818791946309e-08,
      "loss": 0.5052,
      "step": 74980
    },
    {
      "epoch": 2.9996,
      "grad_norm": 2.453728437423706,
      "learning_rate": 6.711409395973154e-09,
      "loss": 0.4763,
      "step": 74990
    },
    {
      "epoch": 3.0,
      "grad_norm": 2.7481253147125244,
      "learning_rate": 0.0,
      "loss": 0.5203,
      "step": 75000
    },
    {
      "epoch": 3.0,
      "eval_loss": 0.649696409702301,
      "eval_runtime": 0.6123,
      "eval_samples_per_second": 3266.203,
      "eval_steps_per_second": 52.259,
      "step": 75000
    }
  ],
  "logging_steps": 10,
  "max_steps": 75000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 9216117964800000.0,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
