{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "import os\n",
    "import torch\n",
    "import whisper\n",
    "import pandas as pd\n",
    "from pytubefix import YouTube\n",
    "from transformers import MarianTokenizer, MarianMTModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_audio(youtube_url, output_path=\".\"):\n",
    "    yt = YouTube(youtube_url)\n",
    "    audio_stream = yt.streams.filter(only_audio=True).first()\n",
    "    audio_file = audio_stream.download(output_path=output_path)\n",
    "    mp3_file = os.path.splitext(audio_file)[0] + \".mp3\"\n",
    "    os.rename(audio_file, mp3_file)\n",
    "    return mp3_file\n",
    "\n",
    "def format_timestamp(seconds):\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    secs = int(seconds % 60)\n",
    "    millis = int((seconds - int(seconds)) * 1000)\n",
    "    return f\"{hours:02}:{minutes:02}:{secs:02},{millis:03}\"\n",
    "\n",
    "def transcribe_audio(audio_path):\n",
    "    print(\"Transcribing audio...\")\n",
    "\n",
    "    model = whisper.load_model(\"large-v3-turbo\")\n",
    "    result = model.transcribe(\n",
    "        audio=audio_path,\n",
    "        language=\"nl\",\n",
    "        verbose=True,\n",
    "        condition_on_previous_text=False\n",
    "    )\n",
    "\n",
    "    segments = result.get(\"segments\", [])\n",
    "    transcript_data = []\n",
    "\n",
    "    for segment in segments:\n",
    "        transcript_data.append({\n",
    "            \"Start_Time\": format_timestamp(segment[\"start\"]),\n",
    "            \"End_Time\": format_timestamp(segment[\"end\"]),\n",
    "            \"Sentence\": segment[\"text\"].strip()\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(transcript_data)\n",
    "    print(\"Transcription completed.\")\n",
    "    return df, result[\"language\"]\n",
    "\n",
    "def load_translation_model(model_path):\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_path)\n",
    "    model = MarianMTModel.from_pretrained(model_path).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    return tokenizer, model\n",
    "\n",
    "def translate(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        translated_ids = model.generate(**inputs)\n",
    "    return tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "def load_emotion_model(emotion_model_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(emotion_model_path)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(emotion_model_path).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    return tokenizer, model\n",
    "\n",
    "def classify_emotion(sentence, tokenizer, model):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = F.softmax(outputs.logits, dim=1)\n",
    "        predicted_class = torch.argmax(probs, dim=1).item()\n",
    "    return model.config.id2label[predicted_class]\n",
    "\n",
    "def save_transcription(sentences_df, translations, emotions, output_file=\"final_output.csv\"):\n",
    "    sentences_df[\"Translation\"] = translations\n",
    "    sentences_df[\"Emotion\"] = emotions\n",
    "    sentences_df.to_csv(output_file, index=False)\n",
    "    return output_file\n",
    "\n",
    "def pipeline(youtube_url, translation_model_path, emotion_model_path, output_dir=\".\"):\n",
    "    audio_file = download_audio(youtube_url, output_path=output_dir)\n",
    "    transcription_df, _ = transcribe_audio(audio_file)\n",
    "\n",
    "    # Load models\n",
    "    translation_tokenizer, translation_model = load_translation_model(translation_model_path)\n",
    "    emotion_tokenizer, emotion_model = load_emotion_model(emotion_model_path)\n",
    "\n",
    "    # Translate and classify\n",
    "    translations = [translate(row[\"Sentence\"], translation_tokenizer, translation_model)\n",
    "                    for _, row in transcription_df.iterrows()]\n",
    "    emotions = [classify_emotion(row[\"Sentence\"], emotion_tokenizer, emotion_model)\n",
    "                for _, row in transcription_df.iterrows()]\n",
    "\n",
    "    # Save\n",
    "    output_file = os.path.join(output_dir, \"final_output.csv\")\n",
    "    save_transcription(transcription_df, translations, emotions, output_file=output_file)\n",
    "\n",
    "    print(\"Pipeline Completed Successfully!\")\n",
    "    print(f\"Audio saved at: {audio_file}\")\n",
    "    print(f\"CSV saved at: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_url = \"https://www.youtube.com/watch?v=mNOksBRpT9g\"\n",
    "translation_model_path = \"translation_model\"\n",
    "emotion_model_path = \"pretrained_bert_model_w_metrics\"\n",
    "\n",
    "pipeline(youtube_url, translation_model_path, emotion_model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
